{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f6c751c626a782",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 700000\n",
    "json_file_path = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Read the JSON file in chunks\n",
    "chunks = pd.read_json(json_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "for i, df_chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    chunk_csv_path = f\"chunk_{i + 1}.csv\"\n",
    "    df_chunk.to_csv(chunk_csv_path, index=False)\n",
    "    df_chunk_from_csv = pd.read_csv(chunk_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the first 100000\n",
    "chunk_size = 700000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Reading the JSON file in chunks\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        first_chunk_csv_path = \"first_chunk.csv\"\n",
    "        x.to_csv(first_chunk_csv_path, index=False)\n",
    "\n",
    "    df= pd.read_csv(\"first_chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Visualization\n",
    "#%%\n",
    "# Data Preprocessing\n",
    "texts = df['text']\n",
    "labels = df['stars'].apply(lambda x: 1 if x > 3 else 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['stars'].value_counts().sort_index().index, df['stars'].value_counts().sort_index(), color='purple')\n",
    "plt.title('Ratings')\n",
    "plt.xlabel('Stars')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['review_length'] = df['text'].apply(len)\n",
    "df['review_length'].hist(bins=50, color='red')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "df.boxplot(column='useful', by='stars', ax=axes[0])\n",
    "axes[0].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='funny', by='stars', ax=axes[1])\n",
    "axes[1].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='cool', by='stars', ax=axes[2])\n",
    "axes[2].set_title('Votes by Stars')\n",
    "\n",
    "plt.suptitle('Features by Stars')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Data Pre-processing\n",
    "#%%\n",
    "#### Drop duplicates\n",
    "df = df.drop_duplicates(subset=['review_id'])\n",
    "df = df.drop_duplicates(subset=['user_id'])\n",
    "df = df.drop_duplicates(subset=['business_id'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Null values \n",
    "nulls= df.isnull().sum()\n",
    "print(\"Nulls:\\n\", nulls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "##%%\n",
    "# Remove special characters \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(texts)\n",
    "s = token.texts_to_sequences(texts)\n",
    "index = token.word_index\n",
    "maximumlength = 100  # Set your desired sequence length\n",
    "data = pad_sequences(s, maxlen=maximumlength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode labels\n",
    "l_encode = LabelEncoder()\n",
    "labels = l_encode.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Train and Test Split\n",
    "#%%\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.18, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### LSTM Model\n",
    "#%%\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(index) + 1, 100, input_length=maximumlength))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Compile the model\n",
    "#%%\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with Data Generator\n",
    "def data_generator(data, labels, batch_size):\n",
    "    samples = len(data)\n",
    "    while True:\n",
    "        s_indices = np.arange(samples)\n",
    "        np.random.shuffle(s_indices)\n",
    "        for i in range(0,samples, batch_size):\n",
    "            b_indices = s_indices[i:i+batch_size]\n",
    "            yield data[b_indices], labels[b_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 33\n",
    "epochs= len(X_train) // batch_size\n",
    "# Training\n",
    "model.fit(data_generator(X_train, y_train, batch_size),\n",
    "          epochs=7,\n",
    "          steps_per_epoch=epochs,\n",
    "          validation_data=(X_test, y_test)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
