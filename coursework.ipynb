{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ],
   "id": "d6f6c751c626a782"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b6d076706c93cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:49:34.322438305Z",
     "start_time": "2023-12-02T06:49:28.267001553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 12:19:32.342109: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-02 12:19:32.467702: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-02 12:19:32.467759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-02 12:19:32.482148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-02 12:19:32.517763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 12:19:33.531692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ],
   "id": "cbd7368cf0d05d61"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:05.692081199Z",
     "start_time": "2023-12-01T23:13:43.537119654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 700000\n",
    "json_file_path = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Read the JSON file in chunks\n",
    "chunks = pd.read_json(json_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "for i, df_chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    chunk_csv_path = f\"chunk_{i + 1}.csv\"\n",
    "    df_chunk.to_csv(chunk_csv_path, index=False)\n",
    "    df_chunk_from_csv = pd.read_csv(chunk_csv_path)"
   ],
   "id": "91ba0966af9aa095"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fff3cc6a84e894f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:31.449091666Z",
     "start_time": "2023-12-01T23:17:10.178554328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "# Using the first 100000\n",
    "chunk_size = 700000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Reading the JSON file in chunks\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        first_chunk_csv_path = \"first_chunk.csv\"\n",
    "        x.to_csv(first_chunk_csv_path, index=False)\n",
    "\n",
    "    df= pd.read_csv(\"first_chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n699995  8nPFAHUg2Maq20h9PxHCSg  fM-DKqmd9Lh08bApyY-dsg   \n699996  siyHQqTd-35kd9fEQkMskg  YYfgEK77PFL6-ErHHElHww   \n699997  iuBOjA0J7B0_yT_3FEgUJQ  7h7SkRAIJMtynNSGntyxbw   \n699998  2WPcCACe-NecwIKDIRRPrA  qMdjazC9quXk6AElD3i9yA   \n699999  Q_BdRCOkQRcVyF7JOaT5Lg  89aoy6WORObOOdU5yPQxoQ   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n699995  IHd8_cnLZe5oILE_oKy-5g      1       0      0     0   \n699996  DvkSkF83xDONjkNIoEPRnQ      5       4      1     0   \n699997  Y3ZCO17N1_T_Ms1JmswwzA      4       0      0     0   \n699998  f4vbnGoGo3eWorVekctVGQ      5       0      0     0   \n699999  L3QX19EWlkS-clS68Boydg      4       0      0     0   \n\n                                                     text                 date  \n0       If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n1       I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n2       Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n3       Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n4       Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n...                                                   ...                  ...  \n699995  Do not go here! The only redeeming quality of ...  2015-05-23 23:16:41  \n699996  This place is amazing. I'm very surprised seei...  2017-07-12 19:57:37  \n699997  Horchata was good, and the tortillas were grea...  2016-11-15 23:50:18  \n699998  Best omelette and grits both my girlfriend and...  2015-12-29 17:38:16  \n699999  Stopped in to grab an order of beignets becaus...  2013-06-06 21:03:14  \n\n[700000 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>699995</th>\n      <td>8nPFAHUg2Maq20h9PxHCSg</td>\n      <td>fM-DKqmd9Lh08bApyY-dsg</td>\n      <td>IHd8_cnLZe5oILE_oKy-5g</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Do not go here! The only redeeming quality of ...</td>\n      <td>2015-05-23 23:16:41</td>\n    </tr>\n    <tr>\n      <th>699996</th>\n      <td>siyHQqTd-35kd9fEQkMskg</td>\n      <td>YYfgEK77PFL6-ErHHElHww</td>\n      <td>DvkSkF83xDONjkNIoEPRnQ</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>This place is amazing. I'm very surprised seei...</td>\n      <td>2017-07-12 19:57:37</td>\n    </tr>\n    <tr>\n      <th>699997</th>\n      <td>iuBOjA0J7B0_yT_3FEgUJQ</td>\n      <td>7h7SkRAIJMtynNSGntyxbw</td>\n      <td>Y3ZCO17N1_T_Ms1JmswwzA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Horchata was good, and the tortillas were grea...</td>\n      <td>2016-11-15 23:50:18</td>\n    </tr>\n    <tr>\n      <th>699998</th>\n      <td>2WPcCACe-NecwIKDIRRPrA</td>\n      <td>qMdjazC9quXk6AElD3i9yA</td>\n      <td>f4vbnGoGo3eWorVekctVGQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Best omelette and grits both my girlfriend and...</td>\n      <td>2015-12-29 17:38:16</td>\n    </tr>\n    <tr>\n      <th>699999</th>\n      <td>Q_BdRCOkQRcVyF7JOaT5Lg</td>\n      <td>89aoy6WORObOOdU5yPQxoQ</td>\n      <td>L3QX19EWlkS-clS68Boydg</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Stopped in to grab an order of beignets becaus...</td>\n      <td>2013-06-06 21:03:14</td>\n    </tr>\n  </tbody>\n</table>\n<p>700000 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:33.137039432Z",
     "start_time": "2023-12-01T23:17:33.073722911Z"
    }
   },
   "id": "d36238e10f1cf45e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb5a9e38a21f4e4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "texts = df['text']\n",
    "labels = df['stars'].apply(lambda x: 1 if x > 3 else 0)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:41.160777236Z",
     "start_time": "2023-12-01T23:17:41.041274592Z"
    }
   },
   "id": "f46808944f15d9df"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n699995  8nPFAHUg2Maq20h9PxHCSg  fM-DKqmd9Lh08bApyY-dsg   \n699996  siyHQqTd-35kd9fEQkMskg  YYfgEK77PFL6-ErHHElHww   \n699997  iuBOjA0J7B0_yT_3FEgUJQ  7h7SkRAIJMtynNSGntyxbw   \n699998  2WPcCACe-NecwIKDIRRPrA  qMdjazC9quXk6AElD3i9yA   \n699999  Q_BdRCOkQRcVyF7JOaT5Lg  89aoy6WORObOOdU5yPQxoQ   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n699995  IHd8_cnLZe5oILE_oKy-5g      1       0      0     0   \n699996  DvkSkF83xDONjkNIoEPRnQ      5       4      1     0   \n699997  Y3ZCO17N1_T_Ms1JmswwzA      4       0      0     0   \n699998  f4vbnGoGo3eWorVekctVGQ      5       0      0     0   \n699999  L3QX19EWlkS-clS68Boydg      4       0      0     0   \n\n                                                     text                 date  \n0       If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n1       I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n2       Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n3       Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n4       Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n...                                                   ...                  ...  \n699995  Do not go here! The only redeeming quality of ...  2015-05-23 23:16:41  \n699996  This place is amazing. I'm very surprised seei...  2017-07-12 19:57:37  \n699997  Horchata was good, and the tortillas were grea...  2016-11-15 23:50:18  \n699998  Best omelette and grits both my girlfriend and...  2015-12-29 17:38:16  \n699999  Stopped in to grab an order of beignets becaus...  2013-06-06 21:03:14  \n\n[700000 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>699995</th>\n      <td>8nPFAHUg2Maq20h9PxHCSg</td>\n      <td>fM-DKqmd9Lh08bApyY-dsg</td>\n      <td>IHd8_cnLZe5oILE_oKy-5g</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Do not go here! The only redeeming quality of ...</td>\n      <td>2015-05-23 23:16:41</td>\n    </tr>\n    <tr>\n      <th>699996</th>\n      <td>siyHQqTd-35kd9fEQkMskg</td>\n      <td>YYfgEK77PFL6-ErHHElHww</td>\n      <td>DvkSkF83xDONjkNIoEPRnQ</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>This place is amazing. I'm very surprised seei...</td>\n      <td>2017-07-12 19:57:37</td>\n    </tr>\n    <tr>\n      <th>699997</th>\n      <td>iuBOjA0J7B0_yT_3FEgUJQ</td>\n      <td>7h7SkRAIJMtynNSGntyxbw</td>\n      <td>Y3ZCO17N1_T_Ms1JmswwzA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Horchata was good, and the tortillas were grea...</td>\n      <td>2016-11-15 23:50:18</td>\n    </tr>\n    <tr>\n      <th>699998</th>\n      <td>2WPcCACe-NecwIKDIRRPrA</td>\n      <td>qMdjazC9quXk6AElD3i9yA</td>\n      <td>f4vbnGoGo3eWorVekctVGQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Best omelette and grits both my girlfriend and...</td>\n      <td>2015-12-29 17:38:16</td>\n    </tr>\n    <tr>\n      <th>699999</th>\n      <td>Q_BdRCOkQRcVyF7JOaT5Lg</td>\n      <td>89aoy6WORObOOdU5yPQxoQ</td>\n      <td>L3QX19EWlkS-clS68Boydg</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Stopped in to grab an order of beignets becaus...</td>\n      <td>2013-06-06 21:03:14</td>\n    </tr>\n  </tbody>\n</table>\n<p>700000 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:42.302400557Z",
     "start_time": "2023-12-01T23:17:42.262369129Z"
    }
   },
   "id": "e6c1147a38f4e35e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['stars'].value_counts().sort_index().index, df['stars'].value_counts().sort_index(), color='purple')\n",
    "plt.title('Ratings')\n",
    "plt.xlabel('Stars')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:47.069229529Z",
     "start_time": "2023-12-01T23:17:46.815144260Z"
    }
   },
   "id": "15acafa680b135c3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAIjCAYAAADFk0cVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKH0lEQVR4nO3deXxU9b3/8fdMQhCBBEhYLsarNphFshNE0iDKokgaLItQZBfFBcUFREVENgnalAsIP6FBU8EgINsVgVrxFqQNYdFEDATZXECsJAESgixZzu8PLuc6QiVfzDITXs/HYx51zvnM+X5nPoS+c/ieMw7LsiwBAAAAqDBnTU8AAAAA8DSEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaABAjRk0aJB+97vf1fQ0AMAYIRoAaoGVK1cqJCREX3zxRU1P5SI//PCDXn/9deXm5tb0VACg0hCiAQBV6ujRo5ozZw4hGkCtQogGAAAADBGiAeAq8cMPP+iFF15QfHy8wsPDlZiYqOXLl7vUbN26VSEhIVq3bp3eeOMN3X777YqIiNCQIUP0zTffXHTM9PR0de7cWZGRkerTp4927NihQYMGadCgQfbx+vTpI0l64YUXFBISopCQEK1cudLlOPv379egQYMUFRWlDh06KDU19aKxFi1apMTEREVFRalt27bq1auX1qxZU1kfDwAY8a7pCQAAql5+fr769u0rh8OhAQMGqEmTJvrkk0/04osvqri4WEOHDnWpT01NlcPh0AMPPKDi4mItWLBAY8aM0XvvvWfXLF68WJMnT1ZcXJyGDh2q7777TiNHjpSvr69atGghSQoKCtKoUaM0e/Zs9evXT23atJEkxcbG2scpLCzUgw8+qK5du+qee+7Rhx9+qJSUFAUHB6tjx46SpGXLlmnq1Km6++67NXjwYJ09e1ZffvmlPv/8cyUlJVXxpwcAFyNEA8BV4L/+679UVlamNWvWqHHjxpKk/v3765lnntGcOXP0hz/8Qddcc41df/bsWa1evVo+Pj6SJF9fX73yyivau3evgoODde7cOc2aNUsRERF6++235e19/v9OQkJC9Pzzz9shOiAgQLfffrtmz56t6Oho3XvvvRfN7ejRo3r11Vf1+9//XpLUp08fderUSStWrLBD9MaNG3XzzTdr9uzZVfYZAYAJlnMAQC1nWZb+9re/qVOnTrIsS8eOHbMfCQkJOnnypHbt2uXyml69etkBWpLi4uIkSYcOHZIk5eTk6MSJE+rbt68doCUpKSlJfn5+RvO79tprXcK1j4+PIiIi7LGk8yH+X//6l3bu3Gl0bACoKpyJBoBa7tixYyoqKtLSpUu1dOnSf1vzUy1btnR57uvrK0kqKiqSJB05ckSS9J//+Z8udd7e3rruuuuM5teiRQs5HA6XbX5+fvryyy/t5w899JAyMjJ033336YYbbtBvf/tb/e53v7OXhwBAdSNEA0AtV15eLknq0aOHevbsecmakJAQl+dO56X/odKyrMqdnCQvL6/L1gQFBemvf/2rNm7cqM2bN+tvf/ubFi9erJEjR2rUqFGVPicAuBxCNADUck2aNFH9+vVVXl6u+Pj4SjnmhTPV3377rW677TZ7e2lpqb777juXUP7zs8xX6tprr1X37t3VvXt3nTt3Tk888YTmzZunhx9+WHXr1q2UMQCgolgTDQC1nJeXl+6++259+OGH2rt370X7f76UoyLCw8PVqFEjLVu2TKWlpfb2NWvWqLCw0KW2Xr16kv5vKciVOH78uMtzHx8fBQUFybIslZSUXPFxAeBKcSYaAGqRFStWaPPmzRdtf/zxx7V161b17dtX9913n1q1aqXCwkLt2rVLW7Zs0bZt24zG8fHx0RNPPKEpU6ZoyJAhuueee/Tdd99p5cqVF62T/s///E/5+vpqyZIlql+/vq699lpFRkbq+uuvr/B4w4cPV0BAgGJjY+Xv76+DBw/qnXfeUceOHdWgQQOjuQNAZSBEA0At8u67715ye69evfTee+9p7ty5+uijj/Tuu++qUaNGatWqlcaMGXNFYw0cOFCWZSktLU2vvvqqQkND9cYbb2jq1Kkuyyvq1Kmj6dOna8aMGZo4caJKS0uVnJxsFKL79eunNWvWKC0tTT/++KNatGihQYMG6bHHHruiuQPAr+WwquIqEQDAVam8vFzt27dX165dNXXq1JqeDgBUGdZEAwCuyNmzZy+6W8fq1at14sQJ3XrrrTU0KwCoHiznAABckezsbCUnJ6tbt25q1KiRdu/ereXLlys4OFjdunWr6ekBQJUiRAMArsh1112nFi1aaNGiRSosLJSfn5/uvfdejRkzxuXbDgGgNmJNNAAAAGCINdEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGuDtHDSgoOKnquJzT4ZD8/RtW23iofPTQs9E/z0cPPR899HzV3cML410OIboGWJaq9Qe5usdD5aOHno3+eT566Pnooedztx6ynAMAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADNVoiN6+fbseeeQRJSQkKCQkRBs2bLD3lZSU6I9//KOSkpIUHR2thIQEjR07Vj/88IPLMU6cOKHRo0crNjZWcXFxGjdunE6dOuVSs2fPHt1///2KiIhQx44dlZqaetFc1q9fr27duikiIkJJSUnatGmTy37LsjRr1iwlJCQoMjJSQ4cO1ddff115HwYAAAA8Ro2G6B9//FEhISF6+eWXL9p35swZ7d69W48++qhWrlypOXPm6KuvvtKjjz7qUjdmzBjt379faWlpmjdvnnbs2KEJEybY+4uLizV8+HC1bNlSK1eu1NixYzVnzhwtXbrUrvnss880evRo9enTR6tXr1bnzp01cuRI7d27165JTU3VokWLNHHiRC1btkz16tXT8OHDdfbs2Sr4ZAAAAODOHJZlWTU9CUkKCQnR3Llz1aVLl39bs3PnTt133336+9//rpYtW+rAgQPq3r27li9froiICEnSJ598ohEjRmjTpk1q3ry5Fi9erJkzZ+of//iHfHx8JEkpKSnasGGD/vrXv0qSnnrqKZ0+fVrz58+3x+rbt69CQ0M1efJkWZalDh06aNiwYRo+fLgk6eTJk4qPj9f06dOVmJho9F7z80+qOj51h0MKCGhYbeOh8tFDz0b/PB899Hz00PNVdw8vjHc53lU/lcpTXFwsh8MhX19fSVJWVpZ8fX3tAC1J8fHxcjqd2rlzp7p27ars7GzFxcXZAVqSEhISlJqaqsLCQvn5+Sk7O1tDhw51GSshIcFeXnL48GHl5eUpPj7e3t+wYUNFRUUpKyvLOEQ7HKbv/MpcGKe6xkPlo4eejf55Pnro+eih56vuHlZ0HI8J0WfPnlVKSooSExPVoEEDSVJ+fr6aNGniUuft7S0/Pz/l5eXZNYGBgS41AQEB9j4/Pz/l5+fb2y7w9/dXfn6+JNnH8vf3/7c1Jvz9L//bTWWq7vFQ+eihZ6N/no8eej566PncrYceEaJLSkr05JNPyrIsTZo0qaan86sVFFTfP0f4+zes0HhOp0ONfa+Rw7vifySs0lIdLzqj8nL+fayqmPQQ7of+eT566Pnooeer7h5eGO9y3D5El5SU6KmnntKRI0f09ttv22ehpfNnlI8dO+ZSX1paqsLCQjVt2tSu+fnZ4gvPL5x9vlRNQUGBvf/CsQoKCtSsWTOXmtDQUOP3ZFmq1h/kiozncDjOB+gBA6Tc3MsfNCxMjvR0ORwOucmy+lqtuv/MoHLRP89HDz0fPfR87tZDtw7RFwL0N998o4ULF6px48Yu+2NiYlRUVKScnByFh4dLkjIzM1VeXq7IyEhJUnR0tGbOnKmSkhLVqVNHkpSRkaGbbrpJfn5+dk1mZqbLuuiMjAxFR0dLkgIDA9W0aVNt2bJFYWFhks6vz/7888/Vv3//qvwIql9urpSVVdOzAAAAcGs1eou7U6dOKTc3V7n/e+bz8OHDys3N1ZEjR1RSUqJRo0YpJydHKSkpKisrU15envLy8nTu3DlJUlBQkDp06KCXXnpJO3fu1KeffqopU6YoMTFRzZs3lyQlJSWpTp06evHFF7Vv3z6tW7dOCxcu1LBhw+x5DB48WJs3b9Zbb72lAwcO6PXXX1dOTo4GDhwo6fxZ2sGDB+uNN97Qxx9/rC+//FJjx45Vs2bNfvFuIgAAAKidavQWd1u3btXgwYMv2t6zZ089/vjj6ty58yVft3DhQrVr107S+S9bmTJliv7nf/5HTqdTd911l8aPH6/69evb9Xv27NHkyZP1xRdfqHHjxho4cKBGjBjhcsz169dr5syZ+u6773TjjTfq2WefVceOHe39lmVp9uzZWrZsmYqKitSmTRu9/PLLuummm4zftzve4s7b26nGjetLsbEVOxMdEyN99pmOHz+l0tLyypkwLsKtmTwb/fN89NDz0UPP5663uHOb+0RfTQjRqCj+8vds9M/z0UPPRw89n7uG6BpdzgEAAAB4IkI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYMi7picAz+blVbHfw8rLLZWXW1U8GwAAgOpBiMaVadFCKiuTr2+9CpVbpaU6VniGIA0AAGoFQjSuTKNGkpeXNGCAlJv7y7VhYXKkp8vpdBCiAQBArUCIxq+TmytlZdX0LAAAAKoVFxYCAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYqtEQvX37dj3yyCNKSEhQSEiINmzY4LLfsizNmjVLCQkJioyM1NChQ/X111+71Jw4cUKjR49WbGys4uLiNG7cOJ06dcqlZs+ePbr//vsVERGhjh07KjU19aK5rF+/Xt26dVNERISSkpK0adMm47kAAADg6lCjIfrHH39USEiIXn755UvuT01N1aJFizRx4kQtW7ZM9erV0/Dhw3X27Fm7ZsyYMdq/f7/S0tI0b9487dixQxMmTLD3FxcXa/jw4WrZsqVWrlypsWPHas6cOVq6dKld89lnn2n06NHq06ePVq9erc6dO2vkyJHau3ev0VwAAABwdajREN2xY0c9/fTT6tq160X7LMvSwoUL9eijj6pLly4KDQ3Va6+9pqNHj9pnrA8cOKDNmzdr6tSpioqKUlxcnMaPH6+1a9fqhx9+kCS9//77Kikp0bRp03TzzTcrMTFRgwYNUlpamj3WwoUL1aFDBz344IMKCgrSU089pVtuuUXvvPNOhecCAACAq4d3TU/g3zl8+LDy8vIUHx9vb2vYsKGioqKUlZWlxMREZWVlydfXVxEREXZNfHy8nE6ndu7cqa5duyo7O1txcXHy8fGxaxISEpSamqrCwkL5+fkpOztbQ4cOdRk/ISHBDsgVmYsJh8Oo/IpdGKe6xqsId5qLJ3DHHqLi6J/no4eejx56vuruYUXHcdsQnZeXJ0ny9/d32e7v76/8/HxJUn5+vpo0aeKy39vbW35+fvbr8/PzFRgY6FITEBBg7/Pz81N+fr697VLjVGQuJvz9Gxq/5teo7vH+ncaN69f0FDyWu/QQV4b+eT566Pnooedztx66bYiuzQoKTsqyqn4ch+P8H7iKjOfl5azykHv8+CmVlZVX6Ri1jUkP4X7on+ejh56PHnq+6u7hhfEux21DdNOmTSVJBQUFatasmb29oKBAoaGhks6fUT527JjL60pLS1VYWGi/PiAg4KKzxReeXzj7fKmagoICe39F5mLCslStP8jVPd4vcZd5eBp36iHM0T/PRw89Hz30fO7WQ7e9T3RgYKCaNm2qLVu22NuKi4v1+eefKyYmRpIUExOjoqIi5eTk2DWZmZkqLy9XZGSkJCk6Olo7duxQSUmJXZORkaGbbrpJfn5+dk1mZqbL+BkZGYqOjq7wXAAAAHD1qNEQferUKeXm5io3N1fS+Qv4cnNzdeTIETkcDg0ePFhvvPGGPv74Y3355ZcaO3asmjVrpi5dukiSgoKC1KFDB7300kvauXOnPv30U02ZMkWJiYlq3ry5JCkpKUl16tTRiy++qH379mndunVauHChhg0bZs9j8ODB2rx5s9566y0dOHBAr7/+unJycjRw4EBJqtBcAAAAcPWo0eUcOTk5Gjx4sP08OTlZktSzZ09Nnz5dDz30kE6fPq0JEyaoqKhIbdq00YIFC1S3bl37NSkpKZoyZYqGDBkip9Opu+66S+PHj7f3N2zYUG+++aYmT56sXr16qXHjxnrsscfUr18/uyY2NlYpKSmaOXOmZsyYoRtvvFFz585VcHCwXVORuQAAAODq4LAsd1pdcnXIz6++hfEBAQ0rNJ639/9eWBgbK2VlXf7g/ftLixdXrD4mRvrsMx0/fkqlpVxYaMKkh3A/9M/z0UPPRw89X3X38MJ4l+O2a6IBAAAAd0WIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMESIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMESIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMESIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMESIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMORd0xPA1cPLq2K/s5WXWyovt6p4NgAAAFeOEI2q16KFVFYmX996FSq3Skt1rPAMQRoAALgtQjSqXqNGkpeXNGCAlJv7y7VhYXKkp8vpdBCiAQCA2yJEo/rk5kpZWTU9CwAAgF+NCwsBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwJBbh+iysjLNnDlTnTp1UmRkpLp06aK5c+fKsiy7xrIszZo1SwkJCYqMjNTQoUP19ddfuxznxIkTGj16tGJjYxUXF6dx48bp1KlTLjV79uzR/fffr4iICHXs2FGpqakXzWf9+vXq1q2bIiIilJSUpE2bNlXJ+wYAAIB7c+sQnZqaqnfffVcTJkzQunXrNGbMGC1YsECLFi1yqVm0aJEmTpyoZcuWqV69eho+fLjOnj1r14wZM0b79+9XWlqa5s2bpx07dmjChAn2/uLiYg0fPlwtW7bUypUrNXbsWM2ZM0dLly61az777DONHj1affr00erVq9W5c2eNHDlSe/furZ4PAwAAAG7DrUN0VlaWOnfurDvuuEOBgYHq1q2bEhIStHPnTknnz0IvXLhQjz76qLp06aLQ0FC99tprOnr0qDZs2CBJOnDggDZv3qypU6cqKipKcXFxGj9+vNauXasffvhBkvT++++rpKRE06ZN080336zExEQNGjRIaWlp9lwWLlyoDh066MEHH1RQUJCeeuop3XLLLXrnnXeq/4MBAABAjXLrEB0TE6PMzEx99dVXks4vufj00091++23S5IOHz6svLw8xcfH269p2LChoqKilJWVJel8EPf19VVERIRdEx8fL6fTaYfx7OxsxcXFycfHx65JSEjQV199pcLCQrumffv2LvNLSEhQdna28ftyOKrvUdHx3FF1fk7u/OCz8OwH/fP8Bz30/Ac99PxHdfewIrwrL/JUvhEjRqi4uFj33HOPvLy8VFZWpqefflo9evSQJOXl5UmS/P39XV7n7++v/Px8SVJ+fr6aNGnist/b21t+fn726/Pz8xUYGOhSExAQYO/z8/NTfn6+ve1S45jw929o/Jpfo7rHqwyNG9ev6Sm4FU/sIf4P/fN89NDz0UPP5249dOsQvX79eq1Zs0Z/+tOf1KpVK+Xm5io5OVnNmjVTz549a3p6V6yg4KR+cm1klXE4zv+Bq8h4Xl5Otwqux4+fUllZeU1Po8aZ9BDuh/55Pnro+eih56vuHl4Y73LcOkS/9tprGjFihBITEyVJISEhOnLkiObPn6+ePXuqadOmkqSCggI1a9bMfl1BQYFCQ0MlnT+jfOzYMZfjlpaWqrCw0H59QEDARWeULzy/cPb5UjUFBQUXnZ2uCMtStf4gV/d4lcUT51xVPLWHOI/+eT566Pnooedztx669ZroM2fOyPGzhSleXl72Le4CAwPVtGlTbdmyxd5fXFyszz//XDExMZLOr6suKipSTk6OXZOZmany8nJFRkZKkqKjo7Vjxw6VlJTYNRkZGbrpppvk5+dn12RmZrrMJSMjQ9HR0ZX3hgEAAOAR3DpE33nnnZo3b542btyow4cP66OPPlJaWpq6dOkiSXI4HBo8eLDeeOMNffzxx/ryyy81duxYNWvWzK4JCgpShw4d9NJLL2nnzp369NNPNWXKFCUmJqp58+aSpKSkJNWpU0cvvvii9u3bp3Xr1mnhwoUaNmyYPZfBgwdr8+bNeuutt3TgwAG9/vrrysnJ0cCBA6v/gwEAAECNcuvlHOPHj9esWbM0adIke8lGv379NHLkSLvmoYce0unTpzVhwgQVFRWpTZs2WrBggerWrWvXpKSkaMqUKRoyZIicTqfuuusujR8/3t7fsGFDvfnmm5o8ebJ69eqlxo0b67HHHlO/fv3smtjYWKWkpGjmzJmaMWOGbrzxRs2dO1fBwcHV82EAAADAbTgsy51Wl1wd8vOrb2F8QEDDCo3n7f2/FxbGxkr/e3vAX9S/v7R4ccXqTWpjYqTPPtPx46dUWsqFhSY9hPuhf56PHno+euj5qruHF8a7HLdezgEAAAC4I0I0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYMi7picAXIqXV8V+vysvt1ReblXxbAAAAFwRouFeWrSQysrk61uvQuVWaamOFZ4hSAMAgGpFiIZ7adRI8vKSBgyQcnN/uTYsTI70dDmdDkI0AACoVoRouKfcXCkrq6ZnAQAAcElcWAgAAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYMg7RR44ckWVdfDsxy7J05MiRSpkUAAAA4M6MQ3Tnzp117Nixi7afOHFCnTt3rpRJAQAAAO7MOERbliWHw3HR9h9//FF169atlEkBAAAA7qzCX7aSnJwsSXI4HJo5c6bq1fu/r2UuKyvTzp07FRoaWvkzBAAAANxMhUP07t27JZ0/E713717VqVPH3ufj46PQ0FA98MADlT9DAAAAwM1UOEQvWrRIkvTCCy/oxRdfVIMGDapsUgAAAIA7q3CIvuDCsg4AAADgamUcon/88Uf9+c9/VmZmpgoKClReXu6y/+OPP660yQEAAADuyDhEjx8/Xtu2bdO9996rpk2bXvJOHQAAAEBtZhyiP/nkE82fP19t2rSpivkAAAAAbs/4PtG+vr5q1KhRFUwFAAAA8AzGIfrJJ5/UrFmzdPr06aqYDwAAAOD2jJdzpKWl6dtvv1V8fLwCAwPl7e16iFWrVlXa5AAAAAB3ZByiu3TpUhXzAAAAADyGcYh+/PHHq2IeAAAAgMcwXhMNAAAAXO2Mz0SHhob+4r2hc3Nzf9WEAAAAAHdnHKLnzJnj8ry0tFS5ublatWqVnnjiiUqbGAAAAOCuKuXCwm7duqlVq1Zat26d7rvvvkqZGAAAAOCuKm1NdHR0tDIzMyvrcAAAAIDbqpQQfebMGS1cuFDNmjWrjMMBAAAAbs14OUfbtm1dLiy0LEunTp3SNddcoz/+8Y+VOjkAAADAHRmH6HHjxrk8dzgcatKkiaKiouTn51dpEwMAAADclXGI7tmzZ1XMAwAAAPAYxiFakoqKirR8+XIdOHBAknTzzTerd+/eatiwYaVODgAAAHBHxhcWfvHFF+ratav+8pe/qLCwUIWFhUpLS1OXLl20a9euqpgjAAAA4FaMz0QnJyerU6dOmjJliry9z7+8tLRU48eP17Rp05Senl7pkwQAAADcifGZ6JycHD344IN2gJYkb29vPfjgg8rJyanUyQEAAADuyDhEN2jQQN9///1F27///nvVr1+/UiYFAAAAuDPjEN29e3e9+OKLWrdunb7//nt9//33Wrt2rcaPH6/ExMSqmCMAAADgVozXRI8dO9b+37KysvMH8fZW//79NWbMmMqdHQAAAOCGjM9E+/j4aPz48dq+fbtWr16t1atXa9u2bRo3bpx8fHwqfYI//PCDxowZo3bt2ikyMlJJSUn64osv7P2WZWnWrFlKSEhQZGSkhg4dqq+//trlGCdOnNDo0aMVGxuruLg4jRs3TqdOnXKp2bNnj+6//35FRESoY8eOSk1NvWgu69evV7du3RQREaGkpCRt2rSp0t8vAAAA3J9xiL6gXr16CgkJUUhIiOrVq1eZc7IVFhaqf//+qlOnjlJTU7V27Vo999xzLt+MmJqaqkWLFmnixIlatmyZ6tWrp+HDh+vs2bN2zZgxY7R//36lpaVp3rx52rFjhyZMmGDvLy4u1vDhw9WyZUutXLlSY8eO1Zw5c7R06VK75rPPPtPo0aPVp08frV69Wp07d9bIkSO1d+/eKnnvAAAAcF/GyznOnj2rRYsWaevWrSooKJBlWS77V61aVWmTS01NVYsWLZScnGxvu/766+3/tixLCxcu1KOPPqouXbpIkl577TXFx8drw4YNSkxM1IEDB7R582YtX75cERERkqTx48drxIgRGjt2rJo3b673339fJSUlmjZtmnx8fHTzzTcrNzdXaWlp6tevnyRp4cKF6tChgx588EFJ0lNPPaWMjAy98847mjx5cqW9ZwAAALg/4xA9btw4/fOf/9Tdd9+tyMhIORyOqpiXJOl//ud/lJCQoFGjRmn79u1q3ry57r//fvXt21eSdPjwYeXl5Sk+Pt5+TcOGDRUVFaWsrCwlJiYqKytLvr6+doCWpPj4eDmdTu3cuVNdu3ZVdna24uLiXJajJCQkKDU1VYWFhfLz81N2draGDh3qMr+EhARt2LDB+H1V4Ud2yXGqa7yaVFvf49XUw9qI/nk+euj56KHnq+4eVnQc4xC9ceNG/fnPf1abNm1MX2rs0KFDevfddzVs2DA98sgj+uKLLzR16lTVqVNHPXv2VF5eniTJ39/f5XX+/v7Kz8+XJOXn56tJkyYu+729veXn52e/Pj8/X4GBgS41AQEB9j4/Pz/l5+fb2y41jgl//+r9evTqHq+6NW5c+2+tWNt7WNvRP89HDz0fPfR87tZD4xDdvHnzarsftGVZCg8P1zPPPCNJuuWWW7Rv3z4tWbJEPXv2rJY5VIWCgpP62SqYKuFwnP8DV5HxvLycHhtGjx8/pbKy8pqeRpUw6SHcD/3zfPTQ89FDz1fdPbww3uUYh+jnnntOKSkpmjRpkq677rormlxFNW3aVEFBQS7bfvOb3+jDDz+090tSQUGBmjVrZtcUFBQoNDRU0vkzyseOHXM5RmlpqQoLC+3XBwQEXHRG+cLzC2efL1VTUFBw0dnpirAsVesPcnWPVxOuhvdX299jbUb/PB899Hz00PO5Ww+N784RERGhs2fPqkuXLoqJidGtt97q8qhMsbGx+uqrr1y2ff3113Z4DwwMVNOmTbVlyxZ7f3FxsT7//HPFxMRIkmJiYlRUVOTyleSZmZkqLy9XZGSkJCk6Olo7duxQSUmJXZORkaGbbrrJvhNIdHS0MjMzXeaSkZGh6OjoynvDAAAA8AjGZ6KfeeYZHT16VE8//bQCAgKq9MLCIUOGqH///po3b57uuece7dy5U8uWLbPvhuFwODR48GC98cYbuuGGGxQYGKhZs2apWbNm9t06goKC1KFDB7300kuaNGmSSkpKNGXKFCUmJqp58+aSpKSkJM2dO1cvvviiHnroIe3bt08LFy7UCy+8YM9l8ODBGjRokN566y117NhR69atU05ODnfmAAAAuAoZh+isrCwtXbrUXi5RlSIjIzVnzhzNmDFDc+fOVWBgoMaNG6cePXrYNQ899JBOnz6tCRMmqKioSG3atNGCBQtUt25duyYlJUVTpkzRkCFD5HQ6ddddd2n8+PH2/oYNG+rNN9/U5MmT1atXLzVu3FiPPfaYfXs76fxZ8ZSUFM2cOVMzZszQjTfeqLlz5yo4OLjKPwcAAAC4F+MQ/Zvf/EZnzpypirlc0p133qk777zz3+53OBx68skn9eSTT/7bmkaNGulPf/rTL44TGhqqxYsX/2LNPffco3vuueeXJwwAAIBaz3hN9OjRozV9+nRt3bpVx48fV3FxscsDAAAAqO2Mz0Rf+Ma+n3/xiGVZcjgcys3NrZSJAQAAAO7KOEQvXLjw3+7bu3fvr5oMAAAA4AmMQ/TPb2NXXFystWvX6r333tOuXbs0cODASpscAAAA4I6MQ/QF27dv1/Lly/W3v/1NzZo1U9euXTVhwoTKnBsAAADgloxCdF5enlatWqXly5eruLhY99xzj86dO6e5c+eqVatWVTVHAAAAwK1UOEQ/8sgj2r59u+644w6NGzdOHTp0kJeXl5YsWVKV8wMuy8ur4jeZKS+3VF7uRt8ZCgAAPFKFQ/Qnn3yiQYMGqX///rrxxhurcEpABbVoIZWVyde3XoVfYpWW6ljhGYI0AAD4VSocohcvXqzly5erV69eCgoK0r333qvu3btX5dyAX9aokeTlJQ0YIFXk1ophYXKkp8vpdBCiAQDAr1LhEB0dHa3o6GiNGzdO69at04oVKzR9+nSVl5frn//8p1q0aKEGDRpU5VyBS8vNlbKyanoWAADgKmL8jYXXXnut+vTpo3fffVfvv/++hg0bptTUVMXHx+uRRx6pijkCAAAAbsU4RP/Ub37zG40dO1abNm3SjBkzKmtOAAAAgFu74vtE/5SXl5e6dOmiLl26VMbhAAAAALf2q85EAwAAAFcjQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGCJEAwAAAIYI0QAAAIAhQjQAAABgiBANAAAAGPKu6QkA1c3Lq2K/O5aXWyovt6p4NgAAwBMRonH1aNFCKiuTr2+9CpVbpaU6VniGIA0AAC5CiMbVo1EjyctLGjBAys395dqwMDnS0+V0OgjRAADgIoRoXH1yc6WsrJqeBQAA8GBcWAgAAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCGPCtF//vOfFRISoldeecXedvbsWU2aNEnt2rVTTEyMnnjiCeXn57u87siRIxoxYoSioqLUvn17vfrqqyotLXWp2bp1q3r27Knw8HB17dpVK1euvGj89PR0derUSREREbrvvvu0c+fOqnmjAAAAcGseE6J37typJUuWKCQkxGX7tGnT9Pe//10zZ87UokWLdPToUT3++OP2/rKyMj388MMqKSnRkiVLNH36dK1atUqzZ8+2aw4dOqSHH35Y7dq103//939ryJAhGj9+vDZv3mzXrFu3TsnJyRo5cqRWrVql0NBQDR8+XAUFBVX/5gEAAOBWPCJEnzp1Ss8++6ymTp0qPz8/e/vJkye1YsUKPf/882rfvr3Cw8M1bdo0ZWVlKTs7W5L0j3/8Q/v379cf//hHhYWFqWPHjnryySeVnp6uc+fOSZKWLFmiwMBAPf/88woKCtLAgQN199136y9/+Ys9Vlpamvr27avevXurVatWmjRpkq655hqtWLGiOj8KAAAAuAGPCNGTJ09Wx44dFR8f77I9JydHJSUlLtuDgoLUsmVLO0RnZ2crODhYAQEBdk1CQoKKi4u1f/9+u6Z9+/Yux05ISLCPce7cOe3atctlHKfTqfj4eGVdwddHOxzV96joePj3qrNfv6aHPNzzQf88/0EPPf9BDz3/Ud09rAjvyosaVWPt2rXavXu3li9fftG+/Px81alTR76+vi7b/f39lZeXZ9f8NEBLsp9frqa4uFhnzpxRYWGhysrK5O/vf9E4Bw8eNH5P/v4NjV/za1T3eLVJ48b1a3oKkuihp6N/no8eej566PncrYduHaK///57vfLKK3rrrbdUt27dmp5OpSkoOCnLqvpxHI7zf+AqMp6Xl9NtAqM7OX78lMrKymtsfJMewv3QP89HDz0fPfR81d3DC+NdjluH6F27dqmgoEC9evWyt5WVlWn79u1KT0/Xm2++qZKSEhUVFbmcjS4oKFDTpk0lnT+j/PO7aFy4e8dPa35+R4/8/Hw1aNBA11xzjZxOp7y8vC66iLCgoOCiM9gVYVmq1h/k6h6vtnGHz44eejb65/nooeejh57P3Xro1muib7vtNq1Zs0arV6+2H+Hh4UpKSrL/u06dOtqyZYv9moMHD+rIkSOKjo6WJEVHR2vv3r0uATgjI0MNGjRQq1at7JrMzEyXsTMyMuxj+Pj4qHXr1i7jlJeXa8uWLYqJiamidw8AAAB35dZnohs0aKDg4GCXbddee60aNWpkb+/du7emT58uPz8/NWjQQFOnTlVMTIwdgBMSEtSqVSuNHTtWzz77rPLy8jRz5kwNGDBAPj4+kqQ//OEPSk9P12uvvabevXsrMzNT69ev1/z58+1xhw0bpueee07h4eGKjIzU22+/rdOnT7ucJQcAAMDVwa1DdEWMGzdOTqdTo0aN0rlz55SQkKCXX37Z3u/l5aV58+Zp4sSJ6tevn+rVq6eePXtq1KhRds3111+v+fPnKzk5WQsXLlSLFi00depUdejQwa7p3r27jh07ptmzZysvL09hYWFasGDBFS3nAAAAgGfzuBC9aNEil+d169bVyy+/7BKcf+66665TamrqLx63Xbt2Wr169S/WDBw4UAMHDqzwXAEAAFA7ufWaaAAAAMAdEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQx73td9AdfLyqtjvmeXllsrLrSqeDQAAcBeEaOBSWrSQysrk61uvQuVWaamOFZ4hSAMAcJUgRAOX0qiR5OUlDRgg5eb+cm1YmBzp6XI6HYRoAACuEoRo4Jfk5kpZWTU9CwAA4Ga4sBAAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEOEaAAAAMAQIRoAAAAwRIgGAAAADBGiAQAAAEPeNT0BoLbw8qr476Tl5ZbKy60qnA0AAKhKhGjg12rRQiork69vvQq/xCot1bHCMwRpAAA8FCEa+LUaNZK8vKQBA6Tc3MvXh4XJkZ4up9NBiAYAwEMRooHKkpsrZWXV9CwAAEA14MJCAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwBAhGgAAADBEiAYAAAAMEaIBAAAAQ4RoAAAAwJB3TU8AuFp5efE7LAAAnooQDVS3Fi2ksjL5+tarWH1ZmZxOh8rKrKqdFwAAqDBCNFDdGjWSvLykAQOk3Nxfrg0Lk9LT5XA4JBGiAQBwF4Toq0BFlg2wtKAG5OZKWVk1PQsAAHAF3Do5zZ8/X71791ZMTIzat2+vxx57TAcPHnSpOXv2rCZNmqR27dopJiZGTzzxhPLz811qjhw5ohEjRigqKkrt27fXq6++qtLSUpearVu3qmfPngoPD1fXrl21cuXKi+aTnp6uTp06KSIiQvfdd5927txZ+W+6EjmdDqmsTI0b17/so8JLCwAAAODeIXrbtm0aMGCAli1bprS0NJWWlmr48OH68ccf7Zpp06bp73//u2bOnKlFixbp6NGjevzxx+39ZWVlevjhh1VSUqIlS5Zo+vTpWrVqlWbPnm3XHDp0SA8//LDatWun//7v/9aQIUM0fvx4bd682a5Zt26dkpOTNXLkSK1atUqhoaEaPny4CgoKqufDuAIOh+P/lg3Exv7y48UXa3q6AAAAHsOtl3O8+eabLs+nT5+u9u3ba9euXWrbtq1OnjypFStWKCUlRe3bt5d0PlR3795d2dnZio6O1j/+8Q/t379faWlpCggIUFhYmJ588kmlpKTo8ccfl4+Pj5YsWaLAwEA9//zzkqSgoCB9+umn+stf/qIOHTpIktLS0tS3b1/17t1bkjRp0iRt3LhRK1as0IgRI6rxU7kCFVk2EBpaPXMBAACoBdw6RP/cyZMnJUl+fn6SpJycHJWUlCg+Pt6uCQoKUsuWLe0QnZ2dreDgYAUEBNg1CQkJmjhxovbv369bbrlF2dnZdgj/ac20adMkSefOndOuXbv08MMP2/udTqfi4+OVdQVrWh0O45dckeoaB1XP4aCfnuhCz+id56KHno8eer7q7mFFx/GYEF1eXq5p06YpNjZWwcHBkqT8/HzVqVNHvr6+LrX+/v7Ky8uza34aoCXZzy9XU1xcrDNnzqiwsFBlZWXy9/e/aJyfr9GuCH//hsavwdWtUaP6NT0F/Ar8zHs+euj56KHnc7ceekyInjRpkvbt26fFixfX9FR+tYKCk7Kq4W5l3t5OwlctceLEKZWWltf0NGDI4Tj/l351/cyj8tFDz0cPPV919/DCeJfjESF68uTJ2rhxo9555x21aNHC3h4QEKCSkhIVFRW5nI0uKChQ06ZN7Zqf30Xjwt07flrz8zt65Ofnq0GDBrrmmmvkdDrl5eV10UWEBQUFF53BrgjLUrX8IeAvi9qjuv7MoGrQP89HDz0fPfR87tZDt747h2VZmjx5sj766CO9/fbbuv766132h4eHq06dOtqyZYu97eDBgzpy5Iiio6MlSdHR0dq7d69LAM7IyFCDBg3UqlUruyYzM9Pl2BkZGfYxfHx81Lp1a5dxysvLtWXLFsXExFTmWwYAAIAHcOsQPWnSJL3//vv605/+pPr16ysvL095eXk6c+aMJKlhw4bq3bu3pk+frszMTOXk5GjcuHGKiYmxA3BCQoJatWqlsWPHas+ePdq8ebNmzpypAQMGyMfHR5L0hz/8QYcOHdJrr72mAwcOKD09XevXr9fQoUPtuQwbNkzLli3TqlWrdODAAU2cOFGnT59Wr169qvtjAQAAQA1z6+Uc7777riRp0KBBLtuTk5Pt8Dpu3Dg5nU6NGjVK586dU0JCgl5++WW71svLS/PmzdPEiRPVr18/1atXTz179tSoUaPsmuuvv17z589XcnKyFi5cqBYtWmjq1Kn27e0kqXv37jp27Jhmz56tvLw8hYWFacGCBVe0nAMAAACeza1D9JdffnnZmrp16+rll192Cc4/d9111yk1NfUXj9OuXTutXr36F2sGDhyogQMHXnZOAAAAqN3cejkHAAAA4I7c+kw0gPO8vCr2+255uaXycje6dBkAgFqKEA24sxYtpLIy+frWq1C5VVqqY4VnCNIAAFQxQjTgzho1kry8pAEDpNzcX64NC5MjPV1Op4MQDQBAFSNEA54gN1fKyqrpWQAAgP/FhYUAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIb6xEKhlvLwq9rtxebnF14MDAHCFCNFAbdGihVRWJl/fehUqt0pLdazwDEEaAIArQIgGaotGjSQvL2nAACk395drw8LkSE+X0+kgRAMAcAUI0UBtk5srZWXV9CwAAKjVuLAQAAAAMESIBgAAAAwRogEAAABDhGgAAADAECEaAAAAMESIBgAAAAxxizvgKlbRbzeU+IZDAAB+ihANXI0Mv91Q4hsOAQD4KUI0cDUy+XZDiW84BADgZwjRwNWMbzcEAOCKcGEhAAAAYIgQDQAAABgiRAMAAACGCNEAAACAIS4sBFBhFb2vNPeUBgDUdoRoAJdneF9p7ikNAKjtCNEALs/kvtLcUxoAcBUgRAOoOO4rDQCAJC4sBAAAAIwRogEAAABDLOcAUCW4kwcAoDYjRAOoXNzJAwBwFSBEA6hc3MkDAHAVIEQDqBrcyQMAUItxYSEAAABgiDPRAGocFyECADwNIRpAzeEiRACAhyJEA6g5XIQIAPBQhGgANY+LEAEAHoYLCwEAAABDnIkG4FEqehGixIWIAICqQ4gG4BkML0KUuBARAFB1CNEAPIPJRYiSfSFinTpeKisrv2w5Z60BACYI0QA8S0UvQuT2eQCAKkSIBlA7cfs8AEAVIkQDqN0Mbp/HNycCACqKEA0ALP0AABgiRAPAFSz9qOgFi5LkdDpUVkbgBoDahBANABdUZOnHFdxqr7HvNZy5BoBahhANACa41R4AQIRoALgyVXirvcLic7KsywdpAjcA1BxCNABUJZMz1wkJcvzXf6lRo2srdGgCNwDUHEI0AFSHipy5Dg11i8AtEboB4HII0QDgbmo4cEuc5QaAyyFEA4Anq+zALVX5We6KIpwDcGeEaEPp6el68803lZeXp9DQUL300kuKjIys6WkBwOVV9GLIqlxWUlYmh5dXxWqrKJwDQGUgRBtYt26dkpOTNWnSJEVFRentt9/W8OHD9de//lX+/v41PT0AqFyVfZb7nnvkeOWVqluCYhLQDWpVVqY6dbxq/Kw4Z+ZRmzidDjmdjpqexq9CiDaQlpamvn37qnfv3pKkSZMmaePGjVqxYoVGjBhRw7MDgBpU0cBtUmuyBMUkoBuGef3Xf8nPr+bDPGfmfx0vL2dNTwH/y+FwyK+BjxzeBjG0rMztvv2VEF1B586d065du/Twww/b25xOp+Lj45VVkX8e/QmnU6qOvwMdF37Bi42V6tf/5eIL/+dWkVrTeneoZR5XXusu8/DEObvLPDx5zvXqVWzOdetWvN6kNiDgfJh/9VXp0KHLzyMuTo6hQytWb1LburUcI0ZUybIZ03p3qDWuLytT48YV+HNUxfNwh1p3mkeFf66uv1567jk5nY5q+SXSUcET5A6LX2kr5IcfftDtt9+uJUuWKCYmxt7+2muvafv27XrvvfdqcHYAAACoTvzbBgAAAGCIEF1BjRs3lpeXlwoKCly2FxQUKCAgoIZmBQAAgJpAiK4gHx8ftW7dWlu2bLG3lZeXa8uWLS7LOwAAAFD7cWGhgWHDhum5555TeHi4IiMj9fbbb+v06dPq1atXTU8NAAAA1YgQbaB79+46duyYZs+erby8PIWFhWnBggUs5wAAALjKcHcOAAAAwBBrogEAAABDhGgAAADAECEaAAAAMESIBgAAAAwRomup9PR0derUSREREbrvvvu0c+fOmp7SVWv79u165JFHlJCQoJCQEG3YsMFlv2VZmjVrlhISEhQZGamhQ4fq66+/dqk5ceKERo8erdjYWMXFxWncuHE6deqUS82ePXt0//33KyIiQh07dlRqampVv7Wrwvz589W7d2/FxMSoffv2euyxx3Tw4EGXmrNnz2rSpElq166dYmJi9MQTTyg/P9+l5siRIxoxYoSioqLUvn17vfrqqyotLXWp2bp1q3r27Knw8HB17dpVK1eurPL3dzVYvHixkpKSFBsbq9jYWPXr10+bNm2y99M/z/LnP/9ZISEheuWVV+xt9NC9vf766woJCXF5dOvWzd7vsf2zUOusXbvWat26tbV8+XJr37591vjx4624uDgrPz+/pqd2Vdq4caM1Y8YM629/+5sVHBxsffTRRy7758+fb7Vp08b66KOPrNzcXOuRRx6xOnXqZJ05c8auGT58uNWjRw8rOzvb2r59u9W1a1frmWeesfefPHnSio+Pt0aPHm3t3bvX+uCDD6zIyEhryZIl1fY+a6sHHnjAWrFihbV3714rNzfXeuihh6w77rjDOnXqlF0zYcIEq2PHjlZGRob1xRdfWH379rX69etn7y8tLbV+97vfWUOHDrV2795tbdy40WrXrp31pz/9ya759ttvraioKCs5Odnav3+/tWjRIissLMz65JNPqvX91kYff/yxtXHjRuurr76yDh48aM2YMcNq3bq1tXfvXsuy6J8n+fzzz60777zTSkpKsqZOnWpvp4fubfbs2VZiYqJ19OhR+1FQUGDv99T+EaJroT59+liTJk2yn5eVlVkJCQnW/Pnza3BWsCzrohBdXl5u/fa3v7UWLFhgbysqKrLCw8OtDz74wLIsy9q/f78VHBxs7dy5067ZtGmTFRISYv3rX/+yLMuy0tPTrbZt21pnz561a/74xz9ad999d1W/patOQUGBFRwcbG3bts2yrPP9at26tbV+/Xq75kLPsrKyLMs6/4tUaGiolZeXZ9csXrzYio2NtXv22muvWYmJiS5jPfXUU9YDDzxQxe/o6tS2bVtr2bJl9M+DFBcXW3fddZf1z3/+0xo4cKAdoumh+5s9e7bVo0ePS+7z5P6xnKOWOXfunHbt2qX4+Hh7m9PpVHx8vLKysmpwZriUw4cPKy8vz6VfDRs2VFRUlN2vrKws+fr6KiIiwq6Jj4+X0+m0l+lkZ2crLi5OPj4+dk1CQoK++uorFRYWVtO7uTqcPHlSkuTn5ydJysnJUUlJiUsPg4KC1LJlS2VnZ0s635/g4GCXL2ZKSEhQcXGx9u/fb9e0b9/eZayEhAT7GKgcZWVlWrt2rX788UfFxMTQPw8yefJkdezY0aVXEj+DnuKbb75RQkKCOnfurNGjR+vIkSOSPLt/fGNhLXP8+HGVlZXJ39/fZbu/v/9F6zhR8/Ly8iTpkv26sB4sPz9fTZo0cdnv7e0tPz8/+/X5+fkKDAx0qbnwl01+fr4d+PDrlJeXa9q0aYqNjVVwcLCk859vnTp15Ovr61Lr7+/v0p+ff7PpheeXqykuLtaZM2d0zTXXVMl7ulp8+eWX+sMf/qCzZ8/q2muv1dy5c9WqVSvl5ubSPw+wdu1a7d69W8uXL79oHz+D7i8yMlLJycm66aablJeXp7lz52rAgAFas2aNR/ePEA0AFTRp0iTt27dPixcvrumpwNBNN92k1atX6+TJk/rwww/13HPP6Z133qnpaaECvv/+e73yyit66623VLdu3ZqeDq5Ax44d7f8ODQ1VVFSU7rzzTq1fv96jfzlhOUct07hxY3l5eamgoMBle0FBwUW/oaHmNW3aVJJ+sV8BAQE6duyYy/7S0lIVFhbarw8ICLjoSuYLz+l75Zg8ebI2btyot99+Wy1atLC3BwQEqKSkREVFRS71BQUFFerP5WoaNGjg0f8n4y58fHx0ww03KDw8XKNHj1ZoaKgWLlxI/zzArl27VFBQoF69eumWW27RLbfcom3btmnRokW65ZZb6KEH8vX11Y033qhvv/3Wo/tHiK5lfHx81Lp1a23ZssXeVl5eri1btigmJqYGZ4ZLCQwMVNOmTV36VVxcrM8//9zuV0xMjIqKipSTk2PXZGZmqry8XJGRkZKk6Oho7dixQyUlJXZNRkaGbrrpJpZy/EqWZWny5Mn66KOP9Pbbb+v666932R8eHq46deq49PDgwYM6cuSIoqOjJZ3vz969e11+WcrIyFCDBg3UqlUruyYzM9Pl2BkZGfYxULnKy8t17tw5+ucBbrvtNq1Zs0arV6+2H+Hh4UpKSrL/mx56llOnTunQoUNq2rSpZ/evyi5ZRI1Zu3atFR4ebq1cudLav3+/9dJLL1lxcXEuV7Wi+hQXF1u7d++2du/ebQUHB1tpaWnW7t27re+++86yrPO3uIuLi7M2bNhg7dmzx3r00UcveYu73//+99bnn39u7dixw7rrrrtcbnFXVFRkxcfHW88++6y1d+9ea+3atVZUVBS3uKsEL7/8stWmTRtr69atLrdnOn36tF0zYcIE64477rC2bNliffHFF1a/fv0ueXumBx54wMrNzbU++eQT67bbbrvk7ZleffVVa//+/dY777zD7bUqSUpKirVt2zbr0KFD1p49e6yUlBQrJCTE+sc//mFZFv3zRD+9O4dl0UN3N336dGvr1q3WoUOHrE8//dQaOnSo1a5dO/s2d57aP0J0LbVo0SLrjjvusFq3bm316dPHys7OrukpXbUyMzOt4ODgix7PPfecZVnnb3M3c+ZMKz4+3goPD7eGDBliHTx40OUYx48ft5555hkrOjraio2NtZ5//nmruLjYpSY3N9fq37+/FR4ebnXo0IFbGlaSS/UuODjYWrFihV1z5swZa+LEiVbbtm2tqKgoa+TIkdbRo0ddjnP48GHrwQcftCIjI6127dpZ06dPt0pKSlxqMjMzrXvvvddq3bq11blzZ5cxcOVeeOEF684777Rat25t3XbbbdaQIUPsAG1Z9M8T/TxE00P39tRTT1m//e1vrdatW1sdOnSwnnrqKeubb76x93tq/xyWZVlVd54bAAAAqH1YEw0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AAAAYIkQDAAAAhgjRAAAAgCFCNAAAAGCIEA0AuCIhISHasGFDTU8DAGoE31gIALXQ888/r1WrVkmSvL291bx5c3Xr1k1PPvmk6tatWylj5OXlyc/PTz4+PpVyPADwJN41PQEAQNXo0KGDkpOTVVpaql27dum5556Tw+HQs88+WynHb9q0aaUcBwA8Ecs5AKCW8vHxUdOmTfUf//Ef6tKli+Lj45WRkSFJKi8v1/z589WpUydFRkaqR48e+utf/2rvu/3227V48WKX4+3evVuhoaH67rvvJF28nOP777/Xk08+qbi4ON1666169NFHdfjwYUnS3r17FRoaqmPHjkmSTpw4odDQUD399NP26//f//t/6t+/vySpsLBQo0eP1m233abIyEjdddddWrFiRRV9UgBgjhANAFeBvXv3KisrS3Xq1JEkzZ8/X6tXr9akSZO0du1aDR06VM8++6y2bdsmp9OpxMREffDBBy7HWLNmjWJjY3XddddddPySkhINHz5c9evXV3p6ut59911de+21evDBB3Xu3DndfPPNatSokbZt2yZJ2rFjhxo1aqTt27fbx9i+fbtuvfVWSdKsWbN04MABpaamat26dZo4caIaN25cVR8PABgjRANALbVx40bFxMQoIiJCSUlJKigo0PDhw3Xu3DnNnz9f06ZNU4cOHXT99derV69e6tGjh5YuXSpJ6tGjhz777DMdOXJE0vmz02vXrlVSUtIlx1q3bp3Ky8v1yiuvKCQkREFBQUpOTtb333+vbdu2yeFwqG3btnaI3rZtm3r16qVz587pwIEDKikpUVZWlh2ijxw5orCwMEVERCgwMFDx8fHq1KlTNXxqAFAxrIkGgFqqXbt2mjhxok6fPq2//OUv8vLy0t133619+/bp9OnTeuCBB1zqS0pKFBYWJkkKCwtTUFCQPvjgA40YMULbtm3TsWPH1K1bt0uOtWfPHn377beKjY112X727Fl9++23kqS2bdtq2bJlks6fdX766af19ddfa9u2bSosLFRpaan9+v79+2vUqFHavXu3fvvb36pLly4XHRsAahIhGgBqqXr16umGG26QJE2bNk333nuv3nvvPQUHB0s6v6SjefPmLq/56Z02kpKStGbNGo0YMUIffPCBEhIS/u2Sih9//FGtW7dWSkrKRfuaNGkiSbr11ls1bdo0ff3119q/f7/atGmjgwcPatu2bSoqKlJ4eLjq1asnSerYsaP+/ve/a9OmTfrnP/+poUOHasCAAXruued+/QcDAJWA5RwAcBVwOp16+OGHNWvWLAUFBcnHx0dHjhzRDTfc4PL4j//4D/s1v/vd77Rv3z7l5OToww8/VI8ePf7t8Vu3bq1vvvlG/v7+Fx2zYcOGks5fiOjn56c33nhDYWFhql+/vtq1a6ft27dr27Zt9lKOC5o0aaKePXsqJSVF48aNs5eaAIA7IEQDwFWiW7ducjqdWrp0qR544AElJydr1apV+vbbb7Vr1y4tWrTIvre0JAUGBiomJkYvvviiysrKfnFNclJSkho3bqxHH31UO3bs0KFDh7R161ZNnTpV//rXvyRJDodDcXFxWrNmjR2YQ0JCdO7cOW3ZskVt27a1jzdr1ixt2LBB33zzjfbt26eNGzcqKCioij4ZADDHcg4AuEp4e3tr4MCBWrBggT7++GM1adJE8+fP1+HDh9WwYUPdcssteuSRR1xek5SUpEmTJun3v/+9rrnmmn977Hr16umdd95RSkqKHn/8cZ06dUrNmzdX+/bt1aBBA7uubdu22rBhgx2inU6n4uLitGnTJpc1z3Xq1NGMGTP03Xff6ZprrlGbNm00Y8aMSv5EAODK8Y2FAAAAgCGWcwAAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYIgQDQAAABgiRAMAAACGCNEAAACAIUI0AAAAYOj/A7YSzbgQEhQwAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['review_length'] = df['text'].apply(len)\n",
    "df['review_length'].hist(bins=50, color='red')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:51.092150155Z",
     "start_time": "2023-12-01T23:17:50.689360948Z"
    }
   },
   "id": "33ee94c6c65f7b87"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1800x600 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdgAAAIxCAYAAABeo9rMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde1yUdfr/8TeMoiLCKiCdHC1bOWQeCgU55GZCCkRqu5kHIuNrWRqUtpZpOeZ5U78eW1ob+RoV1Zb1UwxDq21VgmjTWhMrK8VDKWApiSs6M78/XGadBVNwdBh4PR8PHg/n/nxm5poL9Ybr/tzXx8Nms9kEAAAAAAAAAADqxdPVAQAAAAAAAAAA4I4osAMAAAAAAAAA0AAU2AEAAAAAAAAAaAAK7AAAAAAAAAAANAAFdgAAAAAAAAAAGoACOwAAAAAAAAAADUCBHQAAAAAAAACABqDADgAAAAAAAABAA1BgBwAAAAAAAACgASiwAwAAAM1YSkqKkpKSXB0GAAAA4JZauDoAAAAAwFnWrFmjKVOm1Dk2duxYPf74405/z88++0xbt25VamqqfH19nf76TcXx48dlNpuVn5+v/fv3q1WrVrriiivUp08fjR07VkFBQZKkjz76SF988YUeeeQRF0cMAAAAnB8FdgAAADQ56enpuuaaaxyOdevW7ZK817Zt27R8+XINHTqUAvs5nDp1SqNHj9Z3332nIUOGaPTo0aqqqtI333yj3NxcxcXFORTYX3nlFQrsAAAAcAsU2AEAANDk3HLLLbrxxhtdHcZFqaqqkre3t6vDcIpNmzZp586dWrBgge644w6HsZMnT+rUqVOX9P2tVqtOnTqlVq1aXdL3AQAAQPNDD3YAAAA0Ox999JFGjhypXr16qXfv3nrggQf0zTffOMzZtWuXnnzySd1222268cYbFR0drSlTpuinn36yz1m2bJn+9Kc/SZJuu+02BQcHKzg4WPv379f+/fsVHBysNWvW1Hr/4OBgLVu2zOF1goODtXv3bk2aNEl9+vTRyJEj7eP/7//9Pw0bNkw9evRQ37599dhjj+mHH35weM09e/bokUceUXR0tG688Ubdcssteuyxx1RZWXlBOdmxY4fuuece9ejRQwMGDFBOTo597Pjx4+rVq5dmzZpV63k//vijQkND9cILL5zztfft2ydJuummm2qNtWrVSj4+PpKkJ598Uq+88ook2XMZHBxsn2s2m3XPPfcoIiJCPXr00LBhw7Rhw4ZarxkcHKxnn31Wa9euVWJiom688UZt3rxZkrR+/XoNGzZMvXv31k033aQ77rhDq1evvpAUAQAAALWwgh0AAABNzi+//KIjR444HOvQoYMk6Z133tGTTz6pmJgYPf744zpx4oRycnI0cuRIvf322/bWMgUFBdq3b5+GDRumwMBAffPNN3rjjTe0e/duvfHGG/Lw8FBcXJz27Nmj3NxcTZkyRe3bt7e/13+//4XIyMhQ586d9dhjj8lms0mS/vznP2vJkiUaPHiwfv/73+vIkSN6+eWXNWrUKL3zzjvy9fVVdXW10tLSVF1drdGjRysgIECHDh3S3/72Nx07dkzt2rX71fc9evSoHnjgAQ0ePFiJiYnKy8uTyWRSy5Yt9fvf/15t27bVwIEDlZeXpylTpshgMNifm5ubK5vNVmtl+tmuuuoqe+4ffvhheXh41Dlv+PDhOnz4sLZu3Wq/cHG2l156SQMGDNAdd9yhU6dOaf369crIyNALL7yg3/3udw5zCwsLlZeXp1GjRql9+/a6+uqrtXXrVk2cOFH9+vWz9+P/7rvv9Nlnnyk1NfVXcwQAAADUhQI7AAAAmpz77ruv1rGvvvpKx48f1+zZs/WHP/xBM2fOtI8NHTpUgwYN0gsvvGA/PnLkSN1///0Or9GrVy9NnDhR//jHPxQeHq6QkBCFhYUpNzdXAwcOdOj73pACe0hIiBYuXGh/fODAAS1btkyPPvqoxo0bZz8eHx+voUOH6tVXX9W4ceP07bffav/+/VqyZIkGDRpknzdhwoQLet/Dhw/rySef1JgxYySdKXTffffdWrRoke688061bNlSQ4YM0bp167R161bdcsst9ueuXbtWffr0sRfR6zJw4EBde+21Wrp0qd566y1FRETo5ptv1q233ip/f3/7vN69e6tLly7aunWr7rzzzlqv895776l169b2x6NGjdKwYcOUlZVVq8D+/fffa926dbr++uvtx2bPni0fHx+ZzWaHiwQAAABAQ9EiBgAAAE3OM888o6ysLIcv6cyq9GPHjikxMVFHjhyxf3l6eqpnz54qKiqyv8bZhdyTJ0/qyJEj6tmzpyTpyy+/vCRx33PPPQ6PN27cKKvVqsGDBzvEGxAQoM6dO9vjrWmxsmXLFp04caLe79uiRQsNHz7c/tjLy0vDhw9XRUWF/bNGRUWpY8eOWrdunX3e119/ra+++krJycm/+vqtW7fWX//6V6WlpUmS1qxZo6lTpyomJkYzZ85UdXX1BcV59vfk6NGjqqys1M0336ydO3fWmtunTx+H4rok+fr66sSJE9q6desFvR8AAABwPqxgBwAAQJPTo0ePOjc53bNnjySdsx1ITaFakn7++WctX75c7777rioqKhzmXWhf8/o6ewW8dCZem82m+Pj4Oue3aHHmx/lOnTppzJgxysrK0rp16xQeHq4BAwYoOTn5vO1hJKljx461NlTt0qWLpDOr6Hv16iVPT0/dcccdysnJ0YkTJ9SmTRutW7dOrVq1clg1fy7t2rXT5MmTNXnyZB04cEAff/yxVq1apZdfflk+Pj567LHHzvsaH374of785z+rpKTEoShfV8uZ/86ldOauhLy8PI0dO1ZBQUGKjo7W4MGDHVbkAwAAAPVBgR0AAADNRk1f8z/96U8KDAysNX5225BHH31U27ZtU1pamkJDQ+Xt7S2r1ar/+Z//sb/OrzlXn3GLxXLO57Rq1crhsdVqlYeHh1auXFlnS5Ozi+JPPvmkhg4dqvfff19bt27VrFmz9MILL+iNN97QFVdccd54L8SQIUNkNpu1adMmJSUlKTc3V7/73e8uqIh/tquvvlq///3vFRcXp4EDB2rdunXnLbB/+umneuihh9SnTx9Nnz5dgYGBatmypd566y3l5ubWmn/2avca/v7+euedd7Rlyxb9/e9/19///netWbNGQ4YM0fz58+v1GQAAAACJAjsAAACakU6dOkk6U2iNioo657yjR4/q448/1iOPPOLQx7xmBfzZzlVI9/PzkyQdO3bM4fjBgwcvOF6j0SibzaZrrrlG11577XnnBwcHKzg4WA8//LA+++wzjRgxQjk5OectXh8+fFhVVVUOBfuaz3r11Vfbj3Xr1k1hYWFat26drrjiCh08eFDTpk274M/z3/z8/NSpUyd988039mPnyud7772nVq1ayWw2y8vLy378rbfeqtd7enl5acCAARowYICsVqtMJpNef/11Pfzww+rcuXPDPggAAACaLXqwAwAAoNmIjY2Vj4+PXnjhBZ06darWeM3GpOfaAHP16tW1jrVp00ZS7bYxPj4+at++vT799FOH46+++uoFxxsfHy+DwaDly5fXWjVvs9n0008/SZJ++eUXnT592mG8W7du8vT0vKD+5qdPn9brr79uf1xdXa3XX39dHTp00A033OAw984779TWrVu1evVq/eY3v7mg9iq7du2qc9PXAwcO6Ntvv3W4eFCTz/++MGEwGOTh4eFwB8D+/fv1/vvvn/f9a9Tkq4anp6eCg4Ml6YL7wAMAAABnYwU7AAAAmg0fHx+ZTCZNnjxZw4YNU0JCgjp06KCDBw/qo48+0k033aRnnnlGPj4+6tOnj1588UWdOnVKQUFB2rp1q/bv31/rNWsK0P/7v/+rhIQEtWzZUrfeequ8vb31hz/8QX/5y180depUde/eXZ9++qm+//77C47XaDTq0Ucf1cKFC3XgwAENHDhQbdu21f79+7Vp0ybdfffdSktLU2FhoZ599lkNGjRIXbp0kcVi0f/7f/9PBoNBt99++3nfp2PHjlq5cqUOHDigLl266N1331VJSYlmzpypli1bOsxNSkrSc889p40bN2rEiBG1xuuydetWLVu2TAMGDFDPnj3l7e2t/fv366233lJ1dbUeeeSRWvmcNWuWYmJiZDAYlJiYqP79+ysrK0v/8z//o6SkJFVUVOjVV1+V0WjUV199dUH5nDZtmo4eParIyEgFBQXp4MGDevnllxUaGqquXbte0GsAAAAAZ6PADgAAgGbljjvuUMeOHfWXv/xFZrNZ1dXVCgoKUnh4uIYNG2aft3DhQs2cOVOvvvqqbDaboqOjtXLlSsXGxjq8Xo8ePZSRkaHXXntNmzdvltVq1fvvvy9vb2+NHz9eR44c0Xvvvae8vDzdcsstevHFF9WvX78LjveBBx5Qly5d9H//939asWKFJOmKK65QdHS0BgwYIOlMa5iYmBh9+OGHOnTokNq0aaPg4GCtXLlSvXr1Ou97+Pn5ad68eZo1a5beeOMNBQQE6JlnntHdd99da25AQICio6P10Ucf6c4777ygzxAfH6/jx49r69atKiws1NGjR+Xr66sePXpozJgxioyMdJibkpKi9evXa+3atbLZbEpMTFS/fv00e/ZsrVy5UnPmzNE111yjxx9/XAcOHLjgAntycrLeeOMNvfrqqzp27JgCAwM1ePBgPfLII/L05OZeAAAA1J+H7UJ2aAIAAACAfxs/fry+/vprbdy40dWhAAAAAC7FMg0AAAAAF+zw4cP1Wr0OAAAANGW0iAEAAABwXvv27dNnn32mN998Uy1atNDw4cNdHRIAAADgcqxgBwAAAHBexcXFmjx5svbv36958+YpMDDQ1SEBAAAALkcPdgAAAAAAAAAAGoAV7AAAAAAAAAAANAAFdgAAAAAAAAAAGoACOwAAAAAAAAAADUCBHQAAAAAAAACABqDADgAAAAAAAABAA1BgBwAAAAAAAACgASiwAwAAAAAAAADQABTYgWZg2bJlCg4O1pEjR1wdCgAAuEQ43wMA0DxwzgcaFwrsQB3GjRunnj176pdffjnnnEmTJql79+766aefLug1Dx06pGXLlqmkpMRZYTY6VqtV77zzjv7whz+ob9++6t27t26//XZNnjxZ27dvt8/bvXu3li1bpv3797suWABAs8f5vmE43wMA3A3n/IbhnA9cGArsQB2Sk5P1r3/9S5s2bapz/MSJE/rggw8UExOj9u3bX9BrHj58WMuXL2/SJ99Zs2bpiSeeUGBgoCZMmKDHH39csbGx+vzzz7V582b7vN27d2v58uU6cOCAC6MFADR3nO8bhvM9AMDdcM5vGM75wIVp4eoAgMZowIABatu2rdatW6chQ4bUGn///fdVVVWl5OTkyx9cI1VeXq5XX31Vd999t2bOnOkwZrPZLsutaydOnFCbNm0u+fsAAJoGzvf1x/keAOCOOOfXH+d84MKxgh2oQ+vWrRUfH6/CwkJVVFTUGs/NzVXbtm01YMAASdK+ffuUnp6uvn37qmfPnrr77rv1t7/9zT6/qKhIv//97yVJU6ZMUXBwsIKDg7VmzRr7nM8//1xpaWm6+eab1bNnT40ePVr/+Mc/HN73l19+0ezZszVgwAB1795d/fr105gxY/Tll19e0Of66aeflJGRoZtuukkRERGaNWuWTp48aR8fPXr0OX+guP3225WWlnbO196/f79sNptuuummWmMeHh7y9/eXJK1Zs0YZGRmSpHvvvdeei6KiIknSpk2b9MADDygmJkbdu3fXwIEDtWLFClksFofXTElJUVJSknbs2KFRo0apZ8+eWrRokSTpn//8p9LS0hQREaEePXpowIABmjJlygXlCADQfHC+r43zPQCgKeKcXxvnfMB5WMEOnMMdd9yht99+W3l5eRo9erT9+M8//6wtW7YoMTFRrVu3Vnl5ue655x6dOHFCKSkpat++vd5++2099NBDWrp0qeLi4tS1a1elp6dr6dKlGj58uG6++WZJsp+oPv74Y40dO1bdu3fXhAkT5OHhoTVr1ig1NVWvvvqqevToIUmaPn263nvvPY0ePVpdu3bVzz//rH/84x/69ttvdcMNN5z3Mz366KO6+uqrNWnSJG3fvl3Z2dk6duyY/vSnP0mS7rzzTk2bNk1ff/21unXrZn/eF198oT179uihhx4652tfddVVkqQNGzZo0KBB57zK3KdPH6WkpCg7O1vjxo3TddddJ0nq2rWrJOntt9+Wt7e3xowZI29vbxUWFmrp0qX65Zdf9MQTTzi81s8//6yxY8cqMTFRycnJ8vf3V0VFhdLS0tS+fXs98MAD8vX11f79+7Vx48bz5gcA0Pxwvud8DwBoHjjnc84HLhkbgDqdPn3aFh0dbRs+fLjD8ZycHFu3bt1smzdvttlsNtvs2bNt3bp1sxUXF9vn/PLLL7YBAwbYbr31VpvFYrHZbDbbF198YevWrZvtrbfecng9q9Vqi4+Pt91///02q9VqP37ixAnbgAEDbGPGjLEfu/nmm20zZsyo92dZunSprVu3brZx48Y5HDeZTLZu3brZSkpKbDabzXbs2DHbjTfeaHvuuecc5s2cOdPWq1cv2/Hjx3/1fSZPnmzr1q2brU+fPrbx48fbzGazbffu3bXm5eXl2bp162YrLCysNXbixIlax55++mlbz549bSdPnrQfGz16tK1bt262nJwch7kbN260devWzfbFF1/8aqwAANhsnO/PxvkeANCUcc7/D875gHPRIgY4B4PBoMTERG3bts1hJ+zc3FwFBASoX79+kqSPPvpIPXr0UHh4uH1O27ZtNXz4cB04cEC7d+/+1fcpKSnRnj17dMcdd+inn37SkSNHdOTIEVVVValfv34qLi6W1WqVJPn6+urzzz/XoUOHGvSZRo0a5fC45qr93//+d0lSu3btdNttt2n9+vWy2WySJIvFory8PN12223y9vb+1defO3eunnnmGV1zzTXauHGj5s+fr4SEBKWmpl5wzK1bt7b/+ZdfftGRI0cUHh6uEydO6LvvvnOY6+XlpWHDhjkca9eunSTpb3/7m06dOnVB7wkAaL4433O+BwA0D5zzOecDlwoFduBX3HHHHZLOnHAl6ccff9Snn36qhIQEGQwGSdLBgwd17bXX1npuzW1RBw8e/NX32LNnjyTpiSeeUL9+/Ry+/vrXv6q6ulqVlZWSpMcff1zffPONfve73+n3v/+9li1bpn379l3w5+ncubPDY6PRKE9PT4cfLoYMGaKDBw/q008/lSQVFBSovLxcd95553lf39PTU6NGjdKaNWtUWFio559/XrfccosKCwv12GOPXVCM33zzjcaPH6+bb75ZN998s/r166c//vGPkmTPQ42goCB5eXk5HOvbt69uv/12LV++XJGRkXrooYf01ltvqbq6+oLeHwDQ/HC+53wPAGgeOOdzzgcuBXqwA7+ie/fuuu6667R+/XqNGzdOubm5stls9pOyM9RcRZ48ebJCQ0PrnFNzVTkhIUHh4eHauHGjtm7dKrPZrJUrV2rZsmXq379/vd/bw8Oj1rGYmBgFBARo7dq16tOnj9auXavAwEBFRUXV67Xbt2+v2267TbfddptSUlL0ySef6MCBA7r66qvP+Zxjx45p9OjR8vHxUXp6uoxGo1q1aqUvv/xSCxYssF/lr3H2lfCzP9PSpUu1fft2ffjhh9q8ebOeeuopZWVl6fXXX1fbtm3r9TkAAE0f53vO9wCA5oFzPud84FJgBTtwHnfccYe+/vpr7dq1S7m5uerSpYt9QxLpzMYf33//fa3n1dzqVLMxSF0nOknq1KmTJMnHx0dRUVF1frVs2dI+v2PHjho1apSef/55vf/++/rNb36jzMzMC/ose/furfXYarXqmmuusR8zGAxKSkrSe++9p6NHj2rTpk1KTEy0X81viO7du0uSysrKJJ07F5988ol+/vlnzZs3T6mpqbr11lsVFRUlPz+/er9nr1699Nhjj2nNmjVasGCBvvnmG7377rsN/gwAgKaN8z3newBA88A5n3M+4GwU2IHzqLmSvXTpUpWUlNS6st2/f3998cUX2rZtm/1YVVWV3njjDV199dW6/vrrJcm+4/axY8ccnt+9e3cZjUatWrVKx48fr/X+R44ckXSmT9p/3z7l7++vjh07XvCtUa+88orD45dfflmSdMsttzgcv/POO3X06FE988wzqqqqUnJy8nlfu6ysrM5edNXV1fr444/l6ekpo9Eo6T+5+O/P4+l55r+kmiv+Nc9/9dVXz/v+NY4ePerwfEn2VQPcQgYAOBfO95zvAQDNA+d8zvmAs9EiBjiPTp06qXfv3nr//fclqdbJ94EHHtD69es1duxYpaSkyM/PT++8847279+vZcuW2U8oRqNRvr6+eu2119S2bVt5e3urR48e6tSpk2bNmqWxY8cqKSlJw4YNU1BQkA4dOqSioiL5+PgoMzNTx48fV//+/XX77bcrJCRE3t7eKigo0D//+U89+eSTF/RZ9u/fr3Hjxik2Nlbbt2/X2rVrlZSUpJCQEId5YWFh6tatmzZs2KCuXbvqhhtuOO9r//jjj/rDH/6gyMhI9evXTwEBAaqoqND69eu1a9cupaamqkOHDpLOnAwNBoNWrlypyspKeXl5KTIyUr1795afn5+efPJJpaSkyMPDQ//v//2/WifTX/P2228rJydHAwcOlNFo1PHjx/XGG2/Ix8en1g8ZAADU4HzP+R4A0DxwzuecDzibh60+f6uBZuqVV17Rs88+qx49euivf/1rrfF9+/bpueee08cff6yTJ08qODhY48eP1+9+9zuHee+//74WLVqkPXv26PTp05o7d659h+ySkhI9//zz+uSTT1RVVaXAwED16NFDw4cPV79+/VRdXa3Fixdr69at2rdvn2w2m4xGo4YPH66RI0f+avzLli3T8uXL9e6772rJkiXasmWLWrRooTvuuEOTJ09Wq1ataj3nxRdf1HPPPaeJEyfqwQcfPG+OfvnlF61Zs0YfffSRdu/erYqKCnl5ealbt26666679Pvf/97htrG//vWveuGFF3Tw4EFZLBa99NJLioiI0Geffab58+dr165d8vX1VXJysvr166e0tDT7HElKSUnRTz/9ZN+cpsbOnTtlNpv12Wefqby8XO3atVOPHj00YcIE+21sAADUhfM953sAQPPAOZ9zPuBMFNgB1Gn16tWaO3euPvjgA3uPOQAA0LRwvgcAoHngnA9cOvRgB1CLzWbTm2++qT59+nDiBQCgieJ8DwBA88A5H7i06MEOwK6qqkoffPCBioqK9PXXX+v55593dUgAAMDJON8DANA8cM4HLg9axACw279/v2677Tb5+vpq5MiReuyxx1wdEgAAcDLO9wAANA+c84HLgwI7AAAAAAAAAAANQA92AAAAAAAAAAAagAI7AAAAAAAAAAANQIEdAAAAAAAAAIAGoMAOAAAAAAAAAEADtHB1ABejoqJSjXmLVg8Pyd+/XaOP052QU+cjp85HTp3LnfJZEyucy52+9+4Qqzsgn85HTp2PnDqfu+SU8/2l4y7f+8Yepzshp85FPp2PnDqfu+S0Pud7ty6w22xq1N+IGu4Spzshp85HTp2PnDoX+Wy+3Ol7706xugPy6Xzk1PnIqfOR0+bLXb737hKnOyGnzkU+nY+cOl9TyiktYgAAAAAAAAAAaAAK7AAAAAAAAAAANAAFdgAAAAAAAAAAGoACOwAAAAAAAAAADUCBHQAAAAAAAACABqDADgAAAAAAAABAA7RwdQAAAAAAAODyKy4ultls1o4dO1RWVqYVK1Zo4MCB9vHg4OA6n/fHP/5R//M//yNJGjBggA4cOOAwPmnSJD3wwAOXLnAAABoRCuwAAAAAADRDVVVVCg4O1l133aUJEybUGt+yZYvD47///e+aOnWqbr/9dofj6enpuvvuu+2P27Zte2kCBgCgEaLADgAAAABAM9S/f3/179//nOOBgYEOj99//31FRESoU6dODsfbtm1bay4AAM0FBXYAAAAAAPCrysvL9dFHH2nevHm1xlauXKk///nPuvLKK5WUlKT77rtPLVrUv9zg4eGMSC+dmvgae5zuhJw6F/l0PnLqfO6S0/rER4EdAAAAAAD8qrfffltt27ZVfHy8w/GUlBSFhYXJz89P27Zt06JFi1RWVqYpU6bU+z38/ds5K9xLyl3idCfk1LnIp/ORU+drSjmlwA4AAAAAAH7VW2+9pTvuuEOtWrVyOD5mzBj7n0NCQtSyZUtNnz5dkyZNkpeXV73eo6KiUjabU8K9JDw8zhSEGnuc7oScOhf5dD5y6nzuktOaOC8EBfZLxGKxqKioQFVVR+Xt7aeIiCgZDAZXhwUAAAAAjQ6/PzVun376qb7//nstXrz4vHN79uyp06dPa//+/bruuuvq9T42mxp1saWGu8TpTsipc5FP5yOnztFUz/cU2C+B3Ny1MpmmqrR0r/2Y0dhZJtNsJSUluzAyAAAAAGhc+P2p8XvzzTd1ww03KCQk5LxzS0pK5OnpKX9//8sQGQDAXTTl872nqwNoanJz1yotLUWhoWHKy9ukyspK5eVtUmhomNLSUpSbu9bVIQIAAABAo8DvT651/PhxlZSUqKSkRJK0f/9+lZSU6ODBg/Y5v/zyizZs2KA//OEPtZ6/bds2/d///Z927dqlffv2ae3atZo7d66Sk5Pl5+d32T4HAKBxa+rnew+bzX1vcCgvb1y9eiwWiyIieik0NEyrV+fIYPBUQEA7lZdXymKxKjV1hEpKSlRUtK1J3P7gCh4esue0MX3v3Rk5dT5y6lzulM+aWOFc7vS9d4dY3QH5dD5y6nzk9OK56+9PTel8X1RUpHvvvbfW8aFDh2revHmSpNdff11z5szRli1b1K6d4+f+8ssvNWPGDH333Xeqrq7WNddcozvvvFNjxoypd/91qfGf8/l373zk1LnIp/OR04vXHM73tIhxosLCApWW7lVmplmeno43B3h6eio9faISE+NUWFig6OhYF0UJAAAAAK7H70+uFxERoa+++upX5wwfPlzDhw+vc+yGG27QG2+8cSlCAwA0Ec3hfE+LGCc6dOhHSVJISFid46GhYQ7zAAAAAKC54vcnAACavuZwvqfA7kRBQVdIknbt2lnneEnJTod5AAAAANBc8fsTAABNX3M431Ngd6LIyCgZjZ21ZMlCWa1WhzGr1aqlSxfJaOyiyMgoF0UIAAAAAI0Dvz8BAND0NYfzPQV2JzIYDDKZZis/f4NSU0eouLhIlZWVKi4uUmrqCOXnb5DJNKtRNewHAAAAAFfg9ycAAJq+5nC+Z5NTJ0tKSpbZnC2TaaoSEuLsx43GLjKbs5WUlOzC6AAAAACg8eD3JwAAmr6mfr6nwH4JJCUla/DgRBUVFaiq6qi8vf0UERHl1ldiAAAAAOBS4PcnAACavqZ8vqfAfokYDAZFR8cqIKCdyssrZbO5OiIAAAAAaJz4/QkAgKavqZ7v6cEOAAAAAAAAAEADUGAHAAAAAAAAAKABKLADAAAAAAAAANAAFNgBAAAAAAAAAGgACuwAAAAAAAAAADQABXYAAAAAAAAAABqAAjsAAAAAAAAAAA3QwtUBAAAAAAAAAEBjYrFYVFRUoKqqo/L29lNERJQMBoOrw0IjRIEdAAAAAAAAAP4tN3etTKapKi3daz9mNHaWyTRbSUnJLowMjREtYgAAAAAAAABAZ4rraWkpCg0NU17eJlVWViovb5NCQ8OUlpai3Ny1rg4RjQwFdgAAAAAAAADNnsVikck0VfHxg7R6dY7Cw/vKx8dH4eF9tXp1juLjB8lkmiaLxeLqUNGIUGAHAAAAAAAA0OwVFhaotHSvMjImydPTsWzq6emp9PSJKi3do8LCAhdFiMaIAjsAAAAAAACAZu/QoR8lSSEhYXWOh4aGOcwDJArsAAAAAAAAAKCgoCskSbt27axzvKRkp8M8QKLADgAAAAAAAACKjIyS0dhZS5YslNVqdRizWq1aunSRjMYuioyMclGEaIwosAMAAAAAAABo9gwGg0ym2crP36DU1BEqLi5SZWWliouLlJo6Qvn5G2QyzZLBYHB1qGhEWrg6AAAAAAAAAABoDJKSkmU2Z8tkmqqEhDj7caOxi8zmbCUlJbswOjRGFNgBAAAAAAAA4N+SkpI1eHCiiooKVFV1VN7efoqIiGLlOupEgR0AAAAAAAAAzmIwGBQdHauAgHYqL6+UzebqiNBY1asH+7JlyxQcHOzwNWjQIPv4yZMnNWPGDEVERKh379565JFHVF5e7vAaBw8e1AMPPKCePXuqX79+mj9/vk6fPu2cTwMAAAAAAAAAwGVS7xXsv/3tb5WVlWV/fPatEXPmzNFHH32kxYsXq127dpo5c6YmTJig1157TZJksVj04IMPKiAgQK+99poOHz6sJ554Qi1bttTEiROd8HEAAAAAAAAAALg86rWCXTpTUA8MDLR/dejQQZJUWVmpt956S08++aT69eun7t27a86cOdq2bZu2b98uSdqyZYt2796t5557TqGhoerfv78yMjL0yiuvqLq62qkfDAAAAAAAAACAS6neK9j37t2rmJgYtWrVSr169dKkSZN01VVXaceOHTp16pSioqLsc7t27aqrrrpK27dvV69evbR9+3Z169ZNAQEB9jkxMTEymUzavXu3wsLC6hWLh0d9o7+8auJr7HG6E3LqfOTU+cipc7lTPt0hRgAAAAAA4Dz1KrD36NFDc+fO1bXXXquysjKtWLFCo0aN0rp161ReXq6WLVvK19fX4Tn+/v4qKyuTJJWXlzsU1yXZH9fMqQ9//3b1fo4ruEuc7oScOh85dT5y6lzkEwAAAAAANDb1KrD379/f/ueQkBD17NlTt956q/Ly8tS6dWunB3c+FRWNewdfD48zBaHGHqc7IafOR06dj5w6lzvlsyZWAAAAAADQPNS7RczZfH191aVLF5WWlioqKkqnTp3SsWPHHFaxV1RUKDAwUNKZ1epffPGFw2uUl5dLkn1OfdhsavTFFsl94nQn5NT5yKnzkVPnIp8AAAAAAKCxqfcmp2c7fvy49u3bp8DAQHXv3l0tW7bUxx9/bB//7rvvdPDgQfXq1UuS1KtXL3399deqqKiwzykoKJCPj4+uv/76iwkFAAAAAAAAAIDLql4r2OfPn69bb71VV111lQ4fPqxly5bJ09NTSUlJateune666y7NmzdPfn5+8vHx0axZs9S7d297gT0mJkbXX3+9Jk+erD/+8Y8qKyvT4sWLNWrUKHl5eV2KzwcAAAAAAAAAwCVRrwL7jz/+qIkTJ+rnn39Whw4ddPPNN+uNN95Qhw4dJElPPfWUPD09lZ6erurqasXExGj69On25xsMBmVmZspkMmn48OFq06aNhg4dqvT0dOd+KgAAAAAAAAAALrF6Fdj/93//91fHW7VqpenTpzsU1f/b1VdfrZUrV9bnbQEAAAAAAAAAbsxisaioqEBVVUfl7e2niIgoGQwGV4d10S5qk1MAAAAAAAAAAH5Nbu5amUxTVVq6137MaOwsk2m2kpKSXRjZxbuoTU4BAAAANG4Wi0Vbt25WTk6Otm7dLIvF4uqQAAAA0Izk5q5VWlqKQkPDlJe3SZWVlcrL26TQ0DClpaUoN3etq0O8KBTYAQAAgCYqN3etIiJ6aciQRI0cOVJDhiQqIqKX2/8SAwAAAPdgsVhkMk1VfPwgrV6do/DwvvLx8VF4eF+tXp2j+PhBMpmmufUiEArsAAAAQBPU1FcKAQAAoPErLCxQaeleZWRMks1mc7iz0mazKT19okpL96iwsMDVoTYYPdgBAACAJua/VwoZDJ4OK4VSU0fIZJqmwYMTm8TGUgAAAGicDh36UZK0Z88ejRuXVqsH+xNPTHOY545YwQ4AAAA0MWevFPL0dPyR39PTs0msFAIAAEDjFxR0hSTp4Yf/p847K8ePH+swzx1RYAcAAACamJoVQCEhYXWOh4aGOcwDAAAALoU+fSJkMBgUGNhRL774kk6ePKl169bp5MmTevHFlxQY2FEGQwv16RPh6lAbjBYxAAAAQBNTswJo166dCg/vW2u8pGSnwzwAAADgUiguLpLFYlFZ2WF162bUiRMn7GNt2rSxPy4uLlJ0dKyrwrworGAHAAAOXnjhBd11113q3bu3+vXrp4cffljfffedw5yUlBQFBwc7fD3zzDMOcw4ePKgHHnhAPXv2VL9+/TR//nydPn36cn4UoNmKjIyS0dhZS5YslNVqdRizWq1aunSRjMYuioyMclGEAAAAaA4u9I5Jd76zkhXsAADAwSeffKJRo0bpxhtvlMVi0aJFi5SWlqb169fL29vbPu/uu+9Wenq6/XGbNm3sf7ZYLHrwwQcVEBCg1157TYcPH9YTTzyhli1bauLEiZf18wDNkcFgkMk0W2lpKUpNHaGMjImKiYlQcXGRlixZpPz8DTKbs9ngFAAAAJdUQECgJCkiop/WrMlVcXGhqqqOytvbT336RGro0ER98kmhfZ47osAOAAAcmM1mh8fz5s1Tv3799OWXX6pPnz72461bt1ZgYN0/BG3ZskW7d+9WVlaWAgICFBoaqoyMDC1YsEATJkyQl5fXJf0MAKSkpGSZzdkymaYqISHOftxo7CKzOVtJSckujA4AAADNgYeHx7//ZJPBYFB0dKwCAtqpvLxSFotVNcP/med+KLADAIBfVVlZKUny8/NzOL5u3TqtXbtWgYGBuvXWW/Xwww/bV7Fv375d3bp1U0BAgH1+TEyMTCaTdu/erbCwujderIs7/Jz1nx8KXRtHU0E+neeOO5KVkJCowsIC+0qhyMgoVq47AX9Pnc9dctrY46uP4uJimc1m7dixQ2VlZVqxYoUGDhxoH3/yySf19ttvOzwnJibG4WL8zz//rJkzZ+rDDz+Up6en4uPjNXXqVLVt2/ayfQ4AQONVVnZYklRUVFjnnZVFRYUO89wRBXYAAHBOVqtVc+bM0U033aRu3brZjyclJemqq65Sx44d9dVXX2nBggX6/vvvtXz5cklSeXm5Q3Fdkv1xWVlZvWLw9293kZ/i8nGnWN0B+XSeO+9McHUITRZ/T52PnF4+VVVVCg4O1l133aUJEybUOSc2NlZz5861P/7vu9Aef/xxlZWVKSsrS6dOndJTTz2lZ555RgsXLryksQMA3ENQ0BWSpKlTTcrOzqp1Z+VTT03XnDkz7PPcEQV2AABwTjNmzNA333yjV1991eH48OHD7X8ODg5WYGCg7rvvPpWWlspoNDo1hoqKStlsTn1Jp/PwOFMQcodY3QH5dD5y6nzk1PncJac1cTYF/fv3V//+/X91jpeX1zlbwn377bfavHmz3nzzTd14442SpGnTpumBBx7Q5MmTFRQU5PSYAQDuJTIySkZjZ336aZE+/vizWj3Y779/tIzGLoqMjHJ1qA1GgR0AANTp2Wef1d/+9je9/PLLuuKKX19N0LNnT0nS3r17ZTQaFRAQoC+++MJhTnl5uSSd85f0c7HZ1KgLLWdzp1jdAfl0PnLqfOTU+chp4/LJJ5+oX79+8vX1VWRkpB599FG1b99ekrRt2zb5+vrai+uSFBUVJU9PT33xxReKi4s718sCAJoJg8Egk2m20tJSdP/9o5WRMVEDBiRpy5Yi3X//aOXnb5DZnO3WLQwpsAMAAAc2m00zZ87Uxo0blZ2drU6dOp33OSUlJZL+Uzzv1auXMjMzVVFRIX9/f0lSQUGBfHx8dP3111+64AEAgNPExsYqLi5O11xzjfbt26dFixZp7Nixev3112UwGFReXq4OHTo4PKdFixby8/Ord0s4qfH3t3eXfQLcCTl1LvLpfOTUOe64I1mrVmVr+vSpDi1iOnfuolWrspWUlOzC6OpWn+85BXYAAOBgxowZys3N1fPPP6+2bdvaf0Fu166dWrdurdLSUq1bt079+/fXb37zG3311VeaO3eu+vTpo5CQEElnNkC7/vrrNXnyZP3xj39UWVmZFi9erFGjRtXq3QoAABqnxMRE+5+Dg4MVHBysgQMH2le1O5u7tN5xlzjdCTl1LvLpfOT04t133yilpNyjzZs364cfftCVV16p2NhYt165XoMCOwAAcJCTkyNJSklJcTg+d+5cDRs2TC1bttTHH3+sl156SVVVVbryyisVHx+vhx9+2D7XYDAoMzNTJpNJw4cPV5s2bTR06FClp6df1s8CAACcp1OnTmrfvr327t2rfv36KSAgQEeOHHGYc/r0aR09erTeLeGkxr/virvsE+BOyKlzkU/nI6fOd+ONN+t3vzuT059+qnJ1OOdUnz1XKLADAAAHX3311a+OX3nllXr55ZfP+zpXX321Vq5c6aywAACAi/3444/6+eef7cXz3r1769ixY9qxY4e6d+8uSSosLJTValWPHj3q/fru0n/fXeJ0J+TUucin85FT52tKOaXADgAAAAD1YLFYVFRUoKqqo/L29lNERFSTuL0Zzc/x48dVWlpqf7x//36VlJTIz89Pfn5+Wr58uW6//XYFBARo3759eu6559S5c2fFxsZKkrp27arY2Fg9/fTTmjFjhk6dOqWZM2cqMTFRQUFBrvpYAABcVhTYAQAAAOAC5eaulck0VaWle+3HjMbOMplmN8oNuoBfs2PHDt177732x3PnzpUkDR06VCaTSV9//bXeeecdVVZWqmPHjoqOjlZGRobDfioLFizQzJkzlZqaKk9PT8XHx2vatGmX/bMAABq/prpIgQI7AAAAAFyA3Ny1SktLUXz8IL3wglkxMRHasqVIixcvVFpaiszmbIrscCsRERG/2hrObDaf9zV+85vfaOHChc4MCwDQBDXlRQqerg4AAAAAABo7i8Uik2mq4uMHafXqHIWH95WPj4/Cw/tq9eocxccPksk0TRaLxdWhAgAANCo1ixRCQkI1f/4CrVq1SvPnL1BISKjS0lKUm7vW1SFeFFawAwAAAMB5FBYWqLR0rzIzzfL0dFyn5OnpqfT0iUpMjFNhYYGio2NdFCUAAEDjUrNIoWfPXiop2an8/A32sU6djOrZs5dMpmkaPDjRbdvFsIIdAAAAAM7j0KEfJUkhIWF1joeGhjnMAwAAwH8WKXz++XaFhd2gvLxNqqysVF7eJoWF3aDPP9+u0tI9KiwscHWoDUaBHQAAAADOIyjoCknSrl076xwvKdnpMA8AAADSDz8clCQNGDCwzjZ7AwYMdJjnjiiwAwAAAMB5REZGyWjsrCVLFspqtTqMWa1WLV26SEZjF0VGRrkoQgAAgManoqJckpSYeEedbfYGD05ymOeOKLADAAAAwHkYDAaZTLOVn79BqakjVFxcpMrKShUXFyk1dYTy8zfIZJrltr1DAQAALgV//wBJ0vr16+pcpJCXl+swzx1RYAcAAACAC5CUlCyzOVslJTuVkBAnX19fJSTEqaSkRGZztpKSkl0dIgAAQKNy5ZVXSZLef39jnYsU3n9/o8M8d9TC1QEAAAAAgLtISkrW4MGJKioqUFXVUXl7+ykiIoqV6wAAAHWoabPXoUMH7dz5pRIS4uxjnTp1Vq9evXXkyE9u3WaPAjsAAAAA1IPBYFB0dKwCAtqpvLxSNpurIwIAAM5msVi4oO4ENW320tJSNHBgvAYNSpCHh1U2m6f27Plemzbly2zOduvcUmAHAAAAAAAAgH/LzV0rk2mqSkv32o8ZjZ1lMs2mJVwDJCUl6+GH05WZuVwWi8V+3GBooYcfTnf7nNKDHQAAAAAAAAB0prielpai0NAw5eVtUmVlpfLyNik0NExpaSnKzV3r6hDdTm7uWj3//FLddluc5s9foFWrVmn+/AW67baBev75pW6fUw+bzX1vaGzst2N6eIjbRp2MnDofOXU+cupc7pTPmljhXO70vXeHWN0B+XQ+cup85NT53CWnnO8vHXf53jf2ON0JOXUu8nnxLBaLIiJ6KTQ0TKtX58hg8LTn1GKxKjV1hEpKSlRUtM2tW5pcTu6a0/qc71nBDgAAAAAAAKDZKywsUGnpXmVkTJKnp2PZ1NPTU+npE1VaukeFhQUuitD9nJ1Tm82mrVs3KycnR1u3bpbNZmsSOaUHOwAAAAAAAIBm79ChHyVJISFhdY6HhoY5zMP51eRqz549GjcurVZf+yefnOYwzx2xgh0AAAAAAABAsxcUdIUkadeunXWOl5TsdJiH86vJ1fjxY+vsaz9+/AMO89wRBXYAAAAAAAAAzV5kZJSMxs5asmShrFarw5jVatXSpYtkNHZRZGSUiyJ0P336RMhgMCggIFAvvviSTp48qXXr1unkyZN68cWXFBAQKIOhhfr0iXB1qA1GixgAAAAAAAAAzZ7BYJDJNFtpaSlKTR2hjIyJiomJUHFxkZYsWaT8/A0ym7Mb1WacjV1xcZEsFovKyg6rWzejTpw4YR9r06aN/XFxcZGio2NdFeZFocAOAAAAAAAAAJKSkpJlNmfLZJqqhIQ4+3GjsYvM5mwlJSW7MDr3c6G91d25BzsFdgAAAAAAAAD4t6SkZA0enKiiogJVVR2Vt7efIiKiWLneAIGBHSVJERGRWrNmvYqLC+057dMnUsOGJaqoqNA+zx1RYAcAAAAAAACAsxgMBkVHxyogoJ3Kyytls7k6IvdksyfOo1ZOLRarPa82N04wm5wCAAAAAAAAAJyuvLxMkvTJJ4VKTR2h4uIiVVZWqri4yP747HnuiAI7AAAAAAAAAMDpgoKukCQ99dQzKinZqYSEOPn6+iohIU4lJSWaMuVph3nuiBYxAAAAAAAAAACni4yMktHYWZ9++ok+/vizWj3Y779/tIzGLoqMjHJ1qA3GCnYAAAAAAAAAgNMZDAaZTLOVn79B998/Wl5eXkpKSpKXl5fuv3+08vM3yGSa5dYbyLKCHQAAAAAAAABwSSQlJctszpbJNFUJCXH240ZjF5nN2UpKSnZhdBePAjsAAAAA1IPFYlFRUYH99uaIiCi3XnUFAABwqSUlJWvw4MQm+TMUBXYAAAAAuEC5uWtlMk1Vaele+zGjsbNMptluv/oKAADgUjIYDIqOjlVAQDuVl1fKZnN1RM5BD3YAAAAAuAC5uWuVlpai0NAw5eVtUmVlpfLyNik0NExpaSnKzV3r6hABAABwmVFgBwAAAIDzsFgsMpmmKj5+kFavzlF4eF/5+PgoPLyvVq/OUXz8IJlM02SxWFwdKgAAAC4jCuwAAAAAcB6FhQUqLd2rjIxJ8vR0/DXK09NT6ekTVVq6R4WFBS6KEAAAAK5AgR0AAAAAzuPQoR8lSSEhYXWOh4aGOcwDAABA88AmpwAAAABwHkFBV0iSdu3aqfDwvrXGS0p2OswDAADuzWKxqKioQFVVR+Xt7aeIiCgZDAZXh4VGiBXsAAAAAHAekZFRMho7a8mShbJarQ5jVqtVS5cuktHYRZGRUS6KEAAAOEtu7lpFRPTSkCGJGjlypIYMSVRERC82NEedKLADAAAAwHkYDAaZTLOVn79BqakjVFxcpMrKShUXFyk1dYTy8zfIZJrFyjYAANxcbu5apaWlKDQ0THl5m1RZWam8vE0KDQ1TWloKRXbUQoEdAAAAAC5AUlKyzOZslZTsVEJCnHx9fZWQEKeSkhKZzdlKSkp2dYgAAOAiWCwWmUxTFR8/SKtX5yg8vK98fHwUHt5Xq1fnKD5+kEymabJYLK4OFY0IPdgBAAAA4AIlJSVr8OBEerICANAEFRYWqLR0rzIzzfL0dFyX7OnpqfT0iUpMjFNhYYGio2NdFCUaGwrsAAAAAFAPBoNB0dGxCghop/LyStlsro4IAAA4w6FDP0qSQkLC6tzkNDQ0zGEeIFFgBwAAAAAAAAAFBV0hSTKb/6Ls7CyVlu61jxmNnTV69H0O8wCJHuwAAAAAAAAAoMjIKAUEBGr2bJNCQkIdNjkNCQnVnDkzFBAQqMjIKFeHikaEAjsAAAAAAAAA/BebzWb/As6FAjsAAAAAAACAZq+wsEDl5WWaOnW6du0qUUJCnHx9fZWQEKddu3bpqaeeUXl5mQoLC1wdKhoRerADAAAAAAAAaPZqNi9NS3tQEyY8WmuT0xMnqjRnzrNscgoHF7WC/S9/+YuCg4M1e/Zs+7GTJ09qxowZioiIUO/evfXII4+ovLzc4XkHDx7UAw88oJ49e6pfv36aP3++Tp8+fTGhAAAAAAAAAECD1WxeumvXThkMBkVHx2rEiBGKjo6VwWBQSclOh3mAdBEF9i+++EKvvfaagoODHY7PmTNHH374oRYvXqzs7GwdPnxYEyZMsI9bLBY9+OCDOnXqlF577TXNmzdPb7/9tpYuXdrwTwEAAAAAAAAAFyEyMkpGY2ctWbJQVqvVYcxqtWrp0kUyGruwySkcNKjAfvz4cf3xj3/UrFmz5OfnZz9eWVmpt956S08++aT69eun7t27a86cOdq2bZu2b98uSdqyZYt2796t5557TqGhoerfv78yMjL0yiuvqLq62ikfCgAAAAAAAADqw2AwyGSarfz8DUpNHaHi4iJVVlaquLhIqakjlJ+/QSbTLBkMBleHikakQQX2Z599Vv3791dUlOPVmh07dujUqVMOx7t27aqrrrrKXmDfvn27unXrpoCAAPucmJgY/fLLL9q9e3dDwgEAAAAAAACAi5aUlCyzOVslJTsdNjktKSmR2ZytpKRkV4eIRqbem5yuX79eO3fu1JtvvllrrLy8XC1btpSvr6/DcX9/f5WVldnnnF1cl2R/XDPnQnl41Gv6ZVcTX2OP052QU+cjp85HTp3LnfLpDjECAAAAAH5dUlKyBg9OrLXJKSvXUZd6Fdh/+OEHzZ49W6tWrVKrVq0uVUwXzN+/natDuCDuEqc7IafOR06dj5w6F/kEAAAAAFwuNZucBgS0U3l5pWw2V0eExqpeBfYvv/xSFRUVGjZsmP2YxWJRcXGxXnnlFZnNZp06dUrHjh1zWMVeUVGhwMBASWdWq3/xxRcOr1teXi5J9jkXqqKicf/l9vA4UxBq7HG6E3LqfOTU+cipc7lTPmtiBQAAAAC4N4vFwgp2J2uqOa1XgT0yMlLr1q1zODZlyhRdd911Gjt2rK688kq1bNlSH3/8sW6//XZJ0nfffaeDBw+qV69ekqRevXopMzNTFRUV8vf3lyQVFBTIx8dH119/fb2Ct9nU6IstkvvE6U7IqfORU+cjp85FPgEAAJyruLhYZrNZO3bsUFlZmVasWKGBAwdKkk6dOqXFixfr73//u/bt2ycfHx9FRUVp0qRJCgoKsr/GgAEDdODAAYfXnTRpkh544IHL+lkAwJlyc9fKZJqq0tK99mNGY2eZTLPpwd5ATTmn9drk1MfHR926dXP48vb21m9+8xt169ZN7dq101133aV58+apsLBQO3bs0FNPPaXevXvbC+wxMTG6/vrrNXnyZO3atUubN2/W4sWLNWrUKHl5eV2KzwgAAAAAAP5LVVWVgoODNX369Fpj//rXv7Rz50499NBDWrNmjZYvX67vv/9eDz30UK256enp2rJli/1r9OjRlyN8ALgkcnPXKi0tRaGhYcrL26TKykrl5W1SaGiY0tJSlJu71tUhup2mntN6b3J6Pk899ZQ8PT2Vnp6u6upqxcTEOJysDQaDMjMzZTKZNHz4cLVp00ZDhw5Venq6s0MBAAAAAADn0L9/f/Xv37/OsXbt2ikrK8vh2NNPP60//OEPOnjwoK666ir78bZt29a75SsANEYWi0Um01TFxw/S6tU5Mhg85ePjo/Dwvlq9OkepqSNkMk3T4MGJTaK1yeXQHHJ60QX27Oxsh8etWrXS9OnT67wCXuPqq6/WypUrL/atAQAAAADAZfLLL7/Iw8PDYc81SVq5cqX+/Oc/68orr1RSUpLuu+8+tWhR/3KDh4ezIr00auJr7HG6E3LqXOTz4hUVFai0dK9eeMEsg8HTIacGg6cyMiYqISFORUUFio6OdW2wbsJdc1qff0dOX8EOAAAAAACalpMnT2rBggVKTEyUj4+P/XhKSorCwsLk5+enbdu2adGiRSorK9OUKVPq/R7uslm8u8TpTsipc5HPhquqOipJiomJcPi/rianMTER9nkBAeT5Qpyd0zZt2mjz5s364YcfdOWVVyo2NrZJ5JQCOwAAAAAAOKdTp04pIyNDNptNM2bMcBgbM2aM/c8hISFq2bKlpk+frkmTJtV7n7WKispGvam9h8eZIltjj9OdkFPnIp8Xz9vbT5K0ZUuRwsP71sppcXGRfV55eaUrQ3UbNTmdP3+BXnrp/2ptcpqSkmqf15hyWvO9vxAU2AEAAAAAQJ1OnTqlRx99VAcPHtTq1asdVnTWpWfPnjp9+rT279+v6667rl7vZbPJLYqC7hKnOyGnzkU+Gy4iIkpGY2ctXrzQ3i9cOpNPi8WqJUsWyWjsooiIKHJ8gSIiohQQEKhZs2YoLm6Qxo9PV2Bge5WV/aRNmzZq9uxnFRAQ6NY5pcAOAAAAAABqqSmu7927Vy+99JLat29/3ueUlJTI09NT/v7+lyFCAHAug8Egk2m20tJSlJo6QhkZExUTE6Hi4iItWbJI+fkbZDZnu+1mnK62efPftHHjBvvj1q1buywWZ6LADgAAAABAM3T8+HGVlpbaH+/fv18lJSXy8/NTYGCg0tPTtXPnTr3wwguyWCwqKyuTJPn5+cnLy0vbtm3T559/rsjISLVt21bbtm3T3LlzlZycLD8/P1d9LAC4KElJyTKbs/XMM1OUkBBnP96pk1Fmc7aSkpJdGJ37KSwsUHn5mfOHx3/tHFrzuLy8TIWFjWuT0/qgwA4AABy88MILys/P13fffafWrVurd+/eevzxxx1u8z558qTmzZund999V9XV1YqJidH06dMVEBBgn3Pw4EGZTCYVFRXJ29tbQ4YM0aRJk9SiBT9+AADQGOzYsUP33nuv/fHcuXMlSUOHDtWECRP0wQcfSJLuvPNOh+e99NJLioiIkJeXl959910tX75c1dXVuuaaa3Tfffc59GUHAHf0j38U64cfDjocO3jwgP7xj2IK7PVUk8fbbovTSy+9puLiQlVVHZW3t5/69InUvffeo/ff31gr3+6E33ABAICDTz75RKNGjdKNN94oi8WiRYsWKS0tTevXr5e3t7ckac6cOfroo4+0ePFitWvXTjNnztSECRP02muvSZIsFosefPBBBQQE6LXXXtPhw4f1xBNPqGXLlpo4caIrPx4AAPi3iIgIffXVV+cc/7UxSbrhhhv0xhtvODssAHCpGTOe1ooVSxQY2FFTpkzTiBF/UE7OXzV37iytWLFEkjR9+kwXR+k+KirKJUmJiXeoZcuWio6OVUBAO5WXn9k4dvDgJL3//kb7PHfk6eoAAABA42I2mzVs2DD99re/VUhIiObNm6eDBw/qyy+/lCRVVlbqrbfe0pNPPql+/fqpe/fumjNnjrZt26bt27dLkrZs2aLdu3frueeeU2hoqPr376+MjAy98sorqq6uduGnAwAAAIC6VVdXKzNzuQIDO+rzz3cpJeU+XXHFFUpJuU+ff75LgYEdlZm5gt9p6sHf/8xdzuvXr5PVanUYs1qtysvLdZjnjiiwAwCAX1VZWSlJ9l6qO3bs0KlTpxQVFWWf07VrV1111VX2Avv27dvVrVs3h5YxMTEx+uWXX7R79+56vb+Hh3t8uVOs7vBFPsmpO3yR0+abUwBA05SVtVIWi0VTpkyr1dqyRYsWeuKJqbJYTisra6WLInQ/V155lSTpgw82KTV1hIqLi1RZWani4iKlpo7QBx9scpjnjmgRAwAAzslqtWrOnDm66aab1K1bN0lSeXm5WrZsKV9fX4e5/v7+9s3PysvLHYrrkuyPa+ZcKH//dg0N/7Jzp1jdAfl0PnLqfOTU+cgpAMBV9uz5XpIUFze4zvH4+EEO83B+kZFRMho7q0OHDtq580uHjWONxs7q2bOXjhz5SZGRUb/yKo0bBXYAAHBOM2bM0DfffKNXX33VZTFUVJzpzdeYeXicKQi5Q6zugHw6Hzl1PnLqfO6S05o4AQBNT5cu10qSNm7M0+jR99Uaz8/f4DAP52cwGGQyzVZaWori4m7X+PHpCgxsr7Kyn/TBB5u0ceN7MpuzZTAYXB1qg1FgBwAAdXr22Wf1t7/9TS+//LKuuOIK+/GAgACdOnVKx44dc1jFXlFRocDAQPucL774wuH1ysvPbFpTM+dC2Wxq1IWWs7lTrO6AfDofOXU+cup85BQA4CpjxoyVyTRNc+fO0j33jFbLlv8pnZ4+fVrz58+WwdBCY8aMdWGU7icpKVlmc7ZMpqn2ixSSZDR2kdmcraSkZBdGd/EosAMAAAc2m00zZ87Uxo0blZ2drU6dOjmMd+/eXS1bttTHH3+s22+/XZL03Xff6eDBg+rVq5ckqVevXsrMzFRFRYX8/f0lSQUFBfLx8dH1119/WT8PAAAAAFwILy8vjRs3QStWLFHPnsH6/e+H64YbQvTll7v05puvq6ysTOPHZ8jLy8vVobqdpKRkDR6cqKKiAlVVHZW3t58iIqLceuV6DQrsAADAwYwZM5Sbm6vnn39ebdu2tfdMb9eunVq3bq127drprrvu0rx58+Tn5ycfHx/NmjVLvXv3thfYY2JidP3112vy5Mn64x//qLKyMi1evFijRo3ih1EAAAAAjdb06TP17be7tWHDev35z8sdxgYNStT06TNdFJn7MxgMio6OVUBAO5WXN+6WcPVBgR0AADjIycmRJKWkpDgcnzt3roYNGyZJeuqpp+Tp6an09HRVV1crJiZG06dPt881GAzKzMyUyWTS8OHD1aZNGw0dOlTp6emX74MAAAAAQD3l5q7Ve++9q7i423XttddJskgy6Pvvv9N7772r3Ny1bt/SBM7lYbO577WCxn6lw8NDTe6KjKuRU+cjp85HTp3LnfJZEyucy52+9+4Qqzsgn85HTp2PnDqfu+SU8/2l4y7f+8Yepzshp85FPi+exWJRREQvhYaGafXqHBkMnvacWixWpaaOUElJiYqKtjWJ1iau4C5/T+tzvve8xLEAAAAAAAAAQKNXWFig0tK9ysiYJE9Px7LpmTt4J6q0dI8KCwtcFCEaIwrsAAAAAAAAAJq9Q4d+lCSFhITVOR4aGuYwD5AosAMAAAAAAACAgoKukCTt2rWzzvGSkp0O8wCJAjsAAAAAAAAAKDIySkZjZy1ZslBWq9VhzGq1aunSRTIauygyMspFEaIxosAOAAAAAAAAoNkzGAwymWYrP3+DUlNHqLi4SJWVlSouLlJq6gjl52+QyTSLDU7hoIWrAwAAAAAAAACAxiApKVlmc7ZMpqlKSIizHzcau8hszlZSUrILo0NjRIEdAAAAAAAAAP4tKSlZgwcnqqioQFVVR+Xt7aeIiChWrqNOFNgBAAAAAAAA4CwGg0HR0bEKCGin8vJK2WyujgiNFT3YAQAAAAAAAABoAArsAAAAAAAAAAA0AC1iAAAAAAAAAOAsFouFHuy4IBTYAQAAAAAAAODfcnPXymSaqtLSvfZjRmNnmUyzlZSU7MLI0BhRYAcAAAAAAAAAnSmup6WlKC7udo0fn67AwPYqK/tJ77+/UWlpKTKbsymywwEFdgAAAAAAAADNnsVikck0VT179lJJyU7l52+wj3XqZFTPnr1kMk3T4MGJtIuBHZucXiIWi0Vbt25WTk6Otm7dLIvF4uqQAAAAAAAAAJxDYWGBSkv3avv2bQoLu0F5eZtUWVmpvLxNCgu7Qdu3b1Np6R4VFha4OlQ0IhTYL4Hc3LWKiOilIUMSNXLkSA0ZkqiIiF7KzV3r6tAAAAAAAAAA1OGHHw5Kkm67LU6rV+coPLyvfHx8FB7eV6tX5+i22+Ic5gESBXanq+nTFBoa5nCVKzQ0TGlpKRTZAQAAAAAAgEaooqJckpSYeIc8PR3Lpp6enho8OMlhHiBRYHeqmj5N8fGD6rzKFR8/SCbTNNrFAAAAAAAAAI2Mv3+AJGn9+nWyWq0OY1arVXl5uQ7zUD9NtaU2BXYnqunTlJExqc6rXOnpE+nTBAAAAAAAADRCV155lSTpgw82KTV1hIqLi1RZWani4iKlpo7QBx9scpiHC9eUW2pTYHeiQ4d+lCSFhITVOR4aGuYwDwAAAAAAAEDjEBkZJaOxs3r27KWdO79UQkKcfH19lZAQp5KSnerZs5eMxi6KjIxydahupam31KbA7kRBQVdIknbt2lnneEnJTod5AAAAAAAAABoHg8Egk2m2Pv98u0JDwzRv3gKZzWbNm7dAISGh+vzz7TKZZslgMLg6VLfRHFpqt3B1AE1JzVWuJUsWavXqHBkM/7l+YbVatXTpIq5yAQAAAAAAAI1UUlKyzOZsmUxTlZ+/wX7caOwiszlbSUnJLozO/dS01M7MNJ+zpXZiYpwKCwsUHR3roigvDgV2J6q5ypWWlqLU1BHKyJiomJgIFRcXacmSRcrP3yCzOZurXAAAAAAAAEAjlZSUrMGDE1VUVKCqqqPy9vZTREQUNb0GaA4ttSmwO9nZV7kSEuLsx7nKBQAAAAAAALgHg8Gg6OhYBQS0U3l5pWw2V0fkns5uqR0e3rfWeFNoqU2B/RLgKhcAAAAAAMC5WSwW6iZAM9AcWmpTYL9EuMoFAAAAAABQW27uWplMU1Vautd+zGjsLJNpNnf+A01Mc2ip7Xn+KQAAAAAAAMDFy81dq7S0FIWGhikvb5MqKyuVl7dJoaFhSktLUW7uWleHCMDJalpql5TsVEJCnHx9fZWQEKeSkpIm0VKbFewAAAAAAAC45CwWi0ymqYqPH2RvFeHj46Pw8L5avTpHqakjZDJN0+DBiW69mhVNA22MnKspt9SmwA4AAAAAAIBLrrCwQKWle5WZaZanp2NTBU9PT6WnT1RiYpwKCwsUHR3roigB2hhdKk21pTYtYgAAAAAAAHDJHTr0oyQpJCSszvHQ0DCHeYAr0MYI9UWBHQAAAAAAAJdcUNAVkqRdu3bWOV5SstNhHnC5/Xcbo/Dwvg5tjOLjB8lkmiaLxeLqUNGIUGAHAAAAAADAJRcZGSWjsbOWLFkoq9XqMGa1WrV06SIZjV0UGRnlogjR3NW0McrImHTONkalpXtUWFjgogjRGFFgBwAAAAAAwCVnMBhkMs1Wfv4GpaaOUHFxkSorK1VcXKTU1BHKz98gk2lWk9j0EO6JNkZoCDY5vUTYaRgAAAAAAMBRUlKyzOZsmUxTlZAQZz9uNHaR2ZzNBpJwqbPbGIWH9601Thsj1IUC+yXATsMAAAAAAAB1S0pK1uDBiSxMRKNzdhuj1atzZDD8p/kHbYxwLrSIcTJ2GgYAAAAAAPh1BoNB0dGxGjFihKKjYymuo1GgjREaggK7E7HTMAAAAAAAAOC+atoYlZTsVEJCnHx9fZWQEKeSkhLaGKFOtIhxopqdhjMzzefcaTgxMU6FhQWKjo51UZQAAAAAAAAAzoU2RqgPCuxOxE7DAAAAAAAAgPuraWMUENBO5eWVstlcHREaK1rEONHZOw3XhZ2GAQAAAACNRXFxscaNG6eYmBgFBwdr06ZNDuM2m01LlixRTEyMevToofvuu0979uxxmPPzzz9r0qRJuummmxQeHq6nnnpKx48fv4yfAgAA16LA7kRn7zRstVodxthpGAAAAADQmFRVVSk4OFjTp0+vc3zlypXKzs6WyWTSG2+8oTZt2igtLU0nT560z3n88ce1e/duZWVlKTMzU59++qmeeeaZy/URAABwOVrEOFHNTsNpaSlKTR2hjIyJiomJUHFxkZYsWaT8/A0ym7Pp1wQAAAAAcLn+/furf//+dY7ZbDa99NJLeuihhzRw4EBJ0p/+9CdFRUVp06ZNSkxM1LfffqvNmzfrzTff1I033ihJmjZtmh544AFNnjxZQUFBl+2zAM2dxWKhX7iTkVNcKArsTlaz07DJNFUJCXH240ZjF3YaBgAAAAC4hf3796usrExRUf+5A7tdu3bq2bOntm3bpsTERG3btk2+vr724rokRUVFydPTU1988YXi4uLqeulz8vBwWviXRE18jT1Od0JOnSM3d62mT5+q0tK99mNGY2fNmDGbOlQDkdNLx13+3dcnPgrslwA7DQMAAAAA3FlZWZkkyd/f3+G4v7+/ysvLJUnl5eXq0KGDw3iLFi3k5+dnf359+Pu3a2C0l5e7xOlOyGnDrVmzRvffn6KkpCS9/vpr6t69u3bs2KE5c+bo/vtT9Oabb2rYsGGuDtOtkNPLoyn9u6fAfomw0zAAAAAAABeuoqJx/+7s4XGmINTY43Qn5PTiWCwWPfbYRMXHD9KLL2bLYPCUj4+PfvvbG/Tii9m6994RmjhxkqKjB7Do8wKR00vPXf7d18R5ISiwXyL0aQIAAAAAuKvAwEBJUkVFhTp27Gg/XlFRoZCQEElSQECAjhw54vC806dP6+jRo/bn14fNpkZdbKnhLnG6E3LaMB9/XKDS0r3KzDTLw8PTnkObTfLw8FR6+kQlJsbp448LFB0d69pg3QQ5vXya0r97T1cH0BTl5q5VREQvDRmSqJEjR2rIkERFRPRSbu5aV4cGAAAAAMB5XXPNNQoMDNTHH39sP/bLL7/o888/V+/evSVJvXv31rFjx7Rjxw77nMLCQlmtVvXo0eOyxww0N4cO/ShJCgkJq3M8NDTMYR7O7+ycWiwWbd26WTk5Odq6dbMsFgs5RZ0osDtZbu5apaWlKDQ0THl5m1RZWam8vE0KDQ1TWloKRXYAAAAAQKNw/PhxlZSUqKSkRNKZjU1LSkp08OBBeXh46N5779Wf//xnvf/++/rqq680efJkdezYUQMHDpQkde3aVbGxsXr66af1xRdf6B//+IdmzpypxMREBQUFufKjAc1CUNAVkqRdu3bWOV5SstNhHs6vJldm81/qXDz74ot/cZgHSJKHzea+i/EbW29zi8WiiIheCg0N0+rVOTIYPO092C0Wq1JTR6ikpERFRdtoF9NAHh6ir72TkVPnI6fO5U75rIkVzuVO33t3iNUdkE/nI6fOR06dz11y2pTO90VFRbr33ntrHR86dKjmzZsnm82mpUuX6o033tCxY8d08803a/r06br22mvtc3/++WfNnDlTH3zwgTw9PRUfH69p06apbdu29Y7HXb73jT1Od0JOLw51KOezWCy68cZuKi8vU3z8ID322OOKiYnQli1F+t//XaD8/A0KCAjUP//5NTltIHf5d1+f8329erC/+uqrysnJ0YEDByRJv/3tb/Xwww+rf//+kqSTJ09q3rx5evfdd1VdXa2YmBhNnz5dAQEB9tc4ePCgTCaTioqK5O3trSFDhmjSpElq0cL928EXFv6nT5Onp+PNAZ6e/+nTVFhInyYAAAAAgGtFREToq6++Oue4h4eHMjIylJGRcc45v/nNb7Rw4cJLER6A8zAYDDKZZistLUWpqSOUkTFRMTERKi4u0pIli5Sfv0FmczaF4Itgs9nsX8C51KtFzBVXXKHHH39ca9as0VtvvaXIyEiNHz9e33zzjSRpzpw5+vDDD7V48WJlZ2fr8OHDmjBhgv35FotFDz74oE6dOqXXXntN8+bN09tvv62lS5c691O5CL2vAAAAAAAAcLkkJSXLbM5WSclOJSTEydfXVwkJcSopKZHZnK2kpGRXh+hWCgsLVF5epqlTp9fK6a5dJXrqqWdUXl6mwsICV4eKRqRey8YHDBjg8Pixxx5TTk6Otm/friuuuEJvvfWWFixYoH79+kk6U3BPSEjQ9u3b1atXL23ZskW7d+9WVlaWAgICFBoaqoyMDC1YsEATJkyQl5eX8z6ZC5zd+yo8vG+tcXpfAQAAAAAAwJmSkpI1eHCiiooKVFV1VN7efoqIiGLlegPULIq9+upOtcZsNpuuuaaTwzxAuohNTi0Wi9avX6+qqir17t1bO3bs0KlTpxQVFWWf07VrV1111VXavn27JGn79u3q1q2bQ8uYmJgY/fLLL9q9e3fDP0UjERkZJaOxs5YsWSir1eowZrVatXTpIhmNXRQZGXWOVwAAAAAAAADqx2AwKDo6ViNGjFB0dCzF9QaqWRQ7fvxYhYXdoLy8TaqsrFRe3iaFhd2g8eMfcJgHSPVcwS5JX331le655x6dPHlS3t7eWrFiha6//nqVlJSoZcuW8vX1dZjv7++vsrIySVJ5eblDcV2S/XHNnPrw8Kj3Uy6pFi0MmjFjtu6//0zvq0cfnajo6Ah9+mmRFi8+0/tq1apstWjBf3INVfM9b2zfe3dGTp2PnDqXO+XTHWIEAAAA0PRUV1crK2ulDh06oKCgqzVmzFi37xThCn36RMhgMKhDB39lZb2ili1byMfHR+HhfZWV9Yp69gzRkSNH1KdPhKtDRSNS7wL7tddeq3feeUeVlZV677339MQTT+jll1++FLGdl79/49u5/b77RsnXt40mTZqkwYPj7MevvfZavfnmmxo2bJgLo2s6GuP33t2RU+cjp85FPgEAAACgthkznlZm5nJZLBb7MZNpmsaNm6Dp02e6MDL3U1xcJIvForKywxozZlStjWPLyg7b50VHx7o4WjQW9S6we3l5qXPnzpKk7t2765///KdeeuklDR48WKdOndKxY8ccVrFXVFQoMDBQ0pnV6l988YXD65WXl0uSfU59VFRUqjFu4nvLLXHauvVTrVr1nyuH999/5spheXmlq8Nzax4eZ4psjfV7747IqfORU+dyp3zWxAoAAAAAl8OMGU9rxYolCgzsqClTpmnEiD8oJ+evmjt3llasWCJJFNnroaa3+vPPv6h582YqIeE/i2eNxi5asWKlxo8fSw/2BrJYLE1yr4B6F9j/m9VqVXV1tbp3766WLVvq448/1u233y5J+u6773Tw4EH16tVLktSrVy9lZmaqoqJC/v7+kqSCggL5+Pjo+uuvr/d722xqlMWW3Ny1MpmmqrR0r/3YypWZMplms3uzkzTW7707I6fOR06di3wCAAAAwH9UV1crM3O5AgM76vPPd6llyxYKCGinlJT7dM89o9WzZ4gyM1doypSnaRdzgWp6q3fp0kVFRdtrFYM/++xTh3m4cHXVS43Gzk2iXlqvTU4XLlyo4uJi7d+/X1999ZUWLlyoTz75RHfccYfatWunu+66S/PmzVNhYaF27Nihp556Sr1797YX2GNiYnT99ddr8uTJ2rVrlzZv3qzFixdr1KhRTeYfem7uWqWlpSg0NMxhI4TQ0DClpaUoN3etq0MEAAAAAACAm8vKWimLxaIpU6apRQvHNbQtWrTQE09MlcVyWllZK10UofuJjIyS0dhZS5YslNVqdRizWq1aunSRjMYuioyMclGE7qmp10vrtYK9oqJCTzzxhA4fPqx27dopODhYZrNZ0dHRkqSnnnpKnp6eSk9PV3V1tWJiYjR9+nT78w0GgzIzM2UymTR8+HC1adNGQ4cOVXp6unM/lYtYLBaZTFMVHz9Iq1fnyGDwtG+EsHp1jlJTR8hkmqbBgxObxO0PAAAAAAAAcI09e76XJMXFDa5zPD5+kMM8nJ/BYJDJNFv33z9a119/jU6cOGEfa9OmjU6cOKFVq16mrlcPzaFeWq8C+5w5c351vFWrVpo+fbpDUf2/XX311Vq5smleOSssLFBp6V5lZprl6el4c8CZCw8TlZgYp8LCAjZCAAAAAAAAQIN16XKtJGnjxjyNHn1frfH8/A0O83DhPDw86nUc59Yc6qX1ahGDX1ezwUFISFid46GhYQ7zAAAAAAAAgIYYM2asDAaD5s6dpdOnTzuMnT59WvPnz5bB0EJjxox1UYTu5+zV1rt379c776zXq6++qnfeWa/du/crPn6QTKZpslgsrg7VbZxdL7VYLNq6dbNycnK0detmWSyWJlEvpcDuRDUbHOzatbPO8ZKSnQ7zAABorIqLizVu3DjFxMQoODhYmzZtchh/8sknFRwc7PCVlpbmMOfnn3/WpEmTdNNNNyk8PFxPPfWUjh8/fjk/BgAAANBkeXl5ady4CSorO6yePUP00ktZOnjwoF56KUs9e4aorOywxo0b32T2PbwcalZbZ2RMUsuWLRUdHasRI0YoOjpWLVu2VHr6RJWW7lFhYYGrQ3UbNXVQs/kviojopSFDEjVy5EgNGZKoiIheevHFvzjMc0cU2J2IjRAAAE1FVVWVgoODf7XtW2xsrLZs2WL/WrRokcP4448/rt27dysrK0uZmZn69NNP9cwzz1zq0AEAAIBmY/r0mRo/PkNHjlRo0qQMXX311Zo0KUNHjhzR+PEZmj59pqtDdCt0p3C+yMgoBQQEavZsk0JCQh02OQ0JCdWcOTMUEBDo1vXSevVgx6+r2QghLS1FqakjlJExUTExESouLtKSJYuUn79BZnO22zbsBwA0H/3791f//v1/dY6Xl5cCAwPrHPv222+1efNmvfnmm7rxxhslSdOmTdMDDzygyZMnKygoyOkxAwAAAM3R9OkzNWXK08rKWqlDhw4oKOhqjRkzlpXrDXB2d4rw8L61xulOcfFsNpv9q6mgwO5kSUnJMpuzZTJNVUJCnP240dhFZnO2kpKSXRgdAADO88knn6hfv37y9fVVZGSkHn30UbVv316StG3bNvn6+tqL65IUFRUlT09PffHFF4qLizvXy9biDvsI1cToDrG6A/LpfOTU+cip87lLTht7fACapzPtYsYrIKCdyssr1YRql5fV2d0pVq/OkcHwn+YfdKdomMLCApWXl2nq1OnKzv6/WvXSp556RnPmPOvWm5xSYL8EkpKSNXhwooqKClRVdVTe3n6KiIhi5ToAoMmIjY1VXFycrrnmGu3bt0+LFi3S2LFj9frrr8tgMKi8vFwdOnRweE6LFi3k5+ensrKyer2Xv387Z4Z+SblTrO6AfDofOXU+cup85BQA4Cp0p3C+mnY6aWkP6oEHHtaMGdO0f/9eXXNNZ02fPksWy2nNmfOsW7fdocB+iRgMBkVHx3LlEADQJCUmJtr/XLPJ6cCBA+2r2p2poqLxn0c9PM4UhNwhVndAPp2PnDofOXU+d8lpTZwAgKaJ7hTOVdNO549/fFTvvPOWLBaLfWz16lW68867HOa5IwrsAADgonXq1Ent27fX3r171a9fPwUEBOjIkSMOc06fPq2jR4+es2/7udhsatSFlrO5U6zugHw6Hzl1PnLqfOQUAOBqdKdwnsjIKLVr56u33npDAQGBuvvuEerePUQ7duzSG2/kaM2aN9Suna9bt92hwA4AAC7ajz/+qJ9//tlePO/du7eOHTumHTt2qHv37pKkwsJCWa1W9ejRw5WhAgAAAMB50Z3COSwWi44f/0WSVFlZqeefX2ofa9WqtSTp+PFfZLFY3PYCBgV2AABQy/Hjx1VaWmp/vH//fpWUlMjPz09+fn5avny5br/9dgUEBGjfvn167rnn1LlzZ8XGntmUpmvXroqNjdXTTz+tGTNm6NSpU5o5c6YSExMVFBTkqo8FAACARsJisbA6GGgGsrJWymq1SpI8/muncE/PM4+tVquyslbqwQfHX/b4nIECOwAAqGXHjh2699577Y/nzp0rSRo6dKhMJpO+/vprvfPOO6qsrFTHjh0VHR2tjIwMeXl52Z+zYMECzZw5U6mpqfL09FR8fLymTZt22T8LAAAAGpfc3LUymaaqtHSv/ZjR2Fkm02z6WwNNzHfffStJCgu7QceOHdP+/fvsY/7+AfL19dXOnV/a57kjCuwAAKCWiIgIffXVV+ccN5vN532N3/zmN1q4cKEzwwIAAICby81dq7S0FMXHD9ILL5gVExOhLVuKtHjxQqWlpbCJJNDE1Kxa37nzS91++2CtXJnl8O/+vffyHOa5I09XBwAAAAAAAICmz2KxyGSaqvj4QVq9Okfh4X3l4+Oj8PC+Wr06R/Hxg2QyTZPFYnF1qG7HYrFo69bNysnJ0datm8khGo1evW6SJHl5eWn58r/or399XXfddZf++tfXtXz5X+x3QdfMc0esYAcAAAAAuFR1dbWyslbq0KEDCgq6WmPGjHVoOwagaSgsLFBp6V5lZprl6em45tPT01Pp6ROVmBinwsICRUfHuihK90PLHTRmR4/+LOnMuf63v+3kMLZq1cpa89wRK9gBAAAAAC4zY8bT6tw5SE8/PUXLly/X009PUefOQZox42lXhwbAyQ4d+lGSFBISVud4aGiYwzycX03LndDQMOXlbVJlZaXy8jYpNDRMaWkpys1d6+oQ3RZ3BTiHv3+AU+c1RqxgBwAAAAC4xIwZT2vFiiUKDOyoKVOmacSIPygn56+aO3eWVqxYIkmaPn2mi6ME4CxBQVdIknbt2qnw8L61xktKdjrMw6/775Y7BoOnQ8ud1NQRMpmmafDgRBkMBleH61a4K8B52rdvb//zgAED1bXr9fLwsMpm89S33+7WBx9sqjXP3bCCHQAAAABw2VVXVyszc7kCAzvq8893KSXlPl1xxRVKSblPn3++S4GBHZWZuULV1dWuDhWAk0RGRslo7KwlSxbKarU6jFmtVi1dukhGYxdFRka5KEL3UtNyJyNj0jlb7pSW7lFhYYGLInRP3BXgXO+9t0GSFBjYUd9887VWrszUX/7yF61cmandu79RYGBHh3nuiAI7AAAAAOCyy8paKYvFoilTpqlFC8ebq1u0aKEnnpgqi+W0srJWnuMVALgbg8Egk2m28vM3KDV1hIqLi1RZWani4iKlpo5Qfv4GmUyzWG19gWi543xsxOt8e/Z8J0kqKzus0NAwzZu3QGazWfPmLVBISKjKyg47zHNHFNgBAAAAAJfdnj3fS5Li4gbXOR4fP8hhHoCmISkpWWZztkpKdiohIU6+vr5KSIhTSUmJzOZs2m/Uw9ktd+pCy536464A57vuuq6SpISEO7RrV4mefPJxpaWl6cknH9euXbs0eHCSwzx3RA92AAAAAMBl16XLtZKkjRvzNHr0fbXG8/M3OMwD0HQkJSVr8OBEFRUVqKrqqLy9/RQREcXK9Xo6u+VOTQ/2GrTcaRjuCnC+6dNnadWqldq06T3t2rVHr7yyWocOHVBQ0NUaNSpVISFd7PPcFSvYAQAAAACX3ZgxY2UwGDR37iydPn3aYez06dOaP3+2DIYWGjNmrIsiBHApGQwGRUfHasSIEYqOjqW43gC03HE+7gpwvjZt2mjQoERVV1fruuuu0tNPT9Hy5cv19NNTdN11V6m6ulqDBiWqTZs2rg61wSiwAwAAAAAuOy8vL40bN0FlZYfVs2eIXnopSwcPHtRLL2WpZ88QlZUd1rhx4+Xl5eXqUAGg0aLljnOdfVfAqVOntHXrZuXk5Gjr1s06deoUdwU00N13j7io8cbOw2az2VwdREOVl1eqMUfv4SEFBLRr9HG6E3LqfOTU+cipc7lTPmtihXO50/feHWJ1B+TT+cip85FT55kx42llZi532DDOYGihcePGa/r0mS6MrG6c7y+dxv7viX/3zkdOncdisdByx0lyc9cqLS1FrVu31okTJ+zH27Rpo3/9619cuKgni8WiiIhe8vT01L59pf91vjeoUyejrFabioq2Naq/s/U539ODHQAAAADgMtOnz9SUKU8rK2ulvSfrmDFjWbkOAPVQ03KHCxbOca71yG68TtllajaOlaTAwI6aMmWaRoz4g3Jy/qq5c2fZNzMvLCxQdHSsK0NtMArsAAAAAACXOtMuZjyFIQBoIFawO4fFYpHJNFW33z5YK1eu1urVZvvF39TUNI0dmyqTaZoGD04kvxfowIH9kqSAgAB9/vkutWzZQgEB7ZSScp/uuWe0evTopvLycvs8d0SBHQAAAADgUhSGAKDhcnPXymSaal8lLElGY2eZTLNpZVJPNautU1LuU0xMH4ecrlyZqdGjU/Xee3luvdr6cvvss08lSSNHpqhFC8dSdIsWLXTPPaO1fPliffbZp27bi51NTgEAAAAALpObu1Z9+/bUkCGJGjlypIYMSVTfvj2Vm7vW1aEBQKNX0y88NDRMeXmbVFlZqby8TQoNDVNaWgr/l9bToUM/SpLmzHm2zpzOnTvTYR7Or6atzuefb9e//vUvZWau0COPPKLMzBX617/+pX/+83OHee6IFewAAAAAAJfIzV2r++8frTZt2jgcLy8v0/33j9aqVS+z+hIAzqGmnUl8/CCtXp0jg8FTPj4+Cg/vq9Wrc5SaOoJ2JvUUEBAoSerbN7LOnCYnD9InnxTa5+H8rruuqyTpo48+VOfOQQ6F9Geeecr+uGaeO2IFOwAAAADgsrNYLJo8+TFJUmxsf4dVgrGx/SVJkyc/JovF4sowAaDRqmlnkpExSZ6ejiU+T09PpadPVGnpHhUWFrgoQvfj4eHx7z/VvZq6Zvg/83A+Y8aMtefrv1ep1zz28PDQmDFjL3tszkKBHQAAAABw2W3dulnl5WWKiOinl156TeHhfe2rBF966TX17Rup8vIybd262dWhAkCjVNOmJCQkrM7x0NAwh3k4v7Kyw5KkoqJCpaaOUHFxkSorK1VcXKTU1BEqKip0mAdIFNgBAAAAAC5QUzifPPmpOldeTp48xWEeAMBRUNAVkqRdu3bWOV5SstNhHs6vJldTp5pUUrJTCQlx8vX1VUJCnEpKSvTUU9Md5uH8srJWnre/us1mU1bWyssUkfPRgx0AAAAAcNmd7+76mt/FuQsfAOoWGRklo7GzlixZaO8XXsNqtWrp0kUyGrsoMjLKhVG6l5qcfvppkT7++DMVFxeqquqovL391KdPpO6/fzQ5raevv/7a/ufdu/crJydbhw4dUFDQ1RoxIkXXX39NrXnuhhXsAAAAAIDLLjr6FknSn/40W1ar1WHMarVqwYK5DvMAAI4MBoNMptnKz99QZzuT/PwNMplmscFpPZyd0/vvHy0vLy8lJSXJy8tL998/mpw2QHHxmbY6N9/cR76+vho3bryWLVumcePGy9fXV7173+wwzx1RYAcAAAAAXHZRUTEKCAhUUVGh7r33HofC0L333qOiokIFBAQqKirG1aECQKOVlJQsszlbO3d++V/tTHbKbM5WUlKyq0N0OzU5ratFDDltuO+++1ZVVVXKzFyhRx55RJmZK1RVVaU9e75zdWgXjRYxAAAAAIDLzmAw6E9/+l+lpaVo8+aPlJ+/wT7Wpk0beXh46E9/+l9WCQJAA5yv5zV+XVJSsgYPTlRRUYG9RUxERBTnpAa49trrtGtXiX766Yi6dHHsXf/001Mc5rkrVrADAAAAAFyiZpWgv3+Aw/GAgEBWCQLABcjNXau0tBSFhd2gvLxNqqysVF7eJoWF3aC0tBTl5q51dYhuy2AwKDo6ViNGjFB0dCzF9QZaseLCNi+90HmNEQV2AAAAAIBLebCTKQDUm8Vikck0VfHxg7R6dY7Cw/vKx8dH4eF9tXp1juLjB8lkmiaLxeLqUNGMeXl5OTy+7rquioyM1HXXdf3Vee6EAjsAAAAAwCVYeQkADVdYWKDS0r3KyJgkT0/HEp+np6fS0yeqtHSPCgsLXBQhIGVlnVmZXnO32nfffavCwkJ99923Dsdr5rkjerADAAAAAC67/155aTB4Oqy8TE0dIZNpmgYPTuS2fACow6FDP0qSQkLC6hwPDQ1zmIf6sVgs9GB3gj17vpckVVSU69Zbb9N3332rY8eOytfXT9dd11Uffvi+wzx3RIEdAAAAAHDZ1ay8zMw0y2azaevWzQ5FjPT0iUpMjFNhYYGio2NdHW6zNGDAAB04cKDW8ZEjR2r69OlKSUnRJ5984jA2fPhwPfvss5crRKBZCwo6s2Hkrl07FR7et9Z4SclOh3m4cLm5a2UyTVVp6V77MaOxs0ym2ewPUk+dOnWWdGal+kcffSir1SpJ+umnn7RvX6k6dAjQkSPl9nnuiAI7AAAAAOCyq1lRuWfPHo0bl1ariPHEE9Mc5uHye/PNNx16N3/zzTcaM2aMBg0aZD929913Kz093f64TZs2lzVGoDmLjIyS0dhZS5YstN8JVMNqtWrp0kUyGrsoMjLKhVG6n5r2ZfHxg/TCC2bFxERoy5YiLV68UGlpKWzCXU9hYTdIOrOC3d8/QMOHj1D37qHasaNEr7+eo4qKcod57oge7AAAAACAy65mReXDD/+PQkPDHHqwh4aGafz4sQ7zcPl16NBBgYGB9q8PP/xQRqNRffv+Z6Vs69atHeb4+Pi4MGKgeTEYDDKZZis/f4NSU0eouLhIlZWVKi4uUmrqCOXnb5DJNIu2JvXAxrHOd/jwIfuff/rpiJ5/fpkefvhhPf/8Mv3005E657kbVrADAAAAAC67Pn0iZDAY1KGDv7KyXlHLli3sRYysrFfUs2eIjhw5oj59IlwdKiRVV1dr7dq1GjNmjDw8POzH161bp7Vr1yowMFC33nqrHn744QavYj/rZRulmvgae5zuhJxevDvuSNaqVdmaPn2qEhLi7Mc7d+6iVatYaV1fRUVn2pe98IJZBoOnw99Rg8FTGRkTlZAQp6Ii2pddqM8++9T+Z5vN5jB29uPPPvtUw4ePuGxxnU99/l+iwA4AAAAAuOyKi4tksVhUXl6mMWNGKSNjomJiIlRcXKQlSxapvLxMNptNxcVFFDEagU2bztxhMHToUPuxpKQkXXXVVerYsaO++uorLViwQN9//72WL1/eoPfw92/nrHAvKXeJ052Q04tz332jlJJyjzZv3qwffvhBV155pWJjY1m53gBVVUclSTExEQ535NT8HY2JibDPCwjg7+2F8PI68/cwICDA3mrs22+/VdeuXZWVlaXf/va3Ki8vl5eXwW1zSoEdAAAAAHDZ1fRWX7FipebNm+mw8tJo7KIVK/6ihx8eSw/2RuKtt97SLbfcoqCgIPux4cOH2/8cHByswMBA3XfffSotLZXRaKz3e1RUVOq/Fjc2Kh4eZ4psjT1Od0JOnevGG2/W7353Jp8//VTl6nDckre3nyRpy5YihYf3rfV3tLi4yD6vvLzSlaG6jerqM+10ysvL1b59e/vxf/7zn3rnnXcc5jWmnNZ87y8EBXa4DYvFoqKiAlVVHZW3t58iIqK4GgsAAAC4qZre6l26dFFR0fZaP+vX3FJOD3bXO3DggAoKCrRs2bJfndezZ09J0t69extUYLfZ5BZFVneJ052QU+cinw0XEXFm49jFix03jrXZJIvFqiVLzmwcGxERRY4v0E03hWvVqpWSJE9PT1mtVvvY2Y9vuincbXPKJqdwC7m5axUR0UtDhiRq5MiRGjIkURERvZSbu9bVoQEAAABogMjIM0WMJUsWysPDQ9HRsRoxYoSio2Pl4eGhpUvPFDEiI6NcHWqzt2bNGvn7++t3v/vdr84rKSmRJAUGBl6GqADA+dg41vk6dvzPnU/t23fQQw9N0IoVK/TQQxPUvn2HOue5GwrsaPRyc9cqLS1FoaFhyss70/cvL2+TQkPDlJaWQpEdAAAAcEMUMdyD1WrVmjVrNGTIELVo8Z+b4EtLS7VixQrt2LFD+/fv1/vvv68nnnhCffr0UUhIiAsjBoCLk5SULLM5WyUlO5WQECdfX18lJMSppKREZjMbx9bXzp1fSpL8/f31888/6c9/Xq7x48frz39erp9//ln+/v4O89wRLWLQqFksFplMUxUfP8h+a46Pj4/Cw/tq9eocpaaOkMk0TYMHJ/KDNwAAAOBmaooYJtPUWj3YKWI0DgUFBTp48KDuuusuh+MtW7bUxx9/rJdeeklVVVW68sorFR8fr4cffthFkQKA8yQlJWvw4ERaFTvBvn17JUkVFRWKi7tdXbpcKw8Pq2w2T+3Z8702bnzPYZ47osCORq2wsEClpXuVmWmWp6fjDReenp5KT5+oxMQ4FRYWKDo61kVRAgAAAGgoihiNW0xMjL766qtax6+88kq9/PLLLogIAC4Pg8Gg6OhYBQS0U3k5G/E2VJcu10qS7rsvTR98sMleUJfOXFC/994xeumlLPs8d0SBHY3aoUM/SpJCQsLqHA8NDXOYBwAAAMD9UMQAADQ2FouFi79OMGbMWJlM07R+/ToVFm7T7Nkm7d+/V9dc01lTp5oUGdlbBkMLjRkz1tWhNhgFdjRqQUFXSJJ27dqp8PC+tcZLSnY6zAMAAAAAAAAuRm7uWplMU1Va+p+2JUZjZ5lMs2lfVk9eXl4aN26CVqxYoq5dr3YYW7VqpSRp/PgMeXl5uSI8p2CT00vEYrFo69bNysnJ0datm2WxWFwdkluKjIyS0dhZS5YslNVqdRizWq1aunSRjMYuioyMclGEAAAAAAAAaCpyc9cqLS1FoaFhysvbpMrKSuXlbVJoaJjS0lKUm7vW1SGikfGw2dz35rvGeusgV7mcq+Y/tvj4QcrImKiYmAht2VKkJUsWKT9/A5sfXSQPD3ErrpORU+dyp3zWxArncqfvvTvE6g7Ip/ORU+cjp87lTrfhc76/dBr7vyf+3TsfOXUu8nnxLBaLIiJ6KTQ0TKtX58hg8LTn1GKxKjV1hEpKSlRUtK3Rnqcam+rqanXuHKQOHfxVXPyFsrOzdOjQAQUFXa2UlDHq06eHjhw5or17f2xUq9jrc75nBbuTcZXL+ZKSkmU2Z6ukZKcSEuLk6+urhIQ4lZSUUFwHAAAA3Fxu7lpFRPTSkCGJGjlypIYMSVRERC9+dwIAXHaFhQUqLd2rjIxJ8vR0LJt6enoqPX2iSkv3qLCwwEURup+srJWyWCyaMmWavL29NW7ceC1btkzjxo2Xt7e3nnhiqiyW08rKWunqUBuMArsTWSwWmUxTFR8/SKtX5yg8vK98fHwUHt5Xq1fnKD5+kEymabSLaYCkpGQVFW3XO++s16uvvqp33lmvoqJtFNcBAAAAN8YCJQBAY3Lo0I+SpJCQsDrbP4eGhjnMw/nt2fO9JCkubnCd4/HxgxzmuSM2OXWimqtcmZnmc17lSkyMU2FhgaKjY10UpfsyGAyKjo7ldicAAACgCfjvBUoGg6fDAqXU1BEymaZp8OBEbsMHAFwWQUFXSJLM5heUnf1/tdo/jx6d6jAP59ely7WSpI0b8zR69H21xvPzNzjMc0esYHeis69y1YWrXAAAAABwBrfhAwAam8jIKAUEBGr27BkKDg7R/PkLtGrVKs2fv0DBwSGaM+dZBQQEKjIyytWhuo0xY8bKYDBo7txZOnnypMNdASdPntT8+bNlMLTQmDFjXR1qg7GC3Ylqrl7t2rVT4eF9a42XlOx0mAcAAAAAzRULlAAAjdnmzX/Xxo3v2R+3bt3GhdG4Ly8vL40bN0ErVixR585Bslqt9jFPT09ZrVaNH5/RqDY4rS9WsDtRZGSUjMbOWrJkocNfFkmyWq1aunSRjMYuXOUCAAAA0OydvUCpLixQAgBcboWFBSovL5Mk2Wr1Jj7zuLy8jLur6unmm/tIqp3Tmsc14+6KArsTGQwGmUyzlZ+/QampI1RcXKTKykoVFxcpNXWE8vM3yGSaRf9AAAAAAM0eC5QAAI3NDz8clHSm3/rp06ccxk6dOiWjsbPDPJxfzZ4rt98+WHv3HtLMmXM1YcIEzZw5V3v3HtLttw+WyTRNFovF1aE2GAV2J0tKSpbZnK2Skp1KSIiTr6+vEhLiVFJSIrM5W0lJya4OEQAAAABcjgVKAIDGpqKiXJJUWrpXHTr4a9Gipfrhhx+0aNFSdejgb9/0tGYezu/sPVdat26tcePGa9myZRo3brxat27dJPZcoQf7JZCUlKzBgxNVVFSgqqqj8vb2U0REFD8YAgAAAMBZahYomUxTlZAQZz9uNHZhgRIA4LLz8/uNpDN9w7dt26lWrbwUENBOKSn36e67R+raa6/SqVPV9nk4v+aw5woF9kvEYDAoOjpWAQHtVF5eqVptmwAAAAAALFACADQa27d/Jkmqrq5WWlqKMjImKiYmQsXFRVqyZJFOnaq2z7vnnlGuDNVtnL3nSnh431rjTWHPFQrsAAAAAACXYoESAKAxqNl0Myysu3bs+KfD3VXXXNNJYWHdtXPnjjo2QMW5nL3nyqpVL6u4uNB+Qb1Pn8gmsedKvQrsL7zwgvLz8/Xdd9+pdevW6t27tx5//HFdd9119jknT57UvHnz9O6776q6uloxMTGaPn26AgIC7HMOHjwok8mkoqIieXt7a8iQIZo0aZJatGg69X6LxcIKDAAAAAC4APz+BABoDK67rqskaefOHbXG9u/fJ2mfwzycX82eK/ffP1rXX3+NTpw4YR9r06aNTpw4oVWrXnbr8369Njn95JNPNGrUKL3xxhvKysrS6dOnlZaWpqqqKvucOXPm6MMPP9TixYuVnZ2tw4cPa8KECfZxi8WiBx98UKdOndJrr72mefPm6e2339bSpUud96lcLDd3rSIiemnIkESNHDlSQ4YkKiKil3Jz17o6NAAAAABoVPj9CQDQWIwZM1YeHh6/OsfDw0Njxoy9TBE1HR4eHrJaHVf+22y28+bbHdSrwG42mzVs2DD99re/VUhIiObNm6eDBw/qyy+/lCRVVlbqrbfe0pNPPql+/fqpe/fumjNnjrZt26bt27dLkrZs2aLdu3frueeeU2hoqPr376+MjAy98sorqq6udvoHvNxyc9cqLS1FISGhmj9/gVatWqX58xcoJCRUaWkp/JAIAAAAAP9W8/tTaGiY8vI2qbKyUnl5mxQaGsbvT8D/b+/O46Kq+j+Af5hh31QWTWRxF8wFV1AxcwEX0KyeSkulIFvUR38PmuWWo+bSoqWpWQaG9mRliwlKKpblBmKKiYBpLmC4sKggyDbM7w+ee50rCDN4YWbw8369eDV37lc5nmbmzP3ec76HiBqcWq2utfyLRqOBWq1uoBaZPrVaDZVqHry8WqO8vExyrqysDF5eraFSzTfpPtUrwX6vgoICAECTJk0AACkpKSgrK0P//ndr5rRr1w5ubm5igj05ORkdO3aUlIwJCAjA7du3ce7cOb1+v5mZcf1UVFS+YLp390VaWirefHMWwsLC8Oabs5CWloru3X2xaNF8VFSoDd5WU/4xxv/3pv7DPmWfGvuPKfUnERER6Ua44A4KGoHo6K3o3bsv7O3t0bt3X0RHb0VQ0AiTv+AmIiLTsmjRfFnjCEhIOIyMjEu4ePECnJycsWrVGly5cgWrVq2Bk5MzLl68gIyMi0hIOGzoptZZnYueV1RUYNmyZejZsyc6duwIAMjJyYGFhQUcHR0lsc7OzsjOzhZjtJPrAMRjIUZXzs4OdW1+vdi/fz8yMi4hMzMDISEh+Pbbb9ClSxekpKRg2bJliI2NhUajQVpaMh5//HFDN9ekGdv/+8aAfSo/9qm82J9ERESNi3DBvWFDJBQK6dwvhUKB6dMjEBwciISEwxgwYKCBWklERA+Tc+fOAgCcnJyQlHQKS5eqcPnyJbi7e2HePBV69+6CGzduiHFUu3/+uQygMv97/Php/PHHUfz6669o27Ydjh8/jR49fJCTkyPGmaI6J9gXLVqEs2fP4quvvpKzPXrJzTWu3eXT0ytn4A8ZMgyff74FSqUC9vb26NDhUXz++RY8//wz2LdvL9LTz6FLl14Gbq1pMjOrTLIZ2/97U8Y+lR/7VF6m1J9CW4mIiKh2165dBQB4e3eu9ryPT2dJHBERUX0rLi4GALi5uWPw4P7IyLgknouP3wM3N3fcuHFDjKPaHT9+DADQt28/DBjQW9Knnp5e6NPHH3FxsTh+/BiefXa8oZr5QOpUImbx4sXYv38/oqOj8cgjj4jPu7i4oKysDPn5+ZL43NxcuLq6ijE5OTmS88KxEKMrjca4foR/R3DwaJiZKcREkEYDmJkpMHJkiPjvNXRbTfnHGP/fm/oP+5R9auw/ptSfREREpJsWLSqvJdPTU6s9n5aWKokjIiKqb506eQMAUlL+RMeOnST7g3Ts2AmnT5+SxFHthJr2u3bFoFMnH7z77sr/7Vm5Ep06+SAuLlYSZ4r0msGu0WiwZMkS7N27F1u2bIGHh4fkfJcuXWBhYYEjR45g+PDhAIDz588jKysLvr6+AABfX19s2LABubm5cHZ2BgAcPnwY9vb2aN++vQz/JMNxdq4sdbNzZwyee+4FJCUloKjoFmxtm4h3Y7TjiIiIiIiIHlb+/v3h6emF1atXIirqyyrXT2vWrIKnZ2v4+/ev/S8jIiKSQfv2HcTHv/++Hz4+ndG2rQd27YrF77/vrzaOaubl1UZ8fODAfuzd+7N4bG1tXW2cqdErwb5o0SLExsZi/fr1sLOzE2umOzg4wNraGg4ODnj66aexYsUKNGnSBPb29njnnXfQo0cPMcEeEBCA9u3bY/bs2XjjjTeQnZ2Njz76CC+88AIsLS1l/wc2pJYt3QAA+/btRfv27rhz5454zsbGRjwW4oiIiIxVUlISIiMjkZKSguzsbKxbtw7Dhg0Tz2s0GqxZswbbtm1Dfn4+evbsCZVKhdatW4sxN2/exJIlS/Drr79CoVAgKCgI8+bNg52dnQH+RUREZGyUSiVUqqUIC5tw3+unqKgvoVQqDdhKIiJ6mPj4PAoAMDc3R2lpKT7++CN8/PFH4nlzc3OUl5eLcVS7zp3v9lVNk9S140yNXgn2rVu3AgAmTpwoeX758uV46qmnAABz587934Y001FaWoqAgAAsXLhQjFUqldiwYQNUKhWee+452NjY4Mknn8T06dMf9N9icP7+/eHi4oqcnGxUVFRIzgnHLi6unIFBRERGr6ioCJ06dcLTTz+NadOmVTm/ceNGbNmyBStWrIC7uztWr16N8PBw7Nq1C1ZWVgCAWbNmITs7G5s2bUJZWRnmzp2Lt99+GytXrmzofw4RERkxMzMzvZ4nIiKqL3l5uQCA8vJyNG3aDE2bNkFpaSksLS1x8+Yt3Lx5QxJHtcvJyRYfOzjYo1evXjA3V6K8XI2//joj1rPXjjM1eiXYz5w5U2uMlZUVFi5cKEmq36tVq1bYuHGjPr/a5JiZKWo8JiIiMmaDBg3CoEGDqj2n0WiwefNmvP766+Ks9vfeew/9+/dHfHw8goOD8ffff+PAgQP47rvv0LVrVwDA/Pnz8corr2D27Nlo0aJFg/1biIjIOKnVaqhU8xAUNAIbN0YjOjoS1679gxYtWiE0NByTJ4dCpZqPkSODOYudiIgahLDvR8eOnfDXX2fEhLpAeJ77g+guN7dyz0oPD09kZmZU2ZvT3d0Tly9niHGmSK8EO9UsIeGweLfl3skWwnFOTjYSEg5jwICBDdw6IiIieVy+fBnZ2dno3//uiiwHBwd0794dJ06cQHBwME6cOAFHR0cxuQ4A/fv3h0KhwJ9//onAwECdf58pTGAU2mgKbTUF7E/5sU/lxz59cImJh5GRcQmTJr2EgIA+yMi4JJ7buHEDJk58Ebt3xyEx0biun/j/nIiMkVqtRmLiYXEvCz+//rw5WQf+/v3h4OCIv/46AxcXVwwYEAAnp6bIy7uJQ4cO4q+/zsDBwZHVKfQg7EWZmZkBJydn+Pj4iDPY09LScPlyhiTOFDHBLqMrV7IAAEOHBmLz5q+rbNIzadI47Nu3V4wjIiIyRcIeLMJm5QJnZ2dxNkJOTg6cnJwk583NzdGkSRPxz+vK2dnhAVrbsEypraaA/Sk/9qk81Go1Dhw4gCtXrqBly5YYOHAgkxh1UFR0CwDwzjsqjB49Gt988zW6dOmClJQULFu2DEuXLhLjXFz42iUiup/Y2B1QqeZJblR6enpBpVqKkJAxBmyZ6VGr1SgsvA0A6NGjJ159dQoCAvxw8GAiioqKsHfvbhQW3oZarebYryPtxHlRUSEOHTooHmtvcsoEOwG4u+QhOHg0LCwsMGDAQLi4OCAnpwAaDTByZAj27dtr0kseiIiIGlpubkGNm+EYAzOzysSlKbTVFLA/5cc+lU9s7A4sXFg1ibFoEZMY+rKyqkya+/n54/PPt0CpVMDe3h4dOjyKzz/fgjFjRiAxMQFWVpXXVMZCeD8RERmD2NgdCA+fiKCgEfj000gxGfzRRysRHj4RkZFbOD7pYdOmjaioqMCLL4Zj3769GDXq7spbDw8vTJoUhs2bo7Bp00a8+upUA7bUdKSnp4mPNTV8EU1PT8PgwUMbokmyY2FwGQl3WnbujKl2k9O4uFhJHBERkSlydXUFAOTmSjf2yc3NhYtL5Rjn4uKCvLw8yfny8nLcunVL/PO60mhM48eU2moKP+xP9qkx/sTE7EBY2ET4+HRGXFw8CgoKEBcXDx+fzggLm4iYmB0Gb6Np/Wj+90lvJj537+u08lhjBG2t2i4iIkPT3ssiOnorevfuC3t7e/Tu3RfR0VsRFDQCKtV8qNVqQzfVZFy8eAEA0K1bj2rOauDr20MSR7W7ePG8+LikpERyTtjg9N44U8MEu4xatnQDAOzbtxehoeORlJSIgoICJCUlIjR0PPbt2yuJI6LGR61W49ChA9i6dSsOHTrALzLUKLm7u8PV1RVHjhwRn7t9+zZOnjyJHj0qv3D26NED+fn5SElJEWMSEhJQUVGBbt26NXibiYgeFJMY8hP2rzp6NKHa66ekpERJHBERSSUkVO5lMWPGTCgU0hSfQqHA9OkRyMi4iISEwwZqoelp3boNACAiYho6d35UckO9c+dHERHxb0kc1c5Mx81LdI0zRiwRIyN///7w9PSCk5MTUlNPV1lG4uvbA3l5N7gRAlEjxbp31JgUFhYiIyNDPL58+TLS0tLQpEkTuLm5YdKkSfjkk0/g5eUFd3d3rF69Gs2bN8ewYcMAAO3atcPAgQOxYMECLFq0CGVlZViyZAmCg4PRokULQ/2ziIjqTEhibNgQed8kRnBwIBISjGtDTmPWosUjAIC5c9/Gli1fSK6fPD1bY86cBVi2bLEYR0REUteuXQUAeHt3rva8j09nSRzVbtKkMCxYMAeWlpaIjNwCKytL8YZ6ZOQWtG3rhtLSUkyaFGboppqMLl3uTrD6++9/cOpUsrhnZdeuvmjXrlWVOFPDGewyUiqVUKmW4uTJZHh7+2Dy5NfwyiuvYPLk1+Dt7Y2TJ5OhUr3DTRCIGiGh7l11S8bDwyciNnaHoZtIpJeUlBSMHTsWY8eOBQAsX74cY8eOxZo1awAAkydPxoQJE/D222/jX//6F4qKivD555/DyspK/Ds++OADtG3bFqGhoXjllVfQs2dPLF682BD/HKKHGldXyYNJDPkJE5SOHTuKI0eOY/v2nfjqq6+wfftOHDnyB/74Iwmenq05QYmI6D6EG5Dp6anVnk9LS5XEUe2OHz8GACgtLUWPHp2xefMmZGVlYfPmTejRozNKS0slcVS7vXt/Fh/7+/fA33+fw6BBg/D33+fg79+j2jhTY6apqbq8kRM2DzU2ixYtwIYNayUXL0qlEq+9Ng0LFy4xYMtMn5kZJBvH0oNjnz44tVoNPz9f+Ph0RnT0ViiVCrFP1eoKhIaOR1paGhITT/AGm57UajUSEw+Ld7f9/PobdR8K7yeSlyl8PvGzVF7sT/lwdZV8Dh06gCefDMauXfHo3btvlddpUlIigoMD8eOPOzmDXQ/am/PNmBEhbs63evUq7Nnzs1Fuzsfxvv4Y++c+xyf5sU8fDK9F5ffDD9vw2mvhmDz5dURGfirZY1GhUCA8/FVs3PgJNmyIxFNPPWPAlpqOZ555Ar/99ivc3FohK+ufKueF5wcNGoxt234yQAurp894zxnsMouN3YH169fAwsJC8ryFhQXWr1/DWaxEjRDr3tWP2Ngd8PPzxdixwXj++ecxdmww/Px8+TlK1MhxtrV8uLpKXsJs69WrV0outgGgoqICa9as4mzrOggJGYPIyC1IS0vFqFGBcHR0xKhRgUhLSzPK5DoRkTERKins2fNztXtZ7NnzMysp6EmY7R8b+1O1431s7HZJHNWuXbv2AFBtcl37eSHOFDHBLiO1Wo3Zs/8DjUaDxx57XHIh89hjj0Oj0WD27P/wQpGokeGScfkxKUT0cOKNNflwQ075MYlRf0JCxiAxMVlSIiYx8QST60REOuCNSnn5+/eHubk5rlzJAgAMGTIMR44cwZAhlXtNXblyBebm5ryhroeFC9+RNc4YMcEuo8OHDyInJxt+fv7YtOm/KCkpQUxMDEpKSrBp03/h5+ePnJxsHD580NBNJSIZse6dvJgUIno48caavLi6qn4wiVF/lEolBgwYiPHjx2PAgIG8UUFEpAfeqJTP7du3UV5eDgAYOjQIs2a9iUcffRSzZr2JoUODAADl5eW4ffu2IZtpUnS9djfla3xzQzegMTl06HcAwGOPDUa/fj2r1Lp85pnxSExMwKFDv2PgwEGGaiYRyUx7ybhQ907AJeP6E5JCGzZE3jcpFBwciISEw6xxS9RI3HtjTalUSG6shYaOh0o1HyNHBjPppiOurqo/ISFjMHJksEntEUJERI2fcKOSNe0fzAsvVNZV79KlG86ePYNRowLFc56erfHoo11x+vQpvPDCM4iN3WOoZpqUqVMn6xwXHb21nltTPziDXUbCh9f77y+vdvbVypUrJHFE1Dhwybi8tJNC1dViZlKIqPHhbGv5cXVV/eJsayIiosbp8uVMAMB7763CgQNHERY2GUFBQQgLm4wDBxKxfPn7kjiq3fnzfwOo3J8yPf0iRo4MRteuXTFyZDDS0y/C3NxCEmeKOINdRv36DcCHH76Ppk2b4fPPN+OPP44iJiYGtrZN8Pnnm9G1a0fcvHkD/foNMHRTiUhmwpJxlWpelTvcXDKuHyHZExn5GbZs2VRlNdCECS9K4ojI9HG2tfy4uoqIiIhIf+7uHsjK+gevvhqOrKzLkrIl0dFRcHNzF+NIN0VFRQAAV9fmCAoaJF7jnzp1CqdPp8DV1RVXrmSJcaaICXYZCTOubt68gQ4dPFFcfEc8Z21tIx7fOzOLiBoHLhmXh79/f7i4uGLpUhUCA4dj6tTpcHVthuzsG4iP34NlyxbBxcWVSSGiRkR7tnXv3n2rnOdsa/0Jq6vCwyciNHQ8ZsyIQECAH5KSErF69Srs2fMzIiO3cIyqo9LSUmzatBHXrv2DFi1a4aWXJsPS0tLQzTJparWa36GIiMjg/vvfbejQwQOZmZfg7OyM5557AV26eCMlJR3ffPNfZGZeEuNIN15erZGZmYGsrH8wePBQuLm1Qn7+TTg6NoWNjQ1+/XWfGGeqmGCXUU5Otvi4tLREck77WDuOiBoX1r2T14EDv2Pv3t3isbW1jQFbQ0T1hbOt6wdXV9WPRYsWYMOGtZIZbSrVfLz22jQsXLjEgC0zXbGxO6BSzauyak2lWsrXKRERNSgbm7vXnLm5uVi/fk2tcVSz9u074ODByn0rhWT6/eJMFadSy8jFxRUA0KFDR7i5tZKcc3NzR4cOHSVxRERUVULCYfFGpJmZ9JxwnJOTzVrMRI0I97KoPyEhY5CYmIzt23fiq6++wvbtO5GYeIJJyzpatGgB1q1bDScnZ6xatQZXrlzBqlVr4OTkjHXrVmPRogWGbqLJiY3dgfDwifD29sG7736AqKgovPvuB/D29kF4+ETExu4wdBOJiOghsmnTRlnjCBgxIljWOGPEGewyMvtf5sfJyQk//LATSUkJ4hLHPn388dRTwZI4IiKq6sqVLADA0KGB2LTpv4iOjhSX4IeGhuOll17Avn17xTgiahw427r+cHWVPEpLS7Fhw1q4ujbHyZPpsLAwh4uLAyZOfBHjxk1A9+7e2LBhHebMWcByMTpSq9VQqeahe3dfpKWlYs+en8VzHh6e6N7dFyrVfIwcGcwbbERE1CCEjTYHDRqMFStWYfDgfigpKYGVlRV+/fUI3norAr/99qtJb8jZ0K5evSI+Njc3h4uLKzSaCpiZKZCTk43y8vIqcaaGM9hllJ19HQCQmJiAsLAJsLS0REhICCwtLREWNgGJiQmSOCIiqio3NwdA5YV1QEAfLFgwB2vXrsWCBXMQENAHrVp5SOKIqPHgbGsyZps2bYRarcacOfNhZmaGQ4cOYOvWrTh06ADMzMzw5pvzoFaXc0abHhISDiMj4xJOnkxG586PIi4uHgUFBYiLi0fnzo/i5MlkZGRc5Ko1IiJqMMKk2KNHE9GvXw8UFxdDo9GguLgY/fr1wNGjiZI4qt1XX20BADg4OKC8vBxXr17BtWvXcPXqFZSXl8Pe3kESZ4qYYJeRsPHWvHkqpKaexqhRgXB0dMSoUYFITU3F3LkLJXFERFSVs7MLAOCLLyLh7e0judj29vbB5s1RkjgialyE2dbjx4/HgAEDOWuVjMbFixcAAGZmCvj5+WLs2GA8//zzGDs2GH5+vlAoFJI4qp2wGm3IkGGIjt6K3r37wt7eHr1790V09FYMGTJMEkdEjYtarZbcrNTe24LIUHr27A0AuHOnCADg7u6BJ554Au7uHpLnhTiqXX7+LQBAQUEBBg8eCh+fzmjVqhV8fDpj8OChuH27QBJniphgl5GwQdfOnT9Bc8/aW42mArt27eAGXUREtbj3JqRGoxF/aoojIiKqT61btwEARET8Gz4+nSU3gH18OmPmzOmSOKqdsBotOHi0eINCoFAoMHJkiCSOiBqP2Ngd1d6s5L4LZGiOjg6S48uXM/HTTz/h8uXMGuPo/ry87n43+vXXfUhLS8U///yDtLRUyaan2nGmhgl2GSmVSowePRbJySdQUlKCVavWICsrC6tWrUFJSQmSk09g9OgnOBOLiKgGQiK9Q4eOSEtLlawGSk9PQ/v2HSVxREREDWHSpDAAgIWFBT777AuUlJQgJiYGJSUl+OyzL2BhYSGJo9oJq9F27oxBRUWF5FxFRQXi4mIlcUTUOEg3N175v82NV3JzYzIKS5cukTWOgEmTQsXH1d1Qry7O1HCTUxmp1WrExGyHr28P5ObmIiJiOiIiKmeyeHp6wde3B2JifsL8+Som2YmI7iMnJxsAcO7cWQQGDsfUqdPh6toM2dk38Msv8di7d7ckjoiIqCEcP34MQOVmp23atJQkhBUKhXh8/PgxDBgw0CBtNDUtW7oBAPbt24vQ0PGYMSMCAQF+SEpKxOrVq7Bv315JHDW8jz/+GGvXrpU816ZNG/z8c+WGtCUlJVixYgV27dqF0tJSBAQEYOHChXBx4U0Rqh43NyZjd+3a3Y02ra2tUVxcXO2xdhzV7MaNm+Lj6m6oVxdnaphgl5GwSc+GDZHo0aMXEhMPo6joFmxtm8DPrz+OHz+G4OBAJCQc5pduIqL7EEq/zJ37NrZs+ULypdvTszXmzFmAZcsWs0QMERE1qGvXroqPq14caqqNo5oJJTadnJzEPawEHh6VE5Ty8m6wxKaBdejQAZs2bRKPtZOey5Ytw2+//YaPPvoIDg4OWLJkCaZNm4avv/7aEE0lEyDkTTIyLmH48JH47LMoBAT44eDBRHz00Urs3h0nxjFvQoZgYWEpPr53I1PtY+04qpkwSUGXuGefHV/PrakfLBEjI+HLtLd352o36PLx6SyJIyKiqoSL7WPHjuLIkePYvn0nvvrqK2zfvhNHjvyBP/5I4n4WRETU4FxcXAEAfn79cOHCFYSFTUZQUBDCwibjwoUs9O3rL4mj2imVSqhUS3HyZDJ8fDpjxYoPEBkZiRUrPoCPjw9OnkyGSvUOZ7EamFKphKurq/jj5OQEoHKzuu+//x5vvfUW+vXrhy5dumDZsmU4ceIEkpOTDdtoMlrCpsVDhwZWu7nx0KGBkjiihia8BgHgyJETGDkyGF27dsXIkcE4cuREtXFUs7KyMgCVNygOHjwGOzs7KBQK2NnZ4eDBY+KNCyHOFHEGu4yE2ZTp6ano3btvlfNpaamSOCIiqkq42A4Pn4iXXnoBQ4YMg6trM2RkZGH9+o+xd+9uREZu4cU2ERE1KOHiLy8vF4895ofMzAzx3N69u2FtbS2JI92EhIxBZOQWLFw4955Va16IjNyCkJAxBmwdAcClS5cQEBAAKysr+Pr6YubMmXBzc0NKSgrKysrQv//dSQ/t2rWDm5sbkpOT4evrq/fvMva3j9A+Y2+nMdPe3FipVEj6VKlUYNSoEOzbtxe5uTns5zrga/TB2draio99fb3Fx6dOnUJc3E5JHPtZN2fOpAOo3EctIKC3+HxhYaHk+MyZdKPqU33awgS7jIRZl6tXr0RU1JdISkoQS8T06eOPNWtWcdYlEZEOQkLGYMqU6diwYa3kYlupVGLKlOm82CYiogaXnX0dAHD27F9wdW2OVatWY9y4Z/D119uwfPlSMeEuxJF+7t28/N4yPGQY3bp1w/Lly9GmTRtkZ2dj3bp1eOGFFxATE4OcnBxYWFjA0dFR8mecnZ2RnV23vXKcnR3kaHa9M5V2GqM2bTwAAHv3xmHatNdw6NAhXLlyBS1btsSAAQMQH/+zGOfiwn6uK75G6+7xxwciKmqjTnF8jerGwcFO5zhT7VMm2GWkPeuyfXt33LlzRzxnY2OD4uJizrokItJBbOwOrF+/BpaWVlCr1eLz5uYWWL9+DXr16sMkOxERNSih9EuHDh1RXFyMiIgZiIiYAaCyXniHDh1x9uxfLBGjp9jYHQgLmwAbGxvJ87m5OQgLm4CoqC855hvQoEGDxMfe3t7o3r07Bg8ejLi4OHHVhpxycwtwz70Wo2JmVpm4NPZ2GjN7+2YAgLi4ODRp0qRK3kQ4trdvhpycAoO00ZTxNfrgrKzsdY7ja1Q3ulbyaNHiEaPqU+H9pAvWYK8H986+qO15IiK6S61WY/bs/0Cj0VS7qYxGo8Hs2f+RJN6JiIjqmzAmOTk54dChY1iyZDmmTZuGJUuW49ChJLEuNUvE6E4Y8wFg4MBBiIuLR0FBAeLi4jFwYGVil2O+cXF0dETr1q2RkZEBFxcXlJWVIT8/XxKTm5sLV9e63WjSaIz/x1Taaaw/fn79xRuR90uRuLi4ws+vv8Hbamo/5eVqHDx4AFu3bsXBgwdQXq42eJtM8Ue7DExN4uJ2GrytpvJz7Jhum5weO3bM4G2990dXnMEuI7VaDZVqHoYPH1ltiZiwsAlQqeZj5MhgzmInIrqPw4cPIienclnxwIGPYdiwQLi6OiE7Ow/x8Xuxd+9u5ORk4/Dhg+LFNxERUX0TSr8kJiagUycvyazLZcsWi8csEaO7Q4cOICcnG35+/bB589dQKhXiZoebN3+NMWNG4OjRBBw6dACPPfa4oZtLqKyXm5mZCVdXV3Tp0gUWFhY4cuQIhg8fDgA4f/48srKy6lR/nR4+Awc+hqFDA+Hq2gzZ2Tewb1/ld33SX2zsDqhU85CRcUl8ztPTCyrVUq4C0tP583/LGkfA9evXZI0zRpzBLqOEhMPIyLiEGTNmwsLCAgMGDMT48eMxYMBAWFhYYPr0CGRkXERCwmFDN5WIyGgdOLAfANCuXXukp6fhzTdnISwsDG++OQvp6Wlo27adJI6IiKgh6LO8mXRz6NABAMDs2XOhUEgvTRUKBWbPniOJo4b37rvv4ujRo7h8+TKOHz+OadOmQaFQICQkBA4ODnj66aexYsUKJCQkICUlBXPnzkWPHj2YYKf7Skg4jJycbMybp8KZM+l4661ZCA8Px1tvzcKZM2cwd+5C5ORkM2+ih9jYHQgPnwgfn86SlUA+Pp0RHj4RsbE7DN1Ek1JcXCxrHOk+E1yfGePGhjPYZXTt2lUAgLd352rP+/h0lsQRUeOjVquRmHhYXL3i59efK1b0dPnyZQCVMwKCgkbgs8+iEBDgh4MHE/HRRyvFTU+FOCIioobQp48flEolnJyckZT0J7Zs2YRr1/5BixatMHHiS+jTpxvy8vLQp4+foZtqMrSr6VT3HUq40GbVHcO5evUqIiIicPPmTTg5OaFXr1749ttvxZJIc+dW3hyZPn06SktLERAQgIULFxq41WTMhHxIePgrmDZtRpX3/Z07RVi2bBHzJjoSKikEBY1AdPRWyUqg6OitCA0dz0oKemrTpi2OHk0AAJw6dRYffvgeLl++BHd3L/znP7PRtWsHMY5006mTt9inv/2WiAkTnkFeXi6cnJzx5ZfbMGiQnxhnqphgl5EwWyU9PRU9evSqMlCkpaVK4oioceGyPHm4ubkBABwdm+Dzzzfjjz+OIiYmBra2lcddunTArVs3xTgiIqKGkJSUCLVajezs6/D2boPi4rslYpYuXSweJyUlYsCAgYZqpkkZMOAxrFr1Pt56aybu3LmDy5czxXPu7h7ixqcDBjxmqCY+9D788MMaz1tZWWHhwoVMqpPOtPMmvXv3xYABA+Hi4oCcnMpNOZk30Y9QSWHDhshqVwJNnx6B4OBAJCQc5tiko5MnT4iPhWS6ICpqY7VxVLMrV7LEx0IyHagsO6Z9rB1naphgl5G/f394enph7tw3kJubi8zMDPGch4cnnJ2d4enZGv7+/Q3YSiKqD8KyvKCgEfj000jJjOvw8ImIjNzCJLuOnJ0rNz26desmOnTwlCQwrK1txGMhjoiIqCFoz6YsKZEuC9c+5qxL3fXvHwAHB0ecPftXlcRQVtY/qKiogKOjI/r3DzBQC4nu4kpVeQh5k9WrV4ozrgUVFRVYs2YV8yZ6YCUF+T0M5Uwamo2Nraxxxog12GWkVCoxevRYJCefQHFxMVatWo2srCysWrUaxcXFSE4+gdGjn+AgTNTI3Lssr3fvvpJleUFBI6BSzYdarTZ0U02Cq+vdxHlNCQztOCIiovrm4nJ33Bk2LAjvvvsBoqKi8O67H2DYsKBq4+jBMYFBxiA2dgf8/Hwxdmwwnn/+eYwdGww/P1/Wtq4DpVIJlWop9uz5GaGh45GUlIiCggIkJSUiNHQ89uz5GSrVO8yb6Eh7RUB1uCJAf15erWWNo8qV/XLGGSMm2GWkVqsRE7Mdvr49YG1tjYiIGXBzc0NExAxYW9vA17cHYmJ+YpKNqJHR3uD4fsvyuMGx7rS//FlZWUnOWVlZVxtHRERU3yoqKgAATZs2Q3T0VoSFvYKXXnoJYWGvIDp6K5o2bSaJo9odOnQABQX5aNmy+rJvjzzSEgUF+dzklAyKG0jKLyRkDCIjtyAtLRWjRgXC0dERo0YFIi0tjSt/9aS9IuDe8YcrAuqmb1/d9lLRNY5030vFlPdcYYkYGWnXvqquBvvx48dY+4qoEeKyPHlp/jdVrUOHjlXqsbq6usLa2hpnz/4lxhERETWEhIRDAICbN2/gpZdewIwZEQgI8ENSUiJWr16FmzdviHGPPz7EkE01GULi/MqVLAQFjcDQoYFwdW2G7Owb2Ldvr7ix+aFDB/DYY48bsKX0sOIGkvUnJGQMRo4MZtmdBySsCAgPn4jQ0PFVxqY9e35GZOQW9qseLl26JDlWKpXQaDQwMzOTTJi9N47u7+LFC5JjS0tLsU9LS0vvG2dKmGCXkXaSTalUVtmsg0k2osbp3o167sVlefrJyckGAJw7dxaBgcMxbdoM8WL7l1/isXfvbkkcERFRQxDu677xxhx8881XGDUqUDzn6dkas2a9hQ8+WMGSJnrQaCpnW/bu3RebN38NpVIhXj+FhoYjODgQf/yRJMYRNTRuIFm/qsubkP6EFQEq1bwqYxNXBOgvKSlBcny/KhT3xtH9Xb9+XXKsnVSvKc6UMMEuIybZiB5O2svyoqK+RFJSgjgLo08ffy7L05PwGTl37tvYsuULcfYaUPklcc6cBVi2bDE/S4mIqEENGDAQH374Pn7/fT8OHkxCdHQkrl37By1atEJoaDj+9a8xYhzpplkzZwBAUVFhtefv3CmSxBE1NK5UJVMREjIGQ4cGYtGi+bh8+RLc3b2wcOE7sLGxMXTTTM69K6UVCgXMzMyg0WgkZXi4olp3tra2uHXrpnisUCih0VTAzEyBigq1JM5UMcEuI+6GTfRw0l6W1769O+7cuSOes7GxQXFxMZfl6UH4LD127CiOHDle5YZFWNgEfpYSEVGDq5xl6YrExCPo2NELxcV3x/ulSxejuPgOXFxcmWDXg7BheWrqaUyaNA7/938zxdIGH320EqmppyVxRA2Nk+jIVCxatAAbNqyVzLaOjo7Ca69Nw8KFSwzYMtPj5tYKZ86ki8f321vFza1VQzXJ5LVs2RJXrmSJx0JSXaNRV4kzVUywy0g7yTZp0jgMGTKsSlkDJtmIGi+NRlPtskbe2daP9mdpWNgEzJgRgSFDQnDwYCLCwiawjiARERmEUqnEc889j3XrVqO0tERyTjh+7rnnOT7pQXtz0wMHfpOsWtOedXm/TVCJ6hsn0ZEpWLRoAdatWw1XV1c888w4dOnig5SUNGzb9jXWrVsNAEyy6+Gvv87IGkfQufyTKadOzDQmnPkx1hpd1d05VCrN8dprU/mh9oDMzMD6bDJjnz44tVoNPz9fODk5IScnR7Ipp7u7B1xcXJCXdwOJiSd40a2H2NgdWLhwLjIzM8TnPD29oFItNdo6gsL7ieRlCp9P/CyVF/tTfuzTB1fzeO8JFxdnjvd60u7T3NzcKmO+k5OTUfYpx/v6Y4yfUbGxOxAePhFBQSPEDSQPHpRuIGms301NAcenB1NaWgovrxaws7ODo2OTKtei+fm3UFhYhEuXrsLS0tKALTUdXl6PiCXKamJjY4tLl1geShfdunXC1atXao175JGW+PNP47lxoc94zxnsMouN3YH169cgMHA4hg4dBldXJ2Rn52HfvnisX78GvXr14eBL1MgImx9lZmYgMHA4/v3vGZL3/t69u6HRaLj5kQxM+J4wERGZOO3NDnv06IXExMNiCTM/v/44fvwYNzvUk/aqtSFDhqFVq1bIz78FR8cmsLOzxy+/xHPVGhkcN5AkY7Zp00ao1Wrk5+ejX78B2Lhxk3gT6KOPVmL37jgx7tVXpxq4taZB1421uQG37goK8mWNM0ZMsMtIrVZDpZqHoKAR4vIx4U5saOjLCA0dD5VqPkaODOaXRKJGRKgl1qVLV6SlpUqWN3t4eKJLl644depPSc0xqpn2TKHPPouSfEkMD5/IixkiImpw2psdKpXK/9Vkvzvrkpsd1k1IyBh0794D+/btrXLO17cnx3syCiEhYzByZHCVG2u8ridDO3/+bwDAoEGDERX1JZKSEhATEwNb2yaIivoSzz//L/z2269iHNWuZctWuHChsr/27j2Af/1rNAoLb8POzh7ffReDwMCBYhzpxs7ODoWFlRua79nzO558ciTu3LkDGxsb/PhjHIKCHhPjTBUT7DLSntWiUCgk5xQKBaZPj+CsFqJGKDc3BwCQknKq2oSwkHAX4qhm2jcrq/uSGBY2gTcriRoxtVrNBIbM2Kfy0N7ssLoZ7NzssG4mTRqP5OTjsLCwgL9/f3h5eeDSpUwkJBxGcvJxTJo0Hps3bzV0M4mqvbFGZGhmZmYAADs7e/Tr1xMZGZfEc56eXnj00S6SOKpdeXmZ+FhIpgPArVs3JcfacVSzkpJS8bGQTAeAwsJCybF2nKlhgl1G2rNaqsNZLUSNU7NmTgAAZ2dnfP75Zvzxx1ExIfz555vRo4cPcnJyxDiqmXCzcuLEF6v9kjhhQih2747jzUqiRig2dgdUqnlV3vfGvPeCsWOfykfY7HDu3Deq1Av38PCEs7MzNzvU0507d/Dzzzthbm6ORx5piQMHfsOBA5XnPDw8ceVKFn7+eac4y42IiKR69uyNqKiN2LUrBkOGBKJbt+4oKroNW1t7FBXdQVzcTjGOdOPl1VoyxtcUR7pp3rw5bt26qVOcqVLUHkK60p7VUh3OaiFqnG7cyAMA5OTkoGNHT4wdG4znn38eY8cGo2NHT+Tk5EjiqGbCTchlyxbD29sH7767ElFRUXj33ZXw9vbB8uVLJHFE1DgIpaF8fDojLi4eBQUFiIuLh49PZ4SHT0Rs7A5DN9HksE/lpVQqMXr0WCQnn0BxcTFWrVqDrKwsrFq1BsXFxUhOPoHRo5/g6gA9LFo0HwBQXl6Ozp0flbxOO3d+FOXl5ZI4IiKSatnSTXz8yy97ERu7A7/88sv//ru32jiqmYeHp6xxVLkZvJxxxogJdhkJs1pWr16JsrIyHDp0AFu3bsWhQwdQVlaGNWtWcVbLA1Cr1ZI+VavVhm4SEQDA2dlFfHzvUlHtY+04uj8XF1cAQPv2HZCWloo335yJsLAwvPnmTKSlpaJ9+w6SOCIyfffuY9O7d1/Y29ujd+++iI7eiqCgEVCp5nPs1wP7VH5qtRoxMdvh69sD1tbWiIiYDjc3N0RETIeNjQ18fXsgJuYn9qke/v77HABg4MBBiIr6EiUlJYiJiUFJSQmior5EQMBjkjgiIpLS6FirSNc4As6cOSNrHEGn2ev6xBkjloiRkVKphEq1FGFhE9CunTuKi++I56ytbVBcfAdRUV9yVksdcHkzGTPtVSkDBz6GoUMD4eraDNnZN7Bv317s3bu7Shzdn1Af8OzZv6rUtP/www/EmvasI0jUeHAfG/mxT+V3t4TZS9i8OUpyTqPRYNSoMVi2bBH7VA+2trYAgPJydbVl4dzcWkniiKhx4R4hDy4r6x/x8eDBw2Bra4M7dwphY2OHoqI7+PXX+CpxVLOrV+/2lZmZGRQKJSoq1OJ/hZsV2nFUs8uXM2WNM0acwV5PtJPrlcfFBmqJ6ePyZjJ2wgDboUNHpKWl4q23ZiE8PBxvvTUL6elp6NChoySOaqZd+kWj0eDkyWR8++23OHkyWdKHLBFD1HhwHxv5sU/lJ/TV0qWqasuZLFu2SBJHtRs5MgQAcOTIQXTs2EnSpx07dkJCwmFJHBE1HrGxO+Dn5yspr+nn58vrez3FxcUCADp29Mbvv/+KnTtj8Msvv2Dnzhj8/vt+dOzoLYmj2imVd+ciazQaqNXlkv9WF0c1y83NkTXOGPHVICO1Wo3Zs/8DAAgMHI5hwwLh6uqE7Ow8xMdXzmKdPfs/GDkymHdldXTv8malUiFZ3hwaOh4q1Xz2KRlUTk42AODcubOwsrKWnMvOzkZJSbEkjmomDKqPPz4Uv/wSL64AACq/xAwaNAS//faLSQ++RCSlvY9N7959q5znPjb6Y5/Kz9W1cuMtPz//ar+XPvHECCQmJohxVLtWrdzFx/v3/4Li4mJ4eXng0qVMMbl+bxwRmT5hEl1Q0Ah8+mmkuFr1o49WIjx8IiIjt3Cluo6KiooAAH/9lY7AwBH/y0NVrqauzEP9LImj2gUEPIatW78Ujy0sLGBmZgaNRoOysjJJHOlG19XnprxKnQl2GR0+fBA5Odnw8/PHli3fQKlUwMXFATk5BQgNfVn80n348EEMHDjI0M01CVzeTKZASE5UN0PdzOzu80xi6EaoVb9//z4MGxaEtm3bAVADUOL8+b8RH79HEkdEpk97HxshcSmoqKjgPjZ1wD6V391xvvqLP+E0V6zpTugrKysrlJSU4ODB33Hw4N3zwvPsU6LGg5Po5NWmTVv89tuvACqvPbt27SbesNi3b68kjnTj6eklOdZOqtcUR/dnb2+Pmzdv6hRnqlgiRkaHDv0OAJg9e161yeBZs+ZI4qh2XN5MpqBPHz8olUo4OjrC2dlZcs7JyRmOjo5QKs3Rp4+fgVpoWpo3byE+PnjwAD777BN89tln+OyzT3Dw4IFq44jItAn72OzZ8zNCQ8cjKSkRBQUFSEpKRGjoeOzZ8zNUqnd4oa0H9qn8hJVoR48mYNKkcYiM/AxRUVGIjPwMkyaNQ1JSoiSOaif0VUlJSbXnhefZp0SNhzCJbsaMmSgvL8eGDevw73//Gxs2rEN5eTmmT49ARsZFySoWur8RI4IBVI77KSmnMGpUIBwdHTFqVCBOn04Rx3khjmq3ffv3ssYRoFZXyBpnjDiDXUa1TawQVjpwAobuuLyZTEFSUiLUajXy8/NhZWWNVavWYNy4f+Hrr7/D8uXvID8/X4zjSova1bQsTPuUKS8fI6KqQkLGIDJyC1SqeRg1KlB83tOzNZeK1xH7VF7C982nnnoG27d/L266DVSWMHvyyX/hhx+28XupHpo1cxIfW1lZi2X1AMDa2lrcx0o7johMmzA57scfv8fo0cOhVqvFcyrVfISFvSKJo5rdvHkDQOXKgHs3Mv3nn8tV4qh2V6/q9trTNY6A27cLZI0zRkywy2jAgIH48MP38d57yzBgwMAqS3Hfe2+5GEe64fJmMgVXrmQBqFyOd/PmTURETEdExHQAlcvGunbthlOn/hTjqGbSTU6ld7ArKiqqjSOixiEkZAxGjgxGYuJhFBXdgq1tE/j59ecs6wfAPpWPv39/uLi44vvvv612v6UfftgGFxdXfi/VQ2pqCgDA3t4BaWnn8ccfR8XXaa9efeHj0xa3bxcgNTUFgwcPNXBriUgOwk3IjRs/gatrc8yZMx/jxz+DrVu3Yfnyd7Bx4yeSOKqZrv3E/tRdeXn1JWHqGke6l88z5ZJwLBEjowEDBsLFxRWJiUcwcaJ02ejEieNw9GgCXFxcmWDXA5c3kykQNtvs1atPlXMajQY9e/aWxFHNtPupunJb1cURUeOhVCoxYMBAjB8//n8TFjjGPyj2af3QaCrHeRO+FjQ4oazO7dsFePnlSbC0tERISAgsLS3x8suTxJlsQhwRmT7h2sjS0hInTqRi4sQX8cgjj2DixBdx4kQqLC0tJXFUM6Fcqatrc/z1VwZGjgxG165dMXJkMP76KwOurs1ZrlRP2jcjDhxIgoeHJ+zs7ODh4YkDB5KqjSPdffHF17CwsABQuYHsF198beAWyYMz2GWkVCrx3nsfIixsAuLjd4u7NQN3Sxm8996HvKjRE5c3k7ETNtv84otIBAaOwLRpMyQ7t0dHR0niqGbCMnAXFxecOJFWZTabr68PcnNzuFyciIgaVELCYeTkZOPpp5/F9u3fY+/e3eI5pdIcTz31LH744VskJBzmhBod2draAQBeeGESDhz4rcp3/eefn4ivvtoixhGR6du8ufLaqLS0FOHhEzFjRgQCAvyQlJSI1atXobS0VIx79dWphmyqSRDKlWZnX0f37p1w584dAMCpU6ewf//dY5Yr1Z32pqYDB96dRFdYWCg5vt/mp1SzF18cJz4uKyuTHJsyJtjriZWVlVgzsLpj0g+XN5Mx095s8/ff90turllZWVcbR/d340YeACAnJwcvvzwJM2ZEYMiQEBw8mIiXX54kzlwX4oiIqGZqtZrfoWQglCb74YdtCAwcjqFDA8Ub6vv27cWPP26TxFHtnnlmHL777hvs3BmDP/5IwdKlKly+fAnu7l6YN0+FXr26iHFEhsbPUnlcvHgBALBq1Vp89NH7VW6srVr1MSIi/i3GUc20x5x7N4hkec268fJqg8zMDJ3iiARMsMtIrVZDpZqH4cNHIirqSyQlJYiDb58+/ggLmwCVaj5GjgzmQFwHwvJmFxcH5OQUcDmuDPglUR7am21qb8517zE35dSNMNO/a9fuSE09LfnS7eHhha5du+PUqZNcEUBEpIPY2B1QqeYhI+OS+JynpxdUqqVcBagnFxdXAEDfvv7YtOm/4nf9Tp28MWHCi3jyyWCxJCTp5rHHHoeDgyNu3ryBdu1aSc5FRW0EADg4OOKxxx43QOuI7uJnqXxatxaSkhVITEyucj363/9uvieOauLq2hwA0LRpU9y8eVNyrqSkRHxeiKPaeXi4yxpHgK2tLYqKinSKM1VMsMsoIeEwMjIuYcOGSFhYWFRJBk+fHoHg4EAuGyWjwC+J8tGeDeDi4oJnnx2PRx/1xunT6fj2263IycmpEkf317KlGwAgJeVPBAYOx9Sp08UZgr/8Ei8uyRfiiIioerGxOxAePhFBQSPw6aeRCAjww8GDifjoo5UID5/IUnt6Em6U5+Xlwt+/h2R2m4eHJ6ytrSVxVDthAs3PP++8bwz3DSBDEz5LK7+X/lvc3Hjfvnh+ltbBSy9Nhko1H8uXv4Nx4yZI8iZlZeV4992lUCrN8dJLkw3dVJMgbAp58+ZNWFpawtvbGw4ODigoKEB6erqYdDflzSMb2pkzf8kaR0Dv3n74/fdfdYozVUywy0hInnl7d672vI9PZ0kckaHwglte2dnZAAB3dw+YmZlh/fqPxXMeHl5wd7fB5cuZYhzVzN+/Pzw9veDk5IS0tFTs2XO35I6npxe6d/dFXt4N+Pv3N2AriYiMm7CyMihoBKKjt0KpVMDe3h69e/dFdPRWhIaO58pKPWVnXwcAnD37F1xdm2PVqtUYN+4ZfP31NixfvlRMuAtxVLvS0lLs3r2rxpjdu3ehtLRU3PiQqCEJn6Xdu/tW+V7q4eGJ7t19+VmqJ0tLS7z22jSsW7ca3bt746235mHcuH/h66+/w4oVS5GdfR1Tp87ge15HWVn/iI9LS0vx559/1hpHNcvKypQ1joCLF8/LGmeMFIZuQGMi7CCcnp5a7fm0tFRJHJEhaF9wR0V9iZKSEsTExKCkpARRUV8iKGgEVKr5UKvVhm6qybhxIxcA8MgjLZGQcALbt+/EV199he3bdyIh4bj4nhfiqGZKpRIq1VKcPJkMH5/OWLHiA0RGRmLFig/g7e2DkyeToVK9w4sYIqIaCCsrZ8yYCYVC+pVfoVBg+vQIZGRcRELCYQO10PQIy+s7dOgIa2trRETMgJubGyIiZsDa2gYdOnSUxFHtPvvsE2g0GtjbO+DixatYsmQ5pk2bhiVLluPixauwt3eARqPBZ599Yuim0kNK+Cw9eTIZnTs/iri4eBQUFCAuLh6dOz+KkyeT+VlaBwsXLsHUqTOQl5eLmTNnoFWrVpg5cwby8vIwdeoMLFy4xNBNNBlxcbGyxhGQm6vbdbuucQRJ5QQ54owRE+wyEmZdrl69UrKZBFC5ucSaNavg6dmasy7JoIQvib1790W/fj0xdmwwnn/+eYwdG4x+/XqiV68+/JKoJzOzyo/SP/5IQljYBFhaWiIkJASWlpYIC5uA48ePSeKodiEhYxAZuQXp6Wl4661ZCA8Px1tvzUJ6ejpXWBAR6YArK+UnLK93cnLG778nIixsMoKCghAWNhm//56AZs2cJHFUO6E0zKhRIRg4sC8WLJiDtWvXYsGCORg4sC9GjgyWxBE1tCtXsgAAQ4YMQ3T0VvTu3VeyGmjIkGGSONLdwoVLcOnSNcmNtUuXrjK5rqeCgtvi4zNnLknGpjNnLlUbRzXTzudFR38jln4zMzNDdPQ31caR7jZv/rrGY1PFEjEyEmZdhodPxKRJ4zBkyLAqdYMjI7dw1iUZlHAhvWzZ4mrrCC5fvkQSR7UbMGAgPvzwfbRv36HKppyenl5o164Dzp37i3sv6CkkZAxGjgzmRrxERHWgvbKyd+++Vc5zZaX+cnIqS70lJh5BmzYtJeeEDTm146h2Qrn6b7/dChsbG8m5nJxsbNv2tSSOqKHl5lbupRQcPLra1UAjR4Zg3769Yhzpp7JczFTJ3nWkn5ycu2XJvL1bS27ybtr0ebVxVDPt1fyhoc+JjzUajeSYq/7rZtKkcTUemyom2GUWEjIGU6ZMx4YNayX12ZRKc0yZMp2zLsngXFxcAQDt23eoto6gkAwW4qh2lRvzuOLs2b8wbNhwjBgxCmZmFdBoFLhw4QLi43fDxcWVCfY6EDY/45duIiL9aK+sjIr6EklJCeLNyj59/Lmysg60b0YoFArJzDXtY9600F1Q0CgkJiYAqPw+NXPmbHFvoJUr30N8/B4xjsgQnJ1dAAA7d8bguedeqPJZKpTdEOJIP2q1mpNpHpCra3Okp6cBqLqCSvuY5ct0p1AoUVFRe/JcoeBrle7SO8GelJSEyMhIpKSkIDs7G+vWrcOwYcPE8xqNBmvWrMG2bduQn5+Pnj17QqVSoXXr1mLMzZs3sWTJEvz6669QKBQICgrCvHnzYGdnJ8s/ypBiY3dg/fo1CAwcjqFDA8UZ7Pv27cX69WvQq1cfJtnJoITlTWfP/oWgoBH47LMo8ULmww8/EBPuZpwqpDOlUon33vsQYWETsG/fHskXGaEf33vvQ35ZJCKiBqO9srJdO3cUF98Rz1lb26CkpJgrK/XUs2dvAJUzLtPTL+K//43GtWv/oEWLVnjhhVB4e7dGaWmpGEe1e/TRLuLj5OQTSE09jW7dfJCaehrJySeqjSNqSC1bugEA9u3bi/bt3XHnzt3PUhsbG/FYiCPdxcbugEo1T1Jz2dPTCyrVUuZM9NCmTVscOPCbTnGkG1tbG9y+XXtJHVtbm1pjqFKzZk64cSNPpzhTpXdB4KKiInTq1AkLFy6s9vzGjRuxZcsWqFQqfPvtt7CxsUF4eDhKSkrEmFmzZuHcuXPYtGkTNmzYgGPHjuHtt9+u+7/CSGhvHrl589cIC5uMl156CWFhk7F589fcPJKMwr2lXzQajfhTUxzpxsrKqsZj0o9arcahQwewdetWHDp0gJ+fRER60mg0KCkpljxXUlLMOuF1sHlzFACgtLQUjz7aTlIv/NFH26G0tFQSR7U7evSI+DgnJ1uy2aF2qR3tOKKG5O/fX1zZe7/ZwS4urlwNpKfY2B0ID58IH5/Oko1jfXw6Izx8ImJjdxi6iSbDy6u1rHEE3L5dKGscAdbW1rLGGSO9E+yDBg3Cf/7zHwQGBlY5p9FosHnzZrz++usYNmwYvL298d577+H69euIj48HAPz99984cOAA3nnnHXTv3h29e/fG/PnzsXPnTly7du3B/0UGJGweOWPGzGrrs02fHsHNI8nghPqAL74YjvT0NIwaFQhHR0eMGhWI9PR0TJoUJomj2gk314YPH4mUlHMYOTIYXbt2xciRwUhJOYfhw0fy5lodxMbuQN++3SUb8fbt251fuImIdKBWqzF79n8AAMOGBeHddz9AVFQU3n33AwwbFgQAmD37Pxyb9HDx4gVZ4whi6bcxY56s5vpJiTFjnpTEERnSwIGPY8WKDxAZGYkVKz7AwIGPG7pJJkl7YuLGjdE4diwJc+bMwbFjSdi4MZoTE/V0/vx5yXGLFi3xyCOPoEWLljXGUU10HXQ4OOlKl9nr+sQZI1lrsF++fBnZ2dno3//u3VsHBwd0794dJ06cQHBwME6cOAFHR0d07dpVjOnfvz8UCgX+/PPPahP392NsFSyuX6+c8evj0xlmZnfbJ/y3c+fOYpyxtd1U3NunpD8Xl8r6gJmZGUhIOI6jR+/WEezb1x8TJ44T49jPuklMrLy51qpVK7Rv7y4+f+rUKbRv7w5///7IyLiIxMTDrMOuo9jYHQgLm1DthmdhYROwadOXRrl0lO8ZIjIWhw8fRE5ONvz8/LFlyzdQKhXifhahoS/jiSdGIDExAYcPH8TAgYMM3VyT4OnpBaCyXMmePb9VqcUcGDgIqakpYhzVTtgo/tq1q7h48SqioyPFsjuhoeH417/GiHFEhpCQcBg5OdmYN28htmz5Anv33t2/ytOzNebOfRvLli1GQgK/5+tKmJjYp48f2rRpKUmkq1Tz8cQTT4sTE9mntfv9918BAObm5igvL8e1a1ck54XnhTgiQyguLq49SI84YyRrgj07u3IZn7Ozs+R5Z2dn5ORUzobNycmBk5O0po65uTmaNGki/nldOTs7PEBr5dexY2VNq6tXL8Hf3198Xmjn2bMpYpyLi3G13dQY2/97U+Lt3R5AZR3BV199EXPmzEGXLgORkpKCV199Efv27RXj+DrVTVHRLQDAkSOHYWlpiYiICLz88sv4/PPPsWrVKnHVSlHRLfapDrRnXQ4dOhTz5s1Dly5dkJKSgqVLlyI2NhZvvhmBiRPHsXawAX388cdYu3at5Lk2bdrg558rLzxLSkqwYsUK7Nq1C6WlpQgICMDChQvFm3xEVL8OHfodADB79rxqV1bOmjUHzzzzBA4d+p0Jdh35+DwKAPjnn39gZmYm2YS7rKwcWVn/SOKodsJG8YmJR/Dyy5Pwf/83EwEBU3HwYCJefnkSjh5N4EbxZFBC2czw8Ffx+uv/xqZNG8WbQC+9NBmlpSVYtmwxy2vqQeir77//Fq6uzTFnznyMH/8Mtm7dhuXL38EPP3wriaOaqdXlAIDy8nI4O7vAwcEBJSXFsLKyRkFBgbgyXYij2pmZmelUSo/71unuYehTWRPsDS03t8Colgv6+Pj+b1OOxfj882h88cXdGRgvvhiORYuWwMurNXx8fJGTU2Do5poctVqNhIS7O4z7+3OH8boQXqdOTk5ITj4pWXHi6ekFX98euHHjBl+nelAoKuusm5tb4MKFLFhZWcLZ2QGzZs3Fv/89C15eLVFeXgaFwop9qoPff9+P7Oxs+Pn1w9q1G7F48du4fPkS3N29sHbtRmRnP4nExAT89NMuPPbY44ZuroSZ2cN1A7BDhw7YtGmTeKz9mbxs2TL89ttv+Oijj+Dg4IAlS5Zg2rRp+Prrrw3RVKKHTm3fkYXrF2P6Lm3s8vJyAQA3b95At26d8Mwzz6FLFx+kpKRh27ZvcPPmDUkc1U57o/gDB37Dnj13ZwcLq9i4UTwZUosWjwAAIiM/xZYtX0g25Ny4cQMmTAiVxFHtnJwqJ2Q2bdoMJ0+mw8LCHC4uDpg48UWMGzcBnTu3w82bN8Q4qpmbmzuysrIAVJZ5vV+pVzc392qfp6qsrKx0mknN/dZ0Z21tgzt3inSKM1WyJthdXSs3/8jNzUXz5s3F53Nzc+Ht7Q2gsuxEXp60pk55eTlu3bol/nldaTTGdVGgUCihUi1FWNgEeHq2kNydefvtudBoNIiK+hIKhdKo2m0KuMO4fITXaXj4RAwbFoQRI0bBzKwCGo0CFy9eQHz8HkRGbuHrVA9xcbsAAO7uHlAqzcV+02gApdIcrVq549KlC4iL24UhQ3Qvg/WwOnjwAACgoqICrVtLawdGRW1E795+YhxrXxqWUqmsduwuKCjA999/jw8++AD9+vUDUJlwHzVqFJKTk+Hr69vALSV6+AilN957bxn8/ftXKWfy3nvLxTjSjZBA8/fvj4SEw/jkE+kqHuF5Jtr0ExIyBlFRX2LhwrnIzMwQn3dxaY5Fi/hdnwxL2OR06dJFGDZsOEaMGAVADUCJ8+fPY9myxdzkVE+pqacBAG5uraDRaHDo0AHJ+NSypRtu3ryB1NTTePzxIQZurfELChqBY8eO6hRHuikr0222v65xBJ2S6/rEGSNZE+zu7u5wdXXFkSNH4OPjAwC4ffs2Tp48ifHjxwMAevTogfz8fKSkpKBLly4AgISEBFRUVKBbt25yNscg/vgjCUDV5Q/C8R9/JPFLop6EHcaDgkbg008jERDgh4MHE/HRRysRHj4RkZFb2Kd6CgkZgylTpmPDhrWSmndKpTmmTJnO/tTTpUuVm5ldvHgeoaHjMWNGBAIC/JCUlIjVq1eJ54X/Us2EWZVJSYnVnj92LFESR4Zz6dIlBAQEwMrKCr6+vpg5cybc3NyQkpKCsrIyyQqZdu3awc3NrU4JdlP4f809QuTF/nxwAQF3S2+0a+eO4uI74jlraxsUF9+Bq6srAgIGsp911K9fZaItIeEwhg0bjrZt20I70RYfvxuurq7o168/+1RPo0ePwahRwSazWpX/fx9O8fG7Dd2ERiEzs3LSXGpqCtq3d8edO3fHJxsbG/FYiKOaCeXJ5Ioj3cvpsOwOadM7wV5YWIiMjLszCy5fvoy0tDQ0adIEbm5umDRpEj755BN4eXnB3d0dq1evRvPmzTFs2DAAlRfYAwcOxIIFC7Bo0SKUlZVhyZIlCA4ORosWLeT7lxlAaWkpNmxYC1fX5jh+/DT++OOo+AWxV6++6NnzUWzYsA5z5iyApaWloZtrErR3GI+O3gqlUgF7e3v07t0X0dFbERo6HirVfIwcGWy0X8CNUWzsDqxfvwbDhgWhTZs2ACoAKHDhwgWsX78GvXr1YZJdD23btsP+/b9g1KjROHXqJEaNujtL3dPTCyNHBiMubifatm1nwFaaDmGGOgA0a+aE8eNfEJfgb936X3Fnce04anjdunXD8uXL0aZNG2RnZ2PdunV44YUXEBMTg5ycHFhYWMDR0VHyZ5ydnfXeb6Xyz5lO2R1TaqspYH8+mJdeehHvv/8+SktLJM+XlZUCAF588UW0aNHUAC0zTWq1GgpFZWbVysoc3bo9KiaD/vmn8vrIzMwMLi4O/F5aB6Wlpbhw4S/8/fffaNeuHUaOHMZrJjI4YZNToHL/ioqKCvGccJyTk80NOfXQunUbWeMedrrWqmdNe6L6pXeCPSUlBZMmTRKPly+vXF765JNPYsWKFZg8eTLu3LmDt99+G/n5+ejVqxc+//xzSW2iDz74AEuWLEFoaCgUCgWCgoIwf/58Gf45hrVp00ao1WrMmTMfVlZWko2PNBrgzTfnYdasGdi0aSNefXWqoZtrEoQdxjdsiKx2g67p0yMQHBzILzR6EG5adO/ui/T0NOzde3cmhoeHJ7p39+VNCz0tXPgOoqI2Ys+eOLi6NpecKysrE/t44cJ3DNE8kxMXFys+LiwsxPr1H4vHlpZWkrhhw4IatG1016BBdzdF9Pb2Rvfu3TF48GDExcXB2tpa1t9lbHuu3It7hMhP2M/A2P/fGzO1Wo1vvvkWvr49kJOTg8uXM8Vzbm6t4OzsjG+/3YaZM+fy9aqjQ4cO4Pr163j66Wexffv32Llzp3hOqVTi6aefxffff4vY2N38XqonlWoBPvnkY0nycubMmXj99X9DpVpiwJZV72Hbc+Vh9s8/lwFUlro9cSKtyiQ6X18f5ObmiHFUu0mTwrBgwRxYWlrizz//wvLli8X9lubMeRuPPtoOpaWlmDQpzNBNNQlNmzYTHx8/nornn38a169fQ/PmLfDVV9+jZ8/OVeJId0uXvo9589647zHp7/XXZ+CTT1bf99hU6Z1g9/Pzw5kzZ+573szMDDNmzMCMGTPuG9O0aVOsXLlS319t9C5erCz/EBg4strzQs0rIY5qJ9xl9fbuXO15H5/OkjiqnXDTIjMzA0FBI/DZZ1GSsjt79vwMjUbDmxZ6sLGxga9vTyQnH8eVK1mSc8Kxr29PcbMuqtnhwwfFx/fOutQ+1o4jw3N0dETr1q2RkZGB/v37o6ysDPn5+ZJZ7Lm5uXrvtwIY354r2rhHSP0y5v/3xu7IkbuTFHr06IXExLs3gfz8+uP48WMIDg7EkSMc73V19Wrl983vv/+2yjm1Wi0+f/XqVb5u9bBo0QKsW1f1wrqiokJ8fuFC40uy08Ph+PFjAIDnn59Y7SS68eMnYO3aj3D8+DE8++x4A7fWNAh9Wlpaig4dPCTnoqI2SuI4PtXu77/PiY979+4i3qjMy8tD795dqo0j3d2bTGdy/cHdm0xvDMl1AFDUHkK6EpYw7d0bV+35PXt+lsRR7YRNotLTU6s9n5aWKomj2gkJ3yFDhiEq6kuUlJQgJiYGJSUliIr6EkOGDJPEUe3UarVkZmB1Ll/OlNS7p/szN7eQNY4aRmFhITIzM+Hq6oouXbrAwsICR44cEc+fP38eWVlZjWqDU2GPEB+fzoiLi0dBQQHi4uLh49MZ4eETERu7w9BNpIeY9iQFpVKJAQMGYvz48RgwYCCUSiUnKdSB9iq16lZWVhdHNSstLcX69WsA3L9P169fg9LS0gZvGxEAcV+1P/88ibKyMhw6dABbt27FoUMHUFZWhlOnTkriqHYsaSIv7ZWj974OtY/lXmFKRFJMsMvopZcmQ6lUYvnyd1BeLt3soLy8HO++uxRKpTleemmygVpoevz9+8PT0wurV6+ULBkFKme1rFmzCp6erblrux5yc3MAVJaD6devJ8aODcbzzz+PsWOD0a9fT7Rq5S6Jo9odOnQAOTnZ8PPrhwsXriAsbDKCgoIQFjYZFy5cQd++/sjJycahQwcM3VST0KNHL1njqH68++67OHr0KC5fvozjx49j2rRpUCgUCAkJgYODA55++mmsWLECCQkJSElJwdy5c9GjR49Gk2DX3iOkupuVQUEjoFLN5401MhjtSQpqtVqSFFKr1ZykUAdC7XoAGDIkUHJjbciQwGrjqGYbN24QE0CDBw/DK6+8jldeeQWvvPI6Bg+unPSh0WiwceMGQzaTHmLCHkr79/+C9u3dJddO7du747fffpXEUe2aNXMCANjZ2cHNrZXknJtbK9jZ2UniqGZt27YXH9eUYNeOo5qZ6biTta5x9HDQu0QM3Z+lpSVee20a1q1bje7dvfHWW/Mwbty/8PXX32HFiqXIzr6OqVNncLMePSiVSqhUSxEePhGhoeMxY0YEAgL8kJSUiNWrV2HPnp8RGbmFtUP14OzsAgD44otIBAWNwKefRoolYj788ANs3rxJEke1ExLns2fPhZ2dHd59d6Vk6ejs2XPwr389gUOHDuCxxx43bGNNQHFxkaxxVD+uXr2KiIgI3Lx5E05OTujVqxe+/fZbODlVXgzNnTv3f3tlTEdpaSkCAgKwcOFCA7daPkK5rYkTX0K/fj2rlIiZMOFF7N4dx3JbZDDCJIW5c99Abm4uMjMzxHMeHp5wdnbmJAU9fffd1+JjhcIMGo1G/BE2PxXitBPudH+7dsUAqCwh+uuv8di3b494TqFQoGnTprh58yZ27YrB1KnTDdVMeoi99NJkLFw4DxUVFaioqD55qVAoOIlOD+npaQAqVz/26zcAISGjAVQAUOD8+fOIj98jxg0ePNRwDTURw4ePwBdffK5THOnG3NwcZWVlOsURCfhqkJlQH3DDhrWYOXMGZs6srEWvVJpj6tQZrB9YByEhYxAZuQUq1TyMGnX3YsXTszUiI7ewxq2e7p2ppn1xWFMc3Z/2jWu1Wl2lzq3QtbzBrZvU1OpLQtU1jurHhx9+WON5KysrLFy4sFEl1bUJy5aXLlVh+PCRkpuVH320EsuWLZLEETU0pVKJ0aPHYt261XB1bY5Vq1Zj3Lhn8PXX27B8+VIkJ5/A1KkzOElBD5mZleXgnn12PBISDlf5XvrMM+OwbdvXYhzVrqCgAABw8+bNKucqKirE54U4ooamVCphZ2ePgoL8KqtThNJFdnb2/CzVw8WL58XH+/btRXz83etQ7RnB2nF0f7m5ubLGEXRKrusTRw8HJtjrQa9efdCypZukJnPLli3Rq1cfA7bKtIWEjMHIkcFVEpf8IqM/IZHeoUNHpKWlSi4OPTy80KFDR5w9+xfrCOphwIDHsGrV+3jrrZkoLi6uMktQqHc3YMBjhmqiSblzR7eZ6brGEdUHocayn58/oqK+RFJSAmJiYmBr2wRRUV/iqaeCkZiYwFrMdVDdjUqO9/pTq9WIidkOX98eyM3NRUTEDEREVE788PDwgq9vD8TE/IT581XsXx15eHgiMTEBf/6ZjIMHkxAdHYlr1/5BixatEBoajsDAQWIc6aZTJ29xryVzc3OMGfMkAgL64eDBI9ix40ex7GanTt6GbCY9xBISDqOgIB8AqilZWnm9VFCQzxVretBOopuZmUmuO7WPWX5DN8KmsbrEcSNeovrDBLvMhA3PgoJGYOPGTZLZbOHhEznj+gEIG3Rpl94g/eXkZAMAzp07i8DA4Zg6dTpcXZshO/sGfvklHnv37pbEUe369w+Ag4Mjzp79C87OLpgyZRoefdQHp0+n4ZtvvkZmZgYcHR3Rv3+AoZtqEmxsbGSNI6oPwsVfXl4e/P17VLmxZmVlLYkj3cTG7oBKNa9KyR2Vaim/P+lJKGO0YUMkevToVeWmxfHjxxAcHMikkB6effZ5fPfdt0hPT0PHjl4oLr4jnlu6dLF4/OyzzxuqiSbHxsZWfKxWq/HDD9vwww/bAEiTa9pxRA3pypUsAMDQoYH4/PPNWLLkbVy+fAnu7l5YsGAxXn55Evbt2yvGUe26dfMFUPke//vvf3Dy5HFxfOrevSfatnWDRqMR46hmd+7cHYuSkk7hqaeCkZeXCycnZ/zww0706dO1ShzpLiLiTaxa9e59j0l/ISFPIDb2p/semypucioj7Q3PoqO3onfvvrC3t0fv3n0RHb2VG56RURBKv8yd+zbS09Pw1luzEB4ejrfemoX09HTMmbNAEke6sbKyAgDk5eVi/fq1mDp1KtavX4u8vMqleJaWVoZsnkkREpNyxRHVB+Em5Nmzf6G4uBirVq1GVlYWVq1ajeLiYpw795ckjmonTFLw8eks2TzSx6czwsMnIjZ2h6GbaFKE8kTe3p3FSQrjx4/HgAEDoVQq4ePTWRJHtRs4cJB4c1c7ua59bGNjg4EDBzV420xVcvIf4uOaNufTjqOG9emnn+Lpp59Gjx490K9fP0yZMgXnz0tLd0ycOBGdOnWS/Lz99tsGarG8cnNzAFTePB80yB9RURuxZ88eREVtxKBB/mjVykMSR7X7889kAJXv8b59u+Pvv89h0KBB+Pvvc+jbt7v43hfiqGYnTtz9fOzTpysyMzNQWFiIzMwMMbl+bxzp7t5kOpPrD+7eZHpjSK4DTLDLSpgpNGPGTJSXl2PDhnX497//jQ0b1qG8vBzTp0cgI+MiEhIOG7qp9BATNj07duwojhw5ju3bd+Krr77C9u07ceTIH/jjjyRueqanhITDYhLtfheHOTnZfO/rqEWLFrLGEdUHFxdXAJXltqysrBARMQNubm6IiJgBa2trdOjQURJHNeMkBfkJN8rT01OhVqtx6NABbN26FYcOHYBarUZaWqokjnRjZ2f/QOdJ6t6a1g8aR/I7evQoXnjhBXz77bfYtGkTysvLER4ejqIiaam+Z599FgcPHhR/Zs+ebaAWy8vZ2QUA8MUXkfD29pHcAPb29sHmzVGSOKqdcH3k7u6BvLxczJw5A61atcLMmTOQl5cHd3cPSRwRkSlgiRgZCTOAfvzxe4wePVxyEahSzUdY2CuSONIPa7LKQ6lUQqVaivDwiQgLm4AZMyIwZEgIDh5MRFjYBOzZ8zMiI7ewb/WgvSTUysoKJSUl1R5z6ahubt26KWscUX3QLl1w7wWgdo1W1g/VjXY5E4VCOv9DoVBg+vQIljPRk3BDfe7cN5Cbm1uljJGzszNvqOtJuKE+b95CREdHSfZb8vDwxMSJL2LZssV8neqhbdv2+PvvvwEAzZu3wPXr18RzLVo8Il43tW3b3iDtIyAyMlJyvGLFCvTr1w+nT59Gnz539xiztraGq2vju6l8701IjUYj/tQUR/fXtm07AMDly5kIDByO1q3bwMysAhqNAhcvXhBLlgpxVDN3dw+kp6fpFEdE9YcJdhkJg+rGjZ/A1dUVzzwzDl26+CAlJQ3btn2NjRs/kcSR7liTVV4hIWMQGbkFCxfOlWxy6unpxX0C6iA7+24JiEGDBuM//5kl7r/w4YcfYM+en6vE0f0VF5fUHqRHHFF9yM6+DqCyRMy9CeErV7LEJLsQRzXTLmdSHZYz0Z9SqcTo0WOxbt1quLo2x6pVazBu3L/w9dffYfnyd5CcfAJTp87gDXU9CK+/Vq08qr15JiQv+DrVXadOPmIyTTu5Dkj7sVMnnwZtF91fQUEBAKBJkyaS52NiYrBjxw64urpi8ODBmDJlSp32yzG++9KVifQOHToiLS21yrVThw4dcfbsXwA0Rth24xQWNhkq1XzY2dkhLS1V/AwAKm9WOjo6oqioCGFhk9mnOtB1or9GY4zvL9PHPpWfMfWpPm1hgl1GPXv2BgCYm5vD0tIK69d/LJ5r1cod5ubmKC8vF+NIN9obx376aSQ3jq1HXIZXN0LNxSZNmuKTTyKxdKkKH374HtzdvfDJJ5Ho2fNR3Lp1k7UZdaRQ6DaK6RpHVB9cXZuLjy0trST1mK2srMSNpLTj6P60y5n07t23ynmWM9GfWq1GTMx2+Pr2QG5uLiIipiMiYjqAyqSQr28PxMT8hPnzVUyy60h4/U2dOhmBgcMxbdoMcaP4ffv2YurUVyRxVDsLCwtZ46h+VVRUYNmyZejZsyc6duwoPh8SEgI3Nzc0b94cZ86cwQcffIALFy5g7dq1ev8OZ2cHOZv8wEpKbgMAzp07i5EjR6Jv3z64ceMGmjVrhjt37iAuLk6Mc3ExrrYbs4iICLz//vuwtrbG448/joqKCigUCqSmpiI/Px9vvPEG3NycDd1Mk5CVlSk5trW1hUajgZmZmaSUU1ZWJl+j9YB9Kj9T7VMm2GUk1F8rLy+vUgpCezbb5s1RePXVqQ3ePlN0b01WpVIhqckaGjoeKtV8jBwZzItDPWjftPjssyjetHhAWVn/AKgsWdKuXSvJuaiojVXiqGb9+wfg5MlkneKIDEUoA9e0aTOcOvUX/vjjqFjCrFevvujatSNu3rzBmuE6EsqZrF69UhzvBRUVFVizZhXLmehJu+xOjx69qpTZO378GMvu6KlPHz8olUrY2dkhJeWUuEINANzcWsHBwQGFhUXo08fPgK00Lb6+vrLGUf1atGgRzp49i6+++kry/HPPPSc+7tSpE1xdXfHiiy8iIyMDnp6eev2O3NwCnWfkNgRb28qZ+v7+/bBr164q5/39+yEh4QhsbZsgJ6egoZtnsmbPXoC9e/chOfk4rl+Xrvbz9e2J2bMXsD91lJGRITm+d38E7Tj2qfzYp/Izpj41M9P9xi83OZXR+fN/i4+166/ee6wdRzXT3jj2fjVZuXGsfriRnPzc3d1ljXvYFRXdqT1Ijzii+pCQcAhA5Y218PCJSE9Pw507d5Cenobw8IniHgFCHNVM2B9kz56fERo6HklJiSgoKEBSUiJCQ8djz56foVK9w5vpemDZHfklJSVCrVYjPz+/yk3zrKx/kJ+fD7W6HElJiQZqoelZu3aNrHFUfxYvXoz9+/cjOjoajzxS8yqN7t27AwAuXbpUY1x1NBrj+vHz6w8HB0ccOXIYzs4ueOKJJ/HSSy/hiSeehLOzCxISjsDBwRF+fv0N3lZT+lGpFiA5+TicnV0wYEAABg0ahAEDAuDs7ILk5ONQqRYYvI2m8qNU6jZvVqk0N3hbTeXHlD+zjPXHlPtUV5zBLqOKCt16Xtc4kl4cVrfJKS8O9ceN5OTXt28/8bG1tTWKi4urPdaOo/srLS2VNY6oPghftsaMeRKxsT9JZrIqleYYPXosduz4Ue8vlA8zYX8QlWrePTVuW3NlVR0IZUoiIz/Fli1fVNnHZsKEUEkc1U7Xzcq5qbnutDfflSOO5KfRaLBkyRLs3bsXW7ZsgYdH7RslpqVVbrjYGDY9VavVKCysLBNz+/Zt/PTTj+I5KytrAEBh4W2o1WreBNZRaWkpNmxYC0dHR9jY2ODQoYPiOXd3Dzg6OmLDhnWYM2cBLC0tDdhS09C8eXPk59/SKY6I6g8T7DJycNBt2YCuccSLw/rAGW3y27MnTnx8b9JX+3jPnjgMGxbUYO0yVQcO7Jc1jqg+DBgwEB9++D5++ukHBAYOx7BhgXB1dUJ2dh7i4/dix44fxTjSXUjIGIwcGVzlhjqTFvrz9+8PFxdXLF26CIGBIzB16nSxXnh8/F4sW7YYLi6uLLujh6tX7343Gjo0CO3atQOgBqDE33//jX379lSJo5rduHFDfGxlZYWSkhKtY2uUlBRXiaOGtWjRIsTGxmL9+vWws7NDdnY2gMprWmtra2RkZCAmJgaDBg1C06ZNcebMGSxfvhx9+vSBt7e3gVv/4DZt2iiuRr93c2PhuKKiAps2bWQZWB1t2rRRXA3k59cPo0YFA6gAoMCFCxfETU/Zp7phHorIODDBLqOsrMviYzMzM/zrX89i7ty3sGzZCnz33bcQNpDUjqOaaV8c3rvJ6YcffsCLwzrgRnLyu3DhvPi4pvJQ2nF0f7du1T4DQ584ovrg798fCoVCfI937dpdHJ/i4/cCqFwVxPGJjMGBA/uxd+/dVRbW1tYGa4spS0k5CaCy/86cSRMT6kDlrEth1ZoQR7XT/p6knVyvPC6uNo4a1tatWwEAEydOlDy/fPlyPPXUU7CwsMCRI0ewefNmFBUVoWXLlggKCsKUKVMM0VzZCeVdO3fugvz8W7h8+e6Gki4uLnB0bILU1BSWgdWD0Ffu7h745Zd4SWlSpVIJd3cPXL6cyT7V0T//6LbPl65xRFQ3TLDL6Nq1u5tzKBQKbNv2DbZt+wZA5UAhDBzacaQfjUYj/lDdcCM5+dna2soa97CzsrJCgda+Jm5ubrC1tUVRURGysrIkcUSGkpSUKCZ8Dh78XZxtBQA2NjYAKj9Tk5ISOYtdD7GxO6BSzauyYk2lWsoSMXpKSDiMnJzKmab3m3WZk5PNknB6+OefykkyxcXFkiQbAMmxEEe1s7S0QFlZ7SXfLC0tGqA1VJ0zZ87UeL5ly5b48ssvG6g1DU/4vExNTcHw4SOxceMm8Yb6Rx+txO7dcZI4qp3QV5cvZ8LVtTnmzJmP8eOfwdat27B8+Tvi5yn7VDe5uTmyxhFR3XCTUxkJFzEAMHjwMEye/BpeeeUVTJ78GgYPHlZtHNVMuDicN0+F9PQ0jBoVCEdHR4waFYj09HTMnbtQvDgk3XAjOfkNHz5K1riHnbu7tLZnVlYWzp07J0muVxdH1JCEMlrr138OFxdpjVkXl+ZYt26jJI5qFxu7A+HhE+Ht7YN3312JqKgovPvuSnh7+yA8fCJiY3cYuokmRagDPnRoIFJTzyMsbDKCgoIQFjYZqannMXRooCSOateq1d3Nyu930+LeOKrZ448PkTWOSG6+vj0BAJaWlvjkk0gcO5aEOXPm4NixJHzySSQsLCwlcVS7bt18AVR+bh45chyFhYVYunQpCgsLceTIcfHzVIijmmmPPxs3RkvOaR/zhkXdeHt3q/GY9GdtbVvjsaniDHYZNWnSRHx88ODviI+vOpvt3jiqmZCYCA9/BdOmzahSk/XOnSIsW7aICQw9cSM5ebGkibxu3bopObazs0fr1l64ePGSuMlUdXFEDUkoo9W6dWscOXIcmzZtxLVr/6BFi1Z46aXJOHnyhCSOaqZWq6FSzUP37r5ITT0t2TTW3d0D3bv7QqWaj5Ejg3kDWEfCTDW1ugLt27tLluBHR0chIGCQJI5q17lzF/zww3cAUGU1pfZx585dGrRdpoy1g8nYCd83S0tL0batm+TcggVzqsRR7f78MxlA5edm+/bSG5Laffrnn8l4/nlpaSKqSjtxPnlyqOSc9jET7HWTnv5njcekv+LiohqPTRUT7DIaOTIYR48mAACKi+9Izt25c0cSR7rRrhfeo0evKudZL7zuQkLGIChoRJWkEHdq19+1a1dkjXvYCTOBBIWFt3H69Ola44gaklBua+7cN5Cbm4vMzAzx3GeffQJnZ2eW29JDQsJhZGRckpSGEVy+nCkuF2c5E905O7sAAPbv3wcXF1fMnbtAXIK/bNkS/PbbL5I4ql16eqqscQTs2xcvaxyR3HT9jORnqe50LffKsrC68fRsjb//PqtTHBHVH5aIkdHkya/XelfQzMwMkye/3kAtMn3aCYy+fbtj7NhgPP/88xg7Nhh9+3bHvHmzmcCoo9jYHejfvxcWLJiDtWvXYsGCOejfvxeX4NfBzz/vkjXuYcfZbGQKlEolRo8ei+TkEyguvoMpU6Zh3bp1mDJlGoqL7yA5+QRGj36Cs611pF2mxMXFFatWrcGVK1ewatUaSQkeljPRnXayp0ePXvD29oGdnR28vX0kkxaYFNIdV6zJr7i4uPYgPeKI5Kb9GXnv/j/ax/ws1Z2Hh5escQ+7Fi2ayxpHRHXDBLuMlEolzM1r3oDH3NyCF9t60E5g3LkjTWDcucMERl0JdW59fDojLi4eBQUFiIuLh49PZ9a5rYP8/HxZ4x523DSWTIFarUZMzHa0bt0GeXl5WL9+LaZOnYr169ciLy8PrVu3QUzMT5KyHHR/Qqk3e3sHJCYmIyXlFEJDQ5GScgqJicmwt3eQxFHt0tPTAFSW2ElLO33PPjap4j4WQhzV7vr167LGEWBraydrHJHctD8ja9p7gZ+lutO1VAlLmujm8OFDssYRUd2wRIyMfv99P8rKSmuMKSsrxe+/78fgwUMbqFWmTTuBkZFxCevXrxXPKRQKMYExf76KSXYdCXVug4JGIDp6K5RKBezt7dG7d19ER29FaOh41rnVk729vU6bF9vb2zdAa0wfL7bJFAglTczMzDB0aCCsra1RVHQbtrb2KC4uxr59e6HRaFjSREenTwv1LDVo166V5FxU1EbY2dnfE0e1yci4CABieR1t2iWNhDiqnVCaUK44AvLzb8oaRyS3ixfPi4/vXUmhfawdRzU7f/6c1pEZAE21x9I4IiLjxhnsMtq69UvxsYWFdCa79rF2HNVMSGBcvHih2s2kLl68gIyMi0hIOGygFpoeoU9nzJgJhUL6EaBQKDB9egT7VE8tW7rVHqRH3MOuTZs2ssYR1QehVImHhyd+/XUfYmN34JdffkFs7A78+us+eHh4SuKoZkVFlXvV3L59G2ZmZnjmmeeQnJyMZ555DmZmZuIGx0Ic1a51a90+I3WNI6C0tETWOKrcOFLOOCK5cba1/M6cSRcf39tt2sfacURExo4Jdhn98UeS+LisrExyTvtYO45qpp2YcHZ2wZQp/8b69esxZcq/JXXumMDQnbC83tu7c7XnfXw6S+Kodnl5ebLGPexOnz4laxxRfcjNzQEAZGRcQtOmzTBgQAAGDRqEAQMC0LRpM3GzTiGOaqZdE7xVK3ds2/YNfH19sW3bN2Ipk3vjqGbjxk0QHz/++BCEhIzBkCGV/3388SHVxhE1tHsnezxoHJHctK+ZUlPPY8mS5Zg2bRqWLFmO1NTz1cZRzYTa9ebm5khJOYe+ff3h4eGBvn39kZJyDubm5pI40t3bb79T4zER1R+WiJFReXm5rHEEXL1ameS1traGtbU11q//WDzn7u4Ba2trFBcXi3FUuxYtHgEApKenokePXkhMPIyioluwtW0CP7/+4rJmIY5qp1br9p7WNe5hl5Wl2w0zXeOI6kOTJk0BVM5Yy83NwaFDByXnzczMoNFoxDiqWUbG3ZIl95Y0kZYzyQDpZulSlfh4//5faox7770PG6BFRFUxwU7GLirqM/Fxly7tUVFRIR4vXDhPEvfii+EN2jZTJZTNLC8vx6OPthOfz8zMlByzvKb+Fi+eX+MxEdUfJthl5OTkrNNMaicn5wZoTeMgzFAtLi6uUuM6JydbrHvHmay68/fvD09PL8yd+wZyc3MliQsPD084OzvD07M1/P37G7CVpkWX+uv6xD3s/v5bt3qLusYR1Yfk5OMAUKV8mUB4Pjn5OMaNe6HB2mWqMjMvyRpHd1+jcsUR1YeSEt1Kv+gaRyS3/Px88bF2cv3eY+04qtnIkSGIi9upUxwRkalggl1GTZo0kTWOINZcBWreVEY7jmqmVCoxevRYrFu3Gi4urhgwIABKpQJqdQXOnDmD5OQTmDp1Bjc41cPt27q9/nSNIyLjd28puAeNe9h5eHgBqFwuXt1KP+F5IY5q5+joKD5u1swJnTt3hrm5EuXlaqSmpuLGjbwqcUQNr/qblHWPI5JXq1buyMr6R6c40k3z5s1ljSMiMgZcayej7GzdZqfqGkf3limR7oCivZEMy5noTq1WIyZmOxwdHZGTk41Dhw7i999/x6FDB5GTkw1HR0fExPwEtVpt6KYSERmt06dPyxr3sGvdujWA+5fRE54X4qh27u6e4mMrKyscOnQQv/32Gw4dOiipa6sdR0REUjNmzBQf31uqSPtYO45qFhe3S9Y4IiJjwAS7jG7fLpA1jgAHh7uzqpydnTFlyr+xbt06TJnyb0mpHe04qllCwmFkZFy67zLG/Px8ZGRcRELC4QZumekyN7eQNY6IjJ8us9n0iXvYsT/ld+7cWfHx1atXJOe0j7XjiIhI6ubNG+LjiooK2NrawsHBAba2tpISMdpxVLOLF+9uDjto0GA0b94CdnZ2aN68BQYNGlxtHBGRsWOJGBnl5eXJGkeQ1LS/cSNPssmp9owBXWrfUyXtmuvDhg3HzJlvICDADwcPJmLlyvcRH7+7ShzVrKSkuPYgPeKIyPhpNNI6rC4uLnB0dER+fj5ycnLuG0fVc3NrJWscAffZHqDOcURED6Pjx49JjouKiu4b9+yz4xuiSSbP1tYWAGBnZ4fffvtVfL6wsBDXr1+DnZ0dCgsLxTgiIlPAGewyKi/Xrc6qrnEEmJlVXvU5OTlLSsJUnjMTZ7ELcVS7nTtjAAAtW7rhyy+/Qe/efWFvb4/evfviyy+/wSOPuEniqHb32+SwrnFEZPzu3aciJycH58+flyTXq4uj6l24cAFA5djevHkLybnmzVuI3wGEOKpd69a61avXNY6I6GF07/d3hUIBMzOzKuVi+D1fd8LmpYWFhQCA7t174LnnnkP37j0kz3OTUyIyJZzBLiOFQqFT3ep7B2O6P3f3you+vLzcKufUarX4vBBHtRNm+7u6uiI7OxujRg1FXl4unJycsWvXPri6uuLq1SyuCiAiqoFw8SdX3MPu1KmTACoTFNevX5Oc0z4W4qh2x44dqz1IjzgCLC0tUVpaqlMcETUOLVu2lBwLZWHuTajfG0f35+zsJDk+efIETp48UWscEZExY6ZXRmVlus1M1zWOgIEDB8kaR0CzZs0AAH/+eRJdu3ZAZmYGCgsLkZmZga5dO4jJCyGOiIiq0vVmOW+q66ZJkyayxhGQn39L1jji+57oYbRnz25Z4wjYvDla1jgiImPAb39k1Hr37itrHAGvvz5N1jgiooeRrrOqOPtKN35+/cTH1ZWEqy6Oata0qW43ynWNI6C4WLe9VHSNIyLjl5l5SdY4AjIyLsoaR0RkDJhgJ6P2xReRssYR0LlzV1njiIgeRmq1bpuX6hr3sNu9e5f4+N5l99rH2nFUM85gJyJ6cLdv35Y1jgAHB0dZ44iIjAET7GTUjhw5KGscAcHBw2SNIyJ6GFlY6FZjWde4h11eXp6scQQUFelW/1/XOCKih5Gtra34+Ndfj8DDwxN2dnbw8PDEr78eqTaOahYUNFJ8HBOzF02aNIW5uTmaNGmKmJi91caRbrp08a3xmIjqDxPsZNQ4Y0B+V69ekTWOiOhhxH1X5GVldfdGhFKplJzTPtaOo5qVlur22tM1jojoYVRQUCA+Hjy4n2T/qsGD+1UbRzX74IPl4uPRowNx69ZNlJeX49atmxg9OrDaONJNSkpyjcdEVH+YYCejptHcXVpfU01W7TiqWUWFpvYgPeKIiB5Gly5dkDXuYXflylXxsVqtlpzTPtaOo5qVlZXKGkdE9DCytLSSNY6AkhLdxh1d44iIjAET7GTUsrNzxMc11WTVjqOaWViYyxpHRPQwqqjQ7caurnEPu5IS3TaF1DWOiIhIDq6uLWSNI91Xo3HVGhGZEibYyaixfqj8Skt1mwmgaxwR0cPJrPYQveIebtzwTH5mZrp9zdc1jojoYeTgYCdrHAHW1jayxhERGQN+oyajZmVlLWsccdYlEZE8dC2jxXJbumjdurWscQQolbp9zdc1jojoYZSVlSVrHAG3b+tWr17XOCIiY8Bv1GTUsrOvyRpHRERExqe4WLfSL7rGEVBeXi5rHBHRwyg7+7qsccTxiYgaJybYyajl5+t211rXOCIiIjI+bm6tZI0jIiKSg/a+XytXrpWc0z6+d78w0s2sWW/WeExEZCqYYCcjxyX4REREjd3Zs+dkjSMiIpLbzJnTajwm/X3wwbs1HhMRmQom2ImIiIjIoDIzL8oaR0RERERE1FCYYCciIiIiIiIiIiIiqgMm2ImIiIiIiIiIiIiI6oAJdiIiIiIiIiIiIiKiOmCCnYiIiIiIiIiIiIioDphgJyIiIiIiIiIiIiKqAybYiYiIiIiIiIiIiIjqgAl2IiIiIiIiIiIiIqI6YIKdiIiIiIiIiIiIiKgOmGAnIiIiIiIiIiIiIqoDJtiJiIiIiIiIiIiIiOqACXYiIiIiIiIiIiIiojpggp2IiIiIiIiIiIiIqA6YYCciIiIiIiIiIiIiqgMm2ImIiIiIiIiIiIiI6oAJdiIiIiIiIiIiIiKiOmCCnYiIiIiIiIiIiIioDphgJyIiIiIiIiIiIiKqAybYiYiIiIiIiIiIiIjqwNzQDTBVFy9eQH7+rTr/+T//TK7ynKNjE7Ru3eYBWmXa2KdERERERERERFTfHjQHBTAPda/66FNT6U+DJtj/+9//IjIyEtnZ2fD29saCBQvQrVs3QzZJJ7m5ufD374GKioo6/x3Dhj1W5TmlUomUlHNwdnZ+kOaZJPYpEVHjZarjPREREemO4z0RmQo5clAA81Da6qtPTaU/DZZg37VrF5YvX45Fixahe/fuiI6ORnh4OH7++Wej7zRnZ2ckJJyoclemujfW/cTH/17lOUfHJkb/b68v7FMiosbJlMd7IiIi0g3HeyIyFhk37qCotLyWKEt8FXsQBQX5942Y/NyI+57b+M3P9z3n4OCI7HJLZF8rqLEFtpbm8GxmU0s7jYMh+1TX/gQM26cGS7Bv2rQJzz77LJ5++mkAwKJFi7B//358//33eOWVVwzSJt1eMP9j4wJLG5c6/y7LFu2qPFcMIF2HFwxgOm9EU+lTU+lPfTzMS3PqC/tUfiwN1fgZ43hPRERE8uJ4T0TGIOPGHTwdlSTL3+X1ZiwuvRtS7fPvHK8pz5X3v5/afR/Wx+hzUYbvU937EzBcnxokwV5aWorTp0/j1VdfFZ9TKBTo378/Tpw4ofPfY2YmX5sybtzBvzbvg5m5bgnu8oJcVJQUSZ5rNXU+cmM/qPXPOofMwvgNn1R5XmFlC3MH3e7ua8od8H3oUKN+I5pSn5pCfwriz/2N8zev1hhTePsWPnwjDBpN1aU51l7WOv+ukPAgybHCTIH/ez8KdvZNav2zbZs+gmHtq970MDa69CfAPtXHg/bpg/QnoHuf1kd/yjkuNQbGON4LdH2dXjxzCgU3q36h0+d1On39MsmxQ1MntO7UVac/29je90D1ffog/QmwTw3Zp6bSnwDf93J70NcoYLp9yvFeSq7xHpC/bw35vgca32fpw/y+ry+mMt4DptGnd8rKYWaej1FdbdCqSc19cyP7KoqLCmv+CzesBcwAM4UCmooKQAMAF2v8I9a2dmjm+kiNMTmFpfjxRGFle418TDF0n+rSn0D99Kk+f49BEuw3btyAWq2uslTM2dkZ58+f1/nvcXZ2kK1Nl++oYdE0EVau+x7o72nWp70OUdsf6HcAQEn2UDRrNgYuLnYP/HfVF1PqU1PoTwA4d/023j74hU592k7Vtl7asKPwHaCWz0sAKEkZit5t30H75vb10g456NOfAPtUF6bUp6bQn6bOGMd7QM/XadP//dyj/SJdxqZKKYit8tyRa7r9WVN4ner7vq+uTx+0PwH2qaH61BT6E+D7Xm5yvEYB9mljIdd4X/ln5BvzjeF9DzSez1K+7+VnSuM9YBp9av+/PNTvxfsqSxrURvf7E7qrAKBDn1o0HYpWLYw/D2XwPtWxPwHD9qlBNzl9ULm5BdBo5Pm73G2UeH9IGArLn9QpPvvaFRTVcFdm8ZvTqjz39rtra/w7bW3t4NqipU6//5HezeGACuTk6DY73BBMqU9NoT8BoKkCWBzwIs7fvH/dKkFNdw4/Xz6r1j//8pyqKwd0vXMIAG07PYKmCo1R96k+/QmwT3UhV5/WtT8B3fu0PvrTzEz+ZDDJO94D+r1O7zf7avumj3T+fWNf+j/JsV4zhRrh+766Pn2Q/gTYp4bsU1PoT4Dve7nJ8RoFTLdPOd7XHznHfDne94Bur9PqPkeBxvVZKtf7Hqh7n/KztO6fpQ/6GgVMo0/1yUPVloPSZmtrhaKiEh1jG1ceytB9ashcqT7jvZlGI+clq25KS0vh6+uLNWvWYNiwYeLzb775JvLz8/HJJ1VLfVQnJ0feC265mZkBLi4ORt9OU8I+lU/z5o73PXf9+v03paD7Y5/KyxT7U/iMokqNfbyv6TUqMNbXqjFif8qPfSo/9qn8TLFPOd5LyTXeA6Y55hvb69NUsE/lxf6sP8xDyc9U+lSf8V5Rz22plqWlJR599FEcOXJEfK6iogJHjhxBjx49DNEkoofO/QZZDr51xz6VF/vT9DX28b621yJfq/phf8qPfSo/9qn82Kemr7GP9wC/l9YH9qm82J9EhmWwEjEvvfQS3nzzTXTp0gXdunVDdHQ07ty5g6eeespQTSJ66Fy/nm8ydw5NBftUXuxP09fYx/vr1/OrnTHEi5m6YX/Kj30qP/ap/Ninpq+xj/cAv5fWB/apvNifRIZjsAT7qFGjkJeXhzVr1iA7Oxs+Pj74/PPP4eLiYqgmERERkcwehvGeFzPyYn/Kj30qP/ap/Ninpu1hGO+JiIjux6CbnE6YMAETJkwwZBOIiIionnG8JyIiavw43hMR0cPKIDXYiYiIiIiIiIiIiIhMHRPsRERERERERERERER1wAQ7EREREREREREREVEdMMFORERERERERERERFQHTLATEREREREREREREdUBE+xERERERERERERERHXABDsRERERERERERERUR0wwU5EREREREREREREVAdMsBMRERERERERERER1QET7EREREREREREREREdcAEOxERERERERERERFRHTDBTkRERERERERERERUB0ywExERERERERERERHVgbmhG/AgzMwM3YKaCe0z9naaEvap/Nin8mOfysuU+tMU2miKTKFfTel1agrYn/Jjn8qPfSo/U+lTY2+fKTP2vjWV16gpYZ/Ki/0pP/ap/EylT/Vpn5lGo9HUX1OIiIiIiIiIiIiIiBonloghIiIiIiIiIiIiIqoDJtiJiIiIiIiIiIiIiOqACXYiIiIiIiIiIiIiojpggp2IiIiIiIiIiIiIqA6YYCciIiIiIiIiIiIiqgMm2ImIiIiIiIiIiIiI6oAJdiIiIiIiIiIiIiKiOmCCnYiIiIiIiIiIiIioDphgJyIiIiIiIiIiIiKqAybYiYiIiIiIiIiIiIjqgAn2epKUlITXXnsNAQEB6NSpE+Lj4w3dJJP26aef4umnn0aPHj3Qr18/TJkyBefPnzd0s0zaV199hdGjR6Nnz57o2bMnnnvuOfz222+Gblaj8dlnn6FTp05YunSpoZtisj7++GN06tRJ8jNixAhDN4tIguO9vDjey4/jff3ieP/gON6TKeB4Ly+O9/LjeF+/ON4/uMY+3psbugGNVVFRETp16oSnn34a06ZNM3RzTN7Ro0fxwgsvoGvXrlCr1Vi1ahXCw8Oxc+dO2NraGrp5JumRRx7BrFmz4OXlBY1Gg+3bt2Pq1Kn48ccf0aFDB0M3z6T9+eef+Prrr9GpUydDN8XkdejQAZs2bRKPlUqlAVtDVBXHe3lxvJcfx/v6w/FePhzvydhxvJcXx3v5cbyvPxzv5dOYx3sm2OvJoEGDMGjQIEM3o9GIjIyUHK9YsQL9+vXD6dOn0adPHwO1yrQNGTJEcvyf//wHW7duRXJyMgfgB1BYWIg33ngD77zzDj755BNDN8fkKZVKuLq6GroZRPfF8V5eHO/lx/G+fnC8lxfHezJ2HO/lxfFefhzv6wfHe3k15vGeJWLIJBUUFAAAmjRpYuCWNA5qtRo7d+5EUVERevToYejmmLTFixdj0KBB6N+/v6Gb0ihcunQJAQEBGDp0KGbOnImsrCxDN4mIGhDHe3lxvJcPx3t5cbwnerhxvJcXx3v5cLyXV2Me7zmDnUxORUUFli1bhp49e6Jjx46Gbo5JO3PmDMaNG4eSkhLY2tpi3bp1aN++vaGbZbJ27tyJ1NRUfPfdd4ZuSqPQrVs3LF++HG3atEF2djbWrVuHF154ATExMbC3tzd084ionnG8lw/He3lxvJcXx3uihxvHe/lwvJcXx3t5Nfbxngl2MjmLFi3C2bNn8dVXXxm6KSavTZs22L59OwoKCrB79268+eab+PLLLzkI18GVK1ewdOlSREVFwcrKytDNaRS0l+F6e3uje/fuGDx4MOLi4vDMM88YsGVE1BA43suH4718ON7Lj+M90cON4718ON7Lh+O9/Br7eM8EO5mUxYsXY//+/fjyyy/xyCOPGLo5Js/S0hJeXl4AgC5duuDUqVPYvHkzFi9ebOCWmZ7Tp08jNzcXTz31lPicWq1GUlIS/vvf/+LUqVONagMPQ3B0dETr1q2RkZFh6KYQUT3jeC8vjvfy4Xhf/zjeEz08ON7Li+O9fDje17/GNt4zwU4mQaPRYMmSJdi7dy+2bNkCDw8PQzepUaqoqEBpaamhm2GS/P39ERMTI3luzpw5aNu2LSZPnszBVwaFhYXIzMxstJuiEBHH+4bC8b7uON7XP473RI0fx/uGwfG+7jje17/GNt4zwV5PCgsLJXdhLl++jLS0NDRp0gRubm4GbJlpWrRoEWJjY7F+/XrY2dkhOzsbAODg4ABra2sDt840rVy5Eo899hhatmyJwsJCxMbG4ujRo1V2dCfd2NvbV6kZaGtri6ZNm7KWYB29++67GDx4MNzc3HD9+nV8/PHHUCgUCAkJMXTTiEQc7+XF8V5+HO/lxfFefhzvyRRwvJcXx3v5cbyXF8d7+TX28Z4J9nqSkpKCSZMmicfLly8HADz55JNYsWKFoZplsrZu3QoAmDhxouT55cuXS5bskO5yc3Px5ptv4vr163BwcECnTp0QGRmJAQMGGLppRACAq1evIiIiAjdv3oSTkxN69eqFb7/9Fk5OToZuGpGI4728ON7Lj+M9GTuO92QKON7Li+O9/Djek7Fr7OO9mUaj0Ri6EUREREREREREREREpkZh6AYQEREREREREREREZkiJtiJiIiIiIiIiIiIiOqACXYiIiIiIiIiIiIiojpggp2IiIiIiIiIiIiIqA6YYCciIiIiIiIiIiIiqgMm2ImIiIiIiIiIiIiI6oAJdiIiIiIiIiIiIiKiOmCCnYiIiIiIiIiIiIioDphgJ2rk3nrrLUyZMsXQzSAiIqJ6xPGeiIio8eN4T2ScmGAnIp2UlpYauglERERUzzjeExERNX4c74nkZabRaDSGbgQRPbiff/4Z69atw6VLl2BjYwMfHx/4+PggKipKErd582b4+fnh/fffR3x8PK5evQoXFxeMHj0aU6dOhYWFBQDg448/Rnx8PCZMmIBPPvkEWVlZSE9Pr/b3rF+/Hra2tob4ZxMRET1UON4TERE1fhzviUyLuaEbQEQP7vr165g5cybeeOMNDBs2DIWFhTh27BjGjh2LK1eu4Pbt21i+fDkAoEmTJgAAOzs7LF++HM2bN8dff/2FBQsWwM7ODpMnTxb/3oyMDOzevRtr166FQqG47+/hfToiIqL6x/GeiIio8eN4T2R6mGAnagSys7NRXl6OwMBAtGrVCgDQqVMnAIC1tTVKS0vh6uoq+TPaddvc3d1x4cIF7Ny5UzIAl5WV4b333oOTkxMA4PTp0/f9PURERFS/ON4TERE1fhzviUwPE+xEjYC3tzf69euH0aNHIyAgAAEBARg+fLh4N7s6u3btwubNm5GZmYmioiKUl5fD3t5eEuPm5iYOvnX9PURERCQPjvdERESNH8d7ItPDTU6JGgGlUolNmzZh48aNaN++PbZs2YIRI0YgMzOz2vgTJ05g1qxZGDRoEDZs2IAff/wRr732GsrKyiRxNjY2D/R7iIiISD4c74mIiBo/jvdEpocJdqJGwszMDL169cL06dOxfft2WFhYID4+HhYWFqioqJDEnjhxAm5ubnj99dfRtWtXtG7dGllZWQ/0ucHkZwAAAZVJREFUe4iIiKj+cbwnIiJq/DjeE5kWloghagROnjyJI0eOYMCAAXB2dsbJkyeRl5eHtm3boqSkBAcPHsT58+fRtGlTODg4wMvLC1euXMHOnTvRtWtX7N+/X6dBtKbfQ0RERPWL4z0REVHjx/GeyPQwwU7UCNjb2yMpKQnR0dG4ffs23Nzc8NZbb2HQoEHo2rUrjh49iqeffhpFRUXYvHkzhg4ditDQUCxevBilpaV4/PHH8frrr2Pt2rV1/j1ERERUvzjeExERNX4c74lMj5lGo9EYuhFERERERERERERERKaGNdiJiIiIiIiIiIiIiOqACXYiIiIiIiIiIiIiojpggp2IiIiIiIiIiIiIqA6YYCciIiIiIiIiIiIiqgMm2ImIiIiIiIiIiIiI6oAJdiIiIiIiIiIiIiKiOmCCnYiIiIiIiIiIiIioDphgJyIiIiIiIiIiIiKqAybYiYiIiIiIiIiIiIjqgAl2IiIiIiIiIiIiIqI6YIKdiIiIiIiIiIiIiKgOmGAnIiIiIiIiIiIiIqqD/weMzn0YlfBt7wAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "df.boxplot(column='useful', by='stars', ax=axes[0])\n",
    "axes[0].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='funny', by='stars', ax=axes[1])\n",
    "axes[1].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='cool', by='stars', ax=axes[2])\n",
    "axes[2].set_title('Votes by Stars')\n",
    "\n",
    "plt.suptitle('Features by Stars')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:17:57.651523918Z",
     "start_time": "2023-12-01T23:17:55.307841707Z"
    }
   },
   "id": "423ce5ed6ece5ee1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b21f02aac3bbbe6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n686552  t1GdeuIO_Mcw0E2oEU662w  7y3HeuIWJFoVRe-e9kAi3A   \n689115  Xxr0VMnm0RnXlr5RSMt9HQ  sb3ZzGzoUECo64EZd6DE9g   \n693901  zMgZCG75QDYYCvKKrGGYfw  Q4-WQT0Q3_X4ZtCwDdfH_g   \n695069  bqYN3sifORSPnMIInqYgdg  kLPT0qfgHY5ppF2NbdPvlg   \n696153  0fm6c5UH3YpiKA7T7nQx-w  iEirxmriN2h2NSBkMpMbUA   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n686552  oDaamK_x-XhTOieP-qVvnw      5       1      0     1   \n689115  cBbvS4klOQd221pKQVMGMA      4       2      0     0   \n693901  2FR-xWttfR8qaODqcuExvw      1       1      1     0   \n695069  1q54Dq02nSFa9NFbu0Tbxw      1       3      0     0   \n696153  dbfm8H0KQRrxT_QSXrQZ-A      5       1      0     0   \n\n                                                     text  \\\n0       If you decide to eat here, just be aware it is...   \n1       I've taken a lot of spin classes over the year...   \n2       Family diner. Had the buffet. Eclectic assortm...   \n3       Wow!  Yummy, different,  delicious.   Our favo...   \n4       Cute interior and owner (?) gave us tour of up...   \n...                                                   ...   \n686552  Great selection of Mexican related food stuffs...   \n689115  Found some sweet deals on fruits!!! I came acr...   \n693901  I got one of those prepaid debit card \"Excepta...   \n695069  Absolute worst, most embarrassing experience o...   \n696153  By far the best chicken pot pie I've ever had ...   \n\n                       date  review_length  \n0       2018-07-07 22:09:11            513  \n1       2012-01-03 15:28:18            829  \n2       2014-02-05 20:30:30            339  \n3       2015-01-04 00:01:03            243  \n4       2017-01-14 20:54:15            534  \n...                     ...            ...  \n686552  2014-01-11 13:00:46            103  \n689115  2019-11-14 00:13:54            163  \n693901  2020-04-29 00:09:28            428  \n695069  2019-04-28 15:00:37            894  \n696153  2021-01-17 17:18:22            227  \n\n[14965 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>review_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>513</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>829</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>339</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>243</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>534</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>686552</th>\n      <td>t1GdeuIO_Mcw0E2oEU662w</td>\n      <td>7y3HeuIWJFoVRe-e9kAi3A</td>\n      <td>oDaamK_x-XhTOieP-qVvnw</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Great selection of Mexican related food stuffs...</td>\n      <td>2014-01-11 13:00:46</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>689115</th>\n      <td>Xxr0VMnm0RnXlr5RSMt9HQ</td>\n      <td>sb3ZzGzoUECo64EZd6DE9g</td>\n      <td>cBbvS4klOQd221pKQVMGMA</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Found some sweet deals on fruits!!! I came acr...</td>\n      <td>2019-11-14 00:13:54</td>\n      <td>163</td>\n    </tr>\n    <tr>\n      <th>693901</th>\n      <td>zMgZCG75QDYYCvKKrGGYfw</td>\n      <td>Q4-WQT0Q3_X4ZtCwDdfH_g</td>\n      <td>2FR-xWttfR8qaODqcuExvw</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>I got one of those prepaid debit card \"Excepta...</td>\n      <td>2020-04-29 00:09:28</td>\n      <td>428</td>\n    </tr>\n    <tr>\n      <th>695069</th>\n      <td>bqYN3sifORSPnMIInqYgdg</td>\n      <td>kLPT0qfgHY5ppF2NbdPvlg</td>\n      <td>1q54Dq02nSFa9NFbu0Tbxw</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Absolute worst, most embarrassing experience o...</td>\n      <td>2019-04-28 15:00:37</td>\n      <td>894</td>\n    </tr>\n    <tr>\n      <th>696153</th>\n      <td>0fm6c5UH3YpiKA7T7nQx-w</td>\n      <td>iEirxmriN2h2NSBkMpMbUA</td>\n      <td>dbfm8H0KQRrxT_QSXrQZ-A</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>By far the best chicken pot pie I've ever had ...</td>\n      <td>2021-01-17 17:18:22</td>\n      <td>227</td>\n    </tr>\n  </tbody>\n</table>\n<p>14965 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Drop duplicates\n",
    "df = df.drop_duplicates(subset=['review_id'])\n",
    "df = df.drop_duplicates(subset=['user_id'])\n",
    "df = df.drop_duplicates(subset=['business_id'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:18:02.681694511Z",
     "start_time": "2023-12-01T23:18:02.321506058Z"
    }
   },
   "id": "ec860b4d0dd4f343"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls:\n",
      " review_id        0\n",
      "user_id          0\n",
      "business_id      0\n",
      "stars            0\n",
      "useful           0\n",
      "funny            0\n",
      "cool             0\n",
      "text             0\n",
      "date             0\n",
      "review_length    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove Null values \n",
    "nulls= df.isnull().sum()\n",
    "print(\"Nulls:\\n\", nulls)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:18:05.741415533Z",
     "start_time": "2023-12-01T23:18:05.722845200Z"
    }
   },
   "id": "dcd81e9a795ee1c9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import re\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "##%%\n",
    "# Remove special characters \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:18:08.722816408Z",
     "start_time": "2023-12-01T23:18:08.608572175Z"
    }
   },
   "id": "8df19d2f709cd75b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:18:09.602643342Z",
     "start_time": "2023-12-01T23:18:09.542161009Z"
    }
   },
   "id": "79b8eee080ee2c19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7d0e87ac37b0cbe"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(texts)\n",
    "s = token.texts_to_sequences(texts)\n",
    "index = token.word_index\n",
    "maximumlength = 100  # Set your desired sequence length\n",
    "data = pad_sequences(s, maxlen=maximumlength)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:21.024337719Z",
     "start_time": "2023-12-01T23:18:13.240688661Z"
    }
   },
   "id": "a35ed24848ab6241"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "l_encode = LabelEncoder()\n",
    "labels = l_encode.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:29.808129339Z",
     "start_time": "2023-12-01T23:19:29.762696933Z"
    }
   },
   "id": "30ab1057be0184f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and Test Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f482dc523c794b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.18, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:36.502985810Z",
     "start_time": "2023-12-01T23:19:36.245645232Z"
    }
   },
   "id": "ff432a9e5bb8299e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff7cbf3af4d67126"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(index) + 1, 100, input_length=maximumlength))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:40.952601252Z",
     "start_time": "2023-12-01T23:19:40.511946999Z"
    }
   },
   "id": "7739b5302e7a0a65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414c95ba8fd89d27"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:44.432121337Z",
     "start_time": "2023-12-01T23:19:44.394408434Z"
    }
   },
   "id": "8434b5be39863fd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad8bd4ada616cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    " # Training with Data Generator\n",
    "def data_generator(data, labels, batch_size):\n",
    "    samples = len(data)\n",
    "    while True:\n",
    "        s_indices = np.arange(samples)\n",
    "        np.random.shuffle(s_indices)\n",
    "        for i in range(0,samples, batch_size):\n",
    "            b_indices = s_indices[i:i+batch_size]\n",
    "            yield data[b_indices], labels[b_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:19:47.852802236Z",
     "start_time": "2023-12-01T23:19:47.818063689Z"
    }
   },
   "id": "36795275f60ebf9f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "17393/17393 [==============================] - 2730s 157ms/step - loss: 0.2185 - accuracy: 0.9139 - val_loss: 0.1869 - val_accuracy: 0.9252\n",
      "Epoch 2/7\n",
      "17393/17393 [==============================] - 2636s 152ms/step - loss: 0.1682 - accuracy: 0.9349 - val_loss: 0.1801 - val_accuracy: 0.9297\n",
      "Epoch 3/7\n",
      "17393/17393 [==============================] - 2642s 152ms/step - loss: 0.1433 - accuracy: 0.9449 - val_loss: 0.1916 - val_accuracy: 0.9295\n",
      "Epoch 4/7\n",
      "17393/17393 [==============================] - 2639s 152ms/step - loss: 0.1226 - accuracy: 0.9536 - val_loss: 0.2069 - val_accuracy: 0.9256\n",
      "Epoch 5/7\n",
      "17393/17393 [==============================] - 2672s 154ms/step - loss: 0.1056 - accuracy: 0.9603 - val_loss: 0.2132 - val_accuracy: 0.9242\n",
      "Epoch 6/7\n",
      "17393/17393 [==============================] - 2640s 152ms/step - loss: 0.0909 - accuracy: 0.9664 - val_loss: 0.2436 - val_accuracy: 0.9199\n",
      "Epoch 7/7\n",
      "17393/17393 [==============================] - 2725s 157ms/step - loss: 0.0810 - accuracy: 0.9702 - val_loss: 0.2711 - val_accuracy: 0.9190\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x7efac93c0820>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 33\n",
    "epochs= len(X_train) // batch_size\n",
    "# Training\n",
    "model.fit(data_generator(X_train, y_train, batch_size),\n",
    "          epochs=7,\n",
    "          steps_per_epoch=epochs,\n",
    "          validation_data=(X_test, y_test))    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T04:31:20.651114484Z",
     "start_time": "2023-12-01T23:19:56.743575361Z"
    }
   },
   "id": "ac7145e0c7846845"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938/3938 [==============================] - 49s 13ms/step - loss: 0.2711 - accuracy: 0.9190\n",
      "Accuracy: 91.89761877059937\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(f\"Accuracy: {accuracy*100}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:00:23.845520905Z",
     "start_time": "2023-12-02T04:59:34.216793677Z"
    }
   },
   "id": "f51b6af5cb1ab248"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938/3938 [==============================] - 50s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "y_predicted_value = model.predict(X_test)\n",
    "y_binary_value = (y_predicted_value > 0.5).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:03:37.935548519Z",
     "start_time": "2023-12-02T05:02:46.266327584Z"
    }
   },
   "id": "1e49e0dbe798f3ec"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 92.1470\n",
      "Recall score: 96.2532\n",
      "F1 Score: 94.1554\n"
     ]
    }
   ],
   "source": [
    "# Metrics Calculation\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_binary_value, average='binary')\n",
    "\n",
    "print(f\"Precision score: {prec*100:.4f}\")\n",
    "print(f\"Recall score: {rec*100:.4f}\")\n",
    "print(f\"F1 Score: {f1*100:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:05:34.277298648Z",
     "start_time": "2023-12-02T05:05:34.233163527Z"
    }
   },
   "id": "5ed1941d4577fbe9"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_binary_value)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:05:59.966427199Z",
     "start_time": "2023-12-02T05:05:59.752134485Z"
    }
   },
   "id": "73c1fc91228cce3e"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(y_test, y_predicted_value, alpha=0.5)\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:06:07.291603895Z",
     "start_time": "2023-12-02T05:06:06.854794803Z"
    }
   },
   "id": "e5de6df9a058348c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bert Super-vised\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "360c532733d75665"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46aef73a3182928",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:49:55.734286176Z",
     "start_time": "2023-12-02T06:49:52.365601892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "chunk= 100000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "chunks2 = pd.read_json(file, lines=True, chunksize=chunk)\n",
    "for i, x in enumerate(chunks2):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "    if i == 0:\n",
    "        csv = \"chunk.csv\"\n",
    "        x.to_csv(csv, index=False)\n",
    "    df1= pd.read_csv(\"chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sentiment(stars):  \n",
    "    if 0 <= stars < 3:\n",
    "        return 0 #negative\n",
    "    elif 3 <= stars <= 5:\n",
    "        return 1 #normal\n",
    "    else:\n",
    "        return 'undefined'  \n",
    "\n",
    "df1['sentiment_label'] = df1['stars'].apply(sentiment)\n",
    "le = LabelEncoder()\n",
    "df1['encoded_sentiment'] = le.fit_transform(df1['sentiment_label'])\n",
    "\n",
    "# Extract labels\n",
    "y = df1['encoded_sentiment'].values\n",
    "df1.to_csv('preprocessed_data1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:49:59.450642550Z",
     "start_time": "2023-12-02T06:49:57.936645254Z"
    }
   },
   "id": "744e9395667a6e15"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70b30861e00a4ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:01.077724219Z",
     "start_time": "2023-12-02T06:50:00.986711768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    review_id                 user_id             business_id  \\\n0      KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n1      BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n2      saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n3      AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n4      Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n...                       ...                     ...                     ...   \n99995  pAEbIxvr6ebx2bHc1XvguA  SMH5CeiLvKx61lKwtLZ_PA  lV0k3BnslFRkuWD_kbKd0Q   \n99996  xH1AoE-4nf2ECGQJRjO4_g  2clTdtp-BjphxLjN83CpUA  G0xz3kyRhRi6oZl7KfR0pA   \n99997  GatIbXTz-WDru5emONUSIg  MRrN6DH3QGCFcDv5RENYVg  C4lZdhasjZVQyDlOiXY1sA   \n99998  6NfkodAdhvI89xONXuBC3A  rnNQzeKJbvqVCsYsL10mkQ  dChRGpit9fM_kZK5pafNyA   \n99999  sJ1BMq7lkKgOWEFx3n6ZRw  _BcWyKQL16ndpBdggh2kNA  hMcgO98QaOFmQVTfCUeGzw   \n\n       stars  useful  funny  cool  \\\n0          3       0      0     0   \n1          5       1      0     1   \n2          3       0      0     0   \n3          5       1      0     1   \n4          4       1      0     1   \n...      ...     ...    ...   ...   \n99995      4       0      0     0   \n99996      1       1      0     0   \n99997      4       0      0     0   \n99998      2       0      0     0   \n99999      5       0      0     0   \n\n                                                    text                 date  \\\n0      If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n1      I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n2      Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n3      Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n4      Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n...                                                  ...                  ...   \n99995  Came here for lunch with a group. They were bu...  2018-05-30 22:28:56   \n99996  The equipment is so old and so felty! I just u...  2015-04-05 23:31:52   \n99997  This is one of my favorite Mexican restaurants...  2016-06-04 00:59:15   \n99998  Came here for brunch - had an omlette ($19 + t...  2018-06-11 12:45:08   \n99999  Came in for my 5-6 month prophy and saw Kara -...  2013-06-06 10:10:33   \n\n       sentiment_label  encoded_sentiment  \n0                    1                  1  \n1                    1                  1  \n2                    1                  1  \n3                    1                  1  \n4                    1                  1  \n...                ...                ...  \n99995                1                  1  \n99996                0                  0  \n99997                1                  1  \n99998                0                  0  \n99999                1                  1  \n\n[100000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>sentiment_label</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>pAEbIxvr6ebx2bHc1XvguA</td>\n      <td>SMH5CeiLvKx61lKwtLZ_PA</td>\n      <td>lV0k3BnslFRkuWD_kbKd0Q</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came here for lunch with a group. They were bu...</td>\n      <td>2018-05-30 22:28:56</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>xH1AoE-4nf2ECGQJRjO4_g</td>\n      <td>2clTdtp-BjphxLjN83CpUA</td>\n      <td>G0xz3kyRhRi6oZl7KfR0pA</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>The equipment is so old and so felty! I just u...</td>\n      <td>2015-04-05 23:31:52</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>GatIbXTz-WDru5emONUSIg</td>\n      <td>MRrN6DH3QGCFcDv5RENYVg</td>\n      <td>C4lZdhasjZVQyDlOiXY1sA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>This is one of my favorite Mexican restaurants...</td>\n      <td>2016-06-04 00:59:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>6NfkodAdhvI89xONXuBC3A</td>\n      <td>rnNQzeKJbvqVCsYsL10mkQ</td>\n      <td>dChRGpit9fM_kZK5pafNyA</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came here for brunch - had an omlette ($19 + t...</td>\n      <td>2018-06-11 12:45:08</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>sJ1BMq7lkKgOWEFx3n6ZRw</td>\n      <td>_BcWyKQL16ndpBdggh2kNA</td>\n      <td>hMcgO98QaOFmQVTfCUeGzw</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came in for my 5-6 month prophy and saw Kara -...</td>\n      <td>2013-06-06 10:10:33</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "df1['sentiment_label'].value_counts().sort_index().plot(kind='bar', color=['red', 'yellow', 'green'])\n",
    "plt.title('Sentiment Labels')\n",
    "plt.xlabel('Sentiment Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:16.059048449Z",
     "start_time": "2023-12-02T06:50:15.759300554Z"
    }
   },
   "id": "3b3a2f14f5ae398e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting divided parts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cbd632a3aacc089"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:20.099802908Z",
     "start_time": "2023-12-02T06:50:20.054878065Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = df1[df1['sentiment_label'] == 1].sample(n=2000, random_state=42)\n",
    "negative_reviews= df1[df1['sentiment_label'] == 0].sample(n=2000, random_state=42)\n",
    "\n",
    "# Combine the selected samples\n",
    "new_data = pd.concat([positive_reviews, negative_reviews], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c63405a6148b1b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:23.589230300Z",
     "start_time": "2023-12-02T06:50:23.538320949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                   review_id                 user_id             business_id  \\\n0     LP2B6aJarFIJ7I6BJ1cD9A  QY1R3IciPr-IkKeTFsqVOA  Di6uZDhcwnLsgM66Z4fNfw   \n1     mHShP4zIYe1D2S-EBJOGIw  xFj_h72vn0th96ETHQUY7A  rZceWBN1i0QMmmXJcJLfxw   \n2     2Ok8SYfDBlYi8kckknqEUA  huoocXS_i6g65qESKMo7gQ  4NidY2tw42l6iobbtai_kA   \n3     3YGWryyrTbDH0fiCn4OHBQ  Q3E9uqEA0kcgkkjju6h1KQ  wzIN0IqcNOnUjwuzRoG9AA   \n4     ZYbM8Y34-okwn--e5kiO1g  FtZ2rivohSn-KIj_8138yQ  USekrAG0-4tJUs9V2gBBOQ   \n...                      ...                     ...                     ...   \n3995  WpMd2rE9H3Z_Fp38i9OMFw  yjGTXXUjzpR8Q25ZKcKwXw  8usO-H5uFTzlISfGZN8rSg   \n3996  xDKP0idPe3I2zGZTocGa4A  -kMtxnQmE_S3_U1yZ2cVtQ  eaV07HGOcyb27XobHVl8LQ   \n3997  KmVnM35HQR7sEun610gsag  OZ-T-UTOXUXva4RJ8j61oQ  qcLkiAqlxx0oTqZMSC_y6Q   \n3998  P_NZRZPHP3k-HnYacdUtwQ  KGdV17WL4x2w52VK81D_Iw  SRb3xScVOeYfhZP4U8LMdA   \n3999  ioSOnTOAOzSQ_snexu8msw  I9v7jlsljI9NLnrsqIO4qQ  o9Gru-QFoxGK26FSWRCLXA   \n\n      stars  useful  funny  cool  \\\n0         5       0      0     0   \n1         3       6      1     1   \n2         3       0      0     0   \n3         5       0      0     0   \n4         5       0      0     0   \n...     ...     ...    ...   ...   \n3995      2       4      1     0   \n3996      1       3      0     1   \n3997      1       4      1     0   \n3998      2       0      0     0   \n3999      1       0      0     0   \n\n                                                   text                 date  \\\n0     The type of bar a vigilante would go to after ...  2018-03-05 00:04:30   \n1     This place is OK the therapist my son has is w...  2015-11-09 19:21:43   \n2     Really prompt service even though we were 30 m...  2018-01-01 02:41:07   \n3     I got the pick 3! Had the stuffed pepper , jam...  2018-07-28 23:09:42   \n4     Fantastic!  \\n\\nStaff was amazingly friendly. ...  2016-07-27 22:12:07   \n...                                                 ...                  ...   \n3995  The food was good, but that was the only thing...  2015-07-28 20:36:17   \n3996  So basically, you are going to be having a stu...  2012-09-10 19:52:09   \n3997  Absolutely the worst hair cut I have ever seen...  2015-12-23 16:45:43   \n3998  Unfortunately, the other reviews said it all. ...  2015-07-29 00:28:15   \n3999  What a zoo!  I hate renting from this facility...  2006-10-28 19:30:50   \n\n      sentiment_label  encoded_sentiment  \n0                   1                  1  \n1                   1                  1  \n2                   1                  1  \n3                   1                  1  \n4                   1                  1  \n...               ...                ...  \n3995                0                  0  \n3996                0                  0  \n3997                0                  0  \n3998                0                  0  \n3999                0                  0  \n\n[4000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>sentiment_label</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LP2B6aJarFIJ7I6BJ1cD9A</td>\n      <td>QY1R3IciPr-IkKeTFsqVOA</td>\n      <td>Di6uZDhcwnLsgM66Z4fNfw</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>The type of bar a vigilante would go to after ...</td>\n      <td>2018-03-05 00:04:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mHShP4zIYe1D2S-EBJOGIw</td>\n      <td>xFj_h72vn0th96ETHQUY7A</td>\n      <td>rZceWBN1i0QMmmXJcJLfxw</td>\n      <td>3</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>This place is OK the therapist my son has is w...</td>\n      <td>2015-11-09 19:21:43</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2Ok8SYfDBlYi8kckknqEUA</td>\n      <td>huoocXS_i6g65qESKMo7gQ</td>\n      <td>4NidY2tw42l6iobbtai_kA</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Really prompt service even though we were 30 m...</td>\n      <td>2018-01-01 02:41:07</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3YGWryyrTbDH0fiCn4OHBQ</td>\n      <td>Q3E9uqEA0kcgkkjju6h1KQ</td>\n      <td>wzIN0IqcNOnUjwuzRoG9AA</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>I got the pick 3! Had the stuffed pepper , jam...</td>\n      <td>2018-07-28 23:09:42</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ZYbM8Y34-okwn--e5kiO1g</td>\n      <td>FtZ2rivohSn-KIj_8138yQ</td>\n      <td>USekrAG0-4tJUs9V2gBBOQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Fantastic!  \\n\\nStaff was amazingly friendly. ...</td>\n      <td>2016-07-27 22:12:07</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>WpMd2rE9H3Z_Fp38i9OMFw</td>\n      <td>yjGTXXUjzpR8Q25ZKcKwXw</td>\n      <td>8usO-H5uFTzlISfGZN8rSg</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>The food was good, but that was the only thing...</td>\n      <td>2015-07-28 20:36:17</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>xDKP0idPe3I2zGZTocGa4A</td>\n      <td>-kMtxnQmE_S3_U1yZ2cVtQ</td>\n      <td>eaV07HGOcyb27XobHVl8LQ</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>So basically, you are going to be having a stu...</td>\n      <td>2012-09-10 19:52:09</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>KmVnM35HQR7sEun610gsag</td>\n      <td>OZ-T-UTOXUXva4RJ8j61oQ</td>\n      <td>qcLkiAqlxx0oTqZMSC_y6Q</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Absolutely the worst hair cut I have ever seen...</td>\n      <td>2015-12-23 16:45:43</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>P_NZRZPHP3k-HnYacdUtwQ</td>\n      <td>KGdV17WL4x2w52VK81D_Iw</td>\n      <td>SRb3xScVOeYfhZP4U8LMdA</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unfortunately, the other reviews said it all. ...</td>\n      <td>2015-07-29 00:28:15</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>ioSOnTOAOzSQ_snexu8msw</td>\n      <td>I9v7jlsljI9NLnrsqIO4qQ</td>\n      <td>o9Gru-QFoxGK26FSWRCLXA</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>What a zoo!  I hate renting from this facility...</td>\n      <td>2006-10-28 19:30:50</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load model and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83e734884c57f29"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff61dffb6be7e22",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:34.661298821Z",
     "start_time": "2023-12-02T06:50:31.362694002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification \n",
    "model_name = \"bert-base-uncased\"\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f725cbd7499c1a33"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T06:50:49.924387680Z",
     "start_time": "2023-12-02T06:50:42.504102414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts (reviews)\n",
    "import torch\n",
    "\n",
    "tokenized_vals = []\n",
    "\n",
    "for text in new_data['text']:\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Append individually\n",
    "    tokenized_vals.append({\n",
    "        'input_ids': tokens['input_ids'],\n",
    "        'attention_mask': tokens['attention_mask']\n",
    "    })\n",
    "\n",
    "# Extracting input tensors separately\n",
    "input_ids = torch.cat([entry['input_ids'] for entry in tokenized_vals], dim=0)\n",
    "attention_mask = torch.cat([entry['attention_mask'] for entry in tokenized_vals], dim=0)\n"
   ],
   "id": "c40fe859315047eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train and test split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89000442e487914f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "label = new_data['encoded_sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:10.555681395Z",
     "start_time": "2023-12-02T06:57:10.434254365Z"
    }
   },
   "id": "7f8363980f8ba3c6"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:13.690329057Z",
     "start_time": "2023-12-02T06:57:13.655184577Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_id, X_test_id, X_train_m, X_test_m, y_train, y_test = train_test_split(\n",
    "    input_ids.numpy(),\n",
    "    attention_mask.numpy(),\n",
    "    label,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "c53659815d380684"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8393e04480e9c9ed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:16.421361287Z",
     "start_time": "2023-12-02T06:57:16.397846218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3400, 128) (600, 128) (3400, 128) (600, 128) (3400,) (600,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_id.shape, X_test_id.shape, X_train_m.shape, X_test_m.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:19.589280413Z",
     "start_time": "2023-12-02T06:57:19.567283745Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch tensors \n",
    "X_train_ids= torch.tensor(X_train_id, dtype=torch.long)\n",
    "X_test_ids= torch.tensor(X_test_id, dtype=torch.long)\n",
    "X_train_mask= torch.tensor(X_train_m, dtype=torch.long)\n",
    "X_test_mask= torch.tensor(X_test_m, dtype=torch.long)\n",
    "y_train= torch.tensor(y_train.values, dtype=torch.long) \n",
    "y_test= torch.tensor(y_test.values, dtype=torch.long)  "
   ],
   "id": "89b874ad21459b1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414b8b3f3170c940"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8aea60188055875",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:23.747048985Z",
     "start_time": "2023-12-02T06:57:23.393687854Z"
    }
   },
   "outputs": [],
   "source": [
    "criter = torch.nn.CrossEntropyLoss()   \n",
    "optim= torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bbb21d72ff741",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790920d77b296858",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T06:57:27.595522367Z",
     "start_time": "2023-12-02T06:57:27.539099935Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 3\n",
    "\n",
    "train_data = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_data = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_load= DataLoader(test_data,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c73f3627ce1996",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T08:16:10.468075253Z",
     "start_time": "2023-12-02T06:57:36.441467586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1559, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0956, -0.0702, -0.0916],\n",
      "        [-0.3458, -0.1244, -0.0646],\n",
      "        [-0.4021, -0.1654, -0.0450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9818, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0642, -0.0735, -0.2915],\n",
      "        [ 0.0750, -0.2029, -0.4041],\n",
      "        [ 0.1169, -0.0750, -0.4923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9031, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3122, -0.2497, -0.4161],\n",
      "        [ 0.0363, -0.0084, -0.7948],\n",
      "        [ 0.0949, -0.1671, -0.6357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8314, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4530, -0.2628, -0.6905],\n",
      "        [ 0.1000,  0.1218, -0.9599],\n",
      "        [ 0.1485, -0.1124, -0.6348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8161, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1365,  0.0997, -1.1185],\n",
      "        [ 0.0195, -0.0466, -0.8238],\n",
      "        [ 0.2027,  0.3305, -0.8943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8719, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0586,  0.2288, -0.8153],\n",
      "        [ 0.1115,  0.1770, -0.9498],\n",
      "        [ 0.2124,  0.2956, -0.4925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8184, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0809,  0.1004, -0.8808],\n",
      "        [ 0.0989,  0.1519, -1.1107],\n",
      "        [ 0.0316,  0.3117, -1.2225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7054, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2580,  0.4063, -1.3743],\n",
      "        [ 0.0190,  0.0865, -1.3454],\n",
      "        [ 0.3586, -0.0578, -1.1518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8063, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4446,  0.3232, -1.0691],\n",
      "        [ 0.4309,  0.3904, -1.4134],\n",
      "        [ 0.3330,  0.2502, -1.1749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8280, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2125,  0.3524, -1.2276],\n",
      "        [ 0.2535,  0.4885, -1.3552],\n",
      "        [ 0.3780,  0.2750, -1.1742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8547, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4596,  0.4082, -1.2515],\n",
      "        [ 0.1898,  0.8744, -1.5030],\n",
      "        [ 0.2219,  0.6169, -1.2471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8014, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2835,  0.4526, -1.5180],\n",
      "        [ 0.2472,  0.4322, -1.5801],\n",
      "        [ 0.5130,  0.7217, -1.5394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7250, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3129,  0.6078, -1.1905],\n",
      "        [ 0.3966,  0.5180, -1.8571],\n",
      "        [ 0.2135,  0.7007, -1.4214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8709, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1174,  0.6567, -1.4586],\n",
      "        [ 0.1725,  0.5434, -1.6341],\n",
      "        [ 0.3827,  0.8093, -1.2054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7890, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5794,  0.6730, -1.5344],\n",
      "        [ 0.4060,  0.7731, -1.3855],\n",
      "        [ 0.3375,  0.6614, -1.4740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7187, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5191,  0.3720, -1.5425],\n",
      "        [ 0.4928,  0.4969, -1.6169],\n",
      "        [ 0.3418,  0.4427, -1.5371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8043, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4670,  0.5511, -1.6896],\n",
      "        [ 0.3300,  0.6664, -1.5466],\n",
      "        [ 0.6465,  0.4994, -1.4609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6789, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5280,  0.4919, -1.5741],\n",
      "        [ 0.5975,  0.3961, -1.5550],\n",
      "        [ 0.3860,  0.7138, -1.4787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8550, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7391,  0.3446, -1.2424],\n",
      "        [ 0.8985,  0.6528, -1.4462],\n",
      "        [ 0.6555,  0.3453, -1.2073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6684, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8350,  0.5358, -1.2609],\n",
      "        [ 0.8846,  0.2076, -1.6237],\n",
      "        [ 0.7507,  0.4366, -1.5474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7334, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8784,  0.3574, -1.3360],\n",
      "        [ 1.0098,  0.7239, -1.6243],\n",
      "        [ 0.8755,  0.3036, -1.6930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7732,  0.1006, -1.5489],\n",
      "        [ 0.7054,  0.3338, -1.7929],\n",
      "        [ 0.8432,  0.4596, -1.8020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7150, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5438,  0.6382, -1.8187],\n",
      "        [ 0.9411,  0.1354, -1.4872],\n",
      "        [ 0.6832,  0.3255, -1.9081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4778, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8959,  0.3614, -1.6740],\n",
      "        [ 0.9482,  0.1444, -1.6612],\n",
      "        [ 0.7505,  0.1996, -1.7550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7133, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8001,  0.4157, -1.7993],\n",
      "        [ 0.7329,  0.4640, -1.5550],\n",
      "        [ 0.5926,  0.7181, -1.5693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5945, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7433,  0.5111, -1.8534],\n",
      "        [ 0.9757,  0.6457, -1.8148],\n",
      "        [ 0.7370,  0.3947, -1.7873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8999, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9293,  0.3813, -1.8766],\n",
      "        [ 0.7696,  0.4481, -2.0960],\n",
      "        [ 0.7352,  0.6723, -1.9821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6376, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5948,  0.6794, -2.2776],\n",
      "        [ 1.0740,  0.4017, -2.0013],\n",
      "        [ 0.5891,  0.4796, -1.9091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8627, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6148,  0.7728, -1.8171],\n",
      "        [ 1.0384,  0.1467, -1.9164],\n",
      "        [ 0.7926,  0.2682, -2.0281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5722, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7931,  0.4458, -1.9643],\n",
      "        [ 0.6280,  0.8318, -2.0896],\n",
      "        [ 0.9821,  0.5259, -2.0515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6060, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4239,  0.9840, -2.1347],\n",
      "        [ 0.2301,  0.9939, -2.2054],\n",
      "        [ 0.3439,  1.2436, -2.0078]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5895, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0111,  0.2408, -1.9404],\n",
      "        [ 0.4569,  1.0859, -2.0989],\n",
      "        [ 0.5722,  0.9017, -2.1342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5806, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7198,  0.6794, -1.8351],\n",
      "        [ 0.9141,  0.6330, -2.0063],\n",
      "        [ 1.0952,  0.4214, -2.1628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6647, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5457,  1.2336, -2.3586],\n",
      "        [ 0.5507,  1.0083, -2.3000],\n",
      "        [ 1.1291,  0.1954, -1.8262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3323, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4354,  1.2161, -2.1359],\n",
      "        [ 0.9600, -0.2817, -1.9818],\n",
      "        [ 1.2563,  0.0492, -1.6677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7283, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2398, -0.0449, -1.8375],\n",
      "        [ 0.8429,  0.0612, -1.6020],\n",
      "        [ 1.1649,  0.0051, -1.8114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8324, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3207,  0.1204, -1.8668],\n",
      "        [ 1.4098, -0.2331, -1.6016],\n",
      "        [ 1.1151,  0.0537, -1.6511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9067, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2492, -0.1980, -1.5960],\n",
      "        [ 1.1211,  0.5455, -2.2161],\n",
      "        [ 1.0420, -0.0231, -1.4941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1758, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2592, -0.1363, -1.7274],\n",
      "        [ 1.0642, -0.1055, -1.5966],\n",
      "        [ 1.0567,  0.1838, -2.0938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6321, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5845,  1.0981, -2.4261],\n",
      "        [ 0.8825,  0.6712, -1.9485],\n",
      "        [ 0.6865,  0.9872, -2.4296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5666, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7412,  1.1810, -2.4041],\n",
      "        [ 0.3928,  1.1839, -2.5435],\n",
      "        [ 0.3508,  1.2599, -2.4503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4071, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2567,  1.1115, -2.3601],\n",
      "        [ 0.2823,  1.0845, -2.0888],\n",
      "        [ 0.4520,  1.0810, -2.4328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4698, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2076,  1.3698, -2.3213],\n",
      "        [ 0.8222,  0.1531, -1.6002],\n",
      "        [ 0.7499,  0.5647, -1.7690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7242, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7030,  1.0413, -2.1297],\n",
      "        [ 0.4874,  1.2430, -2.5215],\n",
      "        [ 0.3037,  1.1307, -2.4246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8529, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6688,  1.1482, -2.7312],\n",
      "        [ 0.3136,  1.3847, -2.6112],\n",
      "        [-0.0853,  1.5060, -2.2172]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6573, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9395,  0.4717, -2.0093],\n",
      "        [-0.1390,  1.5578, -2.3433],\n",
      "        [ 0.4942,  1.4164, -2.7097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3418, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1645,  1.3901, -2.5278],\n",
      "        [-0.0626,  1.5387, -2.3563],\n",
      "        [ 0.7848,  0.3824, -1.8978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6553, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7170,  0.3272, -2.0869],\n",
      "        [-0.1647,  1.5289, -2.0844],\n",
      "        [ 0.4964,  1.3510, -2.6458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5272, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1988,  1.6394, -2.6010],\n",
      "        [ 0.5289,  1.3545, -2.7734],\n",
      "        [-0.3692,  1.5352, -2.2908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4503, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4295,  0.4726, -2.7294],\n",
      "        [ 0.8467,  0.7492, -2.4290],\n",
      "        [ 0.4705,  1.3826, -2.7082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3203, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0412,  1.3769, -2.5283],\n",
      "        [ 1.0935,  0.4260, -2.5854],\n",
      "        [ 0.3659,  1.5296, -2.8781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2283, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5994,  0.3126, -2.6478],\n",
      "        [ 1.4660,  0.0485, -2.2549],\n",
      "        [ 0.0117,  1.6324, -2.4987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5365, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0636,  1.7212, -2.4148],\n",
      "        [ 1.1256,  0.1627, -2.7438],\n",
      "        [-0.4569,  1.6044, -2.2016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5792, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5264, -0.0906, -2.2558],\n",
      "        [ 1.3407,  0.2012, -2.2440],\n",
      "        [ 1.4255,  0.5461, -2.5883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1933, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5316, -0.2158, -2.5189],\n",
      "        [-0.1554,  1.6984, -2.4928],\n",
      "        [ 0.2289,  1.5489, -2.9023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5289, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4104,  0.5609, -2.6738],\n",
      "        [ 1.8417,  0.1874, -2.3829],\n",
      "        [ 1.6174, -0.1197, -2.1200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5467, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8203,  1.0762, -2.5827],\n",
      "        [ 0.9050,  0.6034, -2.8908],\n",
      "        [ 1.7138,  0.0377, -2.4815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2212, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5556, -0.2493, -2.0589],\n",
      "        [ 1.7367, -0.2959, -2.1660],\n",
      "        [ 0.4885,  1.4082, -2.6708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1338, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5572,  1.6362, -1.7082],\n",
      "        [-0.5432,  1.8407, -2.1984],\n",
      "        [-0.4281,  1.5172, -1.9621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7868, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1018,  1.8010, -2.4258],\n",
      "        [-0.1313,  1.5225, -2.2152],\n",
      "        [-0.6652,  1.7579, -1.7812]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1931, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8690, -0.3901, -2.1122],\n",
      "        [ 0.0292,  1.7473, -2.1264],\n",
      "        [ 1.1312, -0.2412, -1.5216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1194, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4409,  1.9163, -2.3800],\n",
      "        [ 1.8432, -0.4806, -1.9686],\n",
      "        [ 1.7818, -0.2356, -2.1506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5270, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6434,  1.8647, -1.7225],\n",
      "        [ 1.7650, -0.3685, -2.3519],\n",
      "        [ 1.4188,  0.3932, -2.2259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1465, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5784,  1.7548, -1.4520],\n",
      "        [-0.0161,  2.0120, -2.5224],\n",
      "        [ 1.5719, -0.1741, -2.3524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1093, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9279, -0.2099, -2.4466],\n",
      "        [-0.9055,  1.7081, -1.6848],\n",
      "        [-0.7245,  1.7862, -1.7902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9850, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7806,  2.0138, -1.9738],\n",
      "        [-0.8647,  2.0716, -1.8988],\n",
      "        [-0.5911,  2.1450, -2.4073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0979, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0179,  2.2105, -1.9310],\n",
      "        [ 1.5125, -0.3167, -1.9454],\n",
      "        [-0.8145,  2.1849, -1.9251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0939, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8109,  2.1600, -2.1499],\n",
      "        [-0.8081,  2.1542, -1.8497],\n",
      "        [ 1.6173, -0.1216, -2.1814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2166, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8801,  2.0031, -1.6120],\n",
      "        [ 1.3304,  0.4712, -2.3824],\n",
      "        [ 0.0851,  1.6722, -2.4008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2204, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4756,  2.1653, -2.1162],\n",
      "        [ 1.2276,  0.5265, -2.8625],\n",
      "        [ 1.8620,  0.0909, -2.7596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4248, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5321,  2.2898, -2.5002],\n",
      "        [ 0.6523,  1.3290, -2.8968],\n",
      "        [-0.3638,  1.8966, -2.4028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7102, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4443,  2.0689, -2.4345],\n",
      "        [ 2.1427,  0.0623, -2.5968],\n",
      "        [ 0.0985,  1.8466, -2.7568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4372, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5033,  1.7062, -2.8716],\n",
      "        [ 1.9290, -0.5937, -2.1305],\n",
      "        [ 1.2793,  0.8323, -3.2280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9107, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5245,  1.9080, -2.2608],\n",
      "        [ 0.7668,  1.3956, -2.8246],\n",
      "        [-0.3523,  1.7131, -2.5743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2721, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6166,  2.0480, -2.3838],\n",
      "        [ 1.0685,  0.8401, -2.7403],\n",
      "        [ 1.7597, -0.2371, -2.4550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4290, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1299,  2.1366, -3.0495],\n",
      "        [ 0.8486,  0.9650, -2.9665],\n",
      "        [ 1.3453,  0.9861, -3.1246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1870, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2133,  1.5033, -2.9057],\n",
      "        [-0.0171,  1.7403, -2.2687],\n",
      "        [-0.1961,  1.8654, -2.2773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8033, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4185,  1.8189, -2.9060],\n",
      "        [ 1.1472,  0.8266, -2.6239],\n",
      "        [ 0.4012,  1.3738, -2.9372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1296, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1402, -0.1322, -2.7134],\n",
      "        [ 2.2838, -0.5713, -2.2514],\n",
      "        [ 1.8718,  0.4216, -2.9054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2398, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6080,  0.3504, -2.8838],\n",
      "        [ 0.6594,  1.4874, -2.9757],\n",
      "        [ 2.1210, -0.3239, -2.7801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1313, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1436, -0.4114, -2.6924],\n",
      "        [ 0.2343,  1.5728, -2.8083],\n",
      "        [ 2.1348, -0.7135, -2.1613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1604, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3197,  1.6227, -2.9077],\n",
      "        [-0.0392,  1.6188, -2.5572],\n",
      "        [ 2.4686, -0.7447, -2.6557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1552, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5739,  1.4853, -2.9446],\n",
      "        [ 2.2869, -0.7485, -2.5924],\n",
      "        [ 2.2790, -0.5468, -2.5571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4051, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4350, -0.5295, -2.4923],\n",
      "        [ 1.6137,  0.9678, -3.2611],\n",
      "        [-0.3777,  2.1213, -2.8022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2888, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8167,  1.0790, -3.2249],\n",
      "        [ 0.1497,  2.0155, -3.2137],\n",
      "        [ 1.9413, -0.0081, -2.9428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2697, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4140,  0.8607, -3.1960],\n",
      "        [ 2.2269, -0.2387, -2.7436],\n",
      "        [ 0.3480,  1.6863, -2.8025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0971, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2508,  2.3109, -2.6288],\n",
      "        [ 0.1313,  2.1850, -3.0641],\n",
      "        [-0.3204,  2.1638, -3.0678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6142,  0.4984, -3.0752],\n",
      "        [-0.4089,  2.2853, -2.5511],\n",
      "        [ 1.6672,  0.5155, -3.1570]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5735, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4032,  1.4694, -2.9644],\n",
      "        [-0.6543,  2.2396, -2.5487],\n",
      "        [ 0.2784,  1.4104, -3.0784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6362, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7796,  0.3180, -2.9160],\n",
      "        [ 1.7581,  0.0083, -2.4999],\n",
      "        [-0.7169,  2.2758, -2.2900]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1148, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3073,  0.4761, -2.8969],\n",
      "        [-0.5485,  2.4299, -2.5641],\n",
      "        [-0.5165,  2.3387, -2.8297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6491, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6854,  2.5169, -2.6233],\n",
      "        [ 0.0405,  1.7256, -2.8722],\n",
      "        [-1.0018,  2.4494, -2.4769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9478, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0102,  1.6442, -2.9733],\n",
      "        [-0.4865,  2.2739, -2.5651],\n",
      "        [ 0.5115,  0.9590, -2.9252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1208, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0584,  1.8932, -2.4799],\n",
      "        [-0.3173,  1.7158, -2.4601],\n",
      "        [-0.4450,  2.1212, -2.6300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3654, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8964,  0.8949, -3.0265],\n",
      "        [ 1.4945, -0.0070, -2.4677],\n",
      "        [-0.0381,  1.6918, -2.3935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7658, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2095,  1.7992, -2.6151],\n",
      "        [ 0.5884,  0.9280, -2.6686],\n",
      "        [ 0.5052,  1.4242, -2.5759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4086, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6747,  0.9692, -2.9922],\n",
      "        [ 0.1253,  1.3673, -2.6232],\n",
      "        [ 1.1767,  0.3936, -2.7070]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8215, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6117,  0.9109, -2.4526],\n",
      "        [ 1.1096,  0.2405, -2.6739],\n",
      "        [ 0.7234,  0.8373, -2.5909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3218, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0301,  0.2392, -2.6054],\n",
      "        [-0.1152,  1.6566, -2.6258],\n",
      "        [ 1.0621,  0.3295, -2.9473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3807, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2022,  0.1387, -2.6870],\n",
      "        [ 0.8222,  0.5215, -2.6807],\n",
      "        [ 1.5019,  0.2085, -2.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9229, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1814, -0.0171, -2.3953],\n",
      "        [ 1.2917,  0.1050, -2.3687],\n",
      "        [ 0.8644,  0.3287, -2.8725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4003, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2098, -0.0088, -2.5385],\n",
      "        [ 0.7348,  0.7265, -3.0835],\n",
      "        [ 1.4015, -0.0849, -2.2928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4447, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9848,  0.0830, -2.5069],\n",
      "        [ 0.7995,  0.7928, -2.7642],\n",
      "        [ 1.2065, -0.0524, -2.5380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3716, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5495,  0.8546, -2.7739],\n",
      "        [ 0.1891,  1.1980, -2.7689],\n",
      "        [ 1.3310, -0.1657, -2.3280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2504, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0942,  1.2729, -2.3504],\n",
      "        [ 1.0779, -0.4065, -2.4106],\n",
      "        [ 1.3813, -0.0420, -2.4148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4621, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5480, -0.2617, -2.5603],\n",
      "        [ 0.3192,  0.5676, -2.4549],\n",
      "        [ 0.7547,  0.9423, -2.7714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3744, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0861,  0.1288, -2.6045],\n",
      "        [-0.0856,  1.8558, -2.4714],\n",
      "        [ 0.7612,  0.6093, -2.8162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5105, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8225,  0.5977, -2.7373],\n",
      "        [ 0.4587,  0.9635, -2.8148],\n",
      "        [ 0.1160,  1.5836, -2.7966]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3058, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4319,  0.9355, -2.7738],\n",
      "        [-0.0113,  1.6799, -2.6420],\n",
      "        [ 1.2393, -0.1061, -2.5516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1371, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5087e+00, -1.8973e-01, -2.3679e+00],\n",
      "        [-6.4345e-01,  2.3276e+00, -2.5411e+00],\n",
      "        [-1.7641e-03,  1.7475e+00, -2.8880e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1459, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0132,  1.8243, -2.6413],\n",
      "        [ 1.6736, -0.4968, -2.3767],\n",
      "        [ 1.7510, -0.1011, -2.6235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3655, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7416,  2.4118, -2.3776],\n",
      "        [-0.9924,  2.6252, -2.1313],\n",
      "        [ 0.9647,  0.4292, -2.7208]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1702, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9749, -0.4858, -2.2140],\n",
      "        [-0.6468,  2.2475, -2.5993],\n",
      "        [ 1.3274,  0.4339, -2.8803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0718, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4236,  2.3561, -2.4485],\n",
      "        [-0.9838,  2.5688, -2.2307],\n",
      "        [ 1.8878, -0.4180, -2.1344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0727, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1816, -0.5755, -2.2350],\n",
      "        [ 1.6783, -0.6151, -2.4834],\n",
      "        [-0.8494,  2.6964, -2.2864]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1085, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9914, -0.5358, -2.2099],\n",
      "        [-0.6929,  2.5187, -2.5894],\n",
      "        [ 1.6857,  0.0548, -2.6448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1803, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0246,  2.0322, -1.0181],\n",
      "        [ 1.7668, -1.1902, -1.9595],\n",
      "        [ 1.4120,  0.6048, -2.9908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2448, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3512,  2.3023, -2.9110],\n",
      "        [-1.0345,  2.1888, -0.9679],\n",
      "        [ 2.2565, -1.2818, -1.8986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1904, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1192, -1.2856, -1.8799],\n",
      "        [ 1.1824,  0.5778, -2.9180],\n",
      "        [ 2.0083, -1.1432, -1.3551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6198, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9640, -0.8926, -1.8693],\n",
      "        [-0.6228,  2.5983, -2.5483],\n",
      "        [ 0.1493,  1.6872, -3.2388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0506, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2919, -1.0529, -1.7814],\n",
      "        [ 2.0418, -1.0505, -1.8125],\n",
      "        [-0.9900,  2.5793, -2.1550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0679, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1917,  2.3763, -1.0558],\n",
      "        [-1.3101,  2.0462, -0.7578],\n",
      "        [ 2.1227, -1.2805, -1.6989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6083, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5619,  0.0429, -2.6502],\n",
      "        [-0.6913,  2.6860, -2.1870],\n",
      "        [ 2.1682, -1.0113, -2.0464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0597, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9057, -0.9137, -2.1121],\n",
      "        [ 2.3661, -1.0412, -1.9291],\n",
      "        [-1.2184,  2.4936, -0.8340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0533, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0293,  2.6380, -2.3578],\n",
      "        [ 2.0811, -0.9435, -1.9520],\n",
      "        [ 2.1046, -0.8766, -2.0748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0564, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1822,  2.2830, -2.8091],\n",
      "        [ 2.1980, -0.9179, -1.9975],\n",
      "        [-0.9802,  2.9402, -2.4105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0530, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2502, -1.0016, -2.0111],\n",
      "        [ 2.0509, -1.0850, -2.1597],\n",
      "        [ 2.3199, -0.9720, -1.8783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2229, -0.9016, -2.0787],\n",
      "        [ 2.1851, -1.0747, -1.8491],\n",
      "        [-1.0751,  3.1381, -2.0449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0561, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2199, -0.9820, -2.3820],\n",
      "        [ 1.8983, -0.5444, -2.1599],\n",
      "        [-1.0809,  3.1502, -2.0769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7116, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8293,  0.0730, -3.0373],\n",
      "        [ 0.0380,  1.7683, -3.1122],\n",
      "        [ 2.3420, -1.1624, -1.9251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0540, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0099, -0.7814, -2.1669],\n",
      "        [ 2.4096, -1.0681, -2.1805],\n",
      "        [ 2.2362, -0.9927, -2.4311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1688, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6434,  1.3112, -3.5815],\n",
      "        [-0.6072,  2.5766, -2.7356],\n",
      "        [ 2.3155, -1.0729, -2.3694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4151, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2917,  0.2382, -3.2346],\n",
      "        [-1.1540,  3.0023, -2.3547],\n",
      "        [-0.9983,  2.8958, -2.6663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0417, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9187,  2.7536, -2.4151],\n",
      "        [ 2.0679, -0.7185, -2.3343],\n",
      "        [-1.0449,  2.9061, -2.5291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9017, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2273,  2.1104, -3.0705],\n",
      "        [-0.8671,  2.6538, -2.4943],\n",
      "        [ 1.5267,  0.1616, -3.0868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4863, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3739,  2.1617, -3.1647],\n",
      "        [ 0.6031,  1.4113, -3.1725],\n",
      "        [ 0.2923,  1.8600, -3.2517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1922, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2926,  0.9465, -3.3393],\n",
      "        [ 2.5174, -1.2023, -2.2224],\n",
      "        [-0.1217,  2.0461, -2.6562]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3077,  2.1839, -2.8443],\n",
      "        [-0.2397,  2.0173, -2.9909],\n",
      "        [ 1.0944,  0.4781, -3.0305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5378, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2419,  1.8545, -2.9389],\n",
      "        [ 0.6122,  1.5387, -3.1546],\n",
      "        [ 1.5001,  0.0638, -2.8779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1128, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7579,  0.0460, -2.8408],\n",
      "        [ 2.5877, -0.9205, -2.2226],\n",
      "        [-0.0348,  2.0139, -3.1243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0445, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5809, -1.2794, -2.4915],\n",
      "        [ 2.5700, -0.9850, -2.4395],\n",
      "        [ 2.0727, -0.6655, -2.5573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4116, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0203,  0.7524, -3.2037],\n",
      "        [ 0.1378,  1.5601, -3.1247],\n",
      "        [ 0.1485,  1.8918, -3.1091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1091, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9096, -0.2765, -2.7489],\n",
      "        [ 0.0749,  1.7227, -2.9408],\n",
      "        [ 2.6112, -1.2222, -2.3188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0750, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1202, -1.1873, -2.8650],\n",
      "        [-0.1036,  1.9056, -3.0236],\n",
      "        [ 2.4234, -0.7012, -2.4395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1028, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5002, -1.4731, -2.2482],\n",
      "        [ 1.7139,  0.1199, -3.0738],\n",
      "        [ 1.3842,  0.1549, -3.0035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3889, -0.8289, -2.6538],\n",
      "        [ 2.7039, -1.1807, -2.6956],\n",
      "        [ 2.5366, -1.2294, -2.3674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5649, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1234,  2.1124, -3.2730],\n",
      "        [ 2.1491, -0.5596, -2.8131],\n",
      "        [ 1.5681,  0.3079, -2.9744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1344, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7166, -1.2605, -2.4448],\n",
      "        [ 2.8450, -1.3057, -2.5276],\n",
      "        [ 0.4922,  1.3537, -3.3312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5441, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6743,  0.1222, -3.1017],\n",
      "        [ 1.4479,  0.4297, -3.1350],\n",
      "        [ 2.0204, -0.2984, -3.1123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0586, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4772,  2.3173, -2.9715],\n",
      "        [ 2.8047, -1.0267, -2.5600],\n",
      "        [-0.4097,  2.0780, -2.9766]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2089, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1906,  0.6285, -3.2413],\n",
      "        [ 2.3047, -0.5135, -2.8191],\n",
      "        [-0.2270,  2.0446, -2.9080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1621, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1401,  2.2013, -3.1192],\n",
      "        [ 0.5136,  1.3657, -3.6270],\n",
      "        [ 2.5831, -1.0860, -2.7065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9245, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3097,  2.2158, -3.2277],\n",
      "        [-0.0500,  2.0215, -3.1114],\n",
      "        [-0.5826,  2.6132, -3.1579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0809, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0209,  1.8079, -3.2504],\n",
      "        [-0.6249,  2.5291, -3.1239],\n",
      "        [ 2.4443, -0.7759, -3.0355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1742, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5562,  2.6564, -2.9900],\n",
      "        [ 1.3677,  0.5231, -3.1302],\n",
      "        [ 1.9981, -0.1689, -2.9790]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3603, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5562,  2.3222, -3.1470],\n",
      "        [ 0.7883,  0.7397, -3.2852],\n",
      "        [ 1.2699,  0.3477, -3.0296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1465, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2382, -0.5201, -2.8999],\n",
      "        [-0.1727,  2.2089, -3.4533],\n",
      "        [ 0.4739,  1.6263, -3.1436]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9242, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3374,  2.2815, -3.1728],\n",
      "        [-0.7823,  2.6756, -2.9272],\n",
      "        [-0.4700,  2.6997, -2.9163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7506, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6559,  2.7330, -2.9973],\n",
      "        [-0.2670,  2.4256, -3.1934],\n",
      "        [-0.0599,  1.9566, -3.1818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0039, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4460,  2.7001, -3.1933],\n",
      "        [ 0.4464,  1.3279, -3.1472],\n",
      "        [ 1.7912,  0.2617, -3.2364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3974, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0756, -0.2562, -3.0180],\n",
      "        [ 0.8222,  1.3777, -3.2943],\n",
      "        [-0.2687,  2.3050, -2.8292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9211, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6969,  1.2136, -3.3417],\n",
      "        [ 1.7821,  0.0736, -3.0330],\n",
      "        [ 0.2981,  1.6637, -3.2126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6199, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4839,  1.3517, -3.0008],\n",
      "        [ 0.3375,  1.2996, -3.3379],\n",
      "        [ 0.1555,  1.6640, -3.0140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4833, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4496,  1.5651, -3.1833],\n",
      "        [ 0.9507,  0.9242, -3.4047],\n",
      "        [ 0.7377,  1.2598, -3.1408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5097, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6721,  1.2717, -3.5204],\n",
      "        [ 0.5092,  1.3002, -3.2443],\n",
      "        [ 0.7253,  0.7166, -3.4880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4239, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5203,  1.3013, -3.1906],\n",
      "        [ 0.2131,  1.3704, -3.2160],\n",
      "        [ 0.9638,  1.1622, -3.1827]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2549, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4841, -0.0519, -3.2654],\n",
      "        [ 1.5769,  0.3778, -3.2513],\n",
      "        [ 0.7355,  1.8312, -3.2202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2804, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9076, -0.4856, -3.0797],\n",
      "        [ 0.4602,  1.5557, -3.4862],\n",
      "        [ 0.7591,  1.3298, -3.2709]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5665, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1053,  1.2681, -3.6107],\n",
      "        [ 0.8292,  1.1242, -3.5260],\n",
      "        [ 2.3342, -0.5060, -3.1421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3179, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6360, -0.7405, -3.1174],\n",
      "        [ 0.2453,  1.4279, -3.2107],\n",
      "        [ 0.9241,  0.8045, -3.3610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6693, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9801,  0.9620, -3.3993],\n",
      "        [ 1.8353,  0.0725, -3.6013],\n",
      "        [ 1.2086,  0.4427, -3.2572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3664, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2907,  0.4449, -3.1467],\n",
      "        [ 1.4723,  0.1812, -3.2004],\n",
      "        [ 0.5924,  1.0949, -2.9870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2550, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9787, -0.2845, -3.3044],\n",
      "        [ 0.5152,  1.0821, -3.1921],\n",
      "        [ 1.5751,  0.0376, -2.9912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2683, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4088,  0.1391, -3.3668],\n",
      "        [ 1.2853,  0.3338, -3.3218],\n",
      "        [ 1.5270,  0.0848, -3.5885]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5210, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8081, -0.3107, -3.2008],\n",
      "        [ 1.3349,  0.5788, -3.3344],\n",
      "        [ 0.3696,  1.4658, -3.0902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5097, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5986,  0.1621, -3.4539],\n",
      "        [ 1.2861,  0.5026, -3.4063],\n",
      "        [ 1.1084,  0.6922, -3.6108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0901, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2497, -0.5476, -3.1279],\n",
      "        [ 1.8720,  0.1791, -3.4201],\n",
      "        [ 2.5256, -1.0694, -2.4648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1640, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1993, -0.5939, -3.4799],\n",
      "        [ 0.5568,  1.5278, -3.3676],\n",
      "        [ 1.9415, -0.3255, -3.4278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.4822, grad_fn=<NllLossBackward0>), logits=tensor([[ 9.4889e-01,  9.0780e-01, -3.2377e+00],\n",
      "        [ 2.1259e+00,  3.2067e-04, -2.9143e+00],\n",
      "        [ 2.9296e+00, -1.5360e+00, -2.6016e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6508, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5441, -0.8490, -3.1347],\n",
      "        [ 1.6716,  0.3254, -3.2726],\n",
      "        [ 0.4123,  1.3658, -3.2805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0616, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2236, -0.3650, -3.2897],\n",
      "        [ 2.3647, -0.6755, -2.9316],\n",
      "        [ 2.4553, -0.4412, -3.1591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2042, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0269,  1.8689, -3.1112],\n",
      "        [ 0.2960,  1.4623, -3.4532],\n",
      "        [ 1.7102,  0.0694, -3.2680]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4031, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8571,  0.4285, -3.3401],\n",
      "        [ 1.1629,  1.1935, -3.7846],\n",
      "        [ 1.4745,  0.3181, -3.6493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1510, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3609,  1.7908, -3.5211],\n",
      "        [ 2.4945, -0.2999, -3.4737],\n",
      "        [ 1.8437,  0.1421, -3.2565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1391, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2891,  1.9105, -2.8745],\n",
      "        [ 0.0425,  2.3231, -2.8043],\n",
      "        [ 1.7140,  0.1951, -3.5719]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5198, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1200,  1.5837, -2.9876],\n",
      "        [ 0.5850,  1.1314, -3.4179],\n",
      "        [ 0.4334,  1.3840, -3.3624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0544, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0393, -1.0722, -2.2045],\n",
      "        [ 2.6842, -0.6455, -3.0089],\n",
      "        [ 2.1088, -0.7849, -2.1224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3483, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3107,  2.4476, -2.8100],\n",
      "        [ 0.8852,  0.7808, -3.5970],\n",
      "        [ 0.4483,  1.4098, -3.3155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1097, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2165,  2.1006, -3.3085],\n",
      "        [-0.0737,  2.0515, -3.0337],\n",
      "        [ 2.2394, -0.6255, -2.3129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1362, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0580,  1.9737, -3.2497],\n",
      "        [ 1.7912,  0.2774, -3.6467],\n",
      "        [ 2.1743, -0.5055, -2.1859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1367, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3060,  1.6833, -3.4104],\n",
      "        [ 2.0667,  0.1789, -3.5472],\n",
      "        [ 2.4121, -1.0697, -2.7180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3821, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1717, -0.9130, -2.3319],\n",
      "        [ 0.9778,  1.4867, -3.6234],\n",
      "        [-0.1519,  2.0755, -3.0713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0744, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8954, -0.9221, -1.7989],\n",
      "        [-0.1479,  2.2726, -3.2454],\n",
      "        [ 2.4630, -0.5237, -3.0125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0804, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0743, -0.8425, -2.2316],\n",
      "        [ 2.1558, -0.3538, -3.1101],\n",
      "        [ 0.8394,  1.0835, -3.7058]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5770, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0267,  2.1852, -3.3214],\n",
      "        [ 1.7251,  0.4141, -3.5393],\n",
      "        [-0.4560,  2.2679, -2.7377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1703, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5666, -1.0575, -2.5421],\n",
      "        [ 0.7532,  1.3898, -3.5547],\n",
      "        [ 2.6397, -0.4154, -3.0302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1310, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2964,  1.9134, -3.2561],\n",
      "        [ 0.4151,  1.7564, -3.2199],\n",
      "        [-0.7005,  2.4904, -2.6422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1264, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0046,  0.2248, -3.4372],\n",
      "        [ 0.0094,  1.7543, -3.3921],\n",
      "        [-0.3417,  2.6180, -3.1264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0424, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3767, -0.5444, -2.8600],\n",
      "        [-0.9288,  2.8169, -2.4493],\n",
      "        [-0.5981,  2.7100, -2.4851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0565, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5144,  2.4006, -3.0614],\n",
      "        [ 2.2175, -0.1143, -3.4090],\n",
      "        [-1.0442,  3.2539, -2.5019]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0644, grad_fn=<NllLossBackward0>), logits=tensor([[-6.4061e-01,  2.7941e+00, -2.8912e+00],\n",
      "        [-1.0213e+00,  3.2150e+00, -2.2484e+00],\n",
      "        [ 1.3211e-03,  1.9378e+00, -3.2333e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5040, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7540,  0.7759, -3.3807],\n",
      "        [ 1.4560,  0.8142, -3.4948],\n",
      "        [ 2.2550,  0.1616, -3.4301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3937, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9158,  2.9709, -2.9791],\n",
      "        [-1.0065,  3.1167, -2.7225],\n",
      "        [-1.2042,  3.1642, -2.4421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8926, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1382,  3.1486, -2.1731],\n",
      "        [ 2.0161,  0.0138, -3.1147],\n",
      "        [-0.1047,  2.3361, -3.2959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1067, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9065,  2.9178, -2.4369],\n",
      "        [-0.3436,  2.0804, -2.9466],\n",
      "        [ 0.2760,  1.7889, -3.5691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3263, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4530,  0.4347, -3.5371],\n",
      "        [-0.5224,  2.7030, -2.8810],\n",
      "        [ 1.1696,  1.0153, -3.7336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0877, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9401,  3.0104, -2.4212],\n",
      "        [ 1.9421, -0.1099, -3.2093],\n",
      "        [ 1.9535, -0.2027, -3.4042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2795, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1937,  1.2958, -3.7439],\n",
      "        [-0.7186,  2.5319, -2.2536],\n",
      "        [-0.7332,  2.5736, -2.2607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2861, -0.1694, -3.1872],\n",
      "        [ 1.1120,  0.5459, -3.6950],\n",
      "        [ 0.0469,  1.7778, -3.6298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2268, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7026,  0.9252, -3.4545],\n",
      "        [ 1.8033,  0.3510, -3.3171],\n",
      "        [-0.4212,  2.0849, -3.1939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2260, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7561,  1.1375, -3.6135],\n",
      "        [-0.7017,  2.3902, -2.5226],\n",
      "        [ 2.1576, -0.1243, -3.3008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1022, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9707, -0.1038, -3.3015],\n",
      "        [-0.2439,  2.2302, -2.9962],\n",
      "        [-0.1721,  2.1532, -3.1095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6285,  2.3281, -2.7474],\n",
      "        [ 0.2960,  1.5105, -3.4209],\n",
      "        [ 2.1493, -0.2259, -3.6756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0086, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7674, -0.7204, -3.0041],\n",
      "        [ 2.1588, -0.1322, -3.1065],\n",
      "        [ 0.8638,  1.0712, -3.4844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0514, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7161, -0.7879, -2.8421],\n",
      "        [ 2.7856, -0.8490, -2.7363],\n",
      "        [-0.1727,  2.2228, -3.3094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2265, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2163e+00,  4.9641e-02, -3.5216e+00],\n",
      "        [ 6.7715e-01,  1.3417e+00, -3.4016e+00],\n",
      "        [-7.8296e-04,  1.8758e+00, -3.2965e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3749, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2927,  0.6790, -3.8552],\n",
      "        [ 2.6414, -0.4636, -2.8188],\n",
      "        [ 2.8599, -0.8686, -2.9569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0472, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7795, -0.9319, -2.5657],\n",
      "        [ 2.2685, -0.3789, -3.3721],\n",
      "        [ 2.5334, -0.7298, -3.0943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0570, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0941, -0.3491, -3.0105],\n",
      "        [ 2.4528, -0.4046, -3.3489],\n",
      "        [ 2.8210, -1.0819, -2.8616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8294, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2507, -0.1493, -3.3860],\n",
      "        [ 2.1114, -0.1105, -3.4464],\n",
      "        [ 2.4240, -0.2390, -3.4852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0668, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1985,  2.1393, -3.5020],\n",
      "        [ 2.7371, -0.8170, -2.9533],\n",
      "        [-0.7730,  2.7688, -2.9638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0656, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1024, -0.0869, -3.1900],\n",
      "        [ 2.6700, -0.7569, -3.1114],\n",
      "        [ 2.4961, -0.5236, -3.1101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3821, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9063, -0.0704, -3.2528],\n",
      "        [ 2.5603, -0.4889, -3.0971],\n",
      "        [ 1.1450,  0.6752, -3.4009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6739, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7109, -0.5006, -2.9305],\n",
      "        [ 1.7899,  0.3619, -3.3018],\n",
      "        [ 0.1412,  1.7048, -3.3526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0478, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7382,  2.5063, -2.7215],\n",
      "        [-0.3904,  2.4213, -2.8503],\n",
      "        [-0.7452,  2.6494, -2.8884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0829, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7362,  2.1944, -2.8512],\n",
      "        [ 0.1980,  2.1407, -3.4090],\n",
      "        [-0.4956,  2.5006, -2.8690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0468, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5827,  2.5346, -2.9804],\n",
      "        [ 2.5199, -0.8289, -2.8781],\n",
      "        [-0.4963,  2.4754, -2.9288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0527, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3873, -0.2285, -3.0777],\n",
      "        [ 2.5271, -0.7176, -3.0706],\n",
      "        [-0.7659,  2.5136, -2.7504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6532, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.6835e-02,  1.7053e+00, -3.4909e+00],\n",
      "        [ 2.2799e+00,  2.4993e-03, -3.4176e+00],\n",
      "        [ 2.9741e+00, -1.0151e+00, -2.7583e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0767, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0472,  0.0752, -3.4042],\n",
      "        [ 2.6955, -0.9024, -2.8496],\n",
      "        [-0.4292,  2.3274, -3.1896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6489,  2.7771, -2.7804],\n",
      "        [ 2.6612, -1.2134, -2.8753],\n",
      "        [-0.5824,  2.7143, -2.8556]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3978, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4520,  2.2086, -3.5197],\n",
      "        [ 0.6401,  1.3001, -3.5722],\n",
      "        [ 2.3429, -1.1710, -2.0575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3008, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6227,  2.4756, -2.9700],\n",
      "        [-0.3092,  2.0351, -3.0592],\n",
      "        [ 1.0368,  0.9220, -3.6397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8375, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1360,  0.5014, -3.5002],\n",
      "        [ 2.7360, -1.0642, -2.4812],\n",
      "        [ 0.7018,  1.8389, -3.5873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4784, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7201, -0.9058, -3.1470],\n",
      "        [-0.4253,  1.9548, -3.1569],\n",
      "        [ 0.4568,  1.4498, -3.7280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0659, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3835,  2.3823, -3.1642],\n",
      "        [-0.7721,  2.8607, -2.7074],\n",
      "        [-0.1454,  2.1172, -3.2544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0944, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2515, -0.1560, -3.5232],\n",
      "        [-0.2795,  2.0888, -3.1694],\n",
      "        [-0.1644,  2.1337, -3.2735]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0403, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6445,  2.1817, -3.2121],\n",
      "        [ 2.7933, -0.6638, -3.0691],\n",
      "        [ 2.6998, -1.3291, -2.1629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9248, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6294,  1.3017, -3.9687],\n",
      "        [ 2.8572, -1.2904, -2.7110],\n",
      "        [ 1.7584,  0.3078, -3.2384]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7858, -1.4350, -1.8907],\n",
      "        [ 2.7958, -1.1201, -2.9400],\n",
      "        [ 2.0204, -0.0168, -3.5548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5566, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6281,  0.3025, -3.3025],\n",
      "        [ 2.6162, -1.3959, -2.0991],\n",
      "        [ 2.4047, -0.1692, -3.3848]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0883, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7619,  0.2890, -3.5384],\n",
      "        [ 3.2277, -1.0327, -2.6261],\n",
      "        [ 2.4667, -1.2118, -1.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0895, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7332, -0.4641, -3.2255],\n",
      "        [ 0.3898,  1.8754, -3.8287],\n",
      "        [ 2.8353, -1.5793, -2.0896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1852, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7463,  0.1258, -3.2730],\n",
      "        [-0.6447,  2.7089, -2.9577],\n",
      "        [ 1.4104,  0.4650, -3.8288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6568, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1149,  1.7646, -3.7040],\n",
      "        [ 2.8275, -1.2131, -2.7623],\n",
      "        [ 2.1621,  0.0688, -3.2223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4989, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9796, -1.2883, -2.8651],\n",
      "        [ 2.8092, -0.7746, -3.2178],\n",
      "        [ 1.5756,  0.3982, -3.6222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9113, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3044,  0.2714, -3.7798],\n",
      "        [ 2.1698, -0.2871, -3.7499],\n",
      "        [ 1.9651, -0.2699, -3.5559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0731, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1417,  2.0210, -3.4121],\n",
      "        [-0.3809,  2.5006, -3.5205],\n",
      "        [ 2.7847, -1.6283, -2.6000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6194, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8299, -1.5607, -2.3349],\n",
      "        [ 0.3664,  1.5795, -3.8300],\n",
      "        [ 0.3843,  1.2273, -3.6193]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0855, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1206,  1.9661, -3.4964],\n",
      "        [ 2.1252, -0.4440, -3.3948],\n",
      "        [ 2.6866, -0.9946, -2.8480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1445, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5027, -0.3149, -3.3254],\n",
      "        [ 0.3673,  1.6155, -3.5822],\n",
      "        [-0.2045,  1.9244, -3.5332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0621, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7489,  0.9315, -3.9324],\n",
      "        [ 1.9122, -0.0440, -3.4276],\n",
      "        [ 2.0856, -0.2580, -3.2228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3041, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7465, -1.0353, -2.9819],\n",
      "        [ 2.3770, -0.8782, -3.4254],\n",
      "        [ 0.8373,  1.1146, -3.7544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4706,  2.3471, -3.5351],\n",
      "        [ 0.3226,  1.5864, -3.7203],\n",
      "        [ 1.5717,  0.3895, -3.5152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4748, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7648,  1.0577, -3.9429],\n",
      "        [ 1.0776,  0.6492, -3.8266],\n",
      "        [-0.3077,  2.4416, -3.7388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0274, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4478,  0.8794, -3.7930],\n",
      "        [ 0.3595,  1.5791, -3.7360],\n",
      "        [ 0.5027,  1.2616, -3.6656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7250, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2627,  2.3885, -3.6614],\n",
      "        [ 0.9389,  1.2485, -3.8365],\n",
      "        [ 0.6569,  1.5524, -3.8929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4407, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3521,  1.8993, -3.7116],\n",
      "        [ 0.7950,  0.5927, -3.5733],\n",
      "        [ 0.9044,  0.5102, -3.5955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4478, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1673,  0.6772, -3.6127],\n",
      "        [ 0.6572,  1.2203, -3.5918],\n",
      "        [ 0.4781,  1.1942, -3.4011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2727, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4951,  0.1125, -3.4491],\n",
      "        [ 1.3304,  0.6809, -3.6642],\n",
      "        [ 1.6538, -0.1112, -3.3448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5672, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0809,  0.6810, -3.8229],\n",
      "        [ 0.8248,  1.3835, -3.7399],\n",
      "        [ 1.2971,  0.3336, -3.6761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2955, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1555,  0.4321, -3.5340],\n",
      "        [ 1.4202,  0.2772, -3.7578],\n",
      "        [ 1.6515,  0.1367, -3.4017]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4252, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3450,  0.7241, -3.8077],\n",
      "        [ 0.8999,  0.7252, -3.6951],\n",
      "        [ 1.4484,  0.0467, -3.3686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2342, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0105, -0.5722, -3.4839],\n",
      "        [ 2.1838, -0.7120, -3.3758],\n",
      "        [ 0.7656,  1.0430, -3.8082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1602, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0620, -0.2411, -3.5311],\n",
      "        [ 1.9382,  0.1879, -3.7327],\n",
      "        [ 1.6462,  0.2160, -3.5880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6462, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4310, -0.5668, -3.1463],\n",
      "        [ 2.3748, -0.7938, -3.3349],\n",
      "        [ 1.7369,  0.0712, -3.8770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1033, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8459, -0.0231, -3.3677],\n",
      "        [ 1.7622, -0.1834, -3.2602],\n",
      "        [ 2.9705, -0.9043, -3.2291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0647, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7554, -0.7700, -3.1516],\n",
      "        [ 2.5389, -0.9340, -3.1581],\n",
      "        [ 1.7878, -0.2335, -3.5518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6665, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7357,  0.1194, -3.7999],\n",
      "        [ 2.0864, -0.1944, -3.5802],\n",
      "        [ 2.0020, -0.3117, -3.4293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0626, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7132,  0.2561, -3.5912],\n",
      "        [ 2.7531, -0.9128, -3.5086],\n",
      "        [ 1.5088,  0.2791, -3.7144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6951, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7729, -0.0246, -3.7519],\n",
      "        [ 2.2079, -0.7987, -3.5888],\n",
      "        [ 2.1263, -0.4061, -3.5036]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5395, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5770,  0.2518, -3.6738],\n",
      "        [ 2.7121, -1.3567, -3.1061],\n",
      "        [ 2.5035, -0.9457, -3.5457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8849, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1022e+00, -2.3272e-01, -3.6294e+00],\n",
      "        [ 2.2293e+00, -2.2803e-01, -3.7293e+00],\n",
      "        [ 1.9280e+00,  6.4903e-04, -3.4867e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.1861, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6116,  0.0569, -3.8608],\n",
      "        [ 2.2638, -0.6652, -3.1837],\n",
      "        [ 1.8066,  0.1640, -3.5797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5427, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6121, -0.9143, -3.3005],\n",
      "        [ 2.7186, -0.7597, -3.2959],\n",
      "        [ 1.6269,  0.3019, -3.8989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2626, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2544,  0.5415, -3.5320],\n",
      "        [ 2.1346, -0.4297, -3.3349],\n",
      "        [ 1.2959,  0.2375, -3.3162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0974, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8656, -0.0265, -3.6623],\n",
      "        [ 2.3135, -0.2615, -3.5796],\n",
      "        [ 2.2723, -0.3782, -3.1769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0702, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3745,  2.1871, -3.0618],\n",
      "        [ 2.2076, -0.4198, -3.6832],\n",
      "        [ 2.3142, -0.5443, -3.3119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3301, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1704, -0.2782, -3.8534],\n",
      "        [ 0.9493,  0.9462, -3.9090],\n",
      "        [ 0.5501,  2.0184, -3.7725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8680, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1975,  0.2127, -3.8079],\n",
      "        [ 1.4465,  0.7203, -4.0234],\n",
      "        [ 1.0667,  0.6988, -3.5800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1301, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7199,  0.1819, -3.4008],\n",
      "        [ 1.8493, -0.2350, -3.4882],\n",
      "        [ 2.3092, -0.3611, -3.6395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4569, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8415, -0.1201, -3.4618],\n",
      "        [ 1.1109,  0.9137, -4.1657],\n",
      "        [ 0.6369,  1.2540, -3.8618]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3935, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7613,  0.4679, -3.6620],\n",
      "        [ 0.3979,  1.7575, -3.8347],\n",
      "        [ 0.8762,  0.8841, -3.5293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1425, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0724, -0.6531, -3.4227],\n",
      "        [ 0.4741,  1.3612, -3.9797],\n",
      "        [-0.4327,  2.5247, -3.0054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0577, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1905,  0.4859, -4.0129],\n",
      "        [-0.6004,  2.6909, -2.5775],\n",
      "        [ 0.2034,  2.0803, -3.7981]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1865, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3752,  2.4820, -3.0193],\n",
      "        [ 1.2181,  0.6417, -3.6728],\n",
      "        [ 2.4648, -0.5874, -3.3162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1923, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7232,  0.2572, -3.9703],\n",
      "        [-0.3353,  2.3456, -3.2290],\n",
      "        [ 1.4791,  0.3952, -3.4775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6319, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6278,  2.2784, -2.8688],\n",
      "        [ 0.7505,  1.3577, -3.7568],\n",
      "        [ 0.8298,  1.0096, -3.9095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1963, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5204,  1.2714, -3.4991],\n",
      "        [ 0.0886,  2.1454, -3.5104],\n",
      "        [-0.3045,  2.3158, -3.4478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3535, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8990,  0.0049, -3.7712],\n",
      "        [ 1.1482,  0.8376, -3.8500],\n",
      "        [ 1.2337,  0.3975, -4.0256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4595, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9778,  0.0386, -3.7567],\n",
      "        [ 0.7809,  0.9949, -3.8871],\n",
      "        [ 0.7351,  1.3614, -4.0408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3145, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8972,  1.0055, -3.9117],\n",
      "        [ 1.9729, -0.0171, -3.6981],\n",
      "        [ 1.8646,  0.1474, -3.8430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8527, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8648,  0.8762, -3.9801],\n",
      "        [ 0.5074,  1.5676, -4.0785],\n",
      "        [ 1.2309,  0.7757, -3.8886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2277, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1968,  2.1664, -2.8919],\n",
      "        [ 0.1974,  1.9813, -3.4094],\n",
      "        [ 1.2463,  0.6045, -3.5739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7683, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8273,  1.3397, -3.6993],\n",
      "        [ 1.7577,  0.2184, -3.7965],\n",
      "        [-0.1134,  2.2306, -3.5125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1358, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4887,  1.5323, -3.8134],\n",
      "        [ 2.6891, -0.5208, -3.6375],\n",
      "        [ 2.2532, -0.5623, -3.6305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8490, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0365,  0.6072, -3.8296],\n",
      "        [ 1.0532,  0.6461, -3.4769],\n",
      "        [ 1.0264,  1.0441, -3.8916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5586, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9888,  1.0595, -4.0135],\n",
      "        [ 0.8943,  1.2486, -3.7875],\n",
      "        [ 1.1212,  0.6213, -3.7888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1862, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2768,  2.2165, -3.5097],\n",
      "        [ 1.4642,  0.1027, -3.7268],\n",
      "        [ 0.2271,  1.8201, -3.6095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3772, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2647e+00,  6.2059e-01, -4.0209e+00],\n",
      "        [ 1.0167e+00,  8.0651e-01, -4.0778e+00],\n",
      "        [ 2.1324e-04,  2.1823e+00, -3.7092e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4001, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7359,  1.6774, -3.8366],\n",
      "        [ 1.7181,  0.1464, -3.7359],\n",
      "        [ 1.1486,  1.1054, -3.6606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3912, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4354e+00, -5.1965e-02, -3.7388e+00],\n",
      "        [ 2.2442e+00,  2.2982e-03, -3.5426e+00],\n",
      "        [ 1.2384e+00,  7.2233e-01, -3.8345e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2917, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8757,  0.5321, -4.0194],\n",
      "        [ 2.0232,  0.0541, -3.9903],\n",
      "        [ 1.2148,  0.7928, -3.8062]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4542, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0936,  2.3953, -3.3647],\n",
      "        [-0.4420,  2.7203, -2.8114],\n",
      "        [ 0.5144,  1.4003, -3.9247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3502, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1331,  0.5499, -3.9471],\n",
      "        [ 0.0359,  2.0078, -3.7218],\n",
      "        [ 1.0191,  1.5359, -4.1452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2294, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8118, -1.0523, -3.2378],\n",
      "        [-0.4172,  2.5513, -3.2781],\n",
      "        [ 1.0385,  0.8579, -3.5807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1844, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4136,  2.5929, -2.9877],\n",
      "        [ 1.5532,  0.4660, -3.7060],\n",
      "        [ 1.7686,  0.2815, -3.7595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0720, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9575, -1.5130, -2.7449],\n",
      "        [-0.3847,  2.2903, -2.9594],\n",
      "        [ 0.1093,  2.1096, -3.6383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4651, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7762,  1.2311, -4.0832],\n",
      "        [ 2.9490, -1.4165, -2.6473],\n",
      "        [ 1.2592,  0.6278, -3.9852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0556, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4799,  2.8023, -2.6970],\n",
      "        [ 2.1778,  0.0083, -4.2494],\n",
      "        [ 2.9104, -1.5813, -2.3268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0547, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4973, -0.3555, -3.6548],\n",
      "        [ 0.4364,  1.7895, -3.9831],\n",
      "        [ 2.8367, -1.2716, -2.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1416, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4057,  1.9263, -4.1646],\n",
      "        [-0.0990,  2.2329, -3.5239],\n",
      "        [ 1.9405, -0.0604, -3.7642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8579, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8089,  0.3773, -4.1236],\n",
      "        [ 0.9336,  1.1596, -4.1071],\n",
      "        [ 2.1560, -0.0206, -3.5747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9862, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6035,  0.1739, -3.8902],\n",
      "        [ 1.2478,  0.3926, -3.6285],\n",
      "        [-0.0723,  2.2502, -3.5734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2622, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4936, -0.4379, -3.5986],\n",
      "        [ 0.7535,  1.7071, -4.0421],\n",
      "        [ 0.6014,  1.3096, -3.8719]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1488, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2494,  1.8596, -3.9875],\n",
      "        [-0.5154,  2.7774, -3.4740],\n",
      "        [ 1.6211,  0.2237, -4.1162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6878, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0078,  1.3570, -3.9885],\n",
      "        [ 0.8777,  0.9343, -3.9513],\n",
      "        [ 1.2569,  0.6870, -3.9320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1045, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2721, -1.3433, -3.0766],\n",
      "        [ 0.4178,  2.0466, -3.6002],\n",
      "        [ 2.0852, -0.0037, -3.5642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7933, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1803,  2.3295, -3.3382],\n",
      "        [ 0.1659,  2.1900, -4.0548],\n",
      "        [ 1.7052, -0.1506, -3.8259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0729, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9738,  2.9559, -2.0104],\n",
      "        [ 2.9689, -0.8273, -3.5020],\n",
      "        [ 1.8998,  0.1928, -4.1633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7709, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1657,  2.2732, -4.0940],\n",
      "        [ 1.9091,  0.1608, -4.0079],\n",
      "        [ 0.4583,  1.5758, -4.1847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5654, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1650,  2.7558, -3.5286],\n",
      "        [ 0.3951,  1.7885, -4.0568],\n",
      "        [-0.8846,  2.9693, -2.6286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1070, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5038,  0.1124, -3.5148],\n",
      "        [-0.5009,  2.2743, -3.4371],\n",
      "        [-0.6422,  2.9513, -2.9203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2341, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1855, -0.3140, -3.6671],\n",
      "        [-0.4830,  2.8143, -2.7969],\n",
      "        [ 0.9629,  1.2078, -4.1339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6252, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3413,  0.5297, -4.0798],\n",
      "        [ 0.9216,  1.4120, -4.0675],\n",
      "        [ 1.7559,  0.3047, -4.0019]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3708, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5920, -0.4635, -3.7129],\n",
      "        [ 2.3383, -0.2916, -3.8372],\n",
      "        [ 1.4412,  0.9159, -3.7882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4853, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0813, -0.5326, -3.6157],\n",
      "        [ 1.2849,  0.4682, -3.9864],\n",
      "        [ 1.7820,  0.2321, -3.6902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0822, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1194,  2.0155, -3.8744],\n",
      "        [-0.7227,  2.9680, -2.7062],\n",
      "        [ 2.1783, -0.3924, -3.7039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1330, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1532, -0.2637, -3.9493],\n",
      "        [ 1.8901,  0.0571, -3.8384],\n",
      "        [ 1.8360,  0.0713, -4.1011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1438, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2729,  0.4756, -4.1094],\n",
      "        [ 2.4863, -0.7587, -3.6434],\n",
      "        [ 3.0938, -1.2083, -2.9102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3825, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3972,  0.4562, -4.0053],\n",
      "        [ 1.0024,  1.1069, -3.9532],\n",
      "        [-0.1922,  2.5366, -3.9234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4029, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6378,  2.9456, -3.1056],\n",
      "        [ 1.4283,  0.6457, -4.0909],\n",
      "        [ 2.9530, -1.2554, -3.1115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1511, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7780,  1.4522, -3.9370],\n",
      "        [-1.0481,  3.1862, -1.9666],\n",
      "        [ 2.9855, -1.0630, -3.7440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1454, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4405,  0.7148, -4.0399],\n",
      "        [-0.8555,  3.0324, -2.6471],\n",
      "        [ 3.0479, -1.2821, -3.2506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5745, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8623,  1.3936, -4.1574],\n",
      "        [-1.3804,  3.0009, -1.2411],\n",
      "        [ 0.7223,  1.6066, -4.2312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3091, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1892,  0.9480, -4.2510],\n",
      "        [ 2.1538, -0.3548, -3.7102],\n",
      "        [ 2.8859, -0.9760, -3.1628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3461, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7450,  1.5441, -3.9961],\n",
      "        [-1.4429,  3.0422, -1.6337],\n",
      "        [ 1.0667,  0.9583, -3.9718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3840, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4813,  0.7672, -4.1838],\n",
      "        [-1.0289,  3.1079, -1.8940],\n",
      "        [ 2.9848, -1.4752, -2.7680]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6350, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0680,  1.2322, -4.0281],\n",
      "        [ 1.4617,  0.7634, -4.1342],\n",
      "        [-1.2277,  3.2555, -1.5847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0288, -1.5809, -2.4620],\n",
      "        [-1.2684,  3.0729, -2.2663],\n",
      "        [-1.3556,  3.4004, -1.6162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0370, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2305,  3.1456, -1.3560],\n",
      "        [ 2.8482, -1.6105, -2.5852],\n",
      "        [ 2.1357, -0.4996, -3.7183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0912, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1375, -1.5344, -2.5764],\n",
      "        [ 0.4761,  1.7711, -3.8902],\n",
      "        [ 2.9306, -1.4694, -2.6125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2642, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9024,  0.1724, -4.2303],\n",
      "        [ 1.0760,  1.3275, -4.2673],\n",
      "        [-0.3059,  2.6996, -3.4996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0473, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0533, -0.2012, -3.8766],\n",
      "        [ 2.7030, -1.3477, -2.4621],\n",
      "        [ 2.8783, -1.6823, -2.1312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3972,  3.3364, -1.7459],\n",
      "        [ 2.7740, -1.6803, -2.1491],\n",
      "        [-0.9941,  3.4941, -3.0599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1759, -1.3647, -2.7790],\n",
      "        [ 3.0169, -1.0482, -3.3909],\n",
      "        [-0.7517,  2.9062, -3.3636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8468, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2247,  0.8711, -4.2138],\n",
      "        [ 1.2882,  0.7660, -4.0773],\n",
      "        [ 0.7748,  1.3365, -4.4528]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7034, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3946,  3.4546, -1.5995],\n",
      "        [ 1.8634,  0.2563, -3.8405],\n",
      "        [ 0.2756,  2.0240, -4.1032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6773, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4617,  3.5219, -1.7091],\n",
      "        [ 2.7743, -1.7461, -1.5576],\n",
      "        [-1.6800,  3.6663, -1.2077]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4379, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0142, -1.0599, -3.6488],\n",
      "        [ 1.0757,  0.9703, -4.0456],\n",
      "        [ 1.0595,  1.1513, -4.2689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2175, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4484,  1.6076, -4.1899],\n",
      "        [-1.1526,  3.5093, -1.8128],\n",
      "        [ 1.5888,  0.7548, -4.1483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0201, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6615,  2.9876, -3.3011],\n",
      "        [ 2.9125, -1.6376, -2.2443],\n",
      "        [-1.1135,  3.3273, -1.9664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9089, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7698e-03,  2.5542e+00, -4.0352e+00],\n",
      "        [-7.9229e-01,  3.1217e+00, -2.3262e+00],\n",
      "        [ 2.4076e+00, -2.3500e-01, -3.8362e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2215, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1891, -1.4017, -2.8966],\n",
      "        [ 2.5835, -1.5444, -2.1958],\n",
      "        [ 0.8812,  1.0237, -4.0032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0373, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3806,  2.6788, -3.2531],\n",
      "        [-0.4088,  2.7173, -3.5224],\n",
      "        [ 2.9055, -1.7282, -1.7880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0371, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9223, -1.8154, -1.3935],\n",
      "        [ 2.4833, -0.2776, -4.0235],\n",
      "        [-0.7355,  3.0798, -2.2330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7899, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8982,  1.1732, -4.1201],\n",
      "        [ 1.8079,  0.2431, -4.1825],\n",
      "        [-0.4769,  2.6705, -3.0707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2283, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0798,  1.1687, -4.0861],\n",
      "        [ 2.9557, -1.0970, -3.0972],\n",
      "        [ 3.0403, -1.8923, -2.0846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2077, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8619,  1.1976, -4.1464],\n",
      "        [ 2.3187, -0.4102, -3.8617],\n",
      "        [ 2.8567, -1.7275, -2.3240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1755, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2959,  2.4563, -3.8505],\n",
      "        [ 0.4475,  1.3776, -4.0711],\n",
      "        [ 0.0286,  2.0374, -4.2909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3475, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1587, -0.5856, -3.5882],\n",
      "        [ 1.3924,  0.5394, -3.7183],\n",
      "        [ 2.7428, -1.7517, -1.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1495, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9220,  0.0108, -3.7159],\n",
      "        [ 1.5046,  0.1494, -4.1246],\n",
      "        [-0.1910,  2.3849, -3.8894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0515, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4530, -0.9654, -2.8451],\n",
      "        [-0.3618,  2.5695, -3.6100],\n",
      "        [-0.1079,  2.6415, -3.7195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3123, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0322,  0.9907, -4.0888],\n",
      "        [ 0.0316,  1.9918, -3.8551],\n",
      "        [ 1.9753, -0.0464, -3.8013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2583, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2787,  1.0345, -4.0338],\n",
      "        [ 2.3900, -0.2091, -3.8210],\n",
      "        [ 0.0986,  2.1756, -3.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2036, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3279, -0.2377, -4.0239],\n",
      "        [ 0.6698,  1.2495, -4.1406],\n",
      "        [ 0.0747,  2.4840, -3.9227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5823, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5966,  0.5240, -3.9882],\n",
      "        [ 2.3396, -0.4717, -3.4367],\n",
      "        [ 0.5828,  1.6815, -4.1391]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2579, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7801,  1.3240, -3.9578],\n",
      "        [ 1.7449,  0.5083, -3.7408],\n",
      "        [-0.2571,  2.6697, -3.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0437, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3068, -0.2167, -3.6049],\n",
      "        [ 2.7285, -0.8790, -2.8470],\n",
      "        [ 2.7956, -1.2094, -2.9628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0813, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1796, -1.7836, -2.4048],\n",
      "        [ 3.2448, -1.5396, -2.3775],\n",
      "        [ 1.6638,  0.2566, -4.1694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3224, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4448,  2.0654, -4.3083],\n",
      "        [ 1.0623,  1.1098, -4.3341],\n",
      "        [-0.2243,  2.4998, -3.4563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1995, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6644,  2.8489, -2.8470],\n",
      "        [ 1.0782,  0.7593, -4.1326],\n",
      "        [ 2.9437, -1.2936, -3.1744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0515, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4272,  2.7674, -3.0969],\n",
      "        [ 2.5166, -0.2274, -3.8892],\n",
      "        [-0.3543,  2.7060, -3.4631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0522, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2449,  2.3579, -3.8013],\n",
      "        [ 3.0936, -1.9995, -1.8701],\n",
      "        [-0.2612,  2.3969, -3.5229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0238, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4821,  2.7715, -3.4552],\n",
      "        [ 3.1356, -1.6105, -2.4810],\n",
      "        [ 2.9314, -1.1741, -2.8993]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0299, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1364, -1.8583, -2.0892],\n",
      "        [-0.6616,  2.8638, -2.7778],\n",
      "        [-0.4682,  2.6496, -3.6013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8781, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1715, -1.8721, -2.4882],\n",
      "        [-0.2819,  2.2258, -3.9448],\n",
      "        [-0.4014,  2.9702, -2.8887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1579, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8425,  1.4069, -4.2310],\n",
      "        [ 3.2622, -2.0200, -2.3873],\n",
      "        [ 3.0772, -1.7186, -2.3259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0264, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7909, -1.8652, -1.1510],\n",
      "        [-0.5510,  3.0065, -2.6159],\n",
      "        [ 2.8192, -1.4058, -2.5404]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4913,  2.9685, -3.0292],\n",
      "        [-0.4087,  2.6913, -3.5082],\n",
      "        [ 2.7776, -1.8992, -1.2509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0760, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0099,  0.5026, -4.0144],\n",
      "        [ 3.1957, -1.8837, -2.1935],\n",
      "        [ 3.1330, -1.1935, -3.0187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2049, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0263, -1.1623, -3.5858],\n",
      "        [ 1.0262,  1.2729, -3.9734],\n",
      "        [ 3.1154, -1.8905, -1.3647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0680, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1767,  1.8093, -4.2497],\n",
      "        [ 3.1925, -1.9510, -1.8223],\n",
      "        [ 3.1239, -1.9508, -2.1753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1063, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9950, -2.1221, -1.3697],\n",
      "        [ 0.6359,  1.7402, -4.0046],\n",
      "        [ 3.1754, -1.9784, -1.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2265, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9077, -0.7238, -3.5333],\n",
      "        [ 3.3699, -2.0669, -1.9215],\n",
      "        [ 3.2189, -1.8349, -2.1243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3851, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0712,  2.3995, -3.7680],\n",
      "        [ 1.2684,  0.7816, -3.7921],\n",
      "        [ 2.5136, -0.5410, -3.6940]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0856, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7663,  0.1228, -3.9849],\n",
      "        [ 2.4846, -0.9321, -3.1899],\n",
      "        [ 2.5683, -0.6586, -3.2615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3878, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2942,  2.7708, -3.6770],\n",
      "        [ 0.7387,  1.3000, -4.0389],\n",
      "        [ 2.1161, -0.1667, -3.4753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9609, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1074,  2.1327, -3.7937],\n",
      "        [ 1.8349,  0.2395, -3.6221],\n",
      "        [ 1.3647,  0.4884, -4.2410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1684, grad_fn=<NllLossBackward0>), logits=tensor([[-6.6191e-01,  2.8216e+00, -3.2522e+00],\n",
      "        [ 1.4500e+00,  3.7490e-01, -3.6875e+00],\n",
      "        [ 1.6762e+00, -3.3626e-03, -3.7553e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3403, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3735,  2.7699, -3.5832],\n",
      "        [ 1.2976,  0.4544, -3.9131],\n",
      "        [ 0.8358,  1.0039, -4.3528]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2689, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7653,  0.3632, -4.0149],\n",
      "        [ 1.2540,  0.7189, -4.1703],\n",
      "        [ 1.7899, -0.2949, -3.8394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0939, grad_fn=<NllLossBackward0>), logits=tensor([[-3.3851e-03,  2.0973e+00, -3.9993e+00],\n",
      "        [ 2.1400e+00,  1.9007e-01, -3.4854e+00],\n",
      "        [-6.2659e-01,  3.0121e+00, -3.1685e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1062, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7442,  0.4500, -4.2104],\n",
      "        [-0.3769,  2.8210, -3.6399],\n",
      "        [-0.5460,  2.9041, -3.4685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5514, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2957,  0.5451, -4.3226],\n",
      "        [ 0.6752,  1.1920, -4.2470],\n",
      "        [ 0.9482,  1.1362, -4.0303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0547, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5420,  2.6398, -3.3466],\n",
      "        [ 0.0878,  2.5285, -3.6050],\n",
      "        [-0.5218,  2.8501, -3.5057]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6332, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4205,  2.9206, -3.3714],\n",
      "        [ 1.3258,  0.6660, -4.2325],\n",
      "        [ 0.3081,  1.4796, -4.2487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0948, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7086,  3.0001, -3.0512],\n",
      "        [-0.1902,  2.8411, -3.5669],\n",
      "        [ 0.4109,  1.8793, -4.0897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2150, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8329,  1.8308, -3.9793],\n",
      "        [ 1.5018,  0.0929, -4.0916],\n",
      "        [ 0.0572,  2.2516, -4.0889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2300, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2344,  0.8281, -4.2731],\n",
      "        [-0.7430,  2.7308, -3.4369],\n",
      "        [ 0.1520,  2.0291, -3.9967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5110, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8506,  1.5236, -4.2520],\n",
      "        [ 1.3216,  0.5600, -3.9095],\n",
      "        [-0.1800,  2.6488, -3.8665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1967, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5284,  0.8058, -4.3114],\n",
      "        [-0.4218,  2.6794, -3.5400],\n",
      "        [ 1.8031, -0.0721, -3.5833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7467, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0049,  0.0772, -3.9400],\n",
      "        [-0.2978,  2.6942, -3.6777],\n",
      "        [ 2.0610,  0.0125, -3.8020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7432, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2527e+00,  5.2069e-01, -3.7846e+00],\n",
      "        [ 1.9076e+00, -9.8670e-04, -4.0303e+00],\n",
      "        [ 1.4194e+00,  9.4481e-01, -4.5237e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3640, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9122,  1.1437, -3.9387],\n",
      "        [ 1.4566,  0.5956, -4.0458],\n",
      "        [ 1.7484, -0.1122, -3.5516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1208, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9551,  0.2093, -3.9770],\n",
      "        [-0.4184,  2.8782, -3.8721],\n",
      "        [ 1.8771,  0.1213, -4.0014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1773, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3220,  1.7850, -4.0249],\n",
      "        [ 1.4450,  0.0510, -4.1092],\n",
      "        [ 2.0333, -0.2825, -3.8579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3204, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0778,  1.2602, -4.0777],\n",
      "        [-0.1253,  2.6141, -4.1071],\n",
      "        [ 0.3455,  1.4495, -4.2881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0582, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7252,  3.1144, -2.9405],\n",
      "        [ 1.9290, -0.1011, -3.9144],\n",
      "        [-0.6923,  3.0624, -3.1979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7216, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4565,  0.3070, -4.3267],\n",
      "        [ 1.2504,  1.2511, -4.0407],\n",
      "        [-0.3666,  2.8130, -3.6048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3872, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7219,  1.2109, -4.3419],\n",
      "        [ 1.5805,  0.0941, -3.7623],\n",
      "        [ 1.1829,  0.6722, -4.1583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1611, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7190,  2.8595, -3.4375],\n",
      "        [-0.4589,  2.9138, -3.2373],\n",
      "        [ 0.8692,  1.5310, -4.2326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1385, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9022,  0.1613, -3.7955],\n",
      "        [ 1.8846,  0.0470, -3.6280],\n",
      "        [-0.0260,  2.2411, -4.2099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3110, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4860,  0.1018, -4.4339],\n",
      "        [ 1.1551,  0.9386, -4.1012],\n",
      "        [ 1.9235, -0.2228, -3.6939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3000, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6027,  0.6880, -4.3417],\n",
      "        [-1.0772,  3.1627, -2.2582],\n",
      "        [ 0.9883,  0.6543, -4.4352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0270, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8359,  3.0146, -2.4528],\n",
      "        [-0.4795,  2.8160, -3.8201],\n",
      "        [-1.0905,  3.2142, -2.1147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1563, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5119,  1.3805, -4.4450],\n",
      "        [ 2.0672, -0.1989, -3.7050],\n",
      "        [-1.0632,  3.2767, -2.9089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2725, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9986,  0.8359, -4.2713],\n",
      "        [-1.0809,  3.1700, -2.4022],\n",
      "        [-0.8193,  3.2439, -2.8917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0421, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7294,  3.1427, -2.6603],\n",
      "        [-0.8793,  3.3621, -2.5649],\n",
      "        [ 2.0506, -0.3992, -3.6852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1013, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6787,  0.3774, -3.9169],\n",
      "        [-1.1110,  2.9442, -1.1150],\n",
      "        [-1.2125,  2.9292, -1.5962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4246, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0128, -0.3219, -3.8367],\n",
      "        [ 1.1418,  0.8769, -4.0920],\n",
      "        [ 1.0731,  0.8849, -4.1615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0181, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2207,  3.2165, -1.8730],\n",
      "        [-1.1946,  3.2338, -1.4744],\n",
      "        [-0.9467,  3.4193, -2.3331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0395, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2263, -0.4363, -3.5570],\n",
      "        [-1.2482,  2.9045, -1.3905],\n",
      "        [-1.2167,  3.1788, -1.7497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6016, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4195,  0.4183, -4.0580],\n",
      "        [-1.0849,  3.3717, -2.2870],\n",
      "        [-1.3588,  3.2841, -1.5908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0309, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2721,  3.2841, -2.3444],\n",
      "        [-1.1346,  3.2624, -2.4542],\n",
      "        [ 2.3285, -0.4528, -3.3583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7551, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0391,  3.0911, -1.7504],\n",
      "        [-0.9295,  3.2431, -2.7684],\n",
      "        [ 1.9228, -0.1842, -4.0650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1775, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9998, -0.6114, -3.6467],\n",
      "        [-1.0948,  3.1394, -1.8097],\n",
      "        [ 0.7823,  1.3879, -4.5945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0431, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0836,  3.0512, -1.7391],\n",
      "        [ 2.2576, -0.4067, -3.7017],\n",
      "        [-0.9913,  2.9032, -1.2443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0224, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0021,  3.1464, -2.6047],\n",
      "        [-0.7746,  3.2070, -2.7007],\n",
      "        [-1.0826,  2.8893, -1.8572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2236, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7462,  3.1026, -2.5909],\n",
      "        [ 1.2175,  1.0627, -4.2823],\n",
      "        [-1.0428,  2.8662, -2.2942]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0283, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7582,  3.0845, -2.2525],\n",
      "        [-0.6701,  3.1171, -2.4310],\n",
      "        [-0.8930,  2.8021, -1.9530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4147, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8425,  0.3517, -3.8452],\n",
      "        [-0.7635,  3.2321, -2.5379],\n",
      "        [-0.8750,  3.1796, -2.2579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0980, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4018,  2.7595, -3.8960],\n",
      "        [-0.7165,  3.0662, -2.2052],\n",
      "        [ 2.2717, -0.5315, -3.2875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9536, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3931, -0.5011, -3.7502],\n",
      "        [-0.7017,  3.0236, -2.7976],\n",
      "        [-0.2161,  2.4964, -4.0337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2397, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4712,  2.8225, -2.8744],\n",
      "        [ 2.1952, -0.4307, -3.7469],\n",
      "        [ 1.3363,  0.3266, -4.2283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2180, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5195,  2.7676, -2.9563],\n",
      "        [ 1.7914,  0.1564, -3.8698],\n",
      "        [ 1.4358,  0.8193, -4.7173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5791, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0345,  0.0403, -4.1516],\n",
      "        [ 1.6416,  0.3297, -4.0673],\n",
      "        [-0.3004,  2.6044, -3.8014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0809, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4174, -0.3750, -3.7885],\n",
      "        [ 0.2505,  2.3273, -3.5228],\n",
      "        [-0.4293,  2.4124, -3.0570]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1727, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7272,  1.8554, -4.3854],\n",
      "        [ 0.4012,  2.2334, -4.3099],\n",
      "        [ 2.1535, -0.2813, -3.7139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3193, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0071,  0.8589, -4.0700],\n",
      "        [ 2.0556, -0.0717, -4.0557],\n",
      "        [ 2.3349, -0.3369, -3.3410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1844, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4993, -0.5593, -3.8251],\n",
      "        [ 0.2474,  2.0781, -3.6111],\n",
      "        [ 0.7720,  1.6374, -3.8638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0896, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0215,  0.0307, -3.8695],\n",
      "        [ 2.1156, -0.3688, -3.7596],\n",
      "        [ 2.3330, -0.5734, -3.6171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3463, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1422,  1.8859, -3.7331],\n",
      "        [ 0.6191,  1.8783, -4.1945],\n",
      "        [ 1.0203,  1.1693, -4.5908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5007, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2985,  1.9036, -3.8448],\n",
      "        [ 1.3963,  0.4772, -4.2478],\n",
      "        [ 2.5165, -0.3222, -3.4844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3854, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3398, -0.1400, -3.7361],\n",
      "        [ 1.7126,  0.2518, -4.0207],\n",
      "        [ 1.4673,  1.1565, -4.2723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2846, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3068, -0.1828, -3.7169],\n",
      "        [ 1.0006,  1.3585, -4.3262],\n",
      "        [ 0.6931,  2.0082, -3.9592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8079, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2546, -0.6921, -3.3230],\n",
      "        [ 2.0171, -0.0672, -4.1483],\n",
      "        [ 0.2533,  1.9882, -3.6283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3069, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8844,  0.7488, -4.7078],\n",
      "        [ 0.7289,  1.7480, -4.3351],\n",
      "        [ 1.3818,  0.4368, -4.1643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0973, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3984, -0.6720, -3.4946],\n",
      "        [ 0.3721,  2.1445, -4.1281],\n",
      "        [ 2.2068, -0.2417, -3.6943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7291, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5748,  1.7250, -4.4252],\n",
      "        [ 0.5063,  2.1389, -4.1751],\n",
      "        [ 2.1337, -0.1677, -3.7069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1797, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1654,  2.1422, -3.1806],\n",
      "        [ 1.9686, -0.4709, -3.8884],\n",
      "        [ 1.5355,  0.5495, -4.3854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1263, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1167,  2.0228, -3.2935],\n",
      "        [ 0.1068,  1.9377, -3.5430],\n",
      "        [ 2.2703, -0.2028, -3.4853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0943, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4135, -0.2867, -3.3111],\n",
      "        [ 1.7034, -0.2528, -3.7693],\n",
      "        [ 2.0842, -0.4555, -3.6478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0576, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1388, -0.5013, -3.4999],\n",
      "        [ 2.2949, -0.6732, -3.4858],\n",
      "        [ 2.3827, -0.6956, -3.5668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1760, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5423,  2.1789, -4.5589],\n",
      "        [ 0.6685,  1.7707, -4.2344],\n",
      "        [ 2.4164, -0.4091, -3.3459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0752, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1320, -0.3902, -3.1385],\n",
      "        [ 2.4321, -0.6012, -3.4354],\n",
      "        [-0.0472,  2.3110, -3.2489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0514, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3321, -0.8355, -3.2887],\n",
      "        [ 2.2991, -0.7878, -3.5456],\n",
      "        [ 2.3184, -0.4745, -3.5514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8508, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9893, -0.1958, -3.9574],\n",
      "        [-0.0567,  2.0535, -4.0975],\n",
      "        [ 1.9678,  0.0684, -3.8315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1246, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0369,  2.0602, -3.0023],\n",
      "        [ 0.2889,  2.3412, -4.1027],\n",
      "        [ 0.0577,  2.0834, -2.8850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4838, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3995, -0.6613, -3.5858],\n",
      "        [ 0.1560,  1.9630, -3.4440],\n",
      "        [ 1.6642,  0.7579, -4.3079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1831, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4362,  0.7422, -4.4332],\n",
      "        [-0.0497,  2.5516, -3.6001],\n",
      "        [ 0.8208,  1.3449, -4.3045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0948, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1083,  2.2609, -3.6241],\n",
      "        [-0.0923,  2.2132, -3.5277],\n",
      "        [-0.1654,  2.1806, -3.5627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0792, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4549, -0.6696, -3.5844],\n",
      "        [ 2.1265, -0.2557, -3.2961],\n",
      "        [-0.0619,  2.2207, -3.6061]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2639, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3014,  2.3301, -3.2337],\n",
      "        [ 1.0431,  0.8994, -4.2470],\n",
      "        [-0.0851,  2.3018, -3.1166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8034, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4271,  1.9666, -4.2967],\n",
      "        [-0.1891,  2.2391, -3.1651],\n",
      "        [ 0.9748,  0.7427, -4.4946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1544, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1013,  2.2184, -2.9966],\n",
      "        [ 1.9804,  0.0844, -3.8055],\n",
      "        [ 1.6247,  0.1120, -3.9751]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0794, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3887,  2.2929, -3.2906],\n",
      "        [-0.1420,  2.3582, -3.2314],\n",
      "        [ 0.0191,  2.4448, -4.0145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0719, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1625,  2.3899, -3.8222],\n",
      "        [-0.1058,  2.5008, -3.2551],\n",
      "        [ 2.2501, -0.5034, -3.4815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0613, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1288,  2.4169, -4.0843],\n",
      "        [-0.0703,  2.5506, -3.8844],\n",
      "        [ 0.9726,  1.0334, -4.5503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0732, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0370,  2.4289, -3.3559],\n",
      "        [ 2.5234, -0.6587, -3.8404],\n",
      "        [-0.1561,  2.2661, -3.9614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0651, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1797,  2.4101, -3.1054],\n",
      "        [-0.1494,  2.3046, -3.2866],\n",
      "        [ 2.6704, -0.7927, -3.2803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0629, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2934, -0.3271, -3.6314],\n",
      "        [ 2.6193, -0.7739, -3.3834],\n",
      "        [-0.0410,  2.4564, -4.1085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0771, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2751, -0.6501, -3.7243],\n",
      "        [-0.1339,  2.3345, -3.3879],\n",
      "        [ 0.0677,  2.4253, -3.8023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6679, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4136, -0.8925, -3.4195],\n",
      "        [ 1.8479,  0.1431, -4.2297],\n",
      "        [-0.1323,  2.2405, -3.8573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0622, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0251,  2.5374, -3.6011],\n",
      "        [ 2.1326, -0.5201, -3.6503],\n",
      "        [ 2.4923, -0.8062, -3.2919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8391, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2448,  1.9870, -4.4775],\n",
      "        [ 1.0982,  1.4157, -4.3120],\n",
      "        [-0.1433,  2.6103, -3.6873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0678, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4112, -0.5620, -3.6252],\n",
      "        [-0.1553,  2.4789, -3.7622],\n",
      "        [ 2.3809, -0.1212, -4.2069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1263, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2427, -0.5259, -3.8818],\n",
      "        [ 0.7582,  1.5335, -4.2991],\n",
      "        [ 1.8091,  0.0855, -4.1312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5102, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1121,  0.9814, -4.0383],\n",
      "        [ 1.0550,  1.0505, -4.2919],\n",
      "        [ 2.0580, -0.6138, -3.9266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3827, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7898,  1.3369, -4.4904],\n",
      "        [ 2.0742, -0.5043, -3.6071],\n",
      "        [ 2.1668, -0.5619, -3.5541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0918, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1739, -0.1711, -3.5862],\n",
      "        [ 2.2206, -0.4836, -3.8440],\n",
      "        [ 0.2909,  2.4150, -4.1641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2766, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1305,  2.5036, -4.0500],\n",
      "        [ 2.1583, -0.4378, -3.9013],\n",
      "        [ 1.1676,  1.1469, -4.3105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0541, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4733, -0.7909, -3.5032],\n",
      "        [ 2.1150, -0.7386, -3.7533],\n",
      "        [-0.1216,  2.6224, -3.8528]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8655, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1043, -0.2486, -3.7946],\n",
      "        [ 0.2018,  2.3482, -4.1553],\n",
      "        [ 2.3510, -0.9894, -3.3028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7073, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7284e-03,  2.7568e+00, -3.9431e+00],\n",
      "        [ 1.6396e+00, -2.0333e-01, -4.0067e+00],\n",
      "        [-1.7091e-01,  2.5310e+00, -4.0126e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0821, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9548, -0.5636, -3.9503],\n",
      "        [ 0.1112,  2.4363, -3.8995],\n",
      "        [ 2.3239, -0.3103, -3.7271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0949, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1800, -0.4381, -3.7560],\n",
      "        [ 1.7638,  0.1212, -4.2177],\n",
      "        [ 2.5511, -0.8983, -3.9167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0766, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3896, -0.2119, -3.9342],\n",
      "        [-0.0919,  2.4086, -4.2673],\n",
      "        [ 0.0325,  2.5807, -3.9107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1049, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2899,  2.1081, -4.2994],\n",
      "        [-0.3604,  2.4335, -4.0129],\n",
      "        [ 0.0837,  2.3272, -4.3785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1277, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1457,  2.4938, -4.3122],\n",
      "        [ 0.4093,  1.7033, -4.3070],\n",
      "        [-0.3636,  2.7083, -3.9527]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9242, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4970,  1.4537, -4.1346],\n",
      "        [ 0.3826,  1.4036, -4.1418],\n",
      "        [ 1.7319, -0.0691, -3.6817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2075, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2583,  2.7381, -3.8865],\n",
      "        [-0.3062,  2.4584, -4.2019],\n",
      "        [ 1.0765,  0.6618, -4.2390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1532, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0386,  2.6304, -3.9224],\n",
      "        [ 1.8477, -0.3913, -3.7524],\n",
      "        [ 0.4974,  1.6058, -4.6387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5687, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7256,  0.3637, -4.3914],\n",
      "        [-0.1320,  2.7135, -4.1552],\n",
      "        [ 2.2207, -0.6556, -3.8943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2097, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0546,  2.5395, -4.5076],\n",
      "        [ 1.3843,  0.9284, -4.4351],\n",
      "        [ 2.1131, -0.6551, -3.8156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0736, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4518,  2.9245, -3.6592],\n",
      "        [-0.4334,  2.3099, -3.9709],\n",
      "        [ 1.8857, -0.1771, -4.1810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1149, -0.3215, -3.6571],\n",
      "        [ 0.8328,  1.1784, -4.6313],\n",
      "        [ 2.2474, -0.7374, -3.7691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5082, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2731,  2.7118, -4.1972],\n",
      "        [ 1.0214,  0.9932, -4.4451],\n",
      "        [ 0.9093,  1.0440, -4.5321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5140, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7290,  1.2666, -4.8284],\n",
      "        [ 0.3388,  2.1425, -4.6732],\n",
      "        [ 0.6326,  1.0511, -4.2466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0905, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4406,  2.5983, -4.0723],\n",
      "        [ 0.8760,  1.2132, -4.4431],\n",
      "        [ 1.9739, -0.6355, -3.9600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1650, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7328, -0.2056, -3.9061],\n",
      "        [ 0.4204,  1.5192, -4.5614],\n",
      "        [-0.1783,  2.4913, -4.1777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0976, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3751, -0.3160, -3.7485],\n",
      "        [ 2.2487, -0.6318, -3.8945],\n",
      "        [ 1.6387, -0.0745, -4.1105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1324, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1854,  1.9488, -4.2948],\n",
      "        [-0.7492,  2.9427, -3.3837],\n",
      "        [ 1.6277,  0.3895, -3.9931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2779, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6802,  2.7739, -3.8873],\n",
      "        [-0.2894,  2.6196, -4.3947],\n",
      "        [ 0.9533,  1.0547, -4.3468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2047, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9552, -0.1456, -3.9049],\n",
      "        [-0.4680,  2.8095, -3.6332],\n",
      "        [ 0.9933,  0.4404, -4.3370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2155, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8347e+00, -1.5691e-01, -4.1487e+00],\n",
      "        [ 1.4329e+00,  4.0536e-01, -4.3403e+00],\n",
      "        [ 1.4858e+00,  2.6405e-03, -3.9431e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8639, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4576,  0.3865, -4.1217],\n",
      "        [ 1.3693,  0.7475, -4.3453],\n",
      "        [ 1.7225,  0.0109, -3.8496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6000, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7809,  0.3150, -4.4727],\n",
      "        [ 2.2537, -0.4434, -4.0210],\n",
      "        [ 2.2971, -0.5769, -3.5094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2386, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3168,  2.7799, -3.8026],\n",
      "        [-0.4024,  2.8945, -3.9969],\n",
      "        [ 0.9493,  1.0782, -4.3927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5964, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1692, -0.0968, -3.7650],\n",
      "        [-0.4031,  2.6697, -4.2776],\n",
      "        [ 1.5484,  0.1248, -4.2052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1422, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7452,  0.0645, -4.1327],\n",
      "        [-0.8615,  3.1952, -3.5387],\n",
      "        [ 1.6323,  0.2916, -4.0290]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1800, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6403,  1.8612, -4.7214],\n",
      "        [ 1.3859, -0.0652, -4.1551],\n",
      "        [ 2.3126, -0.4026, -3.4884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0541, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7418,  3.0615, -3.7783],\n",
      "        [-0.5871,  3.1046, -3.6505],\n",
      "        [ 2.0859, -0.0529, -3.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1116, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3648,  0.1811, -4.1439],\n",
      "        [-0.4291,  2.8013, -4.1244],\n",
      "        [-0.8147,  2.9238, -3.6742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1077, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6968,  2.9967, -3.7790],\n",
      "        [ 0.3531,  1.5209, -4.6478],\n",
      "        [-0.7524,  3.0040, -3.1994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2327, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6317,  2.8677, -3.5856],\n",
      "        [ 0.7199,  1.2799, -4.2904],\n",
      "        [ 1.8401,  0.3873, -4.0432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2993, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2264, -0.2256, -4.2489],\n",
      "        [ 1.2476,  0.7993, -4.4933],\n",
      "        [ 0.5762,  1.5635, -4.6770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6701,  0.3139, -4.5367],\n",
      "        [ 1.7654, -0.0475, -3.7537],\n",
      "        [-0.4270,  3.1548, -3.9576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9982, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4315, -0.5993, -3.6083],\n",
      "        [-0.2176,  2.5765, -4.4057],\n",
      "        [ 2.0182, -0.3665, -3.8251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5587, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9161,  3.3156, -3.3861],\n",
      "        [ 0.4843,  1.7799, -4.2979],\n",
      "        [-0.0183,  2.0468, -4.3108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4147, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7186,  1.4413, -4.8993],\n",
      "        [-0.2108,  2.1708, -4.3262],\n",
      "        [-0.4060,  2.9648, -4.1043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5464, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4999,  0.1302, -4.2148],\n",
      "        [ 1.4272,  0.4333, -4.2581],\n",
      "        [ 2.0064, -0.2800, -4.0747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0841, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4703, -0.4885, -3.7855],\n",
      "        [ 1.8881, -0.4734, -3.8629],\n",
      "        [-0.2301,  2.8303, -4.3428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6742, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2643,  1.8065, -4.1167],\n",
      "        [ 1.3300,  0.3516, -4.2958],\n",
      "        [ 1.3798,  0.1300, -4.1195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6875, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5039,  0.3517, -3.8350],\n",
      "        [ 2.2841, -0.8098, -3.7445],\n",
      "        [ 1.1686,  0.9348, -4.3403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1286, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5654, -0.8966, -3.3868],\n",
      "        [ 2.2707, -0.4432, -3.6431],\n",
      "        [ 1.5737,  0.4564, -4.0839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4386, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0456e+00,  7.3927e-01, -4.4879e+00],\n",
      "        [ 4.1238e-01,  1.7205e+00, -4.4957e+00],\n",
      "        [ 1.4470e+00,  3.3537e-03, -4.2665e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2303, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7132,  2.9431, -3.8183],\n",
      "        [ 0.5359,  1.4739, -4.7074],\n",
      "        [ 1.3227,  0.3850, -4.5514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2543, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5861,  0.2459, -4.2850],\n",
      "        [ 1.1933,  0.3614, -4.0909],\n",
      "        [ 1.7934,  0.0529, -4.3892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1054, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4122,  1.6827, -4.4144],\n",
      "        [ 2.3713, -0.9135, -3.4483],\n",
      "        [-0.6131,  3.0095, -3.8532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5804, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2714,  0.7481, -4.3175],\n",
      "        [ 0.3622,  1.6461, -4.3735],\n",
      "        [ 0.8067,  1.2359, -4.2998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5874, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3353,  0.5912, -4.2963],\n",
      "        [ 0.5904,  1.5635, -4.6018],\n",
      "        [ 0.7855,  1.4009, -4.6867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3076, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6799,  2.7988, -3.8739],\n",
      "        [ 1.2810,  0.7391, -4.3142],\n",
      "        [ 1.4861,  0.8612, -4.5113]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5950, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2086, -0.2180, -4.0271],\n",
      "        [ 1.3741, -0.0836, -4.3209],\n",
      "        [-0.3457,  3.2098, -3.8924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1200, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4579,  1.6467, -4.5408],\n",
      "        [-0.5649,  2.9946, -4.0001],\n",
      "        [-0.2201,  2.5160, -4.4633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5078, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0527,  0.4744, -4.4982],\n",
      "        [ 2.6662, -0.8598, -3.3974],\n",
      "        [ 1.1359,  0.6064, -4.1524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1334, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3230,  1.6471, -4.6349],\n",
      "        [ 0.0896,  2.2088, -4.2991],\n",
      "        [ 2.4392, -0.6195, -3.6112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2609, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6688,  0.1954, -4.2469],\n",
      "        [ 0.3337,  1.4258, -4.3909],\n",
      "        [ 1.5412,  0.4141, -4.3697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0881, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7634,  0.0257, -4.2515],\n",
      "        [-0.1457,  2.5164, -4.3724],\n",
      "        [-0.6037,  2.8589, -4.1371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0891, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6948,  2.7997, -3.7040],\n",
      "        [ 2.5639, -0.5328, -3.9033],\n",
      "        [ 1.7059,  0.1302, -4.2065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1148, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4581, -1.1699, -3.2343],\n",
      "        [-0.7188,  2.8500, -4.1640],\n",
      "        [ 1.6245,  0.5138, -4.5098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7031, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1853,  2.0310, -4.3184],\n",
      "        [ 2.3637, -0.7616, -4.0220],\n",
      "        [-0.0872,  2.5374, -4.4043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4601, -0.9591, -3.4454],\n",
      "        [ 0.8974,  1.1294, -4.3709],\n",
      "        [-0.6613,  2.7830, -4.2793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2645, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2301,  2.5928, -4.1985],\n",
      "        [ 2.6797, -0.9856, -3.9849],\n",
      "        [ 1.1132,  1.1394, -4.5433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4248, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3470,  0.5338, -4.5403],\n",
      "        [ 2.3355, -0.8522, -3.4661],\n",
      "        [-0.3498,  2.6683, -3.9723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0499, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6222, -0.9777, -3.4532],\n",
      "        [-0.6798,  2.9282, -3.8526],\n",
      "        [ 0.1497,  2.4912, -4.5763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2003, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4473,  1.7119, -4.5911],\n",
      "        [ 0.4872,  1.5361, -4.8760],\n",
      "        [-0.3854,  2.6223, -4.2405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0775, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0370,  0.1037, -4.3370],\n",
      "        [ 2.2213, -0.3527, -4.4214],\n",
      "        [ 2.7293, -1.2488, -3.2217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4761, -1.3252, -3.3806],\n",
      "        [-0.7322,  2.9736, -3.7614],\n",
      "        [-0.6162,  3.2086, -3.9101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7145, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2049,  2.1474, -4.3554],\n",
      "        [-0.4019,  2.8109, -3.9679],\n",
      "        [ 2.6619, -1.0817, -3.6463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0380, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5834,  2.6736, -3.9970],\n",
      "        [ 2.8108, -1.0926, -3.3692],\n",
      "        [ 2.3431, -0.5951, -4.1557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0408, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5298, -1.1170, -3.5058],\n",
      "        [-0.1972,  2.5403, -4.0634],\n",
      "        [-0.6553,  2.8653, -3.8272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3222, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8979, -0.5061, -3.9755],\n",
      "        [ 1.6513,  0.0569, -4.2660],\n",
      "        [ 0.9131,  0.9226, -4.6344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2910, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0643,  2.4449, -4.4184],\n",
      "        [ 2.5816, -0.9414, -3.7767],\n",
      "        [ 0.4984,  1.8090, -4.5841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1843, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4816,  2.8681, -4.2202],\n",
      "        [ 0.8026,  1.4523, -4.5405],\n",
      "        [ 2.0603, -0.2929, -3.9637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9232,  3.2004, -3.4923],\n",
      "        [ 2.6080, -1.2063, -3.5099],\n",
      "        [-1.0753,  2.9459, -3.8180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3721, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6445,  3.1307, -3.8914],\n",
      "        [ 1.3104,  0.6947, -4.4177],\n",
      "        [-0.5088,  2.6410, -4.3423]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2020, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1258,  1.9003, -4.4827],\n",
      "        [ 1.5956, -0.2962, -4.1870],\n",
      "        [ 1.5093,  0.4710, -4.5916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4650, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1474,  0.7350, -4.4725],\n",
      "        [ 1.0729,  0.4593, -4.6333],\n",
      "        [-0.5496,  2.7418, -4.0881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3766, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6650,  0.3050, -4.2378],\n",
      "        [ 1.7853,  0.2051, -4.6272],\n",
      "        [-0.6966,  2.9881, -3.7968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1011, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6465,  1.8637, -4.5375],\n",
      "        [-0.8851,  3.0068, -3.6964],\n",
      "        [-0.7989,  3.0970, -3.6941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3037, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3704,  0.7235, -4.6110],\n",
      "        [ 0.5654,  1.4504, -4.7394],\n",
      "        [ 1.7857, -0.1122, -4.4545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2369, -0.5584, -3.9580],\n",
      "        [ 1.8662,  0.6698, -4.4058],\n",
      "        [ 0.7337,  1.4546, -4.7192]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8891, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7734,  2.9024, -3.8609],\n",
      "        [-0.0706,  2.3431, -4.7803],\n",
      "        [ 0.2015,  2.0983, -4.5359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3199, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3496,  1.2069, -4.5022],\n",
      "        [ 2.2290, -0.0948, -4.3182],\n",
      "        [ 2.0479, -0.2661, -4.0052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8560, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4349,  2.8555, -4.0460],\n",
      "        [-0.0658,  2.3195, -4.4748],\n",
      "        [-0.2857,  2.5886, -4.1822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4167, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4585,  1.9118, -4.7055],\n",
      "        [ 0.9574,  1.4510, -4.2755],\n",
      "        [-0.1480,  2.5409, -4.3704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1184, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5727,  0.2393, -4.4439],\n",
      "        [-0.2957,  2.4348, -4.2254],\n",
      "        [-0.3092,  2.5816, -4.3837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1593, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0864,  2.3309, -4.4476],\n",
      "        [-0.1321,  2.6635, -4.1339],\n",
      "        [ 0.5302,  1.4718, -4.4378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0369, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7154,  2.7826, -4.1764],\n",
      "        [-0.3464,  2.7244, -4.4116],\n",
      "        [-0.5047,  2.8925, -4.1849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4512, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5453,  0.2477, -4.5339],\n",
      "        [ 0.7532,  1.2540, -4.6256],\n",
      "        [ 0.3140,  2.2653, -4.4349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2842, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4084,  2.8453, -4.0712],\n",
      "        [ 0.5244,  1.2028, -4.6309],\n",
      "        [ 0.7103,  1.4212, -4.5792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0845, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3151,  2.8622, -4.1610],\n",
      "        [-0.4448,  2.5615, -4.3064],\n",
      "        [ 1.9072,  0.1662, -4.8415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2278, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1577,  2.7651, -4.2425],\n",
      "        [ 1.2052,  0.9948, -4.5209],\n",
      "        [-0.4540,  2.9056, -4.3258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0573, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4528,  2.8305, -4.0095],\n",
      "        [-0.3384,  2.5835, -3.9743],\n",
      "        [-0.2028,  2.2956, -4.1962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7005, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2201,  0.9600, -4.8814],\n",
      "        [ 0.4296,  1.8295, -4.6533],\n",
      "        [ 0.6351,  1.6245, -4.5502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8737, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7661, -0.0944, -4.5250],\n",
      "        [ 0.7374,  1.3253, -4.6407],\n",
      "        [ 1.7930,  0.1031, -4.4957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0490, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2402,  2.4920, -4.3751],\n",
      "        [-0.4638,  2.9107, -4.2048],\n",
      "        [-0.3993,  2.6302, -4.0701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3297, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9601,  0.0235, -4.3568],\n",
      "        [ 2.0782, -0.0551, -4.0487],\n",
      "        [ 0.8395,  0.9253, -4.5915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1624, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6097, -0.0987, -4.5266],\n",
      "        [ 0.4609,  1.7690, -4.3808],\n",
      "        [ 2.2293, -0.3026, -4.2062]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2749, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1942,  2.5847, -4.3178],\n",
      "        [-0.4179,  2.7443, -4.0579],\n",
      "        [ 1.0004,  1.0523, -4.7186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3999, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3446,  0.7261, -4.5833],\n",
      "        [ 0.8318,  0.5893, -4.4583],\n",
      "        [ 1.6751,  0.0685, -4.3369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0771, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1921, -0.3350, -4.5645],\n",
      "        [-0.1419,  2.3555, -4.3592],\n",
      "        [-0.1027,  2.4854, -4.6400]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0672, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4920, -0.6306, -3.9979],\n",
      "        [ 2.4994, -0.5882, -4.1405],\n",
      "        [ 0.0620,  2.2100, -4.8544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3192, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4824,  0.2697, -4.5685],\n",
      "        [ 0.9141,  1.3152, -4.8499],\n",
      "        [ 1.7675,  0.1428, -4.3034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7895, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0526,  0.2763, -4.3819],\n",
      "        [ 1.3208,  0.5786, -4.6559],\n",
      "        [ 2.5202, -0.6265, -4.2157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2748, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3456, -0.4773, -4.3744],\n",
      "        [ 0.5949,  1.6443, -4.8890],\n",
      "        [ 0.6976,  1.2285, -4.6393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4127, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1297,  2.5280, -4.8416],\n",
      "        [ 1.9805, -0.3036, -4.2400],\n",
      "        [ 1.1968,  0.5772, -4.7647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8680, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8596,  1.0270, -4.8510],\n",
      "        [ 1.7152,  0.1500, -4.3503],\n",
      "        [ 0.5915,  1.9363, -4.7082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2193, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8341, -0.2296, -4.3028],\n",
      "        [ 1.3807,  0.1040, -4.4818],\n",
      "        [ 0.4410,  1.5445, -4.4877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7476, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6306, -0.5181, -3.9450],\n",
      "        [ 1.8819, -0.0987, -4.2025],\n",
      "        [ 2.1846, -0.2298, -3.8976]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1566, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4370,  2.9964, -3.9804],\n",
      "        [ 1.8241, -0.0376, -4.6156],\n",
      "        [ 0.5837,  1.6730, -4.6869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9571, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8363,  0.9745, -4.8734],\n",
      "        [ 2.3760, -0.4647, -4.0473],\n",
      "        [ 1.7590,  0.3890, -4.5781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0524, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3147, -0.3478, -4.0492],\n",
      "        [ 2.3984, -0.3279, -4.3061],\n",
      "        [-0.8394,  2.9521, -3.6476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0458, -0.4032, -4.3391],\n",
      "        [-0.7908,  2.8830, -3.7271],\n",
      "        [-0.1015,  2.5285, -4.5107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3367, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3443,  1.0169, -4.7546],\n",
      "        [ 2.4936, -0.5942, -4.2044],\n",
      "        [ 1.5787,  0.9256, -4.6685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0953, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7015,  2.9877, -3.8638],\n",
      "        [ 2.5686, -0.2771, -3.9243],\n",
      "        [ 1.6715,  0.1666, -4.4442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6149, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5474,  1.7247, -4.6334],\n",
      "        [ 1.2030,  0.3993, -4.7678],\n",
      "        [-0.8793,  2.8307, -3.6950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0946, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5935,  3.0808, -3.6625],\n",
      "        [-0.5880,  2.9905, -4.2082],\n",
      "        [ 1.5698,  0.2042, -4.4635]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8581, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1920,  2.3949, -4.4511],\n",
      "        [ 2.1581,  0.1365, -4.2478],\n",
      "        [ 1.9974,  0.0907, -4.4880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1795, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5429, -0.7469, -3.9612],\n",
      "        [ 1.7943, -0.1662, -4.5072],\n",
      "        [ 0.6094,  1.4266, -4.8249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3426, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8313,  0.1259, -4.3778],\n",
      "        [-0.6757,  3.0638, -3.5328],\n",
      "        [ 0.8671,  1.1293, -4.7784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0485, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0348,  2.2759, -4.3904],\n",
      "        [-0.9305,  3.0745, -3.6135],\n",
      "        [-0.7911,  2.9735, -3.9403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0348, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2503, -0.8116, -3.7486],\n",
      "        [ 2.6548, -0.7644, -4.1164],\n",
      "        [-0.7768,  3.0364, -3.7027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0380, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7274,  2.9344, -3.6559],\n",
      "        [ 2.6052, -0.9913, -3.7891],\n",
      "        [ 2.3464, -0.4722, -4.8022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1517, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4899,  0.5380, -4.5823],\n",
      "        [-0.7810,  2.9852, -3.9365],\n",
      "        [ 1.9527, -0.2784, -4.5905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8101, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3098,  0.9950, -4.6797],\n",
      "        [ 0.3506,  1.9847, -4.8185],\n",
      "        [ 2.2994, -0.3789, -4.4046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0631, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6746,  2.9715, -3.8152],\n",
      "        [ 2.2122, -0.1715, -4.7510],\n",
      "        [-0.1868,  2.4037, -4.4750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0417, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2334,  2.8130, -4.4768],\n",
      "        [-0.3906,  2.6166, -4.5636],\n",
      "        [ 2.5260, -1.0659, -3.7610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0220, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8395, -1.4674, -3.4420],\n",
      "        [ 2.5744, -1.2663, -3.3571],\n",
      "        [ 2.7103, -0.9556, -3.7294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2459,  2.1500, -4.5622],\n",
      "        [ 2.7510, -1.3331, -3.5975],\n",
      "        [-0.7438,  3.0790, -3.7049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7765,  2.8755, -3.8191],\n",
      "        [-0.7427,  3.1592, -3.6010],\n",
      "        [ 2.5943, -1.0352, -3.8242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0365, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6411, -1.0967, -3.7523],\n",
      "        [-0.3061,  2.7728, -4.1731],\n",
      "        [-0.5806,  2.6850, -4.3265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9239, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8798, -1.0500, -3.8520],\n",
      "        [ 2.0868, -0.5762, -4.0657],\n",
      "        [ 2.7791, -1.3090, -3.4619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0222, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5551, -1.0669, -3.9846],\n",
      "        [ 2.5680, -1.2431, -3.5848],\n",
      "        [ 2.9209, -1.3780, -3.5992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0530, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3774,  2.8923, -4.3036],\n",
      "        [ 0.0243,  2.2799, -4.4151],\n",
      "        [ 2.7168, -1.2495, -3.8349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6251, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6997,  1.5293, -4.6040],\n",
      "        [ 2.5827, -1.0433, -3.8743],\n",
      "        [ 1.4061,  0.1811, -4.8455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6908, -1.4351, -3.3138],\n",
      "        [-0.6460,  3.0777, -4.0267],\n",
      "        [-0.5504,  2.9905, -4.1965]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0795, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4320, -0.6718, -4.1978],\n",
      "        [-0.7258,  3.0942, -3.9442],\n",
      "        [-0.1989,  2.4866, -4.4060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3809, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4738, -1.1384, -3.5723],\n",
      "        [-0.6998,  3.1014, -3.5470],\n",
      "        [ 1.3940,  0.7156, -4.7490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4211, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9017, -1.0764, -3.6643],\n",
      "        [ 2.7971, -1.0250, -3.5872],\n",
      "        [ 1.2788,  0.4106, -5.0104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0821, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7388, -0.9678, -3.8819],\n",
      "        [-0.3450,  2.6811, -4.3435],\n",
      "        [ 0.3492,  2.0250, -4.7723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6475, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1856, -0.5236, -4.1839],\n",
      "        [ 1.6461, -0.0396, -4.4078],\n",
      "        [-0.7892,  3.2198, -3.9023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0697, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8510,  0.0968, -4.4452],\n",
      "        [ 2.6757, -0.9849, -3.8210],\n",
      "        [-0.7584,  3.1413, -3.7512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0653, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0572,  2.6190, -4.4103],\n",
      "        [ 2.1520, -0.3656, -4.4060],\n",
      "        [ 2.2179, -0.7936, -4.2564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0995, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4782, -0.4506, -4.0328],\n",
      "        [-0.7118,  3.2264, -3.3443],\n",
      "        [ 1.5702,  0.1805, -4.4571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3790, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5240,  1.8322, -4.8057],\n",
      "        [ 0.5436,  1.9833, -4.9145],\n",
      "        [ 1.1475,  1.1246, -4.7097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4749, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2519,  0.4758, -4.7131],\n",
      "        [ 1.3060,  1.0409, -4.7490],\n",
      "        [ 1.6428,  0.1723, -4.5769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0963, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9776, -0.3759, -4.2482],\n",
      "        [ 2.0036,  0.3640, -4.9103],\n",
      "        [-0.8458,  3.2149, -3.8022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5750, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7158,  0.4085, -4.9127],\n",
      "        [ 1.8526,  0.0213, -4.4594],\n",
      "        [-0.5899,  3.0440, -3.8768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0835, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9700,  3.2484, -3.3150],\n",
      "        [ 1.4052, -0.0148, -4.5043],\n",
      "        [-1.0209,  3.2092, -3.2695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4668, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1771, -0.3283, -4.5916],\n",
      "        [ 1.5497,  0.5602, -4.6903],\n",
      "        [-0.9524,  3.4054, -3.4530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1603, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1861,  1.7757, -4.8326],\n",
      "        [ 2.1557, -0.4296, -3.8266],\n",
      "        [ 1.8053,  0.3927, -4.7026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0152, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9965,  3.1882, -3.5346],\n",
      "        [-1.0829,  3.2734, -3.4465],\n",
      "        [-0.8053,  3.4196, -3.4764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0861, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0375, -0.1361, -4.2891],\n",
      "        [-0.9999,  3.4466, -3.6122],\n",
      "        [ 1.8843, -0.0501, -4.4956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7268, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6679,  3.1418, -4.2842],\n",
      "        [-1.0258,  3.5373, -3.4469],\n",
      "        [ 0.0888,  2.1098, -4.5646]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6080, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4855, -0.7697, -3.7553],\n",
      "        [-0.3559,  2.7167, -4.3480],\n",
      "        [ 1.7296,  0.2756, -4.4730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6187, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5367,  0.5768, -4.8478],\n",
      "        [ 0.8449,  1.5252, -4.8120],\n",
      "        [ 1.1403,  0.5407, -4.8250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0791, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9671,  3.3357, -3.7398],\n",
      "        [ 0.4141,  1.8711, -4.9027],\n",
      "        [-1.2142,  3.2204, -3.6271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1206, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4796, -0.6052, -4.2654],\n",
      "        [ 0.3379,  1.6683, -4.5972],\n",
      "        [-0.1862,  2.3174, -4.3842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2918,  2.4207, -4.6532],\n",
      "        [ 1.9988, -0.2721, -4.3063],\n",
      "        [ 1.5967,  0.3246, -4.6103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6318, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5632,  1.4977, -4.8671],\n",
      "        [ 0.6417,  1.9487, -4.9050],\n",
      "        [-1.0326,  3.1868, -3.8007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0388, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6286,  2.9732, -4.0669],\n",
      "        [ 2.3274, -0.2731, -4.2945],\n",
      "        [-0.8855,  3.3085, -3.7661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0397, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0560, -0.7391, -3.9732],\n",
      "        [-1.1018,  3.4024, -3.8410],\n",
      "        [ 2.4347, -0.6515, -4.2609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1263, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6341,  0.5095, -4.7481],\n",
      "        [ 2.6616, -1.1040, -3.8105],\n",
      "        [ 2.2950, -0.3148, -4.4956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0978, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3190,  1.6986, -4.4838],\n",
      "        [ 2.7206, -0.8114, -4.3506],\n",
      "        [ 2.4121, -0.9019, -3.8544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1504, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6869, -0.8157, -3.8943],\n",
      "        [ 0.6587,  1.4301, -5.0236],\n",
      "        [ 2.4189, -0.8469, -3.9265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0483, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2771, -0.9446, -3.5311],\n",
      "        [-0.4138,  2.6848, -4.5550],\n",
      "        [-0.2754,  2.5572, -4.3803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7796, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8604,  1.3653, -5.1871],\n",
      "        [ 1.1487,  0.7544, -4.9251],\n",
      "        [ 1.2258,  0.7592, -4.9040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9787,  3.2604, -4.1510],\n",
      "        [-0.7745,  3.0780, -3.7846],\n",
      "        [-1.0158,  3.2221, -3.6215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2036, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4206, -0.5890, -4.1380],\n",
      "        [ 2.1781, -0.4150, -4.5423],\n",
      "        [ 1.2765,  0.8101, -4.9547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0144, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9319, -1.2560, -3.5538],\n",
      "        [-1.0167,  3.3550, -3.8063],\n",
      "        [-1.2222,  3.1616, -3.8454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8096, -1.3588, -3.2376],\n",
      "        [ 2.4814, -0.7018, -3.9830],\n",
      "        [ 2.0554,  0.0416, -4.8147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0291, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4304, -0.5813, -4.2122],\n",
      "        [-0.8083,  3.2594, -4.1795],\n",
      "        [ 2.7458, -1.2669, -3.2244]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0599, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2546,  2.6341, -4.3817],\n",
      "        [ 3.0093, -1.2980, -3.6074],\n",
      "        [ 2.0058, -0.1601, -4.4407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0612, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0997,  3.1532, -4.2040],\n",
      "        [ 1.6781, -0.1180, -4.6512],\n",
      "        [-1.0041,  3.3115, -4.0369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1388, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4701,  0.5360, -4.4280],\n",
      "        [-0.2422,  2.3999, -4.5062],\n",
      "        [-0.9174,  3.4340, -3.9254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1748, -1.6925, -2.6452],\n",
      "        [ 2.6147, -1.0351, -3.9744],\n",
      "        [ 1.8981,  0.0567, -4.6050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0575, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0707,  1.9574, -4.6266],\n",
      "        [-1.0781,  3.3602, -3.9989],\n",
      "        [-0.4481,  2.8910, -4.5572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0170, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9676, -2.0372, -2.5515],\n",
      "        [ 2.7581, -1.9638, -1.6923],\n",
      "        [-0.8482,  3.1014, -3.7628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2625, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8963, -1.7317, -2.5573],\n",
      "        [ 1.1450,  1.0190, -4.7172],\n",
      "        [ 2.8343, -1.8689, -2.4619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6765, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1920, -1.6858, -3.2978],\n",
      "        [ 2.6628, -1.9097, -1.7170],\n",
      "        [ 0.1308,  2.1984, -4.6359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1066, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9842, -1.6329, -3.3136],\n",
      "        [ 1.5395,  0.3935, -4.6745],\n",
      "        [-0.4868,  3.0072, -4.1225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4650, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3042,  3.3134, -3.5648],\n",
      "        [-0.4797,  2.5446, -4.5720],\n",
      "        [ 2.8839, -1.4361, -3.0295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3528, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6890,  1.2526, -4.8175],\n",
      "        [-0.6674,  2.8882, -4.1652],\n",
      "        [-0.9314,  3.3748, -4.2492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0552, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5529, -1.0222, -3.1624],\n",
      "        [ 2.7883, -0.8088, -3.6213],\n",
      "        [ 0.0349,  2.2338, -4.6717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1221, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1519,  1.9000, -4.6453],\n",
      "        [ 2.1938,  0.3085, -4.8940],\n",
      "        [ 2.2923, -0.4793, -3.9325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4129, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1947,  3.1973, -3.5652],\n",
      "        [-0.7933,  3.4037, -3.9711],\n",
      "        [ 1.5124,  0.6590, -4.7258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0969, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2115, -0.6207, -3.6461],\n",
      "        [ 1.6351,  0.2246, -4.7723],\n",
      "        [-0.9964,  3.5677, -3.7426]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0876, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6044,  0.2851, -4.8167],\n",
      "        [-1.1778,  3.5958, -3.8098],\n",
      "        [-0.9920,  3.2166, -3.9587]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9678,  2.9963, -3.8178],\n",
      "        [-1.0998,  3.3353, -3.6634],\n",
      "        [-0.9787,  3.3776, -4.0744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1347, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3187, -0.3063, -3.8564],\n",
      "        [ 1.6477,  0.3744, -4.6997],\n",
      "        [ 2.3413, -0.1299, -3.3685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0892, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1910, -0.4054, -3.5444],\n",
      "        [ 2.2406, -0.0290, -4.0158],\n",
      "        [ 2.0874, -0.2834, -3.5731]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0859, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2478,  2.0492, -4.7089],\n",
      "        [ 2.0516, -0.4603, -3.3831],\n",
      "        [-0.9674,  2.8834, -4.0052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1866, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0392, -0.1270, -4.0845],\n",
      "        [ 1.9509, -0.2929, -3.6316],\n",
      "        [ 0.5071,  1.3987, -4.6763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0727, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8105,  3.3705, -4.0229],\n",
      "        [ 1.9245, -0.1005, -3.9670],\n",
      "        [ 2.2879, -0.2703, -4.2048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5842, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2743, -0.4378, -4.0899],\n",
      "        [ 0.4273,  1.7584, -5.1960],\n",
      "        [ 2.0555, -0.0197, -3.9598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0643, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9864, -0.1635, -3.8142],\n",
      "        [ 2.4465, -0.2544, -3.9830],\n",
      "        [-1.0297,  3.3305, -3.8386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1212, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2168,  3.4150, -3.5592],\n",
      "        [ 0.4888,  1.8367, -4.9092],\n",
      "        [ 2.0181, -0.0510, -4.0333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0055,  3.1518, -3.7033],\n",
      "        [-1.1336,  3.3751, -3.4798],\n",
      "        [-0.8911,  3.4235, -3.3776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7601, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3193, -0.2225, -3.6722],\n",
      "        [ 2.1649,  0.2205, -4.5323],\n",
      "        [ 0.2195,  2.2599, -4.9122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3652, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1491,  3.1933, -3.5658],\n",
      "        [ 1.5122,  0.8604, -5.0192],\n",
      "        [-1.3776,  3.4305, -3.2219]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5331, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7400,  0.8552, -4.6386],\n",
      "        [-1.1013,  3.1436, -3.5641],\n",
      "        [ 0.6186,  1.4840, -5.0553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3348,  3.3990, -3.2455],\n",
      "        [ 2.4566, -0.5817, -3.6899],\n",
      "        [-1.4027,  3.3557, -3.1309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3746, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2916,  3.3971, -3.3055],\n",
      "        [ 0.8451,  1.3818, -4.9597],\n",
      "        [ 2.1241,  0.0094, -4.3474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1532, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1003,  3.1657, -3.3585],\n",
      "        [-0.0779,  2.5811, -4.7891],\n",
      "        [ 1.3238,  0.5360, -5.1300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9505, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2339,  3.2962, -3.3446],\n",
      "        [-0.3573,  2.3897, -4.6419],\n",
      "        [-0.5490,  2.9784, -4.6678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9683, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3594,  2.4612, -4.4885],\n",
      "        [-1.1555,  3.3394, -3.5449],\n",
      "        [-1.0230,  3.3487, -3.8723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9055, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2222, -0.3320, -3.9363],\n",
      "        [ 2.3507, -0.1726, -4.0905],\n",
      "        [ 0.0875,  2.5675, -4.8257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2391, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2953, -0.2265, -3.7897],\n",
      "        [ 1.0944,  0.9547, -5.1369],\n",
      "        [-1.2085,  3.3751, -3.5491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0618, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8473,  3.0909, -3.8679],\n",
      "        [ 1.9380,  0.1108, -4.3736],\n",
      "        [-1.0047,  3.2670, -4.0105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1930, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2980, -0.4699, -3.8671],\n",
      "        [-1.1846,  3.2266, -3.8693],\n",
      "        [ 0.8400,  1.2684, -4.7799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5561, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3857,  2.6755, -4.3531],\n",
      "        [ 0.5078,  1.6846, -4.6488],\n",
      "        [ 1.8500,  0.1873, -4.6225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2893, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6376,  2.6907, -4.0308],\n",
      "        [ 0.9174,  1.0246, -4.8627],\n",
      "        [ 2.1003, -0.3831, -4.1445]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1416, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1299, -0.1036, -4.0219],\n",
      "        [ 2.2237, -0.2713, -4.6082],\n",
      "        [ 1.7503,  0.4432, -4.6125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7836, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9561, -0.3546, -4.2489],\n",
      "        [ 1.4180,  0.4870, -4.9180],\n",
      "        [ 2.3893, -0.1429, -4.1532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3835, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3489,  1.7882, -4.6453],\n",
      "        [ 0.9247,  1.6518, -4.4870],\n",
      "        [ 1.3174,  0.9824, -5.0839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2730, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3667, -0.3306, -4.1049],\n",
      "        [ 1.3691,  0.8360, -5.1713],\n",
      "        [ 2.3974, -0.2921, -4.1480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6355, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4346, -0.1207, -4.2481],\n",
      "        [ 1.2745,  0.7227, -5.1289],\n",
      "        [ 1.1863,  0.9443, -4.7345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2126, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4413,  2.9827, -4.2165],\n",
      "        [-0.4724,  2.7335, -4.2471],\n",
      "        [ 0.9264,  1.2057, -4.9181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4949, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0783, -0.2206, -4.6349],\n",
      "        [ 0.7928,  0.9469, -5.0971],\n",
      "        [ 1.6436,  0.4391, -4.3587]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4124, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1837,  0.8877, -5.1161],\n",
      "        [-0.3120,  2.3305, -4.4471],\n",
      "        [ 0.4275,  1.4304, -4.6201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2726, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1026,  1.8508, -4.8942],\n",
      "        [ 1.8741,  0.0506, -4.4618],\n",
      "        [ 0.8104,  1.2341, -4.7859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3547, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5267,  1.3089, -4.9561],\n",
      "        [ 0.4942,  1.7535, -4.6386],\n",
      "        [ 1.2242,  0.6120, -4.5989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5984, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8399,  0.5405, -4.4349],\n",
      "        [ 1.8063,  0.7566, -5.0669],\n",
      "        [ 0.2115,  1.7134, -4.9090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2859, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7573,  0.2388, -4.4083],\n",
      "        [ 0.2502,  1.4481, -5.0925],\n",
      "        [ 1.4439,  0.7069, -4.4361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3168, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6658,  0.2252, -4.6550],\n",
      "        [ 0.6305,  1.3179, -4.7039],\n",
      "        [ 1.5557,  0.6005, -4.3450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4857, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3556,  0.7124, -4.7225],\n",
      "        [ 1.0174,  1.2814, -4.8002],\n",
      "        [ 1.3406,  0.8048, -4.7141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5699, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6715,  0.6809, -4.3365],\n",
      "        [ 1.5936,  0.8757, -4.5659],\n",
      "        [ 0.7358,  1.2643, -4.8360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5923, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6943,  1.4124, -4.9956],\n",
      "        [ 1.6778,  0.6798, -4.4026],\n",
      "        [ 1.2651,  0.6282, -4.6671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6065, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5598,  0.6958, -4.4067],\n",
      "        [ 0.6501,  1.0688, -5.1629],\n",
      "        [ 0.8104,  1.2844, -4.9187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2617, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4640,  1.4771, -4.8047],\n",
      "        [ 1.4380,  0.6039, -4.4652],\n",
      "        [-0.1376,  2.0068, -5.0056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5156, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5941,  1.6121, -5.2063],\n",
      "        [-0.1027,  1.9308, -4.9027],\n",
      "        [ 0.8744,  1.5888, -5.1652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3394, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6457,  1.7384, -5.0082],\n",
      "        [ 1.7807,  0.8689, -4.9173],\n",
      "        [ 0.4605,  1.2066, -5.0482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1527, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6911,  1.4862, -5.3535],\n",
      "        [-0.5441,  2.6771, -4.7913],\n",
      "        [-0.4649,  2.6260, -4.7149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2858, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7383,  0.3945, -4.6118],\n",
      "        [-0.6193,  2.7784, -4.6054],\n",
      "        [ 1.2201,  1.0013, -4.9823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2284, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3664,  0.5786, -4.7492],\n",
      "        [ 1.5162,  0.2440, -4.2201],\n",
      "        [-0.2828,  2.5303, -4.5093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1527, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8821,  0.4986, -4.1880],\n",
      "        [-0.2462,  2.8167, -4.6390],\n",
      "        [ 1.7113,  0.1114, -4.1361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2061, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9587, -0.1948, -4.3604],\n",
      "        [ 1.9198,  0.0901, -4.2899],\n",
      "        [ 1.3212,  0.4651, -4.3717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0665, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6159,  3.0844, -4.3183],\n",
      "        [-0.7060,  2.6756, -4.3940],\n",
      "        [ 1.8751, -0.0303, -4.4251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5221, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0864,  0.1188, -4.4237],\n",
      "        [-0.0942,  2.2128, -4.7655],\n",
      "        [ 0.7853,  1.8182, -4.6999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2798, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0435, -0.0615, -4.0837],\n",
      "        [ 0.6863,  1.3303, -5.0307],\n",
      "        [ 1.3634,  0.3024, -4.5740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2825, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3111,  1.1797, -4.9642],\n",
      "        [ 2.1121, -0.0673, -3.8519],\n",
      "        [ 2.0760, -0.1173, -4.1455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2507, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9871,  3.1719, -4.1401],\n",
      "        [ 1.9284, -0.2568, -3.6819],\n",
      "        [ 1.4355,  1.2949, -5.0724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0848, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4019, -0.1655, -4.0887],\n",
      "        [-0.7287,  3.1060, -4.3046],\n",
      "        [ 1.8686,  0.0874, -4.4722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1711, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6515,  0.7480, -4.6765],\n",
      "        [ 2.0042, -0.3381, -4.0018],\n",
      "        [ 1.9250, -0.6556, -3.3145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0295, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5720,  2.9461, -4.4390],\n",
      "        [-1.1305,  3.4157, -3.7316],\n",
      "        [ 2.5830, -0.4685, -4.0328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0454, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3885,  2.3027, -4.9184],\n",
      "        [-1.0929,  3.3722, -3.7976],\n",
      "        [ 2.2752, -0.5747, -4.1126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1955, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4801,  1.4723, -4.8710],\n",
      "        [ 2.5961, -0.5236, -3.8614],\n",
      "        [ 0.1251,  2.3280, -4.9274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0407, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8892,  3.2362, -4.0018],\n",
      "        [ 2.5961, -0.7220, -3.7552],\n",
      "        [ 2.2335, -0.4543, -3.7858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8413, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0370, -0.2665, -3.8503],\n",
      "        [-0.1178,  2.3527, -4.5937],\n",
      "        [ 2.2467, -1.0016, -3.6919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1779, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4516,  1.9690, -5.0504],\n",
      "        [ 0.5718,  1.8040, -5.1997],\n",
      "        [-0.0994,  2.4197, -4.8411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0510, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3870, -0.8343, -3.7264],\n",
      "        [-0.2462,  2.6156, -4.7725],\n",
      "        [ 2.2397, -0.6636, -3.7925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0253, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4256, -0.8453, -3.8276],\n",
      "        [-1.1396,  3.4779, -3.5401],\n",
      "        [-0.7280,  2.9312, -4.4590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1805, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5430, -0.7879, -3.6276],\n",
      "        [-0.1227,  2.5487, -4.8341],\n",
      "        [ 0.9201,  1.5235, -5.2767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0374, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6540, -0.5783, -3.6148],\n",
      "        [-0.8276,  3.3199, -4.3142],\n",
      "        [ 2.3461, -0.5712, -3.4889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7829, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3244,  0.7971, -4.9729],\n",
      "        [-1.1290,  3.4261, -3.9725],\n",
      "        [ 1.3492,  0.3079, -4.7688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2135, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2199,  3.6175, -3.4781],\n",
      "        [-1.4173,  3.7543, -3.4411],\n",
      "        [ 1.1360,  0.9911, -4.4715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3526, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4768, -0.4739, -3.7888],\n",
      "        [ 2.6357, -0.6668, -3.5989],\n",
      "        [ 1.2632,  1.2027, -4.8903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7980, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4822, -0.2222, -3.7894],\n",
      "        [-0.0179,  2.1943, -4.5999],\n",
      "        [-1.4179,  3.2522, -3.6530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3277, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9284,  1.3122, -4.8890],\n",
      "        [-1.2878,  3.2830, -3.2918],\n",
      "        [ 2.3716, -0.3220, -4.1041]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4491, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0046, -0.0391, -3.8669],\n",
      "        [ 1.3500,  0.5461, -4.7789],\n",
      "        [ 2.3228, -0.7450, -3.7114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6809, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2604,  2.7341, -5.0074],\n",
      "        [ 0.2117,  1.9578, -5.0835],\n",
      "        [ 2.1635, -0.2801, -3.7270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1990, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5028,  0.7480, -4.9466],\n",
      "        [ 2.0683, -0.3168, -3.7929],\n",
      "        [ 1.9316, -0.1440, -4.3333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9712, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3719, -0.4040, -3.4241],\n",
      "        [-1.2247,  3.3887, -3.9152],\n",
      "        [ 2.3475, -0.4300, -3.8102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0549, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7879,  3.1958, -4.2895],\n",
      "        [ 2.1486, -0.2840, -3.3270],\n",
      "        [-0.3117,  2.5256, -4.7626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0478, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9074, -0.2158, -4.0406],\n",
      "        [-1.0954,  3.3385, -3.8280],\n",
      "        [-1.1353,  3.0819, -3.8433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0529, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1936, -0.1408, -3.9844],\n",
      "        [-1.0848,  3.3364, -4.3293],\n",
      "        [-0.3334,  2.6100, -4.7504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1419, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5739,  1.6161, -5.0992],\n",
      "        [-1.1844,  3.3190, -3.7866],\n",
      "        [ 0.0059,  2.1553, -4.9330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1023, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1979, -0.3757, -3.7577],\n",
      "        [ 2.1407,  0.0167, -3.7145],\n",
      "        [ 2.0540, -0.0590, -4.3637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0612, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0739,  1.9266, -4.8289],\n",
      "        [-0.8679,  3.0659, -4.3120],\n",
      "        [-0.8777,  3.2404, -4.0542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1205, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2591,  0.0176, -3.9238],\n",
      "        [ 1.9921,  0.0137, -4.0263],\n",
      "        [ 2.0643,  0.0495, -4.1598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2953, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0599,  2.3038, -4.6820],\n",
      "        [ 1.1305,  1.1649, -5.0551],\n",
      "        [ 2.1720, -0.0259, -4.0748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0694, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1300, -0.3724, -3.6658],\n",
      "        [ 2.1768, -0.0362, -3.8980],\n",
      "        [-0.8394,  3.0417, -4.5656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1089, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6312,  2.9835, -4.4095],\n",
      "        [-1.2622,  3.3987, -2.9028],\n",
      "        [ 1.5333,  0.4333, -4.9854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2275, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9230,  1.4731, -5.0744],\n",
      "        [ 2.3568, -0.3598, -4.4433],\n",
      "        [ 2.0145,  0.2629, -4.6589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0464, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1487,  0.0636, -4.3173],\n",
      "        [-1.1985,  3.3054, -3.6568],\n",
      "        [-1.1718,  3.6642, -3.4575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0616, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4668,  3.4175, -3.5780],\n",
      "        [ 2.0313, -0.2709, -4.2679],\n",
      "        [ 2.0983, -0.4280, -3.9026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1184, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1026,  3.6255, -3.9479],\n",
      "        [ 1.6477,  0.4405, -4.8058],\n",
      "        [-0.1333,  2.3253, -4.3990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0459, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4414, -0.3763, -4.1486],\n",
      "        [-1.0281,  3.3308, -3.8495],\n",
      "        [ 2.2638, -0.4783, -3.6913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4747, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1615, -0.3390, -4.3256],\n",
      "        [-0.8806,  3.4421, -4.3417],\n",
      "        [-1.4562,  3.5432, -3.3952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0425, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4550, -0.3788, -4.3607],\n",
      "        [ 2.5973, -0.4776, -3.8357],\n",
      "        [-0.7242,  3.0781, -4.4582]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9806, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5476, -0.4193, -3.8673],\n",
      "        [ 2.4330, -0.3554, -3.9863],\n",
      "        [-0.3833,  2.8178, -4.8686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3015, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4728, -0.2280, -4.0456],\n",
      "        [ 1.0423,  1.1610, -5.1711],\n",
      "        [ 2.3411, -0.1309, -3.8715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9863, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9822,  3.2001, -4.3445],\n",
      "        [ 2.3278, -0.2608, -4.2422],\n",
      "        [-0.2302,  2.5804, -4.8183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0331, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4406, -0.2721, -4.3912],\n",
      "        [-0.8036,  3.0957, -4.5353],\n",
      "        [-0.9554,  3.3884, -3.9363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0581, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5367,  2.8051, -4.5177],\n",
      "        [ 2.3383, -0.4127, -4.0461],\n",
      "        [ 2.3260, -0.2503, -3.7671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2224, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5792, -0.4310, -3.9014],\n",
      "        [ 1.2006,  0.9879, -4.8718],\n",
      "        [-0.6244,  3.1181, -4.4433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1668, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1981,  2.2421, -4.8975],\n",
      "        [ 0.4830,  1.5570, -4.9340],\n",
      "        [ 2.3066, -0.1622, -4.1432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1540, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3121, -0.3893, -4.0074],\n",
      "        [ 0.8206,  1.7341, -4.9221],\n",
      "        [-0.3146,  2.5326, -4.7037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0994, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3856,  1.8908, -5.0532],\n",
      "        [ 2.6752, -0.3734, -3.9073],\n",
      "        [ 2.6390, -0.3674, -4.1961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5747, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3118,  2.8011, -4.5017],\n",
      "        [ 1.2084,  1.1572, -5.0976],\n",
      "        [ 1.2673,  0.7921, -4.9601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9903, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3272, -0.4355, -3.8164],\n",
      "        [ 2.1339, -0.2457, -4.1392],\n",
      "        [ 0.9726,  1.5743, -5.0305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1146, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6151,  2.8894, -4.2291],\n",
      "        [ 0.5692,  1.8054, -4.8855],\n",
      "        [-0.3514,  2.4899, -4.6850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0482, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3231, -0.4274, -3.8533],\n",
      "        [ 2.3925, -0.4226, -4.0433],\n",
      "        [-0.8353,  3.0416, -4.2923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0611, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4427,  2.9236, -4.7138],\n",
      "        [ 2.2793, -0.4261, -4.0891],\n",
      "        [ 2.3465, -0.1256, -4.1159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0839, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6659,  2.7649, -4.4917],\n",
      "        [ 2.0146, -0.1104, -4.0831],\n",
      "        [ 2.0814, -0.1381, -4.4387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3455, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1590,  1.2896, -5.1854],\n",
      "        [ 1.9820,  0.3768, -4.5836],\n",
      "        [ 2.0995, -0.2726, -4.1142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5631, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0930, -0.1750, -4.2087],\n",
      "        [ 1.7445,  0.2958, -5.0452],\n",
      "        [ 0.3923,  1.4770, -4.8672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4178, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2409, -0.1315, -4.3306],\n",
      "        [-0.3365,  2.7654, -4.7977],\n",
      "        [ 0.6538,  1.3752, -5.2731]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1712, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2621, -0.4035, -3.9569],\n",
      "        [-0.2418,  2.5005, -4.4249],\n",
      "        [ 1.4506,  0.6792, -4.9415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1298, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6215, -0.4293, -3.9999],\n",
      "        [ 0.4748,  1.6062, -5.0380],\n",
      "        [ 2.4442, -0.3481, -3.7486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0344, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2326, -0.7177, -4.3872],\n",
      "        [-0.6966,  3.1008, -4.3066],\n",
      "        [-0.7022,  2.8852, -4.3241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0445, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3542,  2.5800, -4.8340],\n",
      "        [ 2.7033, -0.5113, -4.0644],\n",
      "        [-0.5709,  2.6310, -4.6237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0557, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3512,  2.8198, -4.5127],\n",
      "        [-0.2248,  2.2833, -4.6160],\n",
      "        [ 2.4214, -0.6580, -4.2237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0693, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6432,  2.8267, -4.2416],\n",
      "        [ 0.1957,  1.9534, -4.9544],\n",
      "        [-0.9997,  3.1489, -3.8118]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2040, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7603,  3.0878, -4.4598],\n",
      "        [ 1.4240,  0.9002, -5.1117],\n",
      "        [ 2.1656,  0.1321, -4.3933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5512, -0.6338, -4.0350],\n",
      "        [ 2.5243, -0.7855, -3.8583],\n",
      "        [ 1.7937,  0.6347, -5.1107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8852, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1298,  1.1551, -4.7546],\n",
      "        [ 0.2699,  2.0618, -4.6558],\n",
      "        [-0.6502,  2.9867, -4.5565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6248, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6082,  2.7705, -4.8315],\n",
      "        [ 0.6390,  1.4686, -4.8552],\n",
      "        [ 1.6802,  0.4634, -4.9729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0661, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5068, -0.7105, -4.2511],\n",
      "        [-0.0995,  1.8977, -4.7978],\n",
      "        [-0.5744,  2.9492, -4.6284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8908, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6205, -1.2117, -3.7919],\n",
      "        [ 2.3062, -0.2238, -4.8358],\n",
      "        [ 2.5621, -0.6243, -3.8277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7610, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1382,  3.1883, -3.9434],\n",
      "        [ 2.1492,  0.1648, -4.7841],\n",
      "        [ 1.8785,  0.0844, -4.7190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0747, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4836, -0.3912, -4.2753],\n",
      "        [ 0.3614,  1.5020, -5.2422],\n",
      "        [-0.8971,  3.3334, -3.9945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0772, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3037,  2.0218, -5.1224],\n",
      "        [-0.9775,  3.1553, -3.9511],\n",
      "        [ 2.6994, -0.3146, -3.9154]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3877, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0240,  0.8554, -5.5070],\n",
      "        [-0.1104,  2.2208, -4.7785],\n",
      "        [ 0.5879,  1.6882, -5.6328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1972, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2022,  0.8265, -5.2642],\n",
      "        [ 2.2238, -0.6566, -4.0014],\n",
      "        [-1.2119,  3.3255, -3.9263]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1660, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1764, -0.2398, -4.4963],\n",
      "        [ 1.3032,  0.5684, -5.2140],\n",
      "        [-0.8991,  3.1079, -4.6678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1638, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2244,  3.2948, -3.6916],\n",
      "        [ 1.5271,  0.8717, -4.8581],\n",
      "        [ 2.4311, -0.3640, -4.3383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0942, -0.0788, -3.9755],\n",
      "        [-1.2756,  3.4151, -3.5696],\n",
      "        [-0.9508,  3.4131, -4.2343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2188, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2354,  0.9662, -5.5264],\n",
      "        [-0.9745,  3.5294, -3.5399],\n",
      "        [ 2.3370, -0.2124, -4.5106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2090, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8504,  2.8997, -3.9411],\n",
      "        [ 1.5886,  0.6457, -5.3749],\n",
      "        [ 1.7580,  0.5960, -4.9639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0377, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7656,  2.9000, -4.4500],\n",
      "        [ 2.2659, -0.2777, -4.4572],\n",
      "        [-1.1355,  3.4793, -3.8141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5349, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1543,  1.9029, -5.5880],\n",
      "        [-0.5229,  3.0777, -4.7240],\n",
      "        [ 0.6133,  1.7509, -5.0335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0693, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0405,  3.0394, -4.3214],\n",
      "        [ 1.9850,  0.3448, -4.8544],\n",
      "        [-1.1652,  3.2988, -3.6594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3954, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3128,  1.9682, -4.9503],\n",
      "        [-0.3244,  2.9678, -4.3415],\n",
      "        [ 0.8266,  1.3242, -5.3064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2913, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6262,  3.0172, -4.4188],\n",
      "        [ 2.3150, -0.0843, -4.5021],\n",
      "        [ 2.1388,  0.0398, -5.2135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1584, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2976,  2.5103, -4.7086],\n",
      "        [ 2.5283, -0.0928, -4.5227],\n",
      "        [ 0.4256,  1.3170, -5.1230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0413, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2777,  2.6563, -4.5700],\n",
      "        [ 2.5099, -0.3035, -4.3310],\n",
      "        [-1.1082,  3.3585, -3.8923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1417, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9407,  0.3825, -5.1719],\n",
      "        [ 1.7675,  0.3475, -4.9250],\n",
      "        [-0.9791,  3.2140, -3.9467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5715, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4934, -0.3153, -4.5763],\n",
      "        [ 0.4134,  1.7739, -5.1651],\n",
      "        [ 2.4524, -0.2570, -4.2314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1074, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3779,  1.6182, -4.7171],\n",
      "        [-0.3709,  2.5925, -4.5173],\n",
      "        [-0.9216,  3.2560, -4.2903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2178, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2724,  2.9403, -4.7829],\n",
      "        [ 1.0453,  1.2968, -5.0832],\n",
      "        [-0.5469,  2.7421, -4.4216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1640, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6274,  2.7608, -4.4577],\n",
      "        [ 0.8352,  1.5566, -5.4031],\n",
      "        [ 2.3513, -0.4294, -4.2829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8776, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6217, -0.5024, -3.9787],\n",
      "        [-0.2434,  2.2220, -4.7875],\n",
      "        [-0.5713,  2.6341, -4.3967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1395, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5248,  0.5158, -4.9015],\n",
      "        [-0.4079,  2.8577, -4.6775],\n",
      "        [ 2.5278, -0.4920, -4.2465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2943, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1476,  3.1607, -3.6999],\n",
      "        [ 2.4911, -0.5216, -4.0809],\n",
      "        [ 1.0722,  0.8370, -4.7235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0866, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6288, -0.7357, -4.2761],\n",
      "        [ 0.1792,  1.8752, -5.0790],\n",
      "        [ 2.3780, -0.5028, -4.3195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2064, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4981, -0.3915, -4.2231],\n",
      "        [-0.8159,  3.1448, -4.3900],\n",
      "        [ 1.4234,  1.0982, -4.9470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5451,  2.7789, -4.3753],\n",
      "        [ 2.7392, -0.7396, -3.9535],\n",
      "        [-0.5027,  2.3537, -4.7018]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0489, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6405, -0.6596, -4.3613],\n",
      "        [-0.1364,  2.8020, -4.9234],\n",
      "        [ 2.4595, -0.3919, -4.1373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0407, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5979, -0.6269, -4.1332],\n",
      "        [ 2.6282, -0.6061, -3.8790],\n",
      "        [ 2.6457, -0.5349, -3.9760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2273, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5568,  2.6846, -4.4559],\n",
      "        [ 1.1723,  1.0241, -4.9489],\n",
      "        [-0.9715,  2.9792, -4.1533]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0565, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2368,  0.0649, -4.9193],\n",
      "        [-0.5587,  2.9301, -4.5313],\n",
      "        [-0.6315,  2.8752, -4.2959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2229, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2380,  0.9217, -5.0105],\n",
      "        [ 2.6105, -0.2988, -4.1049],\n",
      "        [-0.3466,  2.3549, -4.6526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2133, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9385,  3.2008, -3.9325],\n",
      "        [ 1.2445,  1.5089, -4.9937],\n",
      "        [ 2.4111, -0.5359, -3.9795]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0585, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4704, -0.5019, -4.1263],\n",
      "        [-1.1508,  3.3930, -3.9258],\n",
      "        [ 2.3587,  0.2302, -4.9746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0732, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6587, -0.4911, -4.2475],\n",
      "        [ 2.7069, -0.5804, -3.9205],\n",
      "        [ 0.3685,  2.2796, -4.8497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0409, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3386, -0.6476, -4.6218],\n",
      "        [-0.4676,  2.8003, -4.8242],\n",
      "        [ 2.6082, -0.7721, -4.0340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0991, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6276, -0.6921, -3.9609],\n",
      "        [ 0.4872,  1.9219, -5.2822],\n",
      "        [-0.4337,  2.6303, -4.7725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0535, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3874, -0.3272, -4.4401],\n",
      "        [-0.1797,  2.4010, -4.8417],\n",
      "        [-0.7042,  3.1480, -4.0742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1355, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0409,  3.1340, -3.7232],\n",
      "        [ 2.6556, -0.5398, -4.1769],\n",
      "        [ 1.9615,  0.1595, -4.7794]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5315, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4083,  2.4264, -4.5768],\n",
      "        [ 2.5660, -0.6556, -4.1686],\n",
      "        [ 1.7605,  0.5191, -4.9322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7078, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1955,  2.0459, -5.0677],\n",
      "        [ 1.7343,  0.6237, -5.1868],\n",
      "        [ 1.9876,  0.5010, -5.0476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4995, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5567, -0.7547, -4.1849],\n",
      "        [ 1.5933,  0.4717, -4.8962],\n",
      "        [ 2.4424, -0.4123, -4.5020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0329, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4884, -0.4480, -4.3043],\n",
      "        [-0.7901,  2.6963, -4.3882],\n",
      "        [-1.0923,  3.1419, -4.0935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7937, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4050, -0.6373, -4.1545],\n",
      "        [ 2.0874,  0.1498, -4.7636],\n",
      "        [ 1.6672,  0.4497, -5.2079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0689, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5625, -0.6085, -4.4437],\n",
      "        [ 2.3245, -0.6156, -4.9180],\n",
      "        [ 2.1616,  0.0235, -4.5487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4198, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5812,  0.8294, -5.5130],\n",
      "        [ 2.3919, -0.4110, -4.2105],\n",
      "        [ 2.4038, -0.3809, -4.7192]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0806, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3616, -0.2783, -4.6044],\n",
      "        [ 2.0104, -0.1647, -4.8929],\n",
      "        [ 2.5087, -0.2328, -4.2921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0244, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4508, -0.7917, -3.9626],\n",
      "        [-1.1546,  3.2215, -3.8381],\n",
      "        [-0.8418,  3.1028, -4.0174]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5111, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8631,  0.0742, -4.7439],\n",
      "        [ 1.4396,  0.3672, -5.0944],\n",
      "        [-1.3901,  3.3191, -3.6910]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1241, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5892,  2.9277, -4.5184],\n",
      "        [ 2.3234, -0.5904, -4.4023],\n",
      "        [ 1.4342,  0.3342, -5.0373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4555, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3373,  0.7391, -5.2818],\n",
      "        [ 0.5415,  1.7108, -5.1562],\n",
      "        [ 2.3683, -0.4631, -4.6602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4715, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5359,  1.5403, -5.1671],\n",
      "        [-0.9545,  2.9713, -4.1586],\n",
      "        [ 2.2518, -0.2909, -4.5014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0529, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1412,  3.4487, -3.6874],\n",
      "        [ 2.4912, -0.1358, -4.4569],\n",
      "        [ 2.3423, -0.1968, -4.4950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5709, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4381,  0.8263, -5.1580],\n",
      "        [ 0.6901,  1.6298, -5.0021],\n",
      "        [-1.5413,  3.4613, -3.2146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0477, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9603, -0.0734, -4.8565],\n",
      "        [-1.3702,  3.3557, -3.3429],\n",
      "        [-1.3635,  3.4437, -3.4806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2267, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4628,  2.8431, -4.6312],\n",
      "        [ 1.3186,  0.3606, -5.0729],\n",
      "        [-1.1338,  3.3669, -3.8804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5781, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4268,  2.8008, -5.0928],\n",
      "        [-0.9030,  2.9068, -4.3057],\n",
      "        [ 0.3844,  1.8479, -5.3023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0684, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0692,  2.2110, -4.7325],\n",
      "        [-0.5678,  2.4227, -4.4753],\n",
      "        [ 2.5390, -0.3180, -4.0973]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0817, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8269,  3.2988, -3.9448],\n",
      "        [ 0.3226,  1.7784, -4.8473],\n",
      "        [-1.0806,  2.9888, -4.2591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1444, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4522,  1.2443, -5.2221],\n",
      "        [ 2.6372, -0.4726, -4.4397],\n",
      "        [-1.1300,  3.2003, -3.7468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3658, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5455, -0.4897, -4.1446],\n",
      "        [-1.1389,  3.1235, -3.6210],\n",
      "        [ 1.3314,  0.7381, -5.1334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0666, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0793,  2.5976, -5.1806],\n",
      "        [ 2.1691, -0.2573, -4.6203],\n",
      "        [ 2.5733, -0.7635, -4.1380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1201, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4787,  0.4072, -4.9994],\n",
      "        [-0.5465,  2.7102, -4.7465],\n",
      "        [-0.8019,  2.8485, -4.3699]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4399, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7846,  1.5817, -5.4170],\n",
      "        [-0.9517,  3.0008, -4.3455],\n",
      "        [ 1.9981,  0.0214, -5.1682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1097, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3938,  1.9479, -4.8544],\n",
      "        [ 2.1982, -0.1441, -4.3383],\n",
      "        [ 2.4041, -0.7431, -4.3148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5658, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6149,  0.4488, -4.9192],\n",
      "        [ 0.2458,  1.7722, -5.0400],\n",
      "        [ 2.1815, -0.6043, -4.3126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0394, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8697,  2.5777, -4.5866],\n",
      "        [ 2.6123, -0.4047, -4.4505],\n",
      "        [-0.4904,  2.7862, -4.7490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5494, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4972, -0.2383, -4.6324],\n",
      "        [ 0.3987,  1.6675, -4.7373],\n",
      "        [ 2.3650, -0.3225, -4.3729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2844, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8265,  1.0418, -5.1787],\n",
      "        [-0.6796,  2.8667, -4.8611],\n",
      "        [-0.7819,  3.3353, -4.2891]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0310, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7182, -0.5060, -4.4146],\n",
      "        [-1.0711,  3.2311, -4.3532],\n",
      "        [ 2.5805, -0.6748, -3.9324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1980, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0709,  0.6948, -5.2402],\n",
      "        [-1.0507,  3.3509, -3.7948],\n",
      "        [ 2.3770, -0.4729, -4.4500]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2511, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5810, -0.4371, -4.1006],\n",
      "        [-0.9396,  3.3456, -3.6778],\n",
      "        [ 0.8619,  0.8524, -5.0335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2336, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1998,  2.0929, -5.2472],\n",
      "        [ 2.6788, -0.7186, -4.3024],\n",
      "        [ 0.9380,  1.2032, -5.2257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2862, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0589,  3.2375, -4.3833],\n",
      "        [ 0.2167,  1.8581, -5.5776],\n",
      "        [ 1.1018,  1.0476, -5.4366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2135, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4670,  2.5653, -5.0670],\n",
      "        [ 0.4356,  1.5580, -4.8803],\n",
      "        [ 0.5279,  1.5440, -5.0610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1412, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7138, -0.6439, -4.2395],\n",
      "        [ 0.6116,  1.5950, -5.2272],\n",
      "        [ 2.3731, -0.2643, -4.8129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1866, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6912, -0.7845, -3.7116],\n",
      "        [ 1.3033,  0.7905, -5.3326],\n",
      "        [ 2.4179, -0.4220, -4.6497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2935, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3287,  1.6223, -5.2920],\n",
      "        [ 1.1622,  0.9496, -5.5607],\n",
      "        [ 2.4776, -0.6416, -4.5493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8201, -0.9767, -4.1497],\n",
      "        [-0.0503,  2.5803, -4.9364],\n",
      "        [-0.7053,  2.9016, -4.5106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0661, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0607,  1.9386, -5.1536],\n",
      "        [ 2.7840, -0.7710, -4.2297],\n",
      "        [-0.6252,  3.0236, -4.3412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0942, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6639, -0.5430, -4.3612],\n",
      "        [ 1.3011,  1.2085, -5.3047],\n",
      "        [ 2.2903, -0.1239, -4.6462]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0379, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7163, -1.0369, -3.9368],\n",
      "        [-0.4233,  2.7093, -4.5810],\n",
      "        [ 2.5412, -0.5373, -4.2533]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0724, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8660,  0.0781, -4.6781],\n",
      "        [ 2.8669, -0.7425, -4.1414],\n",
      "        [ 2.5811, -0.8229, -4.3787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6853, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9432, -1.1254, -4.0143],\n",
      "        [ 2.8782, -0.3684, -4.4588],\n",
      "        [ 1.8790,  0.0263, -4.9352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1593, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2120,  1.9713, -5.3086],\n",
      "        [ 3.0660, -0.8761, -3.9629],\n",
      "        [ 0.5614,  1.6204, -5.3281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1723, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0530,  3.2767, -3.7190],\n",
      "        [ 0.7461,  1.7365, -5.3778],\n",
      "        [ 0.4363,  2.0248, -5.0088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2970, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0532,  2.3511, -4.7170],\n",
      "        [ 2.7586, -0.5737, -4.3962],\n",
      "        [ 1.1719,  1.0466, -5.1764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1720, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6353e+00,  4.8367e-01, -5.1498e+00],\n",
      "        [ 1.9256e+00,  2.0539e-03, -5.0199e+00],\n",
      "        [ 1.8616e+00,  6.4048e-02, -5.2490e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1940, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3864,  3.2026, -2.5408],\n",
      "        [ 2.1105,  0.0595, -5.0464],\n",
      "        [ 1.1015,  0.5256, -5.4534]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2278, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6455,  0.6673, -5.0082],\n",
      "        [ 1.6518,  0.5619, -5.3622],\n",
      "        [ 2.3291, -0.2628, -4.9578]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1271, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3134,  3.3096, -2.5408],\n",
      "        [ 0.2163,  1.4733, -5.4107],\n",
      "        [ 0.1712,  2.2591, -4.9936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3562, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2641,  0.7892, -5.2098],\n",
      "        [ 1.5752,  0.4644, -5.0693],\n",
      "        [ 1.5886,  0.5293, -5.1586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3351, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2059,  0.5587, -5.4320],\n",
      "        [ 1.1153,  0.8589, -5.3417],\n",
      "        [-1.4121,  3.4684, -2.8379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3079, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4918,  3.1147, -2.1644],\n",
      "        [ 2.3743, -0.3309, -4.7189],\n",
      "        [ 0.9492,  1.2284, -5.5671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5412, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2804,  3.2660, -2.8863],\n",
      "        [-1.3016,  3.3197, -2.1458],\n",
      "        [ 2.5198, -0.4455, -4.9875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0683, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8724,  0.1675, -5.1209],\n",
      "        [-1.1939,  3.0054, -2.2344],\n",
      "        [-1.5104,  3.0256, -2.0136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4009, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6317,  0.8949, -5.1800],\n",
      "        [-1.1213,  3.1076, -2.4641],\n",
      "        [-0.3144,  2.5540, -5.0139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0478, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5277, -0.4114, -4.2834],\n",
      "        [-1.0666,  2.8475, -1.4917],\n",
      "        [ 2.3872, -0.4373, -4.7468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1130, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9609,  3.0453, -2.5910],\n",
      "        [ 1.5786,  0.5088, -5.4312],\n",
      "        [-1.1047,  2.9670, -2.3140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1496, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8341,  0.3756, -5.3287],\n",
      "        [ 1.7860,  0.3286, -5.1005],\n",
      "        [-0.4767,  3.0687, -4.4773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0503, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3962,  0.0966, -4.8795],\n",
      "        [-0.7181,  3.1205, -2.8996],\n",
      "        [-0.8909,  2.7377, -2.5997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7842, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4148, -0.1866, -4.9910],\n",
      "        [ 0.0964,  2.2322, -5.2750],\n",
      "        [-0.6094,  2.8770, -3.1476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0333, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7960,  2.7642, -2.3723],\n",
      "        [-0.8133,  2.7620, -2.7757],\n",
      "        [ 2.6970, -0.6662, -4.3857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1386, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0168,  2.5681, -1.6816],\n",
      "        [ 1.7137, -0.0569, -4.9820],\n",
      "        [ 1.6745,  0.2496, -5.0466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4435, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8892,  2.4087, -1.9467],\n",
      "        [ 1.6407,  0.7384, -5.1455],\n",
      "        [-0.8258,  2.6323, -2.2707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0984, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7280,  0.3748, -5.2522],\n",
      "        [-0.6766,  2.8986, -2.4884],\n",
      "        [-0.5915,  2.9149, -2.8778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8019, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8268,  2.8151, -3.0285],\n",
      "        [ 1.7824,  0.3330, -4.7472],\n",
      "        [-0.0212,  2.0210, -5.0414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7345, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6375,  2.8262, -3.0602],\n",
      "        [ 1.7666, -0.1954, -4.9969],\n",
      "        [ 2.2725, -0.2864, -4.8590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0623, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8739,  3.0651, -2.8726],\n",
      "        [ 2.0009,  0.0799, -5.0289],\n",
      "        [ 2.9238, -0.7067, -3.6402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2264, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2939,  2.9277, -4.8224],\n",
      "        [ 0.6880,  1.2866, -5.4161],\n",
      "        [ 1.6329,  0.1269, -5.3863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4149, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5051,  2.8043, -3.2116],\n",
      "        [ 1.5065,  0.7123, -5.5704],\n",
      "        [-0.5582,  2.7516, -3.0538]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1472, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6646,  1.8677, -5.1466],\n",
      "        [ 2.6630, -0.7322, -3.4587],\n",
      "        [ 1.9291,  0.0522, -4.9181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1051, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2328,  2.1217, -5.2672],\n",
      "        [-0.5210,  2.9208, -3.4745],\n",
      "        [ 1.8737, -0.0243, -4.6373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1166, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0323,  0.0268, -4.9279],\n",
      "        [ 1.9719,  0.1068, -5.0558],\n",
      "        [-0.1110,  2.4106, -5.0403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8695, -0.9760, -3.5009],\n",
      "        [ 2.8250, -0.9629, -4.0438],\n",
      "        [ 2.0766,  0.0560, -4.7577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1285, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2115,  2.2251, -5.2231],\n",
      "        [-0.0702,  2.5857, -5.0208],\n",
      "        [ 1.9113,  0.3512, -5.1215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2468, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1203,  2.6830, -5.0331],\n",
      "        [ 2.1259, -0.2511, -4.8136],\n",
      "        [ 1.1213,  0.9041, -5.2235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0222, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6678, -1.0208, -3.5325],\n",
      "        [ 2.9842, -1.0566, -3.4344],\n",
      "        [-0.8740,  3.0396, -3.7357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3084, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0616,  1.0019, -5.4516],\n",
      "        [ 2.1189, -0.0847, -5.0814],\n",
      "        [ 2.2490, -0.0598, -4.7362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2708, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8486,  3.1915, -3.7415],\n",
      "        [-0.3831,  2.3332, -5.0587],\n",
      "        [ 1.2444,  1.1746, -5.4611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0514, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2755,  0.0197, -4.5949],\n",
      "        [ 2.6043, -0.9363, -3.4777],\n",
      "        [ 2.8123, -0.9956, -4.2211]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4345, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1764,  2.1335, -5.0148],\n",
      "        [ 2.5186, -0.5213, -4.7721],\n",
      "        [ 0.7143,  1.4429, -5.2771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4660, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6267, -1.0007, -3.7147],\n",
      "        [ 2.3725, -0.4976, -4.7549],\n",
      "        [ 1.3276,  0.3274, -5.4144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1915, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8098,  1.2249, -5.2914],\n",
      "        [-0.4668,  2.9130, -4.7562],\n",
      "        [-0.4152,  3.0007, -4.8508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0211, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9405, -1.4889, -3.1490],\n",
      "        [-0.6425,  3.1588, -3.7722],\n",
      "        [-0.7725,  2.9193, -3.6098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0308, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4315, -0.5782, -4.4811],\n",
      "        [-0.4568,  3.1686, -4.2724],\n",
      "        [-0.9451,  3.2605, -3.0954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0211, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6712, -1.1785, -3.3971],\n",
      "        [-0.8746,  3.1927, -3.8722],\n",
      "        [-0.8537,  2.9918, -3.8612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0631, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1406,  0.1185, -4.8728],\n",
      "        [-0.3914,  2.8207, -4.8878],\n",
      "        [-0.6250,  3.1060, -4.3388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1335, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5192,  0.7226, -4.8369],\n",
      "        [-0.9270,  3.5001, -3.4305],\n",
      "        [-1.0169,  3.3176, -3.3346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3893, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5725,  0.2668, -4.9146],\n",
      "        [-0.2778,  2.8257, -4.9048],\n",
      "        [ 1.2059,  0.8584, -5.4663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0771, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8651,  3.3766, -3.6014],\n",
      "        [-0.6036,  3.2084, -4.2276],\n",
      "        [ 0.4295,  1.9761, -5.2716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8817, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7561,  3.0123, -4.2828],\n",
      "        [ 0.4087,  2.4072, -5.2604],\n",
      "        [ 0.9896,  1.4356, -5.1530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2016, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1674, -0.4173, -4.9297],\n",
      "        [ 2.5922, -0.6206, -4.0949],\n",
      "        [ 1.0109,  1.4690, -5.4755]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2230, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8431,  3.5228, -3.2314],\n",
      "        [ 2.5371, -1.4234, -2.6558],\n",
      "        [ 1.2082,  1.0778, -5.4473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0418, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3229, -0.1926, -4.4838],\n",
      "        [ 2.5656, -1.1979, -2.8393],\n",
      "        [ 2.8333, -1.2094, -3.3559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1413, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5321, -1.4022, -2.5820],\n",
      "        [ 2.3749, -1.2220, -2.3471],\n",
      "        [-0.4483,  2.8792, -5.0286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3451, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9814,  3.5858, -3.5588],\n",
      "        [ 0.9455,  1.4998, -5.0240],\n",
      "        [-0.8388,  3.4040, -3.8840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0117,  3.4096, -3.6596],\n",
      "        [-0.8917,  3.5543, -3.7834],\n",
      "        [-0.4733,  3.0161, -4.9119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5089, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5095, -0.2796, -4.5027],\n",
      "        [-0.5214,  3.1282, -4.6976],\n",
      "        [ 0.5864,  1.7551, -5.2326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0559, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0614,  0.1198, -4.5237],\n",
      "        [-0.8632,  3.1119, -3.8518],\n",
      "        [-0.9526,  3.4390, -4.1797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3943,  2.8773, -4.6035],\n",
      "        [ 2.7272, -1.5297, -2.6780],\n",
      "        [ 1.7805,  0.5425, -5.2848]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3624, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8362,  1.3677, -5.2389],\n",
      "        [ 3.0239, -1.4139, -3.5996],\n",
      "        [-0.2057,  2.2932, -4.9951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4204, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5406,  0.6685, -5.3584],\n",
      "        [ 2.9134, -0.9011, -3.9890],\n",
      "        [ 2.8850, -1.3819, -3.1728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5507, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9306,  0.5481, -5.5610],\n",
      "        [ 2.9932, -1.3757, -3.5728],\n",
      "        [ 0.7561,  1.8906, -5.3345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0745, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0632, -1.6177, -3.2633],\n",
      "        [ 1.4814, -0.0396, -5.0368],\n",
      "        [ 3.0931, -1.2738, -3.6239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0550, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2249,  0.0556, -5.1510],\n",
      "        [ 2.7822, -1.3040, -3.1797],\n",
      "        [-0.2701,  3.0158, -4.8020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1282, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6828,  1.7122, -5.4910],\n",
      "        [ 2.3626, -0.5467, -4.4307],\n",
      "        [-0.6215,  3.0923, -4.5805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1059, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4008,  1.7808, -5.3175],\n",
      "        [-0.0467,  2.6186, -5.1509],\n",
      "        [ 2.8169, -0.9074, -3.9160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3525, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0137, -1.2977, -2.6519],\n",
      "        [ 1.2711,  1.0272, -5.6043],\n",
      "        [ 1.8325,  0.4148, -5.1899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5458, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2872,  2.2321, -5.1684],\n",
      "        [ 0.6180,  1.8093, -5.4256],\n",
      "        [-0.1126,  2.9481, -5.0748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0989, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1298, -1.1493, -3.7373],\n",
      "        [ 0.2684,  1.9331, -5.2518],\n",
      "        [ 2.0447, -0.1349, -4.8518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7023, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6471,  0.5050, -4.9982],\n",
      "        [ 3.1058, -1.6676, -3.4521],\n",
      "        [-0.1595,  2.9081, -5.1393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0277, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9165, -1.5308, -3.3914],\n",
      "        [ 2.8990, -1.3632, -3.6948],\n",
      "        [-0.1695,  2.7273, -4.8476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6285, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8496,  1.4290, -5.2095],\n",
      "        [ 0.5293,  1.6784, -5.1404],\n",
      "        [ 3.0228, -1.3112, -3.3829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6027e-03,  2.2340e+00, -5.2264e+00],\n",
      "        [ 2.8284e+00, -1.2387e+00, -2.9789e+00],\n",
      "        [ 3.0178e+00, -1.2001e+00, -3.6216e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0303, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6132, -0.7726, -4.1449],\n",
      "        [ 2.8396, -1.1997, -3.3012],\n",
      "        [ 2.6817, -0.6217, -4.4824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3319, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2076,  1.9640, -5.1986],\n",
      "        [-0.1417,  2.6755, -5.2481],\n",
      "        [ 1.3524,  1.1917, -5.4902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2711, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1756,  2.4096, -5.1985],\n",
      "        [ 1.0503,  1.0260, -5.3020],\n",
      "        [ 2.5726, -0.9848, -2.5694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1229, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1932,  2.3155, -5.1417],\n",
      "        [ 0.6211,  1.9639, -5.1414],\n",
      "        [ 2.7188, -1.1828, -3.2284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8869, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6886, -1.1847, -3.4653],\n",
      "        [-0.5444,  2.8701, -4.7850],\n",
      "        [ 2.4029, -0.1247, -4.7922]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0693, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1431,  0.2230, -5.4349],\n",
      "        [-0.0414,  2.9233, -5.1519],\n",
      "        [ 2.8774, -1.1069, -3.5509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1563, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0765,  1.7450, -5.4269],\n",
      "        [ 2.7413, -0.8399, -3.7392],\n",
      "        [-0.4150,  3.2435, -4.5734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0869, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0497,  2.0948, -5.2366],\n",
      "        [ 0.0468,  2.3241, -5.1839],\n",
      "        [ 2.3181, -1.0292, -2.7933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0287, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9537,  3.5279, -3.5816],\n",
      "        [ 2.5720, -0.6577, -4.2149],\n",
      "        [ 2.7348, -0.6661, -3.7847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3841, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7052,  3.1330, -4.1883],\n",
      "        [ 1.4237,  0.8271, -5.1842],\n",
      "        [ 2.2384, -0.0844, -5.1338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3614,  2.7265, -4.8703],\n",
      "        [ 2.6284, -0.8973, -3.5944],\n",
      "        [ 1.6243,  0.4597, -5.0082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2063, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4210,  3.1480, -4.8743],\n",
      "        [ 2.6551, -0.8940, -3.8288],\n",
      "        [-0.9023,  3.5881, -3.5906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7914, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6754,  1.5417, -5.5752],\n",
      "        [ 0.7974,  1.5547, -5.6017],\n",
      "        [-1.0647,  3.2614, -3.8532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0674, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5784,  2.9801, -4.4294],\n",
      "        [-1.0142,  3.3540, -2.9197],\n",
      "        [ 1.9810,  0.2159, -4.5688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0456, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6979,  3.3479, -4.2961],\n",
      "        [ 2.4572, -0.7276, -4.3883],\n",
      "        [ 2.3043, -0.2249, -4.7064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2680, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1817,  1.2680, -5.3504],\n",
      "        [ 2.6303, -0.3800, -4.2548],\n",
      "        [-0.7769,  3.3380, -3.9748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6540, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1321,  1.2952, -5.3907],\n",
      "        [ 0.9565,  1.7039, -5.2549],\n",
      "        [ 2.6008, -0.4329, -4.6173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0299, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5422, -0.5489, -4.2577],\n",
      "        [-0.6957,  3.2475, -3.4153],\n",
      "        [-0.7556,  2.9868, -4.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1743, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9804,  3.4405, -3.1421],\n",
      "        [ 0.6757,  1.7711, -5.5709],\n",
      "        [ 1.8553,  0.4535, -5.1805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2468, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1793,  2.6735, -5.2332],\n",
      "        [ 1.0443,  1.1371, -5.1889],\n",
      "        [-0.9439,  3.5779, -3.2606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5629, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4894, -0.2495, -4.5279],\n",
      "        [-0.6011,  3.1065, -4.0652],\n",
      "        [ 0.4268,  1.8007, -5.3471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1641, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0822,  1.9179, -5.2813],\n",
      "        [ 2.3138, -0.5916, -4.1331],\n",
      "        [ 0.5600,  1.6565, -5.1630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0260, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7845,  3.6326, -4.1610],\n",
      "        [ 2.6512, -0.4203, -4.0325],\n",
      "        [-0.6189,  3.3609, -4.3723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2163, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1734,  0.9576, -5.4631],\n",
      "        [ 2.6660, -0.6123, -4.0307],\n",
      "        [-0.6334,  3.3537, -4.2314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0668, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2655, -0.3067, -5.1075],\n",
      "        [ 2.3759,  0.0078, -4.9997],\n",
      "        [-0.3339,  2.9810, -4.6126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4996, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1357,  2.9797, -4.9040],\n",
      "        [ 2.6678, -0.4484, -4.0760],\n",
      "        [ 0.4106,  1.5402, -5.2607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6137, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5151, -0.6469, -4.3637],\n",
      "        [ 1.6813,  0.1205, -5.8000],\n",
      "        [ 2.5894, -0.4686, -4.2077]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0698, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0246,  3.4506, -2.5321],\n",
      "        [-0.9559,  3.3818, -3.6187],\n",
      "        [ 0.3658,  1.9826, -5.3085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4862, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4102, -0.3196, -4.9882],\n",
      "        [ 1.5976,  0.5040, -5.4665],\n",
      "        [-1.1050,  3.4462, -3.3671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0840, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8971,  0.1282, -5.1699],\n",
      "        [ 2.9192, -0.3657, -4.5004],\n",
      "        [ 2.3846, -0.4816, -4.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9420, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5407, -0.6642, -4.5299],\n",
      "        [ 1.9932, -0.1301, -5.0284],\n",
      "        [ 0.8626,  1.1790, -5.2640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5948, -0.1580, -4.3278],\n",
      "        [ 2.7038, -0.2560, -4.1777],\n",
      "        [ 0.8541,  1.3423, -5.5109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0314, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1350,  1.2433, -5.3218],\n",
      "        [ 1.8400,  0.6580, -5.5908],\n",
      "        [ 1.1861,  0.6411, -5.5545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6441, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1783,  0.5039, -4.9602],\n",
      "        [-0.7075,  3.3373, -3.1108],\n",
      "        [ 2.2939, -0.3913, -5.0273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1147, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4490,  1.7442, -5.8579],\n",
      "        [ 2.6052, -0.6918, -4.0930],\n",
      "        [ 2.3199, -0.4024, -5.0023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2729, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6437,  1.5179, -5.7222],\n",
      "        [ 1.2738,  0.6483, -5.0846],\n",
      "        [ 2.7388, -0.4857, -4.3930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0861, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0135, -0.0242, -5.3158],\n",
      "        [ 2.4644, -0.2198, -4.9859],\n",
      "        [ 2.2919, -0.3646, -4.9886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3856, -0.4742, -4.4303],\n",
      "        [ 1.6654,  0.4843, -5.5087],\n",
      "        [ 0.0592,  2.3575, -5.1556]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2795, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0550,  1.2231, -5.5036],\n",
      "        [-1.1066,  3.5550, -2.8203],\n",
      "        [ 2.6149, -0.4579, -4.4268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2304, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0868,  0.0929, -4.7297],\n",
      "        [ 1.2954,  0.9072, -5.4422],\n",
      "        [ 2.7286, -0.3791, -4.9135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2585, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7889,  1.0678, -5.3411],\n",
      "        [-0.9042,  3.5618, -3.1712],\n",
      "        [ 1.7454,  0.2247, -5.0081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0063, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1124,  3.4887, -3.7092],\n",
      "        [ 2.5290, -0.4047, -4.4960],\n",
      "        [ 1.5287,  0.4740, -5.3566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0714, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5109, -0.0296, -4.8478],\n",
      "        [ 1.0269,  0.7917, -5.2817],\n",
      "        [-1.1828,  3.2768, -2.7439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4637, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4103,  0.8610, -5.5684],\n",
      "        [-1.0276,  3.2452, -3.0654],\n",
      "        [ 0.6550,  1.4608, -5.4504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4723, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1615,  1.0352, -5.6458],\n",
      "        [ 1.4936,  0.4267, -5.7862],\n",
      "        [ 1.2299,  0.7646, -5.3904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2383, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7858,  3.2590, -2.8189],\n",
      "        [ 1.0812,  1.0450, -5.3889],\n",
      "        [-0.6075,  3.3601, -4.4592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1502, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9248,  0.2432, -5.1184],\n",
      "        [ 0.5844,  1.9567, -5.9083],\n",
      "        [ 2.5214, -0.4039, -4.7635]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2225, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5854,  3.1433, -4.3377],\n",
      "        [ 0.8717,  1.0909, -5.6050],\n",
      "        [ 2.7059, -0.2172, -4.7703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3807, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0027,  0.2789, -5.1943],\n",
      "        [ 1.0340,  1.1315, -5.7795],\n",
      "        [ 1.5732,  0.6355, -5.1746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4507, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9959,  1.1171, -5.2678],\n",
      "        [ 1.0045,  1.2737, -5.7427],\n",
      "        [-0.7240,  2.9291, -3.2372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3876, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6390,  3.0336, -3.4941],\n",
      "        [ 1.4296,  0.8473, -5.4893],\n",
      "        [ 2.1425, -0.0235, -4.6334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0459, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2465,  2.7242, -5.0151],\n",
      "        [-0.6588,  3.1127, -3.4562],\n",
      "        [ 2.5644, -0.1784, -4.6550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1161, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9220,  2.9601, -3.2702],\n",
      "        [ 2.3558, -0.3328, -4.9342],\n",
      "        [ 0.7471,  1.9651, -5.6419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0501, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4541,  2.9756, -4.4430],\n",
      "        [ 2.3122,  0.0186, -5.0163],\n",
      "        [-0.7942,  3.1126, -3.5925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9621, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0033,  1.4651, -5.5235],\n",
      "        [ 0.2849,  2.1620, -5.1824],\n",
      "        [ 1.4728,  0.6901, -5.6841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1529, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4918,  3.0312, -3.5102],\n",
      "        [ 1.5343,  0.7848, -5.3002],\n",
      "        [-0.3011,  2.8992, -4.9064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3805, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7724,  0.1997, -5.1836],\n",
      "        [-0.6385,  3.1347, -4.2061],\n",
      "        [ 2.0372,  0.2534, -5.4348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0896, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3492, -0.0941, -4.3984],\n",
      "        [ 0.0483,  2.3045, -5.3590],\n",
      "        [ 2.3957, -0.0419, -4.8640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2330, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3615, -0.7157, -4.6681],\n",
      "        [ 1.6521,  0.1324, -5.5290],\n",
      "        [ 1.3563,  0.8015, -5.1495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0692, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1521,  0.2212, -5.3138],\n",
      "        [-0.4261,  3.0751, -3.9130],\n",
      "        [-0.3252,  2.8642, -4.2167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0980, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8330,  1.2226, -5.2901],\n",
      "        [ 1.2189,  1.0058, -5.5773],\n",
      "        [ 2.1349,  0.0723, -4.7825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3002, -0.0755, -5.0734],\n",
      "        [-0.4390,  2.8246, -4.2180],\n",
      "        [ 2.6037, -0.5216, -4.5999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9508, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7728,  2.0870, -5.7076],\n",
      "        [ 2.5715, -0.2819, -3.7882],\n",
      "        [ 0.5992,  1.4996, -5.4705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1621, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0126,  0.0674, -5.3911],\n",
      "        [ 0.5317,  1.7540, -5.5161],\n",
      "        [ 0.2573,  2.5847, -5.0988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4072, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3856,  2.2248, -5.4447],\n",
      "        [ 2.1901, -0.2370, -4.9132],\n",
      "        [ 1.2735,  0.7516, -5.4927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0974, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1356,  2.5800, -4.9771],\n",
      "        [ 2.1122,  0.1488, -4.8210],\n",
      "        [ 0.0342,  2.5753, -4.7920]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9211, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1022,  0.6469, -5.1116],\n",
      "        [ 2.6663, -0.3875, -4.4654],\n",
      "        [ 1.9344,  0.3537, -5.0700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5486, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5136,  0.6168, -5.4417],\n",
      "        [ 1.4707,  0.6156, -5.3483],\n",
      "        [ 1.4247,  0.9681, -5.4092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0712, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0058,  2.5272, -4.8289],\n",
      "        [ 0.0580,  2.7836, -5.0322],\n",
      "        [ 0.0307,  2.6234, -4.8609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2057, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0770,  2.8021, -4.7326],\n",
      "        [ 2.3085,  0.2347, -5.0720],\n",
      "        [ 1.3754,  0.7895, -5.6947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1774, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7496,  1.5074, -5.6011],\n",
      "        [ 2.3075, -0.0742, -4.8042],\n",
      "        [-0.0491,  2.7736, -4.7181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5287, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6093,  0.6267, -5.8794],\n",
      "        [ 1.3553,  0.8600, -5.4506],\n",
      "        [ 1.1082,  1.2945, -5.6880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7069, -0.4280, -4.4157],\n",
      "        [ 1.0183,  1.5294, -5.6201],\n",
      "        [ 0.2220,  2.1269, -5.4115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3163, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1969, -0.2469, -4.3587],\n",
      "        [ 0.7383,  1.5086, -5.4972],\n",
      "        [ 1.2467,  0.7691, -5.3682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5255, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5842,  0.9966, -5.3646],\n",
      "        [ 1.0353,  0.9638, -5.7523],\n",
      "        [ 1.3397,  0.8417, -5.6517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1755, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0863,  2.3924, -4.6054],\n",
      "        [ 1.3468,  0.6158, -5.3081],\n",
      "        [-0.0636,  2.8876, -4.7360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2136, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0755,  1.4808, -5.8300],\n",
      "        [-0.1237,  2.7181, -4.7169],\n",
      "        [ 2.2627, -0.3428, -4.6855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0571, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5612, -0.6848, -4.1479],\n",
      "        [-0.1407,  2.5625, -4.9063],\n",
      "        [ 0.0262,  2.7098, -4.6816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4333, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3200, -0.0174, -5.2646],\n",
      "        [ 1.5810,  0.7766, -5.3736],\n",
      "        [ 2.7145, -0.7248, -4.2049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0915,  2.4614, -5.3468],\n",
      "        [ 2.1942, -0.3018, -4.8804],\n",
      "        [ 0.6904,  1.5975, -5.7245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2586, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1707,  2.6565, -4.6974],\n",
      "        [ 1.2320,  1.1057, -5.6522],\n",
      "        [ 2.2106, -0.2228, -4.6057]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0461, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  3.0201, -4.4055],\n",
      "        [-0.0947,  2.8210, -5.3639],\n",
      "        [ 2.5318, -0.5772, -3.6672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7119, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9695,  0.1625, -5.1056],\n",
      "        [ 2.3988, -0.0966, -5.5554],\n",
      "        [ 2.1445, -0.1516, -5.1309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4926, -0.6143, -4.4144],\n",
      "        [ 2.6569, -0.5735, -3.9879],\n",
      "        [-0.5110,  2.8183, -4.3858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3093, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4484,  0.9524, -5.6068],\n",
      "        [ 1.4443,  0.8099, -5.3856],\n",
      "        [ 2.7874, -0.8967, -4.2366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1118, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1996,  0.6147, -5.6421],\n",
      "        [-0.2761,  2.9571, -4.3470],\n",
      "        [ 2.1116, -0.0537, -4.8610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9578, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3995,  2.9202, -4.6014],\n",
      "        [-0.2180,  2.8535, -4.4856],\n",
      "        [ 2.3312, -0.3960, -4.8000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3038, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7958,  0.3019, -5.2122],\n",
      "        [ 0.6874,  1.3744, -5.4939],\n",
      "        [ 1.5509,  0.4980, -5.7039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3543, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9210,  0.3063, -5.1461],\n",
      "        [-0.5473,  3.2784, -4.1537],\n",
      "        [ 1.3386,  1.0321, -5.8743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0372, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6438, -0.4661, -4.5542],\n",
      "        [-0.5227,  2.8387, -4.8049],\n",
      "        [ 2.8392, -0.5952, -4.2518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0974, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3922, -0.8266, -4.2374],\n",
      "        [ 2.7203, -0.8265, -4.4680],\n",
      "        [ 1.7280,  0.3369, -5.7521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5567, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6701, -0.7880, -4.6722],\n",
      "        [ 1.7385,  0.5331, -5.0961],\n",
      "        [ 1.8620,  0.1741, -5.4013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1909, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6667,  0.4903, -5.1955],\n",
      "        [ 0.7052,  1.8748, -5.4156],\n",
      "        [ 2.7989, -0.6378, -4.8513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0910, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7229, -0.4853, -4.2265],\n",
      "        [-0.0833,  2.5214, -5.6100],\n",
      "        [ 0.5176,  2.2677, -5.1983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2725, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0531,  2.6604, -5.3934],\n",
      "        [ 1.3971,  0.7558, -5.7320],\n",
      "        [ 1.6113,  0.6668, -5.0336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3882, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2647,  0.7460, -5.6708],\n",
      "        [ 1.6464,  0.3622, -5.5406],\n",
      "        [ 1.4656,  0.9039, -5.4206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9757, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1148,  2.5750, -5.4037],\n",
      "        [ 2.0801,  0.0791, -5.3778],\n",
      "        [ 1.6635,  0.4364, -5.8580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1653, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7466,  1.5080, -5.7981],\n",
      "        [-0.6381,  3.1816, -4.3008],\n",
      "        [ 2.1775, -0.1904, -5.2504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2854, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0463,  0.8594, -5.5874],\n",
      "        [ 2.7973, -1.0173, -4.2238],\n",
      "        [ 2.4913, -0.6864, -4.4671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0344, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7892,  3.1205, -4.1681],\n",
      "        [ 2.4877, -0.3577, -4.6918],\n",
      "        [ 2.8679, -0.8211, -4.2442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1663, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6744,  1.5352, -5.2770],\n",
      "        [-0.7901,  3.1679, -4.2169],\n",
      "        [ 2.2352,  0.2222, -5.2421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3824, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3584,  2.8707, -4.9036],\n",
      "        [ 1.2976,  0.8234, -5.6845],\n",
      "        [ 2.0429,  0.2133, -5.3560]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0292, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4662,  2.9774, -5.2986],\n",
      "        [ 2.8305, -0.3700, -4.6803],\n",
      "        [-0.7903,  3.3987, -4.4659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0306, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6631, -0.6301, -4.8285],\n",
      "        [ 2.6492, -0.7942, -4.2185],\n",
      "        [-0.6767,  3.1373, -4.3590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4677, -0.4649, -4.7633],\n",
      "        [-0.0283,  2.2208, -5.2940],\n",
      "        [ 1.4033,  0.7818, -5.0970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1164, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1770,  2.6206, -5.1459],\n",
      "        [ 0.0639,  2.0097, -5.2580],\n",
      "        [ 0.2142,  2.0019, -5.2320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1516, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6732,  3.1219, -4.4485],\n",
      "        [ 2.3416,  0.1587, -5.3164],\n",
      "        [ 1.4873,  0.5266, -5.3844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1898, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1120,  0.1568, -5.2183],\n",
      "        [ 1.3966,  0.7109, -5.7852],\n",
      "        [ 2.6213, -0.9866, -4.0288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0377, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0141,  2.6765, -5.2170],\n",
      "        [ 2.6157, -0.2498, -4.8858],\n",
      "        [ 1.6566,  0.6053, -5.1151]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1041, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2167,  2.8490, -4.9273],\n",
      "        [ 0.5225,  1.9259, -5.2810],\n",
      "        [ 2.5858, -0.4843, -4.9199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8941, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8309,  0.3939, -5.2532],\n",
      "        [ 2.6876, -0.5943, -4.8511],\n",
      "        [ 2.3323, -0.0062, -5.4677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4673, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0855,  1.0543, -5.7075],\n",
      "        [ 2.4145, -0.7419, -4.6870],\n",
      "        [ 1.0198,  1.0455, -5.3874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1048, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0316,  2.8475, -4.8471],\n",
      "        [ 0.0840,  2.0160, -5.3756],\n",
      "        [ 0.1802,  2.2140, -5.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2010, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6441,  0.7019, -5.2908],\n",
      "        [ 3.0619, -0.8668, -4.2396],\n",
      "        [ 0.6613,  1.9083, -5.3381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3412, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8994,  0.4773, -5.3737],\n",
      "        [ 0.9882,  0.7810, -5.4140],\n",
      "        [ 1.7719,  0.4212, -5.1928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1974, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1499, -0.9784, -4.3719],\n",
      "        [ 1.1520,  1.4712, -5.6703],\n",
      "        [ 2.8478, -0.6987, -4.4531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8333, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1400,  0.3749, -5.5784],\n",
      "        [ 2.8510, -1.1184, -4.6440],\n",
      "        [ 2.8505, -0.6771, -4.8817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4115, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3928,  1.2508, -5.9212],\n",
      "        [ 0.7363,  1.5703, -5.5740],\n",
      "        [ 2.1216, -0.0736, -4.9365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5317, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6344,  0.2235, -5.3247],\n",
      "        [ 0.5757,  1.2989, -5.4474],\n",
      "        [ 1.6138,  1.1055, -5.5100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1571, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7673, -0.8032, -4.1422],\n",
      "        [ 0.5349,  1.8016, -5.3896],\n",
      "        [ 0.2316,  1.7782, -5.4128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2947, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3999,  0.5672, -5.5579],\n",
      "        [ 1.6359,  0.7641, -5.6239],\n",
      "        [ 0.0875,  1.7622, -5.3140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3088, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2315,  0.0101, -5.2679],\n",
      "        [ 0.7251,  1.6654, -5.4348],\n",
      "        [ 0.9487,  1.4018, -5.5238]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1973, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3855,  2.6531, -4.9910],\n",
      "        [ 0.3197,  1.7426, -5.2743],\n",
      "        [ 1.5750,  0.6260, -5.2144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5439, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6648,  1.6756, -5.8106],\n",
      "        [ 1.0723,  1.1572, -5.2347],\n",
      "        [ 0.9638,  0.7292, -5.5899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7085, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5756,  2.6056, -4.6582],\n",
      "        [ 0.1215,  1.8265, -5.5303],\n",
      "        [ 1.9239,  0.4764, -5.6998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5849, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3746,  0.4021, -5.1834],\n",
      "        [ 1.6835,  0.3598, -5.2351],\n",
      "        [ 0.6672,  1.5024, -5.3111]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1910, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5938,  0.4782, -5.0626],\n",
      "        [ 1.9632,  0.4515, -5.6512],\n",
      "        [ 0.0425,  2.4231, -5.2231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1817, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2184,  2.5186, -5.2277],\n",
      "        [-0.3245,  2.1110, -5.2459],\n",
      "        [ 0.9454,  1.6638, -5.6941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3436, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2003,  2.2429, -5.0511],\n",
      "        [ 1.4644,  0.8420, -5.3225],\n",
      "        [ 0.8455,  1.2387, -5.2405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0768, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6146e+00, -3.0155e-01, -5.0765e+00],\n",
      "        [-3.8796e-01,  2.2372e+00, -5.0292e+00],\n",
      "        [-2.5584e-03,  2.1872e+00, -5.0767e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4517, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9196,  0.8584, -5.4971],\n",
      "        [ 1.3954,  1.0252, -5.3133],\n",
      "        [ 0.2922,  2.0105, -5.4656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0881, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0477,  1.8934, -5.0432],\n",
      "        [ 2.3465, -0.6799, -4.7210],\n",
      "        [-0.1641,  2.4869, -5.0548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0881, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0123,  2.2122, -5.3088],\n",
      "        [-0.1821,  2.3546, -5.3776],\n",
      "        [-0.2661,  2.1708, -4.9979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9553, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6622,  1.5130, -5.6504],\n",
      "        [ 0.4180,  1.8150, -5.3425],\n",
      "        [-0.4854,  2.7221, -4.8758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2212, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0205,  1.9477, -5.0554],\n",
      "        [ 0.3818,  1.7196, -5.3510],\n",
      "        [ 0.5806,  1.6388, -5.5787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0593, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2664,  2.5326, -5.1068],\n",
      "        [ 2.4091, -0.0144, -5.1355],\n",
      "        [-0.7054,  2.7076, -4.6468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1989, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3448, -0.2279, -4.6852],\n",
      "        [ 0.7045,  1.2296, -5.4942],\n",
      "        [ 2.6541, -0.1928, -4.9240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2198, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9328,  1.1935, -5.5857],\n",
      "        [-0.7043,  2.5214, -4.8666],\n",
      "        [-0.4594,  2.5627, -5.2391]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2683, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5184,  0.4717, -5.3757],\n",
      "        [ 1.6089,  0.8362, -5.8864],\n",
      "        [ 0.0674,  2.1038, -5.2561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0507, grad_fn=<NllLossBackward0>), logits=tensor([[-7.1387e-01,  3.0014e+00, -4.2764e+00],\n",
      "        [-8.3958e-01,  3.0322e+00, -4.7547e+00],\n",
      "        [ 3.4073e-03,  2.1961e+00, -5.2064e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0847, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4199, -0.3545, -4.7131],\n",
      "        [-0.5420,  2.6937, -4.6595],\n",
      "        [ 0.2046,  2.0047, -5.1905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9151, -0.8266, -4.0425],\n",
      "        [ 2.8090, -0.6488, -4.6773],\n",
      "        [ 2.5458, -0.6160, -4.5272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2406, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0733,  1.1999, -5.4488],\n",
      "        [ 2.5187, -0.5061, -4.8235],\n",
      "        [-0.7557,  3.3694, -4.4228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0856, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9804,  3.3834, -4.3073],\n",
      "        [ 2.7253, -0.8744, -4.3373],\n",
      "        [ 2.5027, -0.6715, -4.7601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0429, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5717, -0.7105, -4.9565],\n",
      "        [-0.6063,  2.4693, -5.1422],\n",
      "        [ 2.6064, -0.4810, -4.2632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0323, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8438,  2.9199, -4.7925],\n",
      "        [ 2.6711, -0.2666, -4.7458],\n",
      "        [ 2.9586, -0.9119, -4.1324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0938, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1339,  0.1305, -5.0924],\n",
      "        [ 2.2304,  0.2507, -5.5597],\n",
      "        [ 2.6829, -1.0434, -4.5173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2594, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9870,  1.0592, -5.4167],\n",
      "        [-0.7895,  3.0617, -4.5181],\n",
      "        [ 2.0566, -0.2234, -4.9574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1105, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1156,  3.2605, -4.0177],\n",
      "        [-0.5196,  3.0818, -4.7726],\n",
      "        [ 1.7628,  0.6748, -5.4432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3013, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7098,  0.7640, -5.6511],\n",
      "        [-1.0254,  3.2832, -4.0516],\n",
      "        [ 0.9974,  1.2826, -5.5076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1586, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2009,  3.4018, -3.6915],\n",
      "        [-0.9273,  3.2372, -4.2888],\n",
      "        [ 1.2369,  0.6677, -5.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0690, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0998,  2.1163, -5.1373],\n",
      "        [-1.0686,  3.2124, -4.0196],\n",
      "        [ 2.3952, -0.2863, -4.8107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3787, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2349,  3.3312, -3.9626],\n",
      "        [ 1.9720,  0.1159, -5.6428],\n",
      "        [-0.7557,  3.2047, -4.9694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9064, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2526,  2.2815, -4.9964],\n",
      "        [ 2.3653,  0.0572, -4.8946],\n",
      "        [-1.2212,  3.2169, -3.8330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0399, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2377,  2.4994, -5.3909],\n",
      "        [ 2.8577, -0.6757, -4.8554],\n",
      "        [-0.7215,  2.8857, -4.7415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8175, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0756,  3.4804, -4.1834],\n",
      "        [-0.1623,  2.1301, -5.2171],\n",
      "        [-0.5063,  2.4266, -4.8787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1921, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0873,  0.7097, -5.7606],\n",
      "        [ 2.9247, -0.8088, -4.2204],\n",
      "        [-0.7363,  2.8005, -4.7839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3591, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8721,  3.0569, -4.6005],\n",
      "        [ 2.4244, -0.3993, -4.7121],\n",
      "        [ 0.8492,  1.3876, -5.1937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3949, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8328,  1.4189, -5.5209],\n",
      "        [ 2.5555, -0.8641, -4.4419],\n",
      "        [ 2.1955,  0.1530, -5.5086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7673, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6031,  3.0037, -4.2848],\n",
      "        [ 1.9457, -0.1360, -5.2830],\n",
      "        [ 2.1198, -0.4498, -4.8379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3523, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6285,  1.9270, -5.3560],\n",
      "        [ 0.4343,  1.8675, -5.1700],\n",
      "        [ 2.8266, -0.7452, -4.2768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2086, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0934,  0.6790, -5.6925],\n",
      "        [-0.3233,  2.3121, -5.2337],\n",
      "        [ 2.6261, -0.3914, -4.9085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1192, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0989,  1.8877, -4.9323],\n",
      "        [-0.3544,  2.4128, -4.8041],\n",
      "        [ 0.1838,  1.8960, -5.3824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0405, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9753, -0.3232, -4.6170],\n",
      "        [ 2.6703, -0.3320, -5.1524],\n",
      "        [ 2.7192, -0.6078, -4.4401]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1455, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6502, -0.2654, -4.9151],\n",
      "        [ 1.5544, -0.0284, -5.2883],\n",
      "        [ 1.6921,  0.3025, -5.3764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0714, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9521, -0.0724, -5.0700],\n",
      "        [ 2.1932,  0.0176, -5.2883],\n",
      "        [ 1.5070,  1.0354, -5.6514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3276, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3324, -0.1722, -5.0027],\n",
      "        [ 1.1672,  0.8954, -5.6132],\n",
      "        [ 2.5742, -0.1389, -5.0067]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0613, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3456, -0.2796, -4.6391],\n",
      "        [ 2.5249, -0.4245, -4.7679],\n",
      "        [ 2.4400, -0.3320, -4.6797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1269, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1815,  2.1562, -5.1066],\n",
      "        [ 0.0810,  1.8939, -5.2709],\n",
      "        [ 2.0463,  0.1167, -5.3679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0263, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4282,  0.4789, -5.8670],\n",
      "        [-0.2436,  2.0382, -5.2199],\n",
      "        [ 2.2627, -0.3169, -4.7217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4308, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2419e-03,  2.0897e+00, -5.3119e+00],\n",
      "        [ 2.4773e+00, -4.1097e-01, -4.8115e+00],\n",
      "        [ 1.3952e+00,  6.7132e-01, -5.3421e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3620, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3999,  2.5361, -4.7077],\n",
      "        [ 1.7654,  0.3130, -5.0251],\n",
      "        [ 0.9323,  1.1748, -5.6988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2906, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6260,  0.5424, -5.6992],\n",
      "        [-0.3039,  2.3919, -4.7084],\n",
      "        [ 0.8243,  1.2238, -5.8310]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5779, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5909,  0.4489, -5.4248],\n",
      "        [ 0.5720,  1.4836, -5.5985],\n",
      "        [ 1.8168,  0.3378, -5.3479]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1946, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8089,  1.3784, -5.5206],\n",
      "        [-0.4555,  2.3504, -4.9027],\n",
      "        [-0.3485,  2.2063, -5.0119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7723, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5950,  0.9190, -5.4974],\n",
      "        [ 1.6458,  0.3767, -5.5606],\n",
      "        [ 1.5325,  0.7832, -5.5465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1083e+00, -1.4116e-03, -5.2944e+00],\n",
      "        [ 8.2979e-02,  2.1907e+00, -5.4141e+00],\n",
      "        [-1.8045e-01,  1.9509e+00, -5.3223e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2182, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8089,  1.3155, -5.6344],\n",
      "        [ 2.2241,  0.0897, -4.6739],\n",
      "        [-0.1530,  2.4815, -5.1861]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1235, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4161,  2.8440, -4.6034],\n",
      "        [ 2.1356, -0.0966, -5.0898],\n",
      "        [ 1.5982,  0.2424, -5.7373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6041, -0.2855, -4.8881],\n",
      "        [ 0.6008,  1.4216, -5.3738],\n",
      "        [-0.0277,  1.9031, -4.9315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6344, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6831,  0.3353, -5.1888],\n",
      "        [-0.2049,  2.3089, -5.2370],\n",
      "        [ 1.7761,  0.4111, -5.4753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4092, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4659,  0.9697, -5.3748],\n",
      "        [-0.2511,  2.1774, -5.1532],\n",
      "        [ 0.2060,  1.8950, -5.3504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4130, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6376,  0.4684, -5.3865],\n",
      "        [ 1.0176,  1.1385, -5.9236],\n",
      "        [ 1.6363,  0.1896, -5.4084]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1977, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8184,  3.0113, -4.7404],\n",
      "        [ 1.0324,  0.5549, -5.3399],\n",
      "        [-0.0103,  2.3852, -5.2289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5166, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5167,  0.7732, -5.5843],\n",
      "        [ 0.8819,  1.3923, -5.5289],\n",
      "        [ 2.0294,  0.3960, -5.2437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0286, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6991,  2.8565, -4.9089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 1/3, Loss_per_epoch: 424.57976839412004\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4036, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3163,  2.8585, -4.8721],\n",
      "        [ 0.6423,  1.4030, -6.0516],\n",
      "        [-0.8077,  2.8978, -4.4603]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1677, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6537,  2.7200, -4.9401],\n",
      "        [ 1.3913,  0.8020, -5.8119],\n",
      "        [-0.7219,  2.8845, -4.6377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0753, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6955,  2.7794, -4.8209],\n",
      "        [ 1.9945,  0.1504, -4.8457],\n",
      "        [-0.5907,  2.4525, -5.0417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2083, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8743,  0.3823, -5.0495],\n",
      "        [-0.4088,  2.5206, -4.9547],\n",
      "        [ 1.6285,  0.8193, -5.5894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4592, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8678,  1.1127, -5.8099],\n",
      "        [ 1.4900,  0.5399, -5.5098],\n",
      "        [ 1.8219,  0.4493, -5.6945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2013, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8452,  1.4384, -5.4233],\n",
      "        [ 2.4592,  0.0196, -5.2725],\n",
      "        [ 2.2940, -0.2078, -4.9864]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0482, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1667, -0.3033, -4.8620],\n",
      "        [-0.5978,  2.7039, -5.1648],\n",
      "        [-0.7131,  2.9544, -4.1698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3196, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3720,  0.7949, -5.6329],\n",
      "        [ 2.0065,  0.0149, -5.1176],\n",
      "        [ 1.5080,  0.7469, -5.4631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1337, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1541, -0.1193, -4.9267],\n",
      "        [ 1.8466,  0.6162, -5.5695],\n",
      "        [-0.4336,  2.6487, -4.6962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2111, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2840e+00,  7.5343e-01, -5.6399e+00],\n",
      "        [ 1.8473e+00, -2.4620e-03, -5.2719e+00],\n",
      "        [-7.4170e-01,  3.0373e+00, -4.3478e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5123, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4317,  0.8319, -5.8794],\n",
      "        [-0.7458,  3.1923, -4.3170],\n",
      "        [ 1.3905,  0.7276, -5.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1253, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2878, -0.5716, -5.0201],\n",
      "        [ 0.3576,  1.7128, -5.4561],\n",
      "        [ 2.4419,  0.0693, -5.2997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0891, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1579, -0.1028, -5.2889],\n",
      "        [ 2.0604, -0.2141, -5.1193],\n",
      "        [ 2.1915, -0.4727, -4.1943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1086, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0727,  3.2233, -4.1276],\n",
      "        [ 2.2339, -0.5038, -4.7456],\n",
      "        [ 0.4374,  1.7080, -5.5805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1185, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6111,  1.6648, -5.1390],\n",
      "        [ 2.7032, -0.6697, -4.6515],\n",
      "        [-0.7106,  3.1419, -4.8132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0616, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5250e+00, -7.1921e-01, -4.6672e+00],\n",
      "        [-8.7054e-01,  3.1883e+00, -4.4745e+00],\n",
      "        [ 1.9898e+00, -2.7479e-03, -5.6526e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0435, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4027, -0.2659, -4.7043],\n",
      "        [ 2.5269, -0.5849, -4.8651],\n",
      "        [-0.8125,  3.1897, -4.4144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0324, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8743,  2.8046, -4.1201],\n",
      "        [-0.6471,  2.6469, -4.6289],\n",
      "        [ 2.5878, -0.7981, -4.5428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1892, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3128,  0.8516, -5.9186],\n",
      "        [ 2.4862, -0.4894, -4.8341],\n",
      "        [-0.6943,  2.8959, -4.5778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0237, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6429,  2.7184, -4.9214],\n",
      "        [-0.7074,  3.2545, -4.6077],\n",
      "        [-0.8397,  3.2414, -4.1763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8461, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4263,  0.0130, -5.3691],\n",
      "        [ 2.5785, -1.0323, -4.7285],\n",
      "        [-1.2439,  3.2660, -4.0968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0455, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2575,  2.5185, -4.8211],\n",
      "        [-0.8332,  3.1864, -4.7330],\n",
      "        [ 2.4823, -0.3595, -5.0744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8961, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0747,  3.4520, -3.6314],\n",
      "        [-0.9837,  3.3125, -4.6026],\n",
      "        [ 2.0982, -0.4919, -5.3316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2535, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5465,  1.7254, -5.4148],\n",
      "        [ 2.1419, -0.4663, -5.0538],\n",
      "        [ 0.8546,  1.5068, -5.5270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0939, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6242, -0.7935, -4.5493],\n",
      "        [ 2.8704, -0.8755, -4.5803],\n",
      "        [ 0.4416,  1.8225, -5.4254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0276, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1364,  3.3461, -3.8688],\n",
      "        [ 2.6580, -0.9148, -4.5628],\n",
      "        [ 2.5812, -0.5759, -4.6182]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4521, -0.9565, -4.6307],\n",
      "        [-1.0262,  2.8732, -4.2585],\n",
      "        [ 2.7186, -0.7058, -5.0122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0486, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3812, -0.4715, -5.1492],\n",
      "        [ 2.1624, -0.4084, -5.1478],\n",
      "        [-1.0527,  3.2075, -3.7508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8901, -0.8135, -4.1771],\n",
      "        [ 2.9990, -0.9111, -4.1958],\n",
      "        [ 2.3412, -0.4594, -4.8885]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1735, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6558, -0.9123, -4.3722],\n",
      "        [ 2.4087, -0.5691, -4.4604],\n",
      "        [ 1.4375,  0.8463, -5.4241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0477, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2185,  3.2407, -3.4502],\n",
      "        [ 2.2288, -0.1040, -5.4992],\n",
      "        [ 2.6127, -0.6751, -4.7216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0322, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6955, -0.8441, -4.4822],\n",
      "        [-1.2689,  3.5064, -3.3630],\n",
      "        [-0.4192,  2.4137, -5.1167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1030, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7319,  0.3940, -5.5181],\n",
      "        [ 2.6030, -0.5873, -4.8574],\n",
      "        [-0.7298,  2.6381, -4.8567]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0369, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7449, -1.1150, -4.5878],\n",
      "        [ 2.4251, -0.9077, -4.6021],\n",
      "        [ 2.4582, -0.4556, -5.2688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0295, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3918,  3.4653, -3.7306],\n",
      "        [ 2.3744, -0.2637, -4.9013],\n",
      "        [-1.2552,  3.4007, -3.8024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1767,  3.4419, -3.5622],\n",
      "        [ 2.3700, -1.0244, -4.2131],\n",
      "        [ 2.7595, -1.2140, -4.1275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5234,  3.3876, -3.2296],\n",
      "        [ 2.9516, -1.1847, -4.2001],\n",
      "        [-1.0947,  3.3443, -3.9814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0991, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5823, -0.8393, -4.8027],\n",
      "        [ 1.9675,  0.3907, -5.7071],\n",
      "        [ 2.1686, -0.3758, -4.8523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2687,  3.4976, -3.7318],\n",
      "        [-1.2325,  3.3133, -2.9660],\n",
      "        [-1.1795,  3.2824, -2.7151]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0960, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2442, -0.3433, -4.9044],\n",
      "        [ 1.8043,  0.1426, -5.4862],\n",
      "        [ 2.5580, -0.6427, -4.8799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8755, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1534, -0.3198, -5.1712],\n",
      "        [ 2.8475, -0.7027, -4.9711],\n",
      "        [ 2.6496, -0.4827, -5.1255]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0178, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6338,  3.0902, -5.0476],\n",
      "        [-1.2392,  3.6183, -3.3168],\n",
      "        [ 2.9483, -0.9623, -4.3983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3852, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1250,  3.6500, -4.5690],\n",
      "        [-1.4030,  3.5227, -3.4250],\n",
      "        [ 0.6680,  1.4203, -6.0380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0141, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8110, -0.9821, -4.2870],\n",
      "        [-1.2459,  3.3167, -3.1888],\n",
      "        [-1.4111,  3.5786, -3.8873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4308, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3427,  0.6008, -5.7985],\n",
      "        [ 2.3102, -0.1833, -5.1643],\n",
      "        [ 2.2514, -0.2349, -5.2892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0102, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0055,  3.2573, -4.9316],\n",
      "        [-1.2256,  3.7311, -4.1079],\n",
      "        [-1.3155,  3.5064, -3.5529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8301, -1.0068, -3.9884],\n",
      "        [-1.3240,  3.5988, -3.2276],\n",
      "        [-0.9637,  3.4952, -3.8707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0628, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2597,  2.1850, -5.2732],\n",
      "        [ 2.6157, -1.1270, -4.6423],\n",
      "        [-0.5357,  3.0520, -5.0180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1370, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6025,  1.3278, -5.4490],\n",
      "        [-1.4822,  3.6854, -3.5140],\n",
      "        [-1.0732,  3.6875, -4.1764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6596, -0.5112, -5.0876],\n",
      "        [ 2.9224, -1.3621, -3.7699],\n",
      "        [-1.5068,  3.7197, -3.6082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1368, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2918, -0.5342, -5.2244],\n",
      "        [ 0.5667,  1.5907, -5.6893],\n",
      "        [ 2.4007, -0.6977, -4.7242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0443, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9203, -1.3419, -4.1462],\n",
      "        [-1.2787,  3.5492, -3.0935],\n",
      "        [ 2.1969,  0.0288, -5.3140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0288, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4045, -0.5232, -4.8898],\n",
      "        [-1.2707,  3.2535, -4.1256],\n",
      "        [ 2.6197, -1.2612, -3.6978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8674, -1.2392, -4.0658],\n",
      "        [-1.5343,  3.3465, -3.7516],\n",
      "        [ 2.7866, -1.4262, -3.8270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0215, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4060,  3.3773, -3.4113],\n",
      "        [ 2.5098, -0.5332, -5.2061],\n",
      "        [-1.2314,  3.6836, -3.6095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0680, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5456,  3.5298, -3.1482],\n",
      "        [-0.5851,  3.0096, -4.9189],\n",
      "        [ 0.3207,  2.0159, -5.3107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0785, -1.3421, -3.7193],\n",
      "        [ 2.7646, -1.6000, -3.7706],\n",
      "        [-0.4965,  2.6990, -5.5266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8924, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2419, -0.3349, -5.1308],\n",
      "        [-1.2617,  3.7017, -3.0402],\n",
      "        [ 2.5185, -1.5955, -3.6378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0696, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5247, -0.0735, -5.0759],\n",
      "        [ 2.8105, -1.2484, -3.7293],\n",
      "        [-1.6469,  3.8462, -3.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3654,  3.7422, -3.8821],\n",
      "        [ 2.6545, -0.8943, -4.4597],\n",
      "        [ 2.6460, -1.2269, -4.3181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3658, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7095, -1.1419, -4.5520],\n",
      "        [ 0.7456,  1.3927, -5.6849],\n",
      "        [-1.5779,  3.5281, -3.4600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0179, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6125,  3.6323, -2.7901],\n",
      "        [-0.9598,  3.1756, -4.7481],\n",
      "        [ 2.7704, -0.7185, -4.7261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0966, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6149,  3.6184, -3.6856],\n",
      "        [ 0.6568,  1.8053, -5.0828],\n",
      "        [-1.5540,  3.5789, -2.8247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2118, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9536,  1.1492, -5.8384],\n",
      "        [ 2.8616, -1.5673, -4.0014],\n",
      "        [ 2.8501, -0.9859, -4.5476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0731, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4788,  3.6752, -3.5246],\n",
      "        [ 1.8878,  0.3533, -5.6520],\n",
      "        [-0.9950,  3.0892, -4.7060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6347,  3.6390, -3.3764],\n",
      "        [ 2.8961, -1.5179, -4.0158],\n",
      "        [-1.5742,  3.5903, -3.9260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3814, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4134,  3.6760, -3.1799],\n",
      "        [ 1.5158,  0.7815, -5.9520],\n",
      "        [-1.3577,  3.2858, -3.6745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2337, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1629,  1.0346, -5.6244],\n",
      "        [-1.5635,  3.6887, -3.5236],\n",
      "        [ 2.3906, -0.3382, -5.5136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7294, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2466, -0.2071, -4.7904],\n",
      "        [-1.4643,  3.6271, -4.0181],\n",
      "        [-1.3560,  3.7554, -3.1292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5884, -1.1459, -4.9674],\n",
      "        [-1.5175,  3.8735, -3.7086],\n",
      "        [-1.5185,  3.7510, -3.5882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1578, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8998,  3.1996, -4.8397],\n",
      "        [-1.4319,  3.6350, -3.7965],\n",
      "        [ 1.3796,  0.8122, -5.6046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0343, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2122,  3.4777, -3.8027],\n",
      "        [ 2.6439, -0.7993, -4.6333],\n",
      "        [ 2.6336, -0.1369, -5.5622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0072, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2922,  3.6011, -3.9853],\n",
      "        [-1.2511,  3.6077, -4.0204],\n",
      "        [-1.3447,  3.9299, -4.0209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4981, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1499,  3.5582, -3.9837],\n",
      "        [ 2.7982, -1.2647, -3.9584],\n",
      "        [ 1.8481,  0.6441, -5.8173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2701,  3.5941, -3.8670],\n",
      "        [ 2.4048, -0.9593, -4.1735],\n",
      "        [-1.1963,  3.5285, -4.3455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2554,  3.2658, -4.3960],\n",
      "        [ 2.8153, -1.2921, -3.7375],\n",
      "        [-1.2956,  3.6956, -3.9127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0427, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0164,  2.3257, -5.3572],\n",
      "        [-1.0667,  3.2219, -4.5392],\n",
      "        [ 2.8023, -1.2249, -3.8488]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0546, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9976, -1.6011, -3.7159],\n",
      "        [-1.1400,  3.1265, -3.8813],\n",
      "        [ 1.9814,  0.0633, -5.1380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1116, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7795, -1.2714, -3.4638],\n",
      "        [-0.9684,  3.1371, -4.4656],\n",
      "        [ 1.6784,  0.6218, -5.6746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0339, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0395, -1.4895, -3.6968],\n",
      "        [-1.1393,  3.1333, -4.5495],\n",
      "        [ 2.2400, -0.3145, -5.1542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4435,  1.0355, -5.9626],\n",
      "        [-1.0674,  3.6433, -3.8999],\n",
      "        [ 2.8271, -1.3763, -3.7950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0159, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8593, -1.4595, -4.2307],\n",
      "        [-0.9896,  3.1873, -4.5619],\n",
      "        [ 2.8228, -1.2610, -3.8967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1627, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6544,  2.7294, -5.0683],\n",
      "        [ 2.3671, -0.5083, -5.0485],\n",
      "        [ 2.7663, -1.4735, -4.1427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0375, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0834, -0.4035, -5.2374],\n",
      "        [ 3.0338, -1.2860, -3.7977],\n",
      "        [ 2.9267, -1.1677, -3.8130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0999, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4632,  0.2769, -5.5794],\n",
      "        [ 2.7135, -1.3070, -3.8815],\n",
      "        [-1.0263,  3.3009, -4.5173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0566, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3178,  2.2362, -5.5350],\n",
      "        [-0.9567,  2.9217, -4.8341],\n",
      "        [-1.0394,  3.4435, -4.6100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9784, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3509,  2.3056, -5.0463],\n",
      "        [ 1.8041,  0.2655, -5.8269],\n",
      "        [ 2.8810, -1.3671, -3.8193]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0485,  3.1140, -4.6028],\n",
      "        [ 2.9121, -1.4607, -3.6001],\n",
      "        [-0.7218,  2.9002, -4.9218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1350, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6388, -1.7408, -3.0782],\n",
      "        [ 2.6278, -1.3904, -4.4653],\n",
      "        [ 1.4383,  0.6344, -5.4230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5904,  2.8271, -4.7596],\n",
      "        [ 2.9368, -1.3165, -3.9269],\n",
      "        [ 2.7959, -1.4353, -3.8464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4225, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8906, -1.0105, -4.5333],\n",
      "        [ 0.6431,  1.5230, -5.5473],\n",
      "        [-0.8741,  3.0677, -4.9432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3477, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1395,  1.4049, -5.7815],\n",
      "        [-0.6777,  2.5861, -4.5239],\n",
      "        [ 1.4691,  0.8613, -5.9895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2211, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7717,  3.0804, -4.5519],\n",
      "        [-0.2702,  2.6368, -4.9987],\n",
      "        [ 1.0506,  1.2748, -5.3554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2306, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8296, -1.4254, -3.3925],\n",
      "        [ 0.7138,  1.3735, -5.5757],\n",
      "        [ 0.4798,  1.7031, -5.5492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0257, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5728, -0.9785, -4.1026],\n",
      "        [-0.7183,  2.7715, -4.9669],\n",
      "        [ 2.9797, -1.1095, -4.6091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4450,  2.4429, -4.9522],\n",
      "        [-0.4678,  2.7794, -4.9903],\n",
      "        [ 3.0435, -1.4134, -3.6670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0582, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0709, -1.1806, -3.8686],\n",
      "        [-0.0141,  1.9696, -5.3119],\n",
      "        [-0.8164,  2.6937, -4.7022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5082, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1784,  0.9953, -5.6880],\n",
      "        [ 0.9281,  1.0782, -5.4773],\n",
      "        [-0.0547,  2.0698, -5.4066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1104, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9245, -1.7419, -3.4061],\n",
      "        [ 2.4746, -0.7810, -5.1555],\n",
      "        [-0.9584,  2.6954, -4.7653]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7436, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6737,  2.8451, -5.2891],\n",
      "        [ 3.0570, -1.1681, -4.5449],\n",
      "        [ 2.1635,  0.0968, -5.6582]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6419,  2.8669, -4.5943],\n",
      "        [-0.3036,  2.3867, -5.2494],\n",
      "        [-0.7467,  2.7416, -4.5406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4867,  2.8913, -4.9999],\n",
      "        [-0.6961,  3.0116, -5.3324],\n",
      "        [ 3.1018, -1.5148, -3.5840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4199, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4423,  0.6110, -5.6564],\n",
      "        [-0.7455,  3.1574, -4.5820],\n",
      "        [ 2.4143, -0.6542, -5.4030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0236, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5573,  2.6360, -5.2549],\n",
      "        [ 2.8840, -1.2468, -3.8049],\n",
      "        [-1.0650,  3.2827, -4.8088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0267, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9286,  2.7692, -4.7001],\n",
      "        [-1.0053,  3.1000, -4.3743],\n",
      "        [-0.5882,  2.6699, -5.0664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0412, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7220,  2.7383, -5.0055],\n",
      "        [-0.6718,  2.6383, -5.2407],\n",
      "        [-0.3742,  2.4878, -5.4892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8501, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6106,  2.4481, -5.1863],\n",
      "        [ 0.4232,  1.7766, -5.4681],\n",
      "        [ 0.8188,  1.2297, -5.7317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0684, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3619,  2.2392, -5.1129],\n",
      "        [-0.1600,  1.9335, -5.5642],\n",
      "        [ 2.7274, -1.4268, -4.4318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1667, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4169,  0.6998, -6.0992],\n",
      "        [ 2.8062, -0.8246, -4.3766],\n",
      "        [ 2.0520, -0.5053, -5.0224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5646, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2213, -0.4345, -4.5828],\n",
      "        [ 0.4724,  1.6120, -5.2310],\n",
      "        [ 0.2887,  1.7618, -5.6919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1013, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9966,  3.1017, -4.6944],\n",
      "        [ 2.2291, -0.3270, -5.3351],\n",
      "        [ 1.7602,  0.3125, -5.5528]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7372, -0.4528, -4.3318],\n",
      "        [ 2.5166, -1.3369, -3.4030],\n",
      "        [ 0.4555,  1.7122, -5.7961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0864,  3.0984, -4.2939],\n",
      "        [-0.6124,  2.5860, -5.0096],\n",
      "        [-1.0615,  3.2774, -4.7094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2943, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8460,  0.9825, -5.6957],\n",
      "        [ 2.8696, -1.1631, -4.2265],\n",
      "        [-0.0653,  2.1901, -5.4184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3633, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3279, -0.3147, -5.1097],\n",
      "        [ 1.1287,  0.4827, -5.7581],\n",
      "        [ 1.2149,  1.0143, -5.8736]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2899, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8299,  2.8539, -4.7219],\n",
      "        [-0.9395,  3.0728, -4.7920],\n",
      "        [ 1.1856,  0.9363, -5.5859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1187, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8373,  0.0686, -5.4453],\n",
      "        [ 0.3134,  2.0321, -5.3050],\n",
      "        [-0.6458,  2.7756, -5.0464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3461, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9378, -1.0794, -4.0504],\n",
      "        [ 2.9048, -1.2719, -3.9645],\n",
      "        [ 1.4395,  0.8938, -5.4956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0265, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0279,  3.0854, -4.7845],\n",
      "        [-1.1872,  3.3793, -4.1667],\n",
      "        [-0.3070,  2.6301, -5.2596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1350, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5605,  1.5782, -5.4806],\n",
      "        [ 2.9058, -1.1492, -3.9937],\n",
      "        [ 2.1801, -0.3438, -4.9994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2031, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9880,  3.3780, -4.5657],\n",
      "        [ 0.8670,  1.0980, -5.7210],\n",
      "        [-1.2568,  3.2562, -4.5639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0569, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8403, -1.3299, -4.0488],\n",
      "        [ 2.1419,  0.0119, -5.5146],\n",
      "        [ 2.4703, -0.7074, -4.5336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1398, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6393,  1.3971, -5.4029],\n",
      "        [-0.8858,  2.8557, -4.8832],\n",
      "        [ 3.0381, -1.7225, -3.1808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9620, -1.7934, -3.6468],\n",
      "        [ 2.5945, -1.0263, -4.7304],\n",
      "        [-1.2202,  3.5088, -4.5957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0195, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9511,  3.3722, -4.1397],\n",
      "        [-0.6899,  2.8734, -4.8258],\n",
      "        [-0.8995,  3.2143, -4.8274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1711, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8429, -1.1168, -4.3633],\n",
      "        [-1.0312,  2.9699, -4.6540],\n",
      "        [ 0.9934,  1.4919, -5.8937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0404, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2481,  2.2704, -5.4189],\n",
      "        [-1.0114,  3.2558, -4.4718],\n",
      "        [-0.6576,  2.8900, -4.6792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0099, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2164,  3.5234, -4.2873],\n",
      "        [ 2.9679, -1.8167, -3.1874],\n",
      "        [-1.2063,  3.4020, -4.4185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3413,  3.4066, -3.7717],\n",
      "        [-1.0937,  3.4442, -4.5748],\n",
      "        [ 2.9907, -1.2849, -4.2027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0957, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5848,  2.7163, -5.1259],\n",
      "        [ 0.4274,  1.7308, -5.6798],\n",
      "        [-1.1402,  3.5277, -4.3121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2834,  3.3773, -4.2058],\n",
      "        [ 2.8456, -1.8854, -3.0061],\n",
      "        [ 2.9300, -1.1661, -4.3749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0188,  3.5532, -3.9162],\n",
      "        [ 3.1276, -1.7189, -3.5216],\n",
      "        [-0.9944,  3.2139, -4.7403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1558, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7074, -2.0833, -2.6902],\n",
      "        [ 1.2129,  0.6331, -5.9309],\n",
      "        [ 2.9609, -2.0724, -2.8237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3986,  3.4716, -4.2951],\n",
      "        [ 2.5604, -0.8344, -4.2041],\n",
      "        [-1.0747,  3.2494, -4.5049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8657, -1.5366, -3.8990],\n",
      "        [-1.4000,  3.4443, -4.1887],\n",
      "        [-1.2160,  3.4866, -4.0056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1578, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3990,  0.8274, -5.8938],\n",
      "        [-1.0172,  3.0462, -4.7703],\n",
      "        [-1.2407,  3.6714, -4.0024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9949, -1.2499, -4.1628],\n",
      "        [ 2.5089, -0.0394, -5.1832],\n",
      "        [-1.4247,  3.5176, -4.3851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8488,  3.1258, -4.9707],\n",
      "        [ 3.0165, -1.7896, -2.9483],\n",
      "        [-1.3985,  3.4529, -4.3799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0356, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4197,  3.8543, -4.0071],\n",
      "        [ 2.1989, -0.1608, -5.6300],\n",
      "        [ 3.1443, -1.4442, -4.0906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3674, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4636,  1.1359, -5.6817],\n",
      "        [-1.3485,  3.8024, -4.2519],\n",
      "        [-1.2316,  3.3320, -4.5892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7212,  3.6794, -4.0487],\n",
      "        [-1.2011,  3.1730, -4.4159],\n",
      "        [-1.4637,  3.9156, -3.7546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9112, -1.4334, -4.1178],\n",
      "        [-1.2684,  3.2060, -4.3646],\n",
      "        [ 2.7185, -1.8492, -2.9439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0648, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5086,  3.6344, -3.5917],\n",
      "        [ 1.7701,  0.1500, -4.9119],\n",
      "        [ 3.1968, -2.1229, -3.3565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0551, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3812,  3.7743, -3.9576],\n",
      "        [ 0.1827,  2.0416, -5.1632],\n",
      "        [ 2.9995, -1.3411, -4.4854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6252,  3.7281, -3.8063],\n",
      "        [ 3.0120, -1.8010, -3.7316],\n",
      "        [ 3.2684, -1.8680, -3.7604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1471, -2.0137, -3.3428],\n",
      "        [-1.5044,  3.7779, -3.9597],\n",
      "        [-1.5253,  3.7396, -3.9091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0355, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2601,  2.0749, -5.2453],\n",
      "        [ 3.2355, -2.0775, -3.0858],\n",
      "        [-1.5793,  3.5118, -3.6559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2789,  3.7478, -4.0671],\n",
      "        [-1.2644,  3.3772, -4.1647],\n",
      "        [-1.4475,  3.6820, -4.2424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1049, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7423,  0.6942, -5.8255],\n",
      "        [ 3.2364, -1.9096, -3.1854],\n",
      "        [-1.3597,  3.7639, -3.9776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5358, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7792,  0.4104, -5.7569],\n",
      "        [-1.4705,  3.9134, -4.1182],\n",
      "        [-1.5638,  3.5689, -3.4623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6629,  3.7310, -3.8028],\n",
      "        [-1.0970,  3.2055, -5.0878],\n",
      "        [ 3.3028, -2.0842, -2.8705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0631, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8678,  3.1158, -5.0195],\n",
      "        [ 1.8675,  0.1433, -5.8042],\n",
      "        [-1.5494,  3.6431, -3.8265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3132,  3.2502, -4.8229],\n",
      "        [ 3.2435, -2.2552, -3.0372],\n",
      "        [-1.1506,  3.4626, -4.4342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9216, -2.1603, -2.2534],\n",
      "        [ 3.0922, -1.9500, -2.8533],\n",
      "        [-1.7065,  3.8467, -3.9865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2004, -2.0191, -2.9720],\n",
      "        [-1.5816,  3.5736, -3.8515],\n",
      "        [ 2.9748, -1.4523, -3.8323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3731, -0.6709, -5.3934],\n",
      "        [ 1.5436,  0.6819, -5.9140],\n",
      "        [ 2.7389, -0.9585, -5.1064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1780, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0052,  3.2583, -4.5878],\n",
      "        [ 1.4944,  0.4232, -5.6959],\n",
      "        [ 0.2726,  1.6547, -5.5592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2663, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7713,  2.9943, -4.8766],\n",
      "        [-1.4741,  3.8352, -3.6485],\n",
      "        [-1.6830,  3.8240, -3.6532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3679, -2.0937, -3.1032],\n",
      "        [-1.7715,  3.8652, -3.6932],\n",
      "        [-1.7150,  3.7318, -3.7571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1115, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2318, -2.1101, -3.3174],\n",
      "        [-1.5492,  3.7261, -3.3194],\n",
      "        [ 1.6260,  0.6575, -5.9124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6987,  3.8477, -3.7959],\n",
      "        [-0.7472,  2.9166, -5.1296],\n",
      "        [-1.4295,  3.8252, -3.8331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1002, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4121,  3.7674, -3.8711],\n",
      "        [-1.5116,  3.4313, -3.8402],\n",
      "        [ 0.5105,  1.6154, -5.3696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5372, -2.4570, -2.7517],\n",
      "        [ 2.8791, -0.9922, -4.8956],\n",
      "        [-1.4349,  3.8215, -3.8347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5723, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7346,  0.4579, -5.6388],\n",
      "        [ 3.3968, -2.1052, -3.3265],\n",
      "        [ 0.3439,  1.9193, -5.7876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3950, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1385, -2.2007, -2.8957],\n",
      "        [ 1.6356,  0.8365, -6.0312],\n",
      "        [ 3.1892, -1.9503, -3.6444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1386, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8483,  1.7514, -5.5520],\n",
      "        [ 3.2587, -2.0015, -3.1416],\n",
      "        [-0.4246,  2.5958, -5.3825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7883, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4166, -2.3397, -3.1341],\n",
      "        [ 3.3212, -2.0266, -4.3204],\n",
      "        [-1.5777,  3.4141, -4.3649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4779, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6027,  1.4830, -5.4033],\n",
      "        [ 1.8581,  0.2078, -5.7796],\n",
      "        [-0.6447,  2.8611, -5.2000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3131, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3918,  1.6890, -5.6042],\n",
      "        [-0.0912,  2.2034, -5.5568],\n",
      "        [ 3.2037, -1.5747, -4.2537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4451, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5029,  3.3744, -4.5186],\n",
      "        [ 0.5031,  1.3315, -5.7200],\n",
      "        [ 1.9755,  0.0446, -5.7284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1694, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9018, -1.3544, -4.4726],\n",
      "        [-0.7995,  2.8746, -4.8978],\n",
      "        [ 0.9232,  1.4404, -5.7290]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1288, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7024,  0.4539, -5.5417],\n",
      "        [ 1.9618, -0.0518, -5.0624],\n",
      "        [ 3.1536, -1.9212, -3.6344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0687, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1600, -1.4441, -4.7387],\n",
      "        [ 2.8242, -1.6342, -3.4959],\n",
      "        [ 1.7340,  0.1238, -5.9805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0633, -1.4208, -4.2316],\n",
      "        [ 3.0446, -1.5619, -4.4686],\n",
      "        [ 2.0551, -0.0923, -5.5537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4964, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2092, -1.3328, -4.3730],\n",
      "        [ 1.5878,  0.4825, -5.9464],\n",
      "        [ 2.3056, -0.1058, -5.3136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0257, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1698, -1.1036, -4.6254],\n",
      "        [ 2.8767, -1.2728, -4.2712],\n",
      "        [-0.3069,  2.7483, -5.2275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0915, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0260,  1.9708, -5.6049],\n",
      "        [ 3.1274, -1.6865, -3.9237],\n",
      "        [ 2.0754,  0.1089, -5.6519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1463, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9902, -1.3464, -4.2212],\n",
      "        [ 1.8378,  0.3526, -5.7993],\n",
      "        [ 1.6355,  0.2343, -5.7287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0930, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3851,  1.6899, -5.8942],\n",
      "        [ 3.1933, -1.5505, -3.8521],\n",
      "        [ 2.6648, -0.8760, -5.2128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0550, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1032, -1.4272, -4.1121],\n",
      "        [ 2.9946, -1.2346, -4.4930],\n",
      "        [ 2.0100,  0.0994, -5.5730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4782, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1583, -1.4048, -4.4319],\n",
      "        [ 1.5614,  0.4270, -6.0016],\n",
      "        [ 3.0892, -1.5682, -4.5853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0219, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5758, -0.5166, -5.0223],\n",
      "        [ 3.1891, -1.4710, -3.8920],\n",
      "        [ 3.1304, -1.4869, -3.8876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7855, -1.3276, -3.6211],\n",
      "        [ 3.0817, -1.4456, -4.2522],\n",
      "        [-0.3611,  2.7727, -5.0141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2295, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7971,  0.4172, -6.0526],\n",
      "        [ 1.8880, -0.0428, -5.2705],\n",
      "        [ 2.8128, -1.2975, -4.5205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1553, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4988,  0.0844, -5.6784],\n",
      "        [ 0.5551,  1.9042, -5.4525],\n",
      "        [ 2.8424, -1.3023, -4.2449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0195, -1.5530, -3.4292],\n",
      "        [-0.4274,  2.8086, -5.2800],\n",
      "        [ 3.1429, -1.7486, -3.9309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0037, -1.5590, -3.9487],\n",
      "        [ 3.0461, -1.1567, -4.6757],\n",
      "        [-0.0481,  2.4754, -5.3407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1347, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7426, -0.7992, -5.0448],\n",
      "        [ 0.5487,  1.6582, -5.4877],\n",
      "        [ 0.0652,  2.4380, -5.2280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0364, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7131, -1.2999, -4.6811],\n",
      "        [ 2.8424, -1.0416, -4.6520],\n",
      "        [ 2.3445, -0.2872, -5.3996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2554, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9365,  0.1925, -5.4009],\n",
      "        [ 0.4460,  1.7723, -5.4308],\n",
      "        [ 0.6237,  1.4336, -5.6701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0377, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3088, -0.5400, -5.2027],\n",
      "        [ 2.9252, -1.4806, -3.8142],\n",
      "        [ 2.5283, -0.6175, -4.5768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4959,  2.8213, -5.1517],\n",
      "        [ 2.9411, -1.5424, -3.8732],\n",
      "        [ 3.1219, -1.4541, -4.1721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6723, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9491, -1.1092, -4.7643],\n",
      "        [ 0.3173,  2.1411, -5.4617],\n",
      "        [-0.6642,  3.0062, -5.1265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5789, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9351,  0.9480, -5.7247],\n",
      "        [ 2.9524, -1.3871, -4.2233],\n",
      "        [ 1.1950,  0.5988, -5.8972]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0729, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3788,  2.7883, -5.0965],\n",
      "        [ 0.1147,  1.9656, -5.6240],\n",
      "        [-0.7693,  2.7153, -4.9055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1993, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8353,  1.2097, -5.4565],\n",
      "        [-0.3712,  2.6742, -5.3478],\n",
      "        [ 2.6647, -0.9568, -4.9019]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0224, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0964,  3.2986, -4.5221],\n",
      "        [ 3.0917, -1.4785, -3.4902],\n",
      "        [ 2.4608, -0.6828, -4.9615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1722, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1141,  2.3234, -5.3567],\n",
      "        [ 2.6853, -1.3849, -4.0756],\n",
      "        [ 0.6647,  1.3336, -5.6132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1672, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7553,  1.2536, -5.6433],\n",
      "        [-0.9027,  3.2363, -4.7926],\n",
      "        [-1.2475,  3.3634, -4.5935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1896, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0939, -1.3194, -4.2162],\n",
      "        [ 1.2808,  0.5176, -5.7622],\n",
      "        [ 0.1264,  1.8003, -4.9710]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1487, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9620, -0.9804, -4.4928],\n",
      "        [ 1.3504,  0.6389, -5.1398],\n",
      "        [ 2.6407, -1.0374, -4.3440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7572, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9751, -1.5061, -3.5978],\n",
      "        [-0.0224,  2.1051, -5.5539],\n",
      "        [-0.8051,  3.1846, -5.1337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1957, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8216, -1.4199, -3.3067],\n",
      "        [ 2.9056, -1.7838, -2.8108],\n",
      "        [ 1.0931,  0.8007, -5.5492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9500, -1.5645, -3.7944],\n",
      "        [ 2.7608, -1.8388, -2.7621],\n",
      "        [ 2.8841, -1.5710, -3.3946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7815, -1.6763, -3.6554],\n",
      "        [ 2.9272, -0.8838, -4.6621],\n",
      "        [-1.1141,  3.4204, -4.3689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0192, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4807, -0.7734, -4.6599],\n",
      "        [ 3.1339, -1.6491, -3.2013],\n",
      "        [ 3.1054, -1.7803, -3.5586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1684, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4448, -0.6713, -4.8403],\n",
      "        [ 0.4151,  1.5828, -5.6178],\n",
      "        [ 1.7495,  0.1816, -5.7118]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0773, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2077,  2.2948, -5.6798],\n",
      "        [-0.3789,  2.2203, -5.5053],\n",
      "        [-0.5514,  2.5944, -5.0459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0605, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0073,  1.9477, -5.6613],\n",
      "        [ 3.1456, -1.7437, -3.4672],\n",
      "        [-0.6001,  2.6633, -4.8398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0324, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0653, -0.5211, -4.3260],\n",
      "        [ 2.8681, -1.6137, -3.4495],\n",
      "        [ 2.9686, -1.8260, -3.3108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0738, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3579,  2.6546, -5.3443],\n",
      "        [ 1.8690,  0.1284, -5.2489],\n",
      "        [ 2.9993, -1.7125, -3.3545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1439, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9758, -1.4699, -4.0358],\n",
      "        [ 3.3362, -1.7028, -3.7327],\n",
      "        [ 0.8355,  1.5126, -5.0800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2180, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1462,  3.5470, -4.4132],\n",
      "        [ 0.3482,  1.4545, -5.5488],\n",
      "        [ 0.5836,  1.4287, -5.6019]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0891, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4650,  1.7266, -5.9862],\n",
      "        [ 2.8782, -1.9720, -3.4477],\n",
      "        [ 3.2084, -1.7270, -3.8046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0302, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7412,  2.9679, -4.9289],\n",
      "        [-0.2271,  2.5793, -5.2109],\n",
      "        [ 3.2912, -1.8096, -3.6407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1221, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5860,  1.6983, -5.7606],\n",
      "        [ 3.1763, -1.9642, -3.4558],\n",
      "        [-0.1807,  2.3849, -5.4597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1343, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0553,  3.3982, -4.4746],\n",
      "        [ 2.9925, -1.3025, -4.3278],\n",
      "        [ 0.7746,  1.5577, -5.6270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0952, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0433, -1.9758, -3.4074],\n",
      "        [ 0.4389,  1.6032, -5.5550],\n",
      "        [ 3.3472, -2.1662, -3.2859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0897, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3746,  3.3433, -4.0187],\n",
      "        [ 1.5135,  0.2083, -5.4786],\n",
      "        [ 2.8966, -1.0985, -4.4646]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0127, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7982, -1.1370, -3.9630],\n",
      "        [-1.4183,  3.8124, -4.0178],\n",
      "        [-1.0776,  3.3823, -4.3210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0832, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7190,  3.6689, -3.7003],\n",
      "        [ 3.0684, -1.5845, -4.4226],\n",
      "        [ 0.3531,  1.6884, -5.3414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3251, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0141,  3.6292, -3.9767],\n",
      "        [-0.7118,  2.9335, -5.1112],\n",
      "        [ 1.1927,  0.7507, -5.3560]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1281, -1.4671, -3.9091],\n",
      "        [-1.5301,  3.7764, -3.5321],\n",
      "        [-1.0358,  3.4383, -4.7481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1035,  3.4934, -4.1099],\n",
      "        [-0.7702,  2.5470, -5.2985],\n",
      "        [-1.6882,  3.9132, -3.7076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6865,  3.7865, -4.1168],\n",
      "        [ 3.1984, -1.8534, -3.4795],\n",
      "        [-1.6030,  3.8046, -4.1202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4749,  3.8064, -4.0502],\n",
      "        [-1.5879,  3.8689, -3.6158],\n",
      "        [-1.2842,  3.5685, -4.3742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0065, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4879,  3.7386, -4.2306],\n",
      "        [-1.4546,  3.6260, -3.9961],\n",
      "        [ 3.2369, -1.9605, -3.2887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1367, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5730,  0.8449, -5.6296],\n",
      "        [ 3.3468, -1.8272, -3.9245],\n",
      "        [ 3.0679, -1.9056, -2.9908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0171, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2529,  3.5684, -4.5242],\n",
      "        [-0.6929,  2.5966, -5.6127],\n",
      "        [ 3.3451, -2.2065, -2.7642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1848, -2.1834, -3.0816],\n",
      "        [-1.5444,  3.8129, -3.7345],\n",
      "        [ 0.4506,  1.7250, -5.8123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0446, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0783,  2.1319, -5.2762],\n",
      "        [ 3.1500, -1.9521, -3.6221],\n",
      "        [-1.5002,  3.8017, -3.9694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5238,  3.8800, -3.3551],\n",
      "        [ 3.2664, -2.1852, -3.3954],\n",
      "        [ 3.3207, -2.1975, -3.1953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6185, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2046, -1.5934, -3.9798],\n",
      "        [-0.5399,  2.5840, -5.5847],\n",
      "        [ 3.4568, -2.0710, -3.2967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4555,  3.7156, -3.4659],\n",
      "        [ 3.0341, -2.1404, -2.8455],\n",
      "        [ 2.8321, -1.2206, -3.7450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0170, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0610, -2.0234, -3.4679],\n",
      "        [ 2.3626, -0.9555, -3.4037],\n",
      "        [-1.7870,  3.7333, -3.2446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6162,  4.0324, -3.3427],\n",
      "        [ 2.8725, -1.1897, -4.3072],\n",
      "        [ 3.0362, -1.7201, -3.1949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1621, -1.7362, -3.5010],\n",
      "        [ 3.1435, -1.7272, -3.3513],\n",
      "        [ 3.1933, -1.7728, -3.5662]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9384, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3266,  3.7806, -4.1136],\n",
      "        [ 2.5750, -0.1575, -4.6911],\n",
      "        [ 2.8737, -1.7848, -2.8500]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0157, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7627,  3.1174, -5.1947],\n",
      "        [-1.0859,  3.1612, -4.9248],\n",
      "        [ 2.7474, -1.9038, -3.3133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0052, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6946,  3.8813, -3.0088],\n",
      "        [-1.6190,  3.7690, -3.7620],\n",
      "        [-1.5097,  3.7705, -3.5544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6142, -1.3286, -3.4778],\n",
      "        [-1.6521,  3.8161, -4.1575],\n",
      "        [ 2.7855, -1.4239, -2.9944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0440, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8988,  3.9092, -3.2025],\n",
      "        [ 2.3574, -0.2740, -4.1097],\n",
      "        [ 2.3631, -0.4936, -4.1524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0227, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5140, -0.3357, -4.5492],\n",
      "        [-1.6448,  3.5697, -3.8520],\n",
      "        [-1.6664,  3.6840, -4.0602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9812, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3216,  1.5886, -5.6154],\n",
      "        [ 0.7037,  1.6691, -5.6401],\n",
      "        [ 1.9674,  0.0617, -5.1316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1589, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5813, -1.0602, -3.6299],\n",
      "        [-1.4159,  3.9138, -4.1560],\n",
      "        [ 0.8773,  1.4610, -5.6979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7123,  3.7702, -3.3602],\n",
      "        [-1.3888,  3.5897, -4.4087],\n",
      "        [-0.3431,  2.5916, -5.2901]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5203,  3.6878, -3.9377],\n",
      "        [-1.4716,  3.8294, -3.5257],\n",
      "        [-1.4707,  3.8265, -4.0742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1367, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9823, -1.4197, -3.8699],\n",
      "        [-0.7885,  2.5344, -5.2863],\n",
      "        [ 0.7799,  1.6143, -5.7473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0401, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2215, -0.2570, -4.8353],\n",
      "        [-0.4687,  2.8819, -5.2269],\n",
      "        [-1.6717,  3.9492, -3.2971]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0637, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0979,  2.0634, -5.7604],\n",
      "        [ 2.5456, -0.4463, -4.5112],\n",
      "        [-0.5804,  2.8518, -5.3264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4638, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3271,  3.4034, -3.4935],\n",
      "        [-1.6825,  3.6654, -3.7320],\n",
      "        [ 0.3774,  1.4618, -5.5776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0896, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7579,  0.1556, -5.6475],\n",
      "        [ 3.1770, -1.1940, -4.1358],\n",
      "        [ 2.4004, -0.2116, -4.6119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0161, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8036, -1.0976, -4.6132],\n",
      "        [ 2.8833, -0.9863, -4.4013],\n",
      "        [-1.3588,  3.7437, -4.0085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0928, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5244,  1.8179, -5.5709],\n",
      "        [ 2.9595, -1.0058, -4.3503],\n",
      "        [ 3.1527, -1.0020, -4.4411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1121, -1.4267, -4.3107],\n",
      "        [-1.6630,  3.7663, -3.4670],\n",
      "        [-1.2509,  3.7868, -4.5966]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2650, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7752, -0.8924, -4.7204],\n",
      "        [ 0.9716,  1.0706, -6.1434],\n",
      "        [ 2.8725, -0.8206, -4.9803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1506, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7840,  3.8030, -3.3379],\n",
      "        [ 1.3171,  0.6883, -5.4575],\n",
      "        [ 2.8393, -1.1581, -4.2339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5708,  3.6426, -3.8452],\n",
      "        [-1.2316,  3.7526, -4.1605],\n",
      "        [ 3.2688, -1.1900, -4.5311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3227, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9348, -1.1164, -4.4156],\n",
      "        [-0.6828,  3.0069, -5.1208],\n",
      "        [ 1.3263,  0.9066, -5.9833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2692,  0.0645, -5.6123],\n",
      "        [ 2.6584, -1.2215, -4.6277],\n",
      "        [-1.4854,  3.9498, -3.6873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1248, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4467,  1.3766, -5.7388],\n",
      "        [ 3.0142, -1.2591, -4.1056],\n",
      "        [ 2.9243, -0.7057, -5.1370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4775,  3.5211, -4.7665],\n",
      "        [-1.6640,  3.9167, -3.5149],\n",
      "        [-1.5355,  3.6677, -3.3873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5611, -0.3531, -5.4127],\n",
      "        [-1.7167,  3.6343, -3.3181],\n",
      "        [-0.5554,  2.6678, -5.2792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0316, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4083,  2.5989, -5.1529],\n",
      "        [ 3.0310, -1.2788, -4.3670],\n",
      "        [ 2.7061, -1.0072, -4.4537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1231,  3.3876, -4.6819],\n",
      "        [ 2.7548, -0.5833, -5.3623],\n",
      "        [-1.4407,  3.9341, -3.4881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6317,  3.7308, -3.4292],\n",
      "        [-1.5704,  3.5798, -3.4606],\n",
      "        [ 3.3076, -1.3566, -4.2820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0946, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2586, -1.2417, -4.2924],\n",
      "        [ 0.6181,  1.8166, -5.4560],\n",
      "        [ 3.3286, -1.5305, -4.5296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5711, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9130, -1.5846, -4.2960],\n",
      "        [ 1.8693,  0.3804, -5.8820],\n",
      "        [ 3.1916, -1.6307, -4.1667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1702, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4844, -0.6832, -5.2134],\n",
      "        [ 3.4001, -1.4523, -4.2277],\n",
      "        [ 1.0645,  1.6019, -5.3965]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0107, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0409, -1.3508, -4.0942],\n",
      "        [ 3.0937, -1.2027, -4.7012],\n",
      "        [-1.6065,  3.8020, -3.6975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2321, -1.4694, -4.4147],\n",
      "        [-1.5639,  3.6109, -3.6820],\n",
      "        [ 2.8535, -1.3575, -4.2948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0448, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9278e+00, -1.4191e+00, -3.6996e+00],\n",
      "        [ 2.1501e+00, -4.1001e-03, -5.9961e+00],\n",
      "        [ 3.1635e+00, -1.4917e+00, -4.1642e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7135, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1503,  0.1523, -5.5900],\n",
      "        [ 3.2193, -1.5253, -4.1938],\n",
      "        [-1.5413,  3.7700, -3.4214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0597, -1.3428, -4.5966],\n",
      "        [ 3.0647, -1.4444, -4.3718],\n",
      "        [-1.7060,  3.6710, -3.7702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2866, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0820,  1.3325, -5.6104],\n",
      "        [ 3.1921, -1.2129, -4.3859],\n",
      "        [-0.9405,  2.9598, -5.2568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1010, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3834,  3.4999, -4.5861],\n",
      "        [ 3.0157, -1.3170, -4.4947],\n",
      "        [ 0.3190,  1.4452, -5.6225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0312, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5295,  1.9908, -5.6131],\n",
      "        [-1.3771,  3.6073, -4.1515],\n",
      "        [-1.2443,  3.5685, -4.3727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7333,  3.6147, -3.7401],\n",
      "        [ 2.8862, -0.7655, -4.9620],\n",
      "        [-1.8309,  3.9026, -3.3934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0303, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5985,  4.0163, -3.6394],\n",
      "        [ 2.3207, -0.1548, -5.5091],\n",
      "        [-1.6175,  3.6319, -3.9777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2210,  3.2660, -4.6009],\n",
      "        [-0.4910,  2.7242, -5.3737],\n",
      "        [-1.5562,  3.9070, -3.7442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5376,  3.9274, -3.3949],\n",
      "        [-1.6876,  3.8965, -3.6834],\n",
      "        [-1.4654,  3.7183, -3.7884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8344, -0.9635, -5.1992],\n",
      "        [-1.6236,  3.8160, -4.1959],\n",
      "        [-1.5567,  4.1436, -3.8651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0470, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2034,  2.2629, -5.4353],\n",
      "        [-1.7262,  3.8645, -3.8632],\n",
      "        [ 2.9913, -1.1363, -4.9632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5852,  4.0066, -3.8877],\n",
      "        [ 2.8757, -0.8050, -5.2143],\n",
      "        [-1.0730,  3.4674, -5.0232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3613, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4537,  2.0468, -5.6739],\n",
      "        [ 0.9394,  1.4794, -6.1689],\n",
      "        [-1.6505,  3.7399, -3.7732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0066, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7164,  3.6001, -3.7759],\n",
      "        [ 1.0710,  1.0698, -5.7648],\n",
      "        [-1.5556,  3.9413, -3.5931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0159, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5362,  3.4023, -4.5289],\n",
      "        [-1.5693,  3.9573, -3.9711],\n",
      "        [ 2.7458, -0.5736, -4.9745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0724, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1238,  0.5136, -5.7178],\n",
      "        [ 2.7067, -0.9347, -4.4760],\n",
      "        [-1.3524,  3.5189, -4.4895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0598, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9664, -1.1693, -4.3436],\n",
      "        [ 2.0199,  0.2124, -5.7056],\n",
      "        [ 3.0886, -1.5141, -4.4094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0440, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4249,  3.3859, -4.3830],\n",
      "        [ 3.0072, -1.2370, -4.6372],\n",
      "        [-0.0605,  2.1088, -5.3867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1203,  3.6880, -4.5228],\n",
      "        [-1.2822,  3.5000, -4.4302],\n",
      "        [ 3.0959, -1.3298, -3.8935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6273, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1375,  3.2553, -4.3333],\n",
      "        [ 0.2853,  1.8706, -5.9384],\n",
      "        [-0.0156,  2.2693, -5.6213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0128, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2463,  3.3776, -4.2635],\n",
      "        [ 2.9326, -1.1160, -4.4639],\n",
      "        [-1.2650,  3.3653, -4.3682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0552, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1565, -1.5516, -4.5488],\n",
      "        [ 0.0718,  2.0580, -5.7049],\n",
      "        [-0.7845,  2.8264, -4.7921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1744, -1.3633, -4.2199],\n",
      "        [ 3.2258, -1.5552, -4.0296],\n",
      "        [ 3.2014, -1.5804, -3.8259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5363, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0722,  1.9460, -5.6960],\n",
      "        [-0.6918,  2.7824, -4.8378],\n",
      "        [ 0.5348,  1.6967, -5.7025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0968, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2727,  1.5851, -5.9817],\n",
      "        [-0.6746,  2.4957, -5.2641],\n",
      "        [ 2.9775, -1.7214, -3.8688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2354, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8929,  1.1606, -6.0629],\n",
      "        [-1.0174,  3.1373, -4.6050],\n",
      "        [ 0.2035,  2.2513, -5.6670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1574, -1.5037, -4.1488],\n",
      "        [ 3.3210, -1.6486, -3.9848],\n",
      "        [ 3.1799, -1.6964, -4.2089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2328, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9468,  3.2327, -4.6516],\n",
      "        [ 0.1665,  1.9822, -5.7738],\n",
      "        [ 2.7868, -0.7148, -4.9381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0546, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1081, -1.6513, -4.2736],\n",
      "        [ 0.0643,  1.9135, -5.4185],\n",
      "        [ 3.3204, -1.5890, -4.3620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8564,  3.1020, -5.0384],\n",
      "        [ 3.3948, -1.6213, -4.5888],\n",
      "        [-0.8909,  3.2927, -4.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0412, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6361,  2.6615, -5.0442],\n",
      "        [-0.0832,  2.4204, -5.6864],\n",
      "        [ 3.2243, -1.7405, -3.7135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3284, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2665,  0.8055, -6.2073],\n",
      "        [-0.6872,  3.0182, -4.7545],\n",
      "        [ 3.1343, -1.4921, -4.5079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0196, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6638,  2.9624, -5.0182],\n",
      "        [-0.7514,  3.0014, -4.7923],\n",
      "        [ 3.2916, -1.5025, -4.4135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1079, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3072, -1.4492, -4.6841],\n",
      "        [ 2.9128, -1.4855, -4.8369],\n",
      "        [-0.6379,  2.6263, -5.1972]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2348, -1.3241, -4.6410],\n",
      "        [-0.8956,  3.4801, -4.4420],\n",
      "        [ 2.1044, -0.1171, -5.6897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2153, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1689,  0.9322, -5.9225],\n",
      "        [ 2.6812, -0.3711, -5.2149],\n",
      "        [-0.9726,  3.1229, -4.4732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0487, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9195, -1.2815, -4.4619],\n",
      "        [-0.9988,  3.3170, -4.4095],\n",
      "        [ 2.1818,  0.0906, -5.5732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0571, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6542,  2.7523, -5.4431],\n",
      "        [ 1.9884, -0.0373, -5.6721],\n",
      "        [-0.9973,  3.2828, -4.3886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1009, -0.8568, -4.8836],\n",
      "        [ 2.7985, -0.6969, -5.2018],\n",
      "        [ 2.9179, -1.2772, -4.9509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3642, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0351,  0.0800, -5.8728],\n",
      "        [ 0.8509,  1.2770, -5.8323],\n",
      "        [-0.7051,  2.7911, -4.4161]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6505, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9671, -0.9347, -5.1761],\n",
      "        [ 0.3326,  2.0645, -5.4816],\n",
      "        [-0.6452,  2.6791, -4.9366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1015, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0530,  2.0178, -5.3918],\n",
      "        [ 2.9930, -0.9516, -4.6484],\n",
      "        [ 0.4414,  2.1587, -5.4076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0975, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9544,  0.1795, -5.8498],\n",
      "        [-0.4131,  2.4479, -5.2005],\n",
      "        [-0.1601,  2.3395, -5.1867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0960, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8920,  3.1307, -4.8170],\n",
      "        [ 3.1542, -1.0934, -4.7035],\n",
      "        [ 0.0425,  2.1614, -5.5150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0251, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4119, -0.6587, -5.3238],\n",
      "        [ 3.2867, -1.1558, -4.5756],\n",
      "        [ 3.1189, -0.9306, -4.9364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0477, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1831,  2.3398, -5.2842],\n",
      "        [ 2.8733, -0.9041, -5.0753],\n",
      "        [ 3.1724, -1.4437, -4.9908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4529, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2621,  0.5469, -5.7148],\n",
      "        [ 3.2537, -1.2752, -4.8170],\n",
      "        [ 1.3335,  0.8754, -5.4833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1393, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1031, -1.3570, -4.5628],\n",
      "        [ 0.5379,  1.2736, -5.6329],\n",
      "        [ 3.1388, -1.1650, -4.8958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0809, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1099, -0.9611, -4.8481],\n",
      "        [ 3.0465, -1.1049, -4.5811],\n",
      "        [ 0.3386,  1.7983, -5.7115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2525, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8488,  1.4876, -5.7357],\n",
      "        [ 3.0160, -0.8927, -5.5230],\n",
      "        [ 0.6844,  1.6868, -5.5028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0749, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0229,  2.5791, -5.1789],\n",
      "        [ 0.1899,  2.0957, -5.4934],\n",
      "        [ 3.1017, -1.5052, -4.5747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0353, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1182, -0.3153, -5.4183],\n",
      "        [ 3.0705, -1.3135, -4.8414],\n",
      "        [ 3.4474, -1.3595, -4.7641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1275, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2454, -0.2911, -5.3034],\n",
      "        [-0.1109,  2.4542, -5.7620],\n",
      "        [ 0.4206,  1.7685, -5.5617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1847, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2290, -1.4426, -4.5983],\n",
      "        [ 0.0396,  2.0138, -5.2379],\n",
      "        [ 0.7882,  1.4584, -5.7351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0603, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0009, -1.6641, -4.3638],\n",
      "        [ 0.2257,  2.0500, -5.2167],\n",
      "        [ 2.9322, -0.9523, -4.7697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2553, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1477, -1.4702, -4.1518],\n",
      "        [ 1.0858,  0.9882, -5.5621],\n",
      "        [ 2.9641, -1.5491, -4.1821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0407, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0884, -1.4117, -4.4070],\n",
      "        [ 2.1939, -0.0525, -5.7468],\n",
      "        [ 3.1363, -1.5875, -4.0773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3970, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6692,  1.6668, -5.3902],\n",
      "        [ 3.1994, -1.4912, -4.0854],\n",
      "        [ 1.0767,  0.7564, -5.7615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1395, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2137,  1.5927, -5.6800],\n",
      "        [ 0.0599,  1.8649, -5.3729],\n",
      "        [ 2.6736, -0.5202, -5.6696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3099, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1804,  2.4993, -5.3872],\n",
      "        [ 2.3909, -0.3163, -5.3384],\n",
      "        [ 1.1019,  0.9026, -6.1983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1468, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1517,  1.9882, -5.5073],\n",
      "        [ 0.4855,  1.6112, -5.6306],\n",
      "        [ 2.9060, -1.7116, -4.3061]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0425, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1028, -1.7431, -3.9186],\n",
      "        [-0.0497,  2.3635, -5.3549],\n",
      "        [-0.6430,  2.7793, -5.0603]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5722, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2400, -1.3602, -4.1677],\n",
      "        [ 0.2962,  1.7726, -5.7045],\n",
      "        [ 2.9516, -0.8070, -5.0685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3634, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2589, -1.6462, -4.6290],\n",
      "        [ 1.4036,  0.7822, -5.9461],\n",
      "        [-0.5063,  2.9685, -5.3405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0582, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0483,  2.4647, -5.3598],\n",
      "        [-0.6926,  2.4651, -5.0282],\n",
      "        [-0.4190,  2.4822, -5.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7924, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1050,  2.2534, -5.5691],\n",
      "        [-0.0749,  2.2596, -5.5745],\n",
      "        [ 2.8069, -0.8740, -5.0458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4709,  2.7318, -5.3834],\n",
      "        [-0.2280,  2.6885, -5.2636],\n",
      "        [ 3.0463, -1.3890, -4.5608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0295, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5048,  2.7110, -4.9393],\n",
      "        [-0.7120,  2.4871, -5.0629],\n",
      "        [ 3.1700, -1.7690, -3.7929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0400, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1655,  2.7072, -5.4429],\n",
      "        [-0.3487,  2.4960, -5.2567],\n",
      "        [ 3.1701, -1.8156, -3.8855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0272, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1177, -1.6731, -4.1878],\n",
      "        [ 2.3824, -0.4191, -5.4265],\n",
      "        [ 3.0964, -1.2435, -4.6141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0500, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6133, -0.3939, -5.4393],\n",
      "        [ 2.1858, -0.2540, -5.5641],\n",
      "        [ 2.9703, -1.0994, -4.7870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0961, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0734,  2.1676, -5.3378],\n",
      "        [-0.2384,  2.6305, -5.7630],\n",
      "        [ 0.1004,  2.1978, -5.5153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9294, -0.8589, -4.8641],\n",
      "        [ 3.2081, -1.6008, -3.7118],\n",
      "        [ 3.4515, -1.6347, -4.0490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6270,  2.7821, -4.9862],\n",
      "        [-0.4091,  2.8026, -5.3187],\n",
      "        [-0.5650,  2.8249, -4.9482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1740, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3909, -1.7857, -3.9666],\n",
      "        [-0.6531,  2.7074, -5.2691],\n",
      "        [ 1.0493,  0.5665, -5.7768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0569, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2284,  2.6371, -5.4911],\n",
      "        [-0.2190,  1.9838, -5.4651],\n",
      "        [ 3.1554, -1.5300, -4.2331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1502, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8079,  2.9322, -5.1098],\n",
      "        [ 3.0759, -1.9053, -3.5758],\n",
      "        [ 0.6377,  1.2928, -6.0359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4503, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2067, -1.6412, -3.7554],\n",
      "        [ 3.1036, -1.1937, -4.9008],\n",
      "        [-0.5715,  2.8901, -5.2810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9691, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2683,  2.4613, -5.3511],\n",
      "        [ 0.6875,  1.4680, -6.1911],\n",
      "        [ 0.3303,  1.8104, -5.5609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0919, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6691,  0.4277, -5.6012],\n",
      "        [ 3.1383, -1.7542, -4.0282],\n",
      "        [ 2.9626, -1.4280, -4.4402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8701,  2.8309, -4.9257],\n",
      "        [ 3.1895, -1.4926, -3.6199],\n",
      "        [ 2.9663, -1.3230, -4.3802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6596, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4151,  2.7065, -5.4121],\n",
      "        [-0.8020,  3.0583, -5.1393],\n",
      "        [ 3.4252, -1.4812, -3.9252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0837, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1446,  2.0187, -5.5659],\n",
      "        [-0.1979,  2.1586, -5.5095],\n",
      "        [ 2.9390, -1.1586, -4.6048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6846, -0.6248, -4.9046],\n",
      "        [ 2.9513, -0.9200, -4.7314],\n",
      "        [-0.3519,  2.6818, -5.2596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0870, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2744,  2.1199, -5.3457],\n",
      "        [-0.2121,  1.7737, -5.5203],\n",
      "        [ 2.5637, -0.5605, -4.5558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3719, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7241,  1.3348, -5.7924],\n",
      "        [-0.4388,  2.7311, -5.0949],\n",
      "        [-0.6513,  2.8829, -4.9650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7436,  2.7579, -4.8911],\n",
      "        [-1.0859,  3.1560, -4.9482],\n",
      "        [ 2.3860, -0.3939, -5.1919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0632, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1286,  0.2482, -5.8649],\n",
      "        [ 2.7869, -0.9204, -4.3364],\n",
      "        [-0.7017,  3.1011, -5.0047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1673, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0735,  1.5100, -5.4673],\n",
      "        [ 1.7558,  0.3544, -6.1086],\n",
      "        [ 2.5474, -0.1236, -5.1105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0416, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6094,  2.5971, -5.3971],\n",
      "        [ 2.9189, -0.7660, -4.6422],\n",
      "        [ 2.3448, -0.4588, -4.9704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0418, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9336, -0.7031, -5.1137],\n",
      "        [ 2.7758, -0.8968, -4.7641],\n",
      "        [ 2.2893, -0.2962, -4.9313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3072, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0833,  1.1175, -6.0746],\n",
      "        [ 2.9359, -0.7450, -4.7133],\n",
      "        [ 0.3949,  1.8003, -5.7725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4855, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4401,  0.3137, -5.7075],\n",
      "        [-0.6353,  3.1882, -5.1904],\n",
      "        [ 2.8989, -0.7208, -4.7334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0748, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1111,  1.9774, -5.7769],\n",
      "        [-0.3958,  2.5314, -5.3632],\n",
      "        [-0.7190,  2.8707, -5.1033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2364, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0527,  0.0737, -5.6112],\n",
      "        [-0.7494,  2.6653, -4.9561],\n",
      "        [ 1.1236,  0.8045, -6.2160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1015, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4815,  1.7562, -5.7036],\n",
      "        [-0.8331,  3.0593, -4.8380],\n",
      "        [ 2.7103, -0.5830, -4.9829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0365, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9314, -0.6853, -5.0585],\n",
      "        [ 2.9649, -0.6876, -4.9916],\n",
      "        [ 2.4308, -0.4230, -4.8354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9412, -0.7336, -4.6088],\n",
      "        [ 2.8636, -0.6301, -5.1068],\n",
      "        [ 2.5750, -0.4292, -5.1710]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0235, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9430,  2.7636, -5.0471],\n",
      "        [-0.9511,  3.0891, -4.7975],\n",
      "        [-0.7348,  2.8434, -5.1080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0322, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9694, -0.5171, -5.1344],\n",
      "        [ 2.6219, -0.6765, -4.6004],\n",
      "        [ 2.7793, -0.7596, -4.7256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9099, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7250, -0.5757, -5.1651],\n",
      "        [ 2.4935, -0.0860, -5.0605],\n",
      "        [ 2.7859, -0.4201, -4.8026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0520, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2686,  0.0285, -5.3473],\n",
      "        [-1.0909,  2.9626, -4.7421],\n",
      "        [ 2.6011, -0.7011, -4.8759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0614, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9476,  3.3356, -4.6947],\n",
      "        [-0.3404,  2.4296, -5.4042],\n",
      "        [-0.1271,  2.0429, -4.9753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1755, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8402, -0.3315, -4.8268],\n",
      "        [ 1.2052,  0.6548, -5.4898],\n",
      "        [-0.6706,  2.8762, -5.2157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0594, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9201,  3.3029, -4.7544],\n",
      "        [ 2.3036,  0.0577, -5.3605],\n",
      "        [ 2.5484, -0.2059, -4.6772]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0420, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8536, -0.9134, -4.9843],\n",
      "        [-0.2624,  2.4871, -5.3742],\n",
      "        [ 2.7274, -0.4716, -5.0178]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1240, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2189,  1.6089, -5.7903],\n",
      "        [ 2.4837, -0.1420, -5.2689],\n",
      "        [ 2.3178, -0.1881, -5.2721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6591, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3407, -0.3529, -4.9046],\n",
      "        [ 2.4064, -0.2940, -4.6535],\n",
      "        [ 0.0141,  1.6867, -5.6641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0479, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0908, -0.0563, -4.7696],\n",
      "        [-0.8352,  3.2207, -5.2027],\n",
      "        [-1.0817,  3.1264, -5.1544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0833, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0492,  1.9713, -5.9511],\n",
      "        [ 2.3551, -0.2312, -5.1495],\n",
      "        [ 2.5912, -0.6215, -4.7689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3292, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6153, -0.6071, -5.1454],\n",
      "        [ 0.6763,  1.0783, -6.1637],\n",
      "        [ 2.7278, -0.6643, -4.7693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3541,  3.4561, -4.6427],\n",
      "        [-1.0398,  3.5182, -4.7844],\n",
      "        [ 3.0919, -0.9797, -4.7865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1094, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9464, -0.8759, -4.7443],\n",
      "        [ 1.3050,  0.1832, -5.0778],\n",
      "        [-0.9371,  2.8404, -5.0779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0307,  3.4635, -4.6315],\n",
      "        [-0.9083,  3.2837, -4.5942],\n",
      "        [-0.7671,  3.1962, -4.8374]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0740, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8474, -0.9167, -5.0052],\n",
      "        [ 2.6084, -0.2026, -5.3674],\n",
      "        [ 2.1342,  0.2358, -5.6052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0608, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3278,  2.3310, -5.1379],\n",
      "        [ 2.2752, -0.3630, -5.5581],\n",
      "        [ 2.4709, -0.6230, -5.1242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0692,  3.2196, -4.5288],\n",
      "        [ 2.9013, -0.8488, -4.9535],\n",
      "        [ 2.8258, -0.8936, -4.4409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4355, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5368, -0.2642, -5.7517],\n",
      "        [ 3.0194, -0.8349, -4.8995],\n",
      "        [ 1.5610,  0.6832, -6.0147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0741, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9739,  3.3123, -4.6568],\n",
      "        [ 0.3988,  1.9805, -5.7080],\n",
      "        [ 2.7534, -1.1337, -4.4201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0207, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7219, -0.8966, -4.2304],\n",
      "        [-0.9929,  3.4723, -4.1780],\n",
      "        [ 2.8750, -0.9297, -4.2286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0709, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8137, -1.1304, -4.7842],\n",
      "        [ 1.8754,  0.1439, -5.7711],\n",
      "        [-0.5881,  2.9321, -4.8851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8722, -0.8613, -5.2500],\n",
      "        [ 2.8940, -1.3282, -4.4623],\n",
      "        [ 3.0048, -1.0596, -4.6895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9499, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1075, -1.0639, -4.7879],\n",
      "        [ 2.2869, -0.4624, -5.5051],\n",
      "        [ 2.7312, -1.0889, -4.3077]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0315, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0902, -1.1365, -4.6562],\n",
      "        [ 2.4483, -0.3859, -5.3635],\n",
      "        [ 2.9406, -0.8885, -4.5345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5420, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0947, -0.9849, -4.5591],\n",
      "        [ 1.6156,  0.2457, -5.8979],\n",
      "        [-1.2033,  3.2533, -4.6955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0391, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3443,  2.3434, -5.7702],\n",
      "        [-1.1354,  3.7436, -4.7771],\n",
      "        [ 2.4364, -0.6882, -5.0217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9332, -1.4823, -4.4291],\n",
      "        [-1.0895,  3.4829, -4.4776],\n",
      "        [ 3.1946, -1.2878, -4.5661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8707, -1.3045, -4.8029],\n",
      "        [-0.8688,  3.0887, -5.3725],\n",
      "        [-1.2774,  3.4803, -4.3304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1236, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1134, -1.0637, -4.5894],\n",
      "        [-1.1799,  3.2398, -4.4424],\n",
      "        [ 0.6235,  1.5197, -6.1674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6204,  2.8589, -5.5199],\n",
      "        [-1.1357,  3.5480, -4.4821],\n",
      "        [ 2.8670, -1.2219, -4.8773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4387, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0837,  3.7336, -4.8375],\n",
      "        [ 1.5019,  0.5240, -5.9898],\n",
      "        [-1.2123,  3.4001, -4.5222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7147, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1664,  2.1547, -5.7601],\n",
      "        [-1.4577,  3.5734, -4.1550],\n",
      "        [ 2.9999, -0.9250, -4.4498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9365,  3.5961, -4.7375],\n",
      "        [ 2.7758, -1.2237, -4.3603],\n",
      "        [ 2.9096, -1.2237, -4.4397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0292, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2560,  2.6524, -5.6105],\n",
      "        [ 2.8714, -0.8738, -5.0093],\n",
      "        [-1.1784,  3.3979, -4.9924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0586, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8679,  2.8449, -5.4124],\n",
      "        [-1.0192,  3.4543, -4.9880],\n",
      "        [ 2.1080,  0.2098, -5.7838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0296, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9678, -1.1637, -4.8852],\n",
      "        [ 2.6525, -0.2720, -5.0394],\n",
      "        [ 2.9018, -1.0366, -4.5552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3242, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9960, -1.1473, -4.5797],\n",
      "        [-1.0155,  3.3067, -4.9066],\n",
      "        [ 1.3075,  0.8587, -5.8983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0182, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8933, -1.1796, -4.3370],\n",
      "        [ 2.9601, -1.0226, -4.3517],\n",
      "        [ 2.8945, -1.1535, -4.6664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0269, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9226, -0.6656, -4.4627],\n",
      "        [ 3.0865, -1.0114, -4.9018],\n",
      "        [-0.8361,  2.4841, -5.0820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1119, -1.0444, -4.7107],\n",
      "        [ 3.1842, -0.6543, -4.8990],\n",
      "        [-0.5859,  2.7592, -5.6671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1229, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2899,  3.4448, -4.6291],\n",
      "        [ 1.6208,  0.7312, -5.9835],\n",
      "        [-1.0941,  3.1184, -4.8749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0226, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9989, -0.9161, -4.6643],\n",
      "        [-0.5337,  2.6819, -5.4660],\n",
      "        [-1.2891,  3.6182, -3.9289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0157, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1151,  3.4023, -4.0378],\n",
      "        [ 2.9796, -0.9770, -4.4227],\n",
      "        [ 2.9377, -1.2424, -4.0202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0679, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1589, -1.1268, -4.4512],\n",
      "        [ 3.0491, -1.0172, -4.5324],\n",
      "        [ 1.6419, -0.0361, -5.2651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1053, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6557,  0.5851, -5.9345],\n",
      "        [ 3.1384, -1.2620, -4.5628],\n",
      "        [-1.4094,  3.4572, -4.2333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6991, -1.4577, -4.0679],\n",
      "        [-1.2190,  3.2654, -4.8477],\n",
      "        [ 2.4534, -0.9015, -3.8963]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9695, -1.2383, -4.0645],\n",
      "        [-1.2407,  3.5081, -4.3655],\n",
      "        [-1.6844,  3.5467, -4.1254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0808, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1060, -1.4545, -4.1610],\n",
      "        [ 1.7764,  0.3561, -5.6771],\n",
      "        [ 3.1209, -1.1399, -4.7632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0003, -0.9873, -4.8206],\n",
      "        [ 3.1724, -1.5545, -4.1934],\n",
      "        [ 3.1602, -1.4565, -4.2463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4550,  3.4540, -4.0773],\n",
      "        [-1.3363,  3.5536, -4.0685],\n",
      "        [-0.9670,  3.7864, -4.7194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1673, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9144,  1.4127, -5.7267],\n",
      "        [ 2.9100, -1.1821, -3.7905],\n",
      "        [-1.2412,  3.5097, -4.8976]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0759, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1605,  3.3493, -4.8447],\n",
      "        [ 3.1567, -1.5745, -4.0553],\n",
      "        [ 0.0486,  1.5216, -5.7550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0328, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4160,  2.2124, -5.8244],\n",
      "        [-1.3370,  3.7958, -4.0256],\n",
      "        [ 2.7491, -1.0913, -4.2641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0942, -1.6683, -3.8602],\n",
      "        [-1.4375,  3.5665, -4.3677],\n",
      "        [ 3.0783, -1.5258, -4.6080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0074, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1145,  3.7830, -4.5649],\n",
      "        [-1.3714,  3.6418, -4.2725],\n",
      "        [-1.3477,  3.5680, -4.3491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5462, -1.4405, -4.4941],\n",
      "        [-1.1033,  3.4584, -4.7317],\n",
      "        [-1.1892,  3.9420, -4.5931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0083, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2740, -1.7479, -4.4881],\n",
      "        [-1.0881,  3.6548, -4.8040],\n",
      "        [ 3.1977, -1.5835, -3.9507]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0152, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4648,  3.5297, -4.2508],\n",
      "        [-0.6596,  2.8051, -5.2169],\n",
      "        [ 3.3489, -1.6798, -3.9669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9895, -1.8864, -3.8907],\n",
      "        [ 3.1776, -1.6088, -3.8519],\n",
      "        [ 3.5699, -1.8228, -3.7479]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2126, -1.6265, -3.5760],\n",
      "        [-1.3954,  3.7020, -4.2945],\n",
      "        [-1.2061,  3.3762, -4.4213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1148, -1.5549, -3.8386],\n",
      "        [ 3.0258, -1.7688, -3.5525],\n",
      "        [-1.5124,  3.6669, -4.5945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2866,  3.7004, -4.3351],\n",
      "        [ 3.3382, -1.4715, -4.1314],\n",
      "        [-1.2107,  3.6292, -4.7671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0074, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1683,  3.5956, -4.7793],\n",
      "        [-1.2582,  3.8591, -4.3969],\n",
      "        [ 3.1982, -1.8680, -3.7350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3204, -1.7126, -3.9165],\n",
      "        [-1.4672,  4.0315, -4.0042],\n",
      "        [ 3.2729, -1.8755, -4.1190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0170, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3731, -1.6890, -4.0644],\n",
      "        [ 3.3674, -1.9060, -4.1624],\n",
      "        [-0.3880,  2.8562, -5.6712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3021, -1.6459, -3.8712],\n",
      "        [ 3.5544, -1.8160, -4.2649],\n",
      "        [-1.2794,  3.7034, -4.5209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4096, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1736, -1.4868, -4.0680],\n",
      "        [ 2.8553, -1.3433, -3.2472],\n",
      "        [ 2.8717, -1.3134, -3.7535]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1478, -1.5630, -4.0050],\n",
      "        [ 3.2657, -1.7910, -3.8172],\n",
      "        [ 2.7674, -1.3986, -3.8164]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7551,  3.0794, -5.2999],\n",
      "        [-1.1218,  3.3469, -5.1915],\n",
      "        [ 2.4542, -1.2377, -3.1115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0379, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0613, -1.3824, -3.4140],\n",
      "        [ 2.1768, -0.3216, -3.3972],\n",
      "        [ 2.9756, -1.1012, -3.7959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0198, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8521, -1.0393, -3.7455],\n",
      "        [ 2.5516, -1.0136, -3.1801],\n",
      "        [-1.2893,  3.7291, -4.5534]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0365, -1.7058, -3.5195],\n",
      "        [-1.3709,  3.5381, -4.4644],\n",
      "        [ 2.7432, -1.5256, -3.5767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0335, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0069, -0.3900, -3.7825],\n",
      "        [-1.4572,  3.8090, -4.3667],\n",
      "        [-1.5727,  3.8177, -3.6831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4110,  3.5982, -4.2846],\n",
      "        [ 3.1528, -0.9989, -4.1546],\n",
      "        [ 2.4713, -1.1749, -4.0035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0280, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5759,  3.9246, -4.2352],\n",
      "        [ 2.2739, -0.8925, -3.6104],\n",
      "        [ 2.4127, -1.0628, -2.7910]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0046, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7516,  3.9964, -4.4693],\n",
      "        [-1.4143,  3.9105, -4.1387],\n",
      "        [-1.4336,  3.8585, -4.2958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1390, -1.4206, -3.5604],\n",
      "        [-1.5949,  3.7434, -4.5252],\n",
      "        [ 2.6230, -1.3482, -3.6866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0683, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6560, -1.4328, -3.4082],\n",
      "        [ 1.6393, -0.0579, -3.4734],\n",
      "        [ 2.9587, -1.4772, -4.0827]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2331, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5705, -0.5985, -3.8892],\n",
      "        [ 1.1508,  1.0640, -4.6218],\n",
      "        [-1.5887,  3.9356, -3.8544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1450, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8527,  0.4075, -4.6630],\n",
      "        [-1.1472,  4.0930, -4.4663],\n",
      "        [ 2.1827,  0.5981, -5.5948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0488, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1373, -1.2036, -4.2639],\n",
      "        [ 0.0907,  2.0831, -5.3324],\n",
      "        [-1.5488,  3.9529, -4.0327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8832, -1.1388, -4.0497],\n",
      "        [-1.6618,  3.5967, -3.7226],\n",
      "        [ 2.9903, -0.9774, -4.2796]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3731,  3.6073, -4.7414],\n",
      "        [ 3.1709, -1.4405, -4.6151],\n",
      "        [-1.5234,  3.7960, -4.1718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0829, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0478, -0.0096, -4.5640],\n",
      "        [-1.2778,  3.8171, -4.1337],\n",
      "        [ 2.2002,  0.1438, -5.4702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0294, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4698,  3.8333, -4.2323],\n",
      "        [-1.2399,  3.6175, -5.1109],\n",
      "        [ 2.4581, -0.1009, -4.9202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0356, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6650, -0.2834, -4.7450],\n",
      "        [ 2.9053, -1.1592, -4.5135],\n",
      "        [ 2.4758, -0.8058, -4.7569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7897, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7932,  3.0005, -5.1660],\n",
      "        [ 0.2799,  1.5878, -5.8894],\n",
      "        [-1.6213,  3.7528, -4.1487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0501, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5769,  2.4391, -5.6181],\n",
      "        [ 3.0915, -1.2695, -4.8438],\n",
      "        [ 2.4378, -0.1520, -4.6211]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1677, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5372,  3.7825, -4.7130],\n",
      "        [ 1.2110,  0.7443, -5.6714],\n",
      "        [-1.1899,  3.3704, -4.8411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3541,  3.5235, -4.3801],\n",
      "        [-0.9095,  3.2990, -5.2947],\n",
      "        [ 3.0664, -1.1895, -5.1269]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1320, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2948,  3.3602, -5.0808],\n",
      "        [-1.4483,  3.7136, -4.3183],\n",
      "        [ 1.5433,  0.7707, -5.5987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0068, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5178,  3.6271, -4.6110],\n",
      "        [-1.3311,  3.5641, -4.4645],\n",
      "        [-1.3702,  3.6773, -4.4780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2573,  3.6982, -4.4652],\n",
      "        [-1.2052,  3.2772, -4.9435],\n",
      "        [-1.5942,  3.4636, -4.3573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2003, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2215, -0.9395, -5.3143],\n",
      "        [-0.9153,  3.0540, -4.9222],\n",
      "        [ 1.1545,  0.8806, -5.8604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3389, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1749, -1.3890, -4.9095],\n",
      "        [ 1.4808,  0.9455, -5.2667],\n",
      "        [-1.4178,  3.3386, -4.1405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0810, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7717,  0.3851, -5.8383],\n",
      "        [ 3.3701, -1.5115, -4.9436],\n",
      "        [-1.0386,  3.4282, -4.8256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0549, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0138,  1.8915, -5.6869],\n",
      "        [ 3.0941, -1.0147, -5.2728],\n",
      "        [-1.3377,  3.4110, -4.5613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0763, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1682,  1.6813, -5.4639],\n",
      "        [-1.0611,  3.2227, -5.0603],\n",
      "        [ 3.0119, -1.1869, -4.8031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2169,  3.0575, -4.5859],\n",
      "        [ 3.2042, -1.0891, -5.1771],\n",
      "        [ 2.9036, -0.9188, -5.1600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0574, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6312,  2.4457, -5.4595],\n",
      "        [ 2.8304, -1.1409, -5.0281],\n",
      "        [ 2.3962,  0.2193, -5.5646]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0145, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1250,  3.2502, -5.2003],\n",
      "        [ 2.9704, -1.0053, -5.2340],\n",
      "        [ 3.2107, -1.2305, -4.8557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1454, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1305,  1.5819, -5.4773],\n",
      "        [-1.1220,  3.1852, -4.8305],\n",
      "        [ 0.3599,  1.8078, -6.0661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0346, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1185, -1.3817, -4.7347],\n",
      "        [-0.3078,  2.1661, -5.5557],\n",
      "        [ 3.1831, -1.3444, -4.6835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0532, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3701, -1.4287, -4.8261],\n",
      "        [ 3.1067, -1.3304, -4.8596],\n",
      "        [ 1.9904,  0.0847, -5.6626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1459, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5888,  1.4675, -5.4757],\n",
      "        [-0.2243,  2.2804, -5.7339],\n",
      "        [-1.1915,  3.3596, -4.7818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1528, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0610, -1.2981, -5.0228],\n",
      "        [ 3.1748, -1.2471, -5.1408],\n",
      "        [ 1.3790,  0.7644, -5.2482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0093, -0.9733, -5.1367],\n",
      "        [ 3.2283, -1.6662, -4.6842],\n",
      "        [ 3.2913, -1.3419, -4.8117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0961, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3416, -1.5581, -4.4883],\n",
      "        [ 0.1990,  1.3679, -5.3058],\n",
      "        [ 3.3295, -1.4323, -4.7724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1178, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5844,  1.5252, -6.0504],\n",
      "        [ 3.4147, -1.6977, -4.7795],\n",
      "        [-0.9860,  3.0841, -5.2169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6788, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1286,  0.3732, -5.8317],\n",
      "        [-0.0579,  2.1320, -5.8227],\n",
      "        [-0.9362,  3.2753, -5.3105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9643, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2090,  2.0794, -5.4568],\n",
      "        [ 2.5780, -0.0620, -5.4958],\n",
      "        [ 2.3163, -0.0899, -5.8787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0273, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4572,  2.4376, -5.5795],\n",
      "        [-1.2203,  3.2570, -4.7582],\n",
      "        [-1.0917,  3.0406, -5.3525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0980, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3338,  3.3509, -4.9236],\n",
      "        [ 0.1830,  2.0269, -5.4001],\n",
      "        [ 0.0346,  1.9546, -5.6815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1099, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9867, -1.1606, -5.1686],\n",
      "        [-0.1588,  2.0282, -5.5257],\n",
      "        [ 0.1937,  1.6678, -5.5134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1846, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7867, -0.5746, -5.4666],\n",
      "        [-0.7287,  2.7362, -5.2914],\n",
      "        [ 2.9118, -0.8480, -5.1272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3368, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7436,  1.2724, -5.8625],\n",
      "        [-1.4680,  3.7473, -4.9321],\n",
      "        [-1.1142,  3.3041, -4.9625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1720, -1.0363, -4.9120],\n",
      "        [ 3.0954, -0.9104, -4.5853],\n",
      "        [ 2.7631, -1.2629, -5.1742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0202, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2257,  3.7818, -4.7586],\n",
      "        [-1.0292,  3.0228, -5.1655],\n",
      "        [ 2.6101, -0.6994, -5.3790]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0462, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2835,  3.5335, -4.7099],\n",
      "        [-0.1928,  1.8890, -5.8253],\n",
      "        [ 3.1512, -1.2635, -4.6573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0799, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6477e-01,  2.4483e+00, -5.8266e+00],\n",
      "        [ 2.2304e-03,  1.8082e+00, -5.6671e+00],\n",
      "        [-1.0092e+00,  3.1204e+00, -5.2256e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1102, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0613,  1.8562, -5.7740],\n",
      "        [-1.2057,  3.6827, -5.1216],\n",
      "        [ 1.9842,  0.2894, -6.0742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2837, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7883,  0.9024, -5.8621],\n",
      "        [-0.9425,  3.2385, -5.0799],\n",
      "        [ 2.1939, -0.2558, -5.3457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1371, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6360,  1.4653, -6.4677],\n",
      "        [ 2.6465, -0.7734, -5.2683],\n",
      "        [ 3.0099, -1.1085, -5.1086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2019, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0776,  0.8362, -6.1588],\n",
      "        [ 3.1279, -1.0339, -5.3656],\n",
      "        [-1.3469,  3.2733, -4.9561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3928, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1584,  3.4112, -5.1888],\n",
      "        [ 1.3979,  0.9738, -6.3861],\n",
      "        [ 1.2112,  1.1518, -6.1097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4851, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5269,  1.5469, -6.0467],\n",
      "        [-0.1223,  2.0019, -5.6703],\n",
      "        [ 3.1115, -1.2068, -4.7986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0345, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2662,  2.2499, -5.4495],\n",
      "        [ 3.0024, -1.1377, -5.2087],\n",
      "        [-1.3017,  3.4076, -4.6758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7064, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3697,  3.5893, -4.9029],\n",
      "        [ 2.0167,  0.0783, -5.6278],\n",
      "        [-0.4208,  2.8192, -5.6525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2515, -0.9337, -5.2206],\n",
      "        [-1.2924,  3.5738, -5.1626],\n",
      "        [-0.9028,  3.0179, -5.3813]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0293,  2.9328, -5.7290],\n",
      "        [ 2.2915, -0.2499, -5.6297],\n",
      "        [-1.0820,  3.5316, -5.3292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1925, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0845, -1.2882, -4.6054],\n",
      "        [ 1.5465,  0.6568, -5.8227],\n",
      "        [ 0.2826,  1.6875, -5.7336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0438, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0069, -1.1727, -4.8984],\n",
      "        [-0.3527,  1.8368, -5.8331],\n",
      "        [-1.1633,  3.5268, -5.0551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5603, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3103,  1.7456, -6.0205],\n",
      "        [ 1.6002,  0.5756, -6.0337],\n",
      "        [ 0.0553,  1.9883, -5.9042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4404, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4049,  1.3724, -5.9693],\n",
      "        [-1.4622,  3.6224, -4.7364],\n",
      "        [ 2.9275, -0.7728, -5.0497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5572, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6276,  1.1514, -6.3169],\n",
      "        [-0.9284,  3.1025, -5.4175],\n",
      "        [ 3.1050, -1.0678, -5.1379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1036, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5426,  0.3859, -5.9411],\n",
      "        [ 2.8132, -1.1250, -5.3819],\n",
      "        [ 2.9068, -1.1593, -5.3407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0742, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9625,  0.0550, -5.9101],\n",
      "        [-0.2978,  2.2844, -6.0428],\n",
      "        [-1.2514,  3.2972, -4.9260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3764, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3510,  0.6064, -5.6670],\n",
      "        [ 2.7655, -0.8720, -5.1345],\n",
      "        [ 1.0122,  1.0520, -6.0451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0822, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9575, -1.0585, -5.4472],\n",
      "        [ 0.3558,  1.7713, -6.0163],\n",
      "        [-1.1981,  3.3254, -5.4043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0737, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7949, -0.1231, -5.8266],\n",
      "        [ 2.5541, -0.8532, -5.7039],\n",
      "        [ 2.5983, -0.3628, -5.4597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0244, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3656,  3.4167, -4.8553],\n",
      "        [-0.4379,  2.4119, -5.7420],\n",
      "        [-1.4322,  3.4179, -4.8450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0840, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2735,  1.8163, -5.8058],\n",
      "        [ 2.8423, -0.9212, -5.2119],\n",
      "        [ 2.7110, -0.6437, -5.4364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4739, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2529,  3.7405, -4.4253],\n",
      "        [ 1.5076,  0.7714, -5.8839],\n",
      "        [ 1.5318,  0.4280, -5.8553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1627, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4467, -0.8510, -5.4421],\n",
      "        [ 2.9447, -0.4647, -5.6535],\n",
      "        [ 0.8431,  1.4984, -5.6399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2223,  3.1789, -5.0846],\n",
      "        [-1.5235,  3.5598, -4.3418],\n",
      "        [-1.2707,  3.2449, -4.8815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1419, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6227,  0.1844, -5.6673],\n",
      "        [ 2.1573,  0.2913, -5.7881],\n",
      "        [-0.6551,  2.0064, -5.3550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0237, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7491,  2.8240, -5.5614],\n",
      "        [ 2.4013, -1.0221, -5.3469],\n",
      "        [-1.1971,  3.3742, -4.6412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0562, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3442,  3.9090, -4.9029],\n",
      "        [ 1.9558, -0.0574, -5.6854],\n",
      "        [-0.6755,  2.5934, -5.8361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0352, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0624,  3.4165, -4.9137],\n",
      "        [ 2.6055, -0.5284, -5.4404],\n",
      "        [ 2.5456, -0.4054, -5.4233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2021, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7290, -0.5668, -5.4540],\n",
      "        [-1.2750,  3.6252, -4.6392],\n",
      "        [ 0.6828,  0.9660, -6.1410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0717, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4957,  2.4854, -5.9650],\n",
      "        [ 2.6852, -0.1582, -5.0608],\n",
      "        [ 2.0858, -0.0847, -5.6595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0473, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6810, -0.5672, -5.6220],\n",
      "        [-1.4850,  3.8490, -4.5495],\n",
      "        [-0.1514,  2.1191, -5.7642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0649, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2709, -0.2933, -5.6013],\n",
      "        [ 2.6742, -0.5142, -5.4274],\n",
      "        [ 2.2235, -0.2717, -5.7575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2030, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2613,  0.5557, -5.6191],\n",
      "        [ 2.1529, -0.1656, -5.6059],\n",
      "        [-0.0809,  2.0472, -5.8714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1798, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1765,  3.3319, -5.2309],\n",
      "        [ 2.8558, -0.6290, -5.3688],\n",
      "        [ 1.2255,  0.7862, -6.1775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4980,  3.8707, -3.8816],\n",
      "        [-1.5085,  3.8130, -3.9011],\n",
      "        [-1.5564,  3.3977, -3.8951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7844, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8016, -1.1847, -4.9999],\n",
      "        [-1.4673,  3.8567, -4.4401],\n",
      "        [-1.4473,  3.8697, -4.2732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2208, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7147, -0.6331, -5.1601],\n",
      "        [ 2.8108, -1.1467, -5.1856],\n",
      "        [ 0.7451,  0.9238, -6.1085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9939, -0.9305, -5.3386],\n",
      "        [-1.1424,  3.2347, -5.2103],\n",
      "        [-1.6409,  3.5054, -4.4264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1862, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3448,  2.6082, -5.8702],\n",
      "        [-0.1407,  2.1608, -5.7874],\n",
      "        [ 1.3046,  0.7622, -6.1432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3453, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3083,  3.3818, -4.6823],\n",
      "        [ 0.8978,  1.4492, -6.0982],\n",
      "        [ 2.8846, -1.0556, -4.8220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0231, -1.1638, -5.0641],\n",
      "        [-1.4228,  3.5252, -5.1762],\n",
      "        [ 2.1976, -0.0663, -5.7538]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1342, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1779, -0.0539, -5.9756],\n",
      "        [ 1.6530,  0.0986, -5.4124],\n",
      "        [-0.1261,  2.0489, -5.7054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3248, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1919, -1.2411, -4.9323],\n",
      "        [ 3.0363, -1.3130, -4.5720],\n",
      "        [ 1.2122,  0.7538, -5.8658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7187, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1245,  0.1652, -5.9467],\n",
      "        [ 1.9103,  0.0408, -5.6228],\n",
      "        [ 3.4278, -1.1226, -4.7360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0651, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0703,  3.6203, -5.0503],\n",
      "        [ 3.0389, -1.4226, -4.8011],\n",
      "        [ 0.2248,  1.8858, -6.1089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1354, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2687, -0.8654, -4.5451],\n",
      "        [ 2.8442, -1.0726, -4.9943],\n",
      "        [ 1.3114,  0.5048, -5.6925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3824, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3578,  1.9031, -5.8665],\n",
      "        [ 2.8371, -1.2238, -4.9398],\n",
      "        [ 1.1167,  0.6794, -5.9410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1905, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1861, -1.2319, -4.8136],\n",
      "        [ 2.6824, -0.5261, -5.2120],\n",
      "        [ 0.8040,  1.1899, -5.8941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0285, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8455, -0.7647, -5.1464],\n",
      "        [-0.8187,  3.2973, -5.3336],\n",
      "        [-0.5239,  2.6269, -5.4074]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1557, -1.3995, -4.6476],\n",
      "        [ 3.0884, -1.2793, -4.6873],\n",
      "        [-0.6264,  3.1763, -5.2158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0257, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5888, -0.5025, -5.3831],\n",
      "        [-0.8161,  3.2155, -4.8640],\n",
      "        [ 3.0459, -1.2189, -4.6633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0317, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2719,  2.4868, -6.0287],\n",
      "        [ 3.0009, -1.0810, -5.0161],\n",
      "        [-0.9519,  3.1581, -5.0564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4146, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5292,  0.0517, -5.4607],\n",
      "        [ 3.0586, -1.2848, -4.7991],\n",
      "        [ 1.6479,  0.8795, -5.9947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0981, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2088,  1.9838, -5.8410],\n",
      "        [ 2.2945,  0.2495, -5.3582],\n",
      "        [-0.9073,  3.2859, -4.9933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0274, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2315,  2.5015, -6.0904],\n",
      "        [ 3.1949, -1.5335, -4.4519],\n",
      "        [ 3.2094, -1.4673, -4.3840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3007, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7417,  3.2680, -5.0264],\n",
      "        [ 2.8625, -0.6991, -5.0176],\n",
      "        [ 3.0312, -0.8025, -5.3493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0510, -1.2373, -4.7869],\n",
      "        [ 2.9832, -0.8147, -4.8010],\n",
      "        [ 3.2067, -1.4677, -4.6663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0412, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2620, -1.4917, -4.6502],\n",
      "        [ 0.1795,  2.4576, -5.9101],\n",
      "        [-0.7615,  3.3334, -5.0768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0161, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0407,  3.1940, -4.9401],\n",
      "        [-1.0091,  3.5715, -4.8728],\n",
      "        [-0.7433,  3.0185, -5.4561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3655, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6596,  1.2822, -6.1806],\n",
      "        [-0.8178,  3.1246, -5.0249],\n",
      "        [-0.6388,  3.0650, -5.2542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8452, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1467,  2.0465, -5.9963],\n",
      "        [-1.1603,  3.1079, -5.1681],\n",
      "        [ 1.1842,  0.7027, -5.6802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0254, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4927, -0.8512, -5.3040],\n",
      "        [-0.8309,  3.1894, -5.3574],\n",
      "        [ 2.9105, -0.8616, -5.0959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8448, -1.0587, -4.8823],\n",
      "        [ 2.9833, -0.7357, -5.0811],\n",
      "        [ 2.7723, -0.9234, -5.0574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4683, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8869, -0.7991, -5.2952],\n",
      "        [ 0.7541,  1.7868, -6.3399],\n",
      "        [-0.5324,  2.6144, -5.3939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2292, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6876, -0.6407, -5.4072],\n",
      "        [-0.7568,  3.2414, -5.1744],\n",
      "        [ 0.8734,  0.9972, -5.9808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6999,  2.9634, -5.5581],\n",
      "        [ 2.5673, -0.5594, -5.4591],\n",
      "        [ 2.4805, -0.2699, -5.9239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2333, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7282,  3.2391, -5.3105],\n",
      "        [ 0.9834,  1.0465, -6.1797],\n",
      "        [-0.9121,  3.0839, -5.2477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7478, -0.4929, -5.4089],\n",
      "        [ 1.2087,  0.7658, -5.7844],\n",
      "        [-0.6989,  2.9241, -5.2544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9614, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8643,  3.2826, -5.2664],\n",
      "        [ 2.7194, -1.0636, -5.1093],\n",
      "        [ 2.3956, -0.3897, -5.5759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0409, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6751,  3.0065, -5.7255],\n",
      "        [ 2.8356, -0.4652, -5.5135],\n",
      "        [ 2.5215, -0.2486, -5.2254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0442, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5047, -0.7251, -5.2455],\n",
      "        [ 2.7714, -0.4891, -5.2434],\n",
      "        [-0.2489,  2.6195, -6.0896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9733, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4065,  1.8599, -5.8319],\n",
      "        [-0.6430,  2.9404, -5.6300],\n",
      "        [ 1.5163,  0.6347, -6.2399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3705, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8475, -0.3554, -5.7151],\n",
      "        [-0.7597,  3.1745, -5.2419],\n",
      "        [ 1.2315,  0.6099, -5.8620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3553, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2282,  0.0805, -6.2519],\n",
      "        [ 1.2109,  0.7994, -6.0050],\n",
      "        [-0.7515,  2.5915, -5.3230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0781, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8561,  0.0472, -5.7868],\n",
      "        [ 2.3123, -0.4294, -5.6355],\n",
      "        [-0.7745,  3.1694, -5.5092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1794, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9767,  0.2217, -5.9843],\n",
      "        [-0.4145,  2.4638, -5.4664],\n",
      "        [ 1.3943,  0.4294, -5.8441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1101, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2946, -0.0211, -5.8643],\n",
      "        [ 1.8462, -0.1602, -5.7115],\n",
      "        [-0.1037,  2.0589, -5.9087]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0645, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9934,  3.2710, -5.6431],\n",
      "        [ 2.0956,  0.3985, -5.8909],\n",
      "        [-1.1561,  3.3772, -4.8764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3281, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4401,  2.2535, -6.0835],\n",
      "        [-1.0693,  3.1888, -5.2595],\n",
      "        [ 0.8603,  1.2448, -5.9791]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4294, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0819,  1.0578, -6.4462],\n",
      "        [-1.0047,  3.0992, -4.7236],\n",
      "        [ 0.9605,  1.1796, -6.0802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3257, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8764,  0.9889, -5.9704],\n",
      "        [ 2.0273, -0.0756, -5.6053],\n",
      "        [ 2.1093, -0.0493, -5.2484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6370, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3487,  2.7991, -5.8165],\n",
      "        [ 0.9844,  1.3779, -6.4585],\n",
      "        [ 0.8281,  1.3038, -6.2273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1728, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7330,  1.5756, -6.2267],\n",
      "        [ 0.1158,  1.9638, -6.1558],\n",
      "        [-0.9432,  3.3696, -5.0144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1581, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2760,  0.5326, -5.9454],\n",
      "        [-1.1618,  3.4564, -4.7319],\n",
      "        [-0.1055,  2.4539, -5.4949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8480, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1062,  1.6547, -5.7316],\n",
      "        [ 0.9507,  0.9178, -6.4187],\n",
      "        [ 2.2060,  0.1879, -5.7921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0918, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1664,  3.3092, -4.6078],\n",
      "        [ 1.7652,  0.5035, -6.0570],\n",
      "        [-0.8653,  3.4151, -4.3894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0489, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9653,  3.3137, -4.9978],\n",
      "        [ 2.0501, -0.0386, -5.9454],\n",
      "        [-1.0013,  3.1613, -5.1974]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1570, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3821,  0.3298, -5.9908],\n",
      "        [-0.7337,  3.1604, -5.5013],\n",
      "        [ 1.7620, -0.0586, -5.6172]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2207, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2096,  1.0569, -6.2308],\n",
      "        [-0.6937,  3.2528, -5.3531],\n",
      "        [-0.7314,  3.0496, -5.5709]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0925, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8603,  0.1067, -6.0809],\n",
      "        [ 2.1618, -0.5173, -5.2695],\n",
      "        [-0.5140,  2.4489, -5.8914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1126, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3061,  2.3083, -5.9854],\n",
      "        [ 1.9915, -0.0915, -5.5848],\n",
      "        [ 1.8904,  0.0594, -5.5325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4629, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3799,  1.4615, -6.0556],\n",
      "        [ 0.5955,  1.3872, -6.3837],\n",
      "        [ 1.0329,  0.9759, -6.1822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0812, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7387, -1.0206, -4.8174],\n",
      "        [ 0.3563,  2.0041, -5.9057],\n",
      "        [ 2.5450, -0.5741, -5.3593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1637, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0826,  3.4805, -4.8852],\n",
      "        [ 1.5214,  0.8936, -6.1082],\n",
      "        [-0.3079,  2.6186, -5.3720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2076, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9471,  0.2505, -6.1079],\n",
      "        [ 3.0511, -1.0271, -4.8649],\n",
      "        [ 1.1854,  0.5836, -6.4975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1171, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5424,  1.5194, -6.0710],\n",
      "        [-0.9824,  3.3723, -4.9386],\n",
      "        [-0.6725,  3.3250, -5.1497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0286, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3163, -0.5084, -5.9466],\n",
      "        [ 3.1099, -1.5359, -4.6905],\n",
      "        [-0.7851,  3.2364, -5.1852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0537, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9730,  3.1840, -5.1275],\n",
      "        [ 2.0826, -0.1025, -5.9169],\n",
      "        [-0.4984,  2.7510, -5.4497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0187, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3112, -1.5065, -4.5792],\n",
      "        [ 2.9233, -1.0845, -5.1286],\n",
      "        [-0.4277,  3.0959, -5.5008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9687,  3.5302, -4.6735],\n",
      "        [ 3.3921, -1.1441, -5.4602],\n",
      "        [-1.0001,  3.4433, -4.5973]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2123, -1.3102, -4.4570],\n",
      "        [-0.9756,  3.2427, -5.4276],\n",
      "        [ 2.6946, -1.1071, -4.8314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0102, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1755,  3.4812, -4.7531],\n",
      "        [-0.9921,  3.5199, -4.6709],\n",
      "        [ 3.3170, -1.3449, -4.9066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0790, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7106,  0.1500, -6.2553],\n",
      "        [-1.1186,  3.1911, -5.3106],\n",
      "        [-0.7303,  2.6884, -5.6343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5256, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9025, -1.5721, -4.5664],\n",
      "        [ 1.6393,  0.3217, -6.1872],\n",
      "        [ 3.2410, -1.4203, -4.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4472, -1.6611, -4.3981],\n",
      "        [ 3.2550, -1.4635, -4.9388],\n",
      "        [-1.2010,  3.4806, -4.1867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0457,  3.3866, -4.5883],\n",
      "        [-0.9684,  3.1044, -4.7989],\n",
      "        [ 3.1373, -1.4451, -4.9339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0618, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0313e+00,  8.3554e-04, -6.1210e+00],\n",
      "        [ 3.2619e+00, -1.4735e+00, -4.2661e+00],\n",
      "        [-3.2571e-01,  2.6004e+00, -5.8011e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0239, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2109,  3.4726, -4.6383],\n",
      "        [ 3.4272, -1.3496, -5.0391],\n",
      "        [-0.3372,  2.5694, -5.7190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0163, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4181,  2.9789, -6.0411],\n",
      "        [ 3.3924, -1.3643, -4.3664],\n",
      "        [ 3.3687, -1.6786, -4.4836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2005, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7453,  2.8176, -5.4424],\n",
      "        [ 3.5418, -2.1013, -4.0713],\n",
      "        [ 3.3875, -1.7706, -4.1365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1536, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0161,  3.4354, -4.8684],\n",
      "        [-0.8057,  3.1715, -5.4141],\n",
      "        [ 0.6937,  1.3149, -6.4063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0229, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2675,  3.4060, -4.5508],\n",
      "        [-0.5633,  3.0233, -5.6295],\n",
      "        [ 2.7448, -0.7080, -5.1776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0480, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2456, -0.3770, -6.0992],\n",
      "        [ 3.4333, -1.5312, -4.1696],\n",
      "        [ 2.2778, -0.4085, -5.8181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2135,  3.4638, -5.0243],\n",
      "        [ 3.5251, -1.8209, -4.0389],\n",
      "        [ 3.4769, -1.8476, -4.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0669, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2221, -1.4189, -5.1021],\n",
      "        [-0.7857,  3.7190, -4.8087],\n",
      "        [ 1.7969,  0.1702, -6.3604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6175, -1.6677, -4.0022],\n",
      "        [-0.7980,  3.0970, -5.6149],\n",
      "        [-1.0928,  3.6547, -4.6736]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0894, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8130,  3.4915, -5.3928],\n",
      "        [ 3.3976, -1.7049, -4.1771],\n",
      "        [ 0.5708,  1.8397, -5.9667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3746, -2.0314, -3.7144],\n",
      "        [ 2.2455, -0.7229, -5.6390],\n",
      "        [-1.0149,  3.4285, -5.1318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3895, -2.1016, -4.2873],\n",
      "        [ 2.9853, -1.0063, -5.4194],\n",
      "        [-1.1464,  3.4544, -4.7779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9422,  2.9291, -5.2350],\n",
      "        [ 3.4793, -1.6504, -4.4954],\n",
      "        [ 3.3704, -1.5323, -4.2563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0053, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7308, -1.7776, -4.0593],\n",
      "        [ 3.2947, -1.7699, -4.2922],\n",
      "        [ 3.3990, -2.1047, -4.2679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0342, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2702,  2.4648, -5.7985],\n",
      "        [ 3.4536, -1.4271, -4.6152],\n",
      "        [ 2.6775, -0.7732, -5.5078]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1046, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2234,  3.3568, -5.0983],\n",
      "        [ 1.4431,  0.3845, -5.7519],\n",
      "        [ 3.5593, -1.8366, -4.2386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0465, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4176, -1.6720, -4.2086],\n",
      "        [ 2.0721,  0.0809, -6.0493],\n",
      "        [ 3.5997, -1.9035, -3.8206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1100, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4395, -0.5436, -5.9202],\n",
      "        [ 2.1899, -0.4152, -5.6856],\n",
      "        [ 1.8503,  0.4755, -6.4095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0512, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9982,  3.1763, -5.4757],\n",
      "        [ 0.1057,  2.1108, -5.7714],\n",
      "        [-1.1420,  3.3271, -5.1767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0933, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6307,  0.4108, -6.3210],\n",
      "        [-1.0279,  3.5897, -4.8349],\n",
      "        [-1.1818,  3.3416, -5.5082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0835, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6156,  0.2498, -6.2404],\n",
      "        [-1.2687,  3.4722, -5.2135],\n",
      "        [-1.0029,  3.2738, -4.9592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8754,  3.1262, -4.9692],\n",
      "        [-1.0397,  3.3896, -4.9267],\n",
      "        [-1.1144,  3.4899, -4.9640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4655, -1.5115, -4.6535],\n",
      "        [-0.9687,  3.2174, -4.9995],\n",
      "        [ 3.0638, -1.6916, -4.6184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0271, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5367, -0.1887, -5.4645],\n",
      "        [-1.0816,  3.5657, -4.9261],\n",
      "        [ 3.2564, -1.6175, -4.8289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1240, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2968, -1.5976, -4.7914],\n",
      "        [ 3.2021, -1.4211, -4.3153],\n",
      "        [ 1.4294,  0.5720, -6.2384]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9579, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1708,  3.2582, -5.0811],\n",
      "        [-1.0508,  3.3210, -5.0705],\n",
      "        [ 2.3512, -0.4376, -5.7345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0282, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3622,  2.3535, -5.7073],\n",
      "        [ 3.3570, -1.3075, -4.8278],\n",
      "        [-1.2029,  3.3679, -5.1097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0500, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2986, -0.5230, -5.6872],\n",
      "        [ 2.3338, -0.1472, -5.6721],\n",
      "        [ 3.2850, -1.2286, -4.6212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5429, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3914,  1.7707, -5.8330],\n",
      "        [-1.0297,  3.3045, -5.2075],\n",
      "        [-1.2987,  3.2004, -5.1368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2245, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1641,  3.1811, -4.9972],\n",
      "        [-0.7876,  2.8043, -5.7180],\n",
      "        [ 1.1313,  1.0060, -6.2622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1021, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7767,  0.5442, -6.1230],\n",
      "        [-1.1239,  3.3711, -4.9568],\n",
      "        [-0.5006,  2.7374, -5.5824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0210, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2289, -1.6175, -4.6586],\n",
      "        [-0.6618,  2.6668, -5.8926],\n",
      "        [-0.7904,  3.1520, -5.2711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3970, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3810,  0.6126, -6.1877],\n",
      "        [ 2.9784, -1.2709, -5.2101],\n",
      "        [-0.5942,  3.0157, -5.4515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1589, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0183, -1.5109, -5.0611],\n",
      "        [-1.1471,  3.4107, -4.8034],\n",
      "        [ 0.7076,  1.2602, -6.4251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1067, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8272,  2.7767, -5.4743],\n",
      "        [ 0.5194,  1.6445, -6.0966],\n",
      "        [ 3.1866, -1.3026, -4.9135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0942, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6457,  2.5761, -5.5160],\n",
      "        [ 1.5269,  0.1841, -6.2479],\n",
      "        [-1.0618,  3.4731, -4.7460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0975, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9720, -1.1533, -5.1967],\n",
      "        [-0.6983,  2.7385, -5.4383],\n",
      "        [ 1.7127,  0.4266, -6.3016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0557, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2748, -0.4920, -5.9633],\n",
      "        [ 2.0393, -0.2304, -5.9096],\n",
      "        [-1.2943,  3.6695, -4.8946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0457, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3294,  3.6269, -4.7599],\n",
      "        [-1.1496,  3.3850, -4.7863],\n",
      "        [ 2.0580, -0.0141, -5.8190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0656,  3.1270, -5.1745],\n",
      "        [-1.1710,  3.2449, -5.2141],\n",
      "        [ 3.2031, -1.2972, -5.1433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7739, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2820,  1.9576, -5.9254],\n",
      "        [ 1.4504,  0.9142, -6.3811],\n",
      "        [-1.1235,  3.2128, -5.0665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0815, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5016, -1.0674, -5.3394],\n",
      "        [ 1.6492,  0.1894, -6.0801],\n",
      "        [-1.4126,  3.5649, -4.5502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0704,  3.5038, -5.1669],\n",
      "        [-1.3637,  3.7261, -4.6594],\n",
      "        [-0.5964,  3.0414, -5.2675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2758, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3220,  0.7317, -5.8871],\n",
      "        [-1.1440,  3.4987, -4.4646],\n",
      "        [ 0.5132,  1.2991, -5.9648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0721, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7672, -0.1059, -6.0471],\n",
      "        [-0.8990,  2.8201, -5.3914],\n",
      "        [ 2.4624, -0.5405, -5.4484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1233, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1395,  2.3970, -5.8342],\n",
      "        [ 1.9971, -0.0580, -5.7946],\n",
      "        [ 1.8560,  0.1851, -6.4329]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0317, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3068, -0.3558, -5.9211],\n",
      "        [-0.9924,  2.9138, -5.4976],\n",
      "        [-1.4029,  3.5687, -4.5955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0314, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1602,  3.5820, -4.7206],\n",
      "        [-1.4124,  3.5886, -3.9923],\n",
      "        [ 2.1737, -0.3396, -5.9282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0279, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3771,  2.3494, -5.8468],\n",
      "        [ 3.1178, -1.4065, -5.1435],\n",
      "        [-1.3545,  3.3967, -4.5592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.2431, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0025e+00,  1.9935e-03, -6.2330e+00],\n",
      "        [ 1.7973e-01,  1.9717e+00, -6.2888e+00],\n",
      "        [ 2.9842e+00, -1.4508e+00, -4.2994e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2878, -0.4649, -5.9338],\n",
      "        [-0.9365,  3.4883, -4.8926],\n",
      "        [-1.0543,  3.3476, -4.9795]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3373,  3.4148, -3.8589],\n",
      "        [ 1.9381, -0.1768, -6.2983],\n",
      "        [-1.2974,  3.6519, -4.2511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8970, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9079,  3.5525, -4.0982],\n",
      "        [ 2.2527, -0.2691, -5.8096],\n",
      "        [ 2.1378, -0.3584, -5.6167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0245, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0945,  3.7290, -4.4393],\n",
      "        [-1.4702,  3.6747, -4.5988],\n",
      "        [ 2.2478, -0.5535, -5.9025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0322, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4866,  3.5355, -4.2549],\n",
      "        [-1.1753,  3.5892, -3.9716],\n",
      "        [ 2.1865, -0.2945, -6.1946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0535, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4526, -0.5550, -5.8077],\n",
      "        [-0.2687,  2.2471, -5.7283],\n",
      "        [ 2.6824, -0.6910, -5.7093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1588, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8913,  0.0398, -5.9345],\n",
      "        [ 0.3267,  1.8093, -6.2222],\n",
      "        [ 1.8900, -0.1267, -5.6676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0383, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6533,  2.9257, -5.3154],\n",
      "        [-0.3532,  2.1617, -5.9013],\n",
      "        [-1.2319,  3.5276, -4.2758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2027, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3609,  3.5528, -4.1678],\n",
      "        [-0.5606,  2.5691, -5.6262],\n",
      "        [ 1.3623,  1.0686, -6.4168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0842, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3192,  3.4772, -3.8327],\n",
      "        [ 1.5009,  0.1721, -6.2278],\n",
      "        [-1.3452,  3.4920, -4.2905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0623, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4064, -0.3942, -6.1439],\n",
      "        [ 1.9584, -0.0974, -6.1038],\n",
      "        [-1.3840,  3.6436, -3.8392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0842, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1138,  2.0516, -5.7478],\n",
      "        [-0.4231,  2.5947, -5.4760],\n",
      "        [ 1.9956, -0.3087, -6.0446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7734, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1964,  0.1269, -5.9935],\n",
      "        [-0.6506,  2.9065, -5.3421],\n",
      "        [-0.2065,  2.0163, -5.9877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0292, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4970,  3.6679, -4.2160],\n",
      "        [-1.3292,  3.5852, -3.6070],\n",
      "        [ 2.1653, -0.4124, -5.5917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1643, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3017,  3.6469, -3.5521],\n",
      "        [-1.6006,  3.5756, -4.0549],\n",
      "        [ 1.3723,  0.8848, -6.3717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0333, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1396,  3.5536, -4.4667],\n",
      "        [-1.2460,  3.6506, -3.8413],\n",
      "        [ 2.4415, -0.0146, -6.0519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2075, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1315,  3.5014, -3.4841],\n",
      "        [ 2.0262, -0.2946, -5.9042],\n",
      "        [ 0.7810,  1.1700, -6.2280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1090, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0363, -0.0109, -5.7944],\n",
      "        [ 1.8362, -0.0808, -6.1174],\n",
      "        [-0.0750,  2.5863, -5.7950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3393, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4323, -0.6754, -5.7539],\n",
      "        [ 0.8068,  1.0251, -6.2423],\n",
      "        [ 1.8124,  0.0944, -6.2511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7806, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1277,  1.9490, -5.5384],\n",
      "        [ 1.8396, -0.0644, -6.2402],\n",
      "        [-1.4340,  3.5930, -3.6235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0857, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6410,  0.3512, -6.0032],\n",
      "        [-1.4210,  3.7419, -3.9753],\n",
      "        [-1.3412,  3.6315, -4.0912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3472, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1368, -0.5773, -5.3966],\n",
      "        [ 1.3899,  0.3756, -6.1380],\n",
      "        [ 1.0978,  1.0447, -6.6090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0630, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4443,  3.8559, -4.1620],\n",
      "        [ 2.5066, -0.5098, -5.6242],\n",
      "        [ 2.1342,  0.2026, -6.3605]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4896, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4474,  0.4108, -6.2805],\n",
      "        [ 2.3627, -0.0311, -5.8567],\n",
      "        [-0.6503,  2.5359, -5.5492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0657, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0695, -0.1529, -6.1763],\n",
      "        [ 2.0498, -0.3986, -5.8076],\n",
      "        [-1.2550,  3.3148, -4.5587]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0902, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6978, -0.9362, -5.4465],\n",
      "        [ 2.7746, -1.0855, -5.4924],\n",
      "        [ 0.3948,  1.7820, -6.2668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6340, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1892, -0.4765, -5.7129],\n",
      "        [ 1.9922, -0.0071, -6.2118],\n",
      "        [ 2.3146, -0.8402, -5.7888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3355, -0.6658, -5.7756],\n",
      "        [ 2.4517, -0.4462, -5.5533],\n",
      "        [ 2.7413, -0.7480, -5.5503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0321, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3944,  3.8486, -4.5098],\n",
      "        [ 2.3133, -0.7177, -5.4164],\n",
      "        [ 2.4864, -0.6399, -5.3304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0527, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2923,  0.7874, -6.0507],\n",
      "        [ 2.1319, -0.5432, -6.0161],\n",
      "        [ 1.9443, -0.0405, -6.6395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0294, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3416,  2.3392, -6.1592],\n",
      "        [-1.0634,  3.1935, -4.8189],\n",
      "        [-1.4570,  3.5509, -3.9287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6055, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1219,  3.6629, -4.3907],\n",
      "        [ 2.1248, -0.3895, -6.0518],\n",
      "        [ 1.8708,  0.3367, -6.0561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0473, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6454, -0.6614, -5.6809],\n",
      "        [ 2.5335, -0.7226, -5.7129],\n",
      "        [ 2.1500, -0.5131, -5.9497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0521, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4510, -0.7010, -5.6447],\n",
      "        [ 2.2489, -0.1813, -5.7350],\n",
      "        [ 2.5989, -0.9175, -5.5317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0326, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2029, -0.2124, -6.0038],\n",
      "        [-1.5890,  3.9738, -3.7661],\n",
      "        [-1.2821,  3.6223, -4.3501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1127, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4636,  2.5551, -5.4116],\n",
      "        [ 0.4385,  1.5551, -5.9219],\n",
      "        [-1.4213,  3.7118, -3.7925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3828, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3031,  0.9223, -6.3228],\n",
      "        [ 2.0728, -0.3561, -5.9913],\n",
      "        [ 1.7847,  0.0428, -5.7689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0388, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2622,  3.5666, -3.9868],\n",
      "        [-0.0092,  2.2359, -5.7935],\n",
      "        [-1.1558,  3.8712, -4.0559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0286, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1792, -0.4359, -6.1993],\n",
      "        [-1.3102,  3.5162, -3.6656],\n",
      "        [-1.4134,  3.7491, -4.0733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0864, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7705,  0.4907, -6.3496],\n",
      "        [-1.6370,  3.7491, -3.7807],\n",
      "        [-1.3005,  3.4919, -4.8414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9674, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0085,  1.2174, -6.3263],\n",
      "        [ 1.5750,  0.9290, -5.9616],\n",
      "        [ 1.6978,  0.2279, -6.2779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3256,  3.6786, -3.9617],\n",
      "        [-1.7080,  3.7417, -3.9966],\n",
      "        [-1.5138,  3.7109, -3.5850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2486, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4754,  3.8408, -3.9211],\n",
      "        [ 0.9091,  0.9917, -6.5426],\n",
      "        [-1.4901,  3.9131, -3.6768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1432, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5515,  2.8706, -5.5990],\n",
      "        [ 1.5008,  0.7646, -6.4730],\n",
      "        [-1.3300,  3.9020, -3.7521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0467, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4266,  3.7979, -4.1676],\n",
      "        [-1.2868,  3.6702, -3.6741],\n",
      "        [ 1.9303, -0.0741, -5.8549]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1517, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3976,  3.7103, -3.7716],\n",
      "        [ 1.0751,  0.3675, -6.1534],\n",
      "        [-0.2757,  2.7515, -6.0307]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3635,  3.5589, -3.8316],\n",
      "        [ 2.6076, -0.9881, -5.2274],\n",
      "        [-1.5985,  3.7152, -3.6346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5648,  3.7602, -4.0016],\n",
      "        [-1.5117,  3.9004, -3.6023],\n",
      "        [ 2.5154, -0.5630, -6.0919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0456, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3132, -0.4184, -5.7209],\n",
      "        [-1.3285,  4.0365, -4.1385],\n",
      "        [-0.2043,  2.4471, -5.8663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1803, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4262,  1.3464, -6.3855],\n",
      "        [-1.3642,  3.8017, -3.8187],\n",
      "        [ 1.8115,  0.2973, -6.2181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0439, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6523, -0.5790, -5.9540],\n",
      "        [ 2.8032, -0.7584, -5.6897],\n",
      "        [ 2.2521, -0.4578, -5.9259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0150, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2302,  3.5988, -4.7251],\n",
      "        [ 2.5029, -0.9199, -5.3913],\n",
      "        [-1.7386,  3.7989, -4.0345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1130, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1242, -0.3370, -5.7503],\n",
      "        [ 2.8906, -0.8891, -5.4784],\n",
      "        [ 1.7974,  0.4632, -6.4608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0228, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4605,  3.7958, -4.3283],\n",
      "        [ 2.8752, -0.9111, -5.5256],\n",
      "        [ 2.4463, -0.7513, -5.6583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6021, -1.0406, -5.5027],\n",
      "        [ 2.9492, -0.9684, -5.6061],\n",
      "        [-1.5076,  3.7443, -4.4604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6784, -0.7059, -5.3672],\n",
      "        [ 2.7485, -0.6092, -5.7052],\n",
      "        [-1.6743,  3.5467, -3.7002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0417, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5128,  3.6975, -3.9750],\n",
      "        [ 1.8988, -0.2874, -6.1678],\n",
      "        [ 3.1020, -1.2929, -5.3616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0150, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0268, -1.0059, -5.4971],\n",
      "        [ 2.7762, -1.0338, -5.4027],\n",
      "        [-1.4292,  3.9250, -4.2182]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0603, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8461, -0.8071, -5.5729],\n",
      "        [ 2.8527, -0.8576, -5.4986],\n",
      "        [ 1.9578, -0.0122, -6.2442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3841,  3.4507, -4.6110],\n",
      "        [-1.5169,  3.9074, -3.7504],\n",
      "        [ 2.6951, -1.0515, -5.3854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4469, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5170,  0.7127, -6.6016],\n",
      "        [ 1.7574,  0.0213, -6.0036],\n",
      "        [-1.5407,  4.0899, -4.0914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3710,  3.7938, -4.1704],\n",
      "        [ 3.0841, -1.3449, -5.3639],\n",
      "        [ 3.1265, -1.5362, -4.8077]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2276, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1839, -1.4217, -4.7520],\n",
      "        [ 2.7753, -1.1568, -5.2446],\n",
      "        [ 1.0994,  1.0161, -6.4571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1334, -1.5235, -4.9899],\n",
      "        [-1.8237,  3.9641, -3.6303],\n",
      "        [ 3.1293, -1.1566, -5.0670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5182,  4.0074, -3.9292],\n",
      "        [ 3.2036, -1.3165, -4.9312],\n",
      "        [-0.3288,  2.4124, -6.0414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0135, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8113, -0.9420, -5.5921],\n",
      "        [-1.4222,  3.8669, -4.3250],\n",
      "        [ 3.1339, -1.3268, -5.2701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0063, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4140,  3.9399, -4.4750],\n",
      "        [-1.6779,  3.9880, -3.5981],\n",
      "        [ 3.1679, -1.4646, -5.1946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2479, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9017,  3.6055, -3.6920],\n",
      "        [ 2.7647, -1.0402, -5.4501],\n",
      "        [ 0.9238,  0.8779, -6.3492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4948, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8705, -0.1050, -6.3115],\n",
      "        [ 2.2592, -0.0176, -6.0222],\n",
      "        [-1.6162,  4.0204, -3.7894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5832,  3.8088, -3.7792],\n",
      "        [ 3.1241, -1.1763, -4.9875],\n",
      "        [ 2.9648, -1.4070, -4.9377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2284, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0491,  0.9864, -6.3428],\n",
      "        [-1.7349,  3.8199, -4.0519],\n",
      "        [ 2.9864, -1.0129, -5.4381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0370, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7775,  3.8499, -3.4674],\n",
      "        [-0.0833,  2.1433, -6.2452],\n",
      "        [-1.7083,  3.9120, -3.7395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2424, -1.3848, -4.6589],\n",
      "        [ 3.0853, -1.5354, -4.4512],\n",
      "        [-1.5851,  3.9776, -4.3563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0319, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1237,  2.2622, -6.2326],\n",
      "        [-1.7234,  3.8137, -3.7912],\n",
      "        [-1.7899,  4.1697, -3.4622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5937,  3.7522, -3.9692],\n",
      "        [-1.7108,  3.8486, -4.0677],\n",
      "        [ 2.9604, -1.1793, -5.0704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3767, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0787,  2.0862, -6.2685],\n",
      "        [ 1.0270,  1.0284, -6.3879],\n",
      "        [ 1.5639,  0.5494, -6.4247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1681, -1.4463, -4.5843],\n",
      "        [-1.4445,  3.5726, -4.2932],\n",
      "        [-1.7264,  3.8911, -3.7645]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7842,  3.7941, -3.9350],\n",
      "        [-1.7230,  3.9143, -3.6691],\n",
      "        [ 3.3132, -1.5229, -4.4282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1373, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5178,  0.7920, -6.4194],\n",
      "        [ 3.2290, -1.1591, -5.4654],\n",
      "        [-1.6493,  3.8875, -3.7209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5694,  4.1204, -3.6271],\n",
      "        [-0.1645,  1.9578, -6.3029],\n",
      "        [-1.7188,  4.0581, -3.5608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1472, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9208, -1.1623, -5.5074],\n",
      "        [-1.8300,  4.1666, -3.7420],\n",
      "        [ 1.4120,  0.7665, -6.3277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8083, -0.9827, -4.8270],\n",
      "        [-1.8839,  4.0886, -3.7077],\n",
      "        [-1.6950,  3.9996, -3.9669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0187, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7410,  3.9350, -3.5945],\n",
      "        [ 2.5926, -0.7006, -5.4403],\n",
      "        [ 3.1050, -1.0780, -5.1669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1597, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1946,  3.3312, -5.0817],\n",
      "        [ 1.5943,  0.1942, -6.0109],\n",
      "        [ 1.7596,  0.4879, -6.3260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5808,  4.0129, -3.9734],\n",
      "        [ 3.1686, -1.5410, -4.6154],\n",
      "        [-1.5659,  4.0499, -3.4831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0393, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7548,  4.0256, -3.4733],\n",
      "        [ 3.1156, -1.1416, -4.7462],\n",
      "        [ 1.9547, -0.3030, -6.0670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0074, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0717, -1.4813, -4.8219],\n",
      "        [-1.7851,  3.7795, -4.1596],\n",
      "        [ 3.4409, -1.5349, -4.8576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3132, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7970,  0.5358, -6.0492],\n",
      "        [ 1.2011,  1.1738, -6.5741],\n",
      "        [ 3.2037, -1.4326, -4.6266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7959e+00, -4.6383e-03, -5.8710e+00],\n",
      "        [ 3.2422e+00, -1.6160e+00, -4.4280e+00],\n",
      "        [ 3.0312e+00, -9.0208e-01, -5.1775e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0206, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4523, -0.6797, -5.7572],\n",
      "        [ 3.5781, -1.5069, -4.6406],\n",
      "        [ 3.0290, -1.3675, -5.4124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0862, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2098, -1.4993, -5.1986],\n",
      "        [ 1.6523,  0.3780, -6.3034],\n",
      "        [-1.8362,  4.2691, -3.6121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2617, -1.8496, -4.2350],\n",
      "        [-1.7105,  4.2368, -3.6329],\n",
      "        [ 3.1812, -0.9529, -5.5332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7337,  3.7317, -4.0084],\n",
      "        [ 3.4505, -1.8628, -4.3701],\n",
      "        [-1.5806,  4.1746, -3.8203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7762,  4.1406, -3.2410],\n",
      "        [ 3.2714, -1.4720, -5.1243],\n",
      "        [ 3.5050, -2.1248, -4.3048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0587, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2170, -0.5155, -5.7726],\n",
      "        [ 2.2537, -0.3893, -6.1287],\n",
      "        [ 2.6287, -0.4867, -5.3737]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1646, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8772,  1.3483, -6.3716],\n",
      "        [ 3.4961, -1.8909, -4.4097],\n",
      "        [-1.7461,  4.0611, -3.7868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2172, -1.2979, -5.2900],\n",
      "        [ 3.2313, -1.7164, -4.4083],\n",
      "        [-1.7361,  4.1249, -3.6597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5326, -1.7337, -4.6299],\n",
      "        [ 3.2249, -1.5884, -4.2045],\n",
      "        [-1.9014,  3.8697, -3.9547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0370, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4553,  4.1789, -4.2869],\n",
      "        [ 3.5407, -1.6418, -4.6576],\n",
      "        [ 2.1434, -0.0979, -6.1069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0603, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9745,  4.2413, -3.8762],\n",
      "        [-1.7754,  3.9751, -3.5754],\n",
      "        [ 1.9018,  0.2450, -6.2947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7169,  4.0529, -3.7290],\n",
      "        [ 3.2139, -1.5281, -4.7473],\n",
      "        [-1.9461,  4.1863, -3.6547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5817, -2.1227, -3.9502],\n",
      "        [ 3.4319, -1.7237, -4.4328],\n",
      "        [ 3.3632, -2.2788, -4.0162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9013,  4.2700, -3.6590],\n",
      "        [ 2.8834, -1.2124, -4.9936],\n",
      "        [ 3.4164, -2.1307, -3.5628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3036, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9558,  4.1964, -3.8280],\n",
      "        [ 3.6104, -1.9332, -4.7046],\n",
      "        [ 2.8151, -1.0685, -5.4178]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4171, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3306, -1.5907, -4.8661],\n",
      "        [-1.8735,  4.0886, -3.5698],\n",
      "        [ 1.4786,  0.5795, -6.6088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7273, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8758,  4.2397, -4.1702],\n",
      "        [-1.8055,  4.1226, -3.7143],\n",
      "        [ 2.0436, -0.0122, -6.1721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0880, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6071,  4.0363, -3.7703],\n",
      "        [-1.6055,  4.2586, -3.8042],\n",
      "        [ 1.6730,  0.4445, -6.3613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0074, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9082,  3.9966, -3.7917],\n",
      "        [ 3.3386, -1.4740, -4.5900],\n",
      "        [ 3.1841, -1.3698, -4.9968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3342, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4120, -1.7261, -4.5788],\n",
      "        [ 1.4065,  0.8756, -6.5046],\n",
      "        [-1.7990,  4.2476, -3.9392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0556, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0737, -1.1808, -5.1252],\n",
      "        [-1.6116,  4.0661, -3.7314],\n",
      "        [ 1.9459,  0.1125, -6.4328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1848, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7443,  4.0331, -3.7674],\n",
      "        [ 1.3316,  1.0077, -6.2641],\n",
      "        [ 3.3972, -1.6931, -4.8805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0368, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7067,  4.2494, -3.8189],\n",
      "        [-1.9429,  4.0961, -3.8600],\n",
      "        [ 2.0218, -0.1853, -5.9641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0810, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1248,  0.0803, -6.2676],\n",
      "        [ 1.9973, -0.0802, -6.3487],\n",
      "        [-1.7211,  4.2422, -3.7194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0258, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9417,  4.2474, -3.6807],\n",
      "        [ 2.3304, -0.2849, -5.2777],\n",
      "        [-1.8190,  3.8560, -3.7507]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8886,  4.0437, -3.6649],\n",
      "        [ 1.7568, -0.1420, -6.1691],\n",
      "        [ 1.1123,  0.6032, -6.1927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0998, -1.2463, -4.8806],\n",
      "        [-1.9190,  3.8776, -3.8889],\n",
      "        [ 2.8517, -0.9557, -5.5714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0410, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4473, -1.5339, -4.5922],\n",
      "        [ 2.0587, -0.1003, -5.9496],\n",
      "        [ 3.4428, -1.6553, -4.6873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2881, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0293, -0.1502, -6.2036],\n",
      "        [-1.8583,  4.1637, -3.6528],\n",
      "        [ 1.1584,  1.2762, -6.4262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7538,  4.0521, -3.2089],\n",
      "        [-1.5352,  3.9153, -3.9043],\n",
      "        [ 3.3273, -1.4172, -4.8313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0391, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2314, -1.5609, -4.5725],\n",
      "        [ 1.8737, -0.4074, -6.0132],\n",
      "        [ 3.1715, -1.3559, -4.6676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0684, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7476,  4.0696, -3.6517],\n",
      "        [ 2.0992, -0.4324, -6.0948],\n",
      "        [ 1.9466, -0.0736, -5.9580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8138,  4.1111, -3.0618],\n",
      "        [-1.8751,  4.1844, -3.4927],\n",
      "        [ 3.3909, -1.4789, -4.8265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0647, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0655, -1.3704, -4.3208],\n",
      "        [ 2.2436, -0.3602, -6.2253],\n",
      "        [ 2.0707, -0.0830, -6.0679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9473, -1.0159, -4.9381],\n",
      "        [-1.6305,  3.9396, -3.6689],\n",
      "        [ 3.3671, -1.4428, -4.9089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1538, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8115e+00,  3.0003e-04, -6.2230e+00],\n",
      "        [ 1.5977e+00,  5.7370e-01, -6.3561e+00],\n",
      "        [-2.0431e+00,  4.0660e+00, -3.7248e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2370, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6360,  1.1107, -6.4914],\n",
      "        [ 1.6859,  0.2997, -5.9624],\n",
      "        [-1.8327,  4.0435, -3.5067]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0585, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6928, -0.0551, -6.4425],\n",
      "        [ 3.4591, -1.7488, -4.4789],\n",
      "        [ 3.2528, -1.4933, -4.9984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6894, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8507, -0.9863, -5.3000],\n",
      "        [ 1.8917,  0.0676, -6.2313],\n",
      "        [ 2.3023, -0.2909, -5.6515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6237, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8675,  4.2473, -3.8486],\n",
      "        [ 1.7749,  0.0879, -6.6675],\n",
      "        [ 2.9958, -1.4826, -5.1101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1519, -1.2075, -5.6279],\n",
      "        [-1.5799,  3.8928, -4.6910],\n",
      "        [-1.4817,  4.1366, -3.7133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6508, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8307, -0.3534, -5.5001],\n",
      "        [ 1.7574,  0.0875, -6.0764],\n",
      "        [-1.8821,  4.2463, -3.8348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0031, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7666,  4.2153, -3.5571],\n",
      "        [-1.8905,  4.1003, -3.5185],\n",
      "        [-1.7283,  4.0816, -3.7458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0195, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0053, -1.2211, -5.3066],\n",
      "        [-1.6336,  4.1178, -3.5197],\n",
      "        [ 2.5574, -0.6472, -5.9366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1186,  3.2545, -5.0282],\n",
      "        [ 3.1238, -0.8997, -5.4193],\n",
      "        [ 2.5947, -0.8784, -5.6563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0461, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1924,  0.0510, -5.6832],\n",
      "        [-0.3439,  2.4713, -5.7999],\n",
      "        [-1.8292,  4.1365, -3.8736]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1919, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7244,  1.7783, -5.8640],\n",
      "        [ 1.6527,  0.4926, -6.5357],\n",
      "        [-1.7827,  4.0432, -3.6802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2699, -0.6715, -6.0772],\n",
      "        [ 3.2661, -1.5763, -4.5263],\n",
      "        [-1.5774,  3.8476, -4.0122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4206,  3.8951, -4.3246],\n",
      "        [ 3.3175, -1.6466, -4.5680],\n",
      "        [-1.6105,  3.9196, -4.3371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0128, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3919, -1.6742, -4.6263],\n",
      "        [ 2.9351, -1.4842, -4.4382],\n",
      "        [ 3.0617, -0.8912, -5.8597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3429,  3.9106, -4.6864],\n",
      "        [-1.4811,  3.7182, -4.3732],\n",
      "        [-1.3512,  3.6296, -4.4149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0664, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8316,  0.2076, -6.0885],\n",
      "        [ 2.9866, -1.3888, -4.3565],\n",
      "        [-1.4370,  3.7466, -4.5451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4216,  3.2842, -4.7201],\n",
      "        [-1.0954,  3.2389, -5.2325],\n",
      "        [-1.1105,  3.5984, -4.3842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2777,  3.6900, -4.5118],\n",
      "        [-1.3718,  3.7568, -4.3935],\n",
      "        [ 3.1001, -0.9505, -5.4044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0931, -1.5224, -4.8507],\n",
      "        [-1.3026,  3.6238, -4.5479],\n",
      "        [-1.1472,  3.6632, -4.9746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8687, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2389,  3.5765, -4.4347],\n",
      "        [ 0.7203,  1.5586, -6.3391],\n",
      "        [-1.1195,  3.2675, -4.8932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0601, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5835, -0.6978, -5.7826],\n",
      "        [ 2.1164,  0.1781, -6.5849],\n",
      "        [-1.3790,  3.4251, -4.7137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3551, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7456,  1.3371, -6.2687],\n",
      "        [-0.8192,  3.2940, -4.3552],\n",
      "        [-0.9956,  3.1416, -4.5467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7866,  3.2744, -4.9624],\n",
      "        [ 2.7288, -1.0343, -5.3128],\n",
      "        [ 3.3134, -1.9404, -3.6435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0140, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9802,  3.1513, -4.9264],\n",
      "        [ 3.1545, -1.8263, -4.3134],\n",
      "        [-0.9369,  3.0805, -4.6137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0207, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6045, -0.4652, -5.6797],\n",
      "        [ 3.3483, -1.7048, -4.5307],\n",
      "        [ 3.2247, -1.4555, -4.4410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0960, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9099,  2.6314, -4.4894],\n",
      "        [ 3.1968, -1.7748, -4.1855],\n",
      "        [ 1.5603,  0.3052, -6.3550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8055,  2.8220, -4.9667],\n",
      "        [-0.7729,  3.1463, -5.0759],\n",
      "        [-0.7799,  3.0054, -5.1672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8568, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0217, -0.3431, -6.0471],\n",
      "        [-0.5327,  2.5514, -5.4848],\n",
      "        [ 2.2520, -0.0912, -6.0196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2568, -0.4452, -5.8345],\n",
      "        [ 3.3438, -1.9031, -3.8804],\n",
      "        [-0.4774,  2.5341, -6.0685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0339, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1269, -1.2845, -4.7484],\n",
      "        [-0.5750,  2.9795, -5.4211],\n",
      "        [ 2.3568, -0.4157, -6.1276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7582,  2.8937, -5.1677],\n",
      "        [ 3.3345, -1.9862, -4.1641],\n",
      "        [ 3.3961, -1.7791, -4.4377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1730, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2993, -1.3837, -4.4800],\n",
      "        [-0.4628,  2.6469, -5.3339],\n",
      "        [ 0.7044,  1.2286, -6.1942]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1947, -1.2709, -4.5666],\n",
      "        [ 3.2428, -1.5712, -4.3674],\n",
      "        [ 3.2333, -1.7007, -4.8276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2788, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6359,  0.3661, -6.2186],\n",
      "        [ 0.9013,  1.4405, -6.6310],\n",
      "        [ 2.1481,  0.1620, -6.1448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8587, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2100, -0.1665, -6.1267],\n",
      "        [ 3.2953, -1.6973, -4.6712],\n",
      "        [-0.0286,  2.1923, -5.8316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0817, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7201,  3.3580, -5.2719],\n",
      "        [-0.7309,  2.9805, -5.0524],\n",
      "        [ 0.1786,  1.6673, -6.4476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1037, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7686,  2.5972, -5.0849],\n",
      "        [ 0.5654,  1.7921, -6.2608],\n",
      "        [ 2.9051, -1.0391, -5.4343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0556, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5903,  2.7353, -5.2020],\n",
      "        [ 0.2253,  2.3907, -5.8484],\n",
      "        [ 3.1267, -0.6745, -5.6977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2200, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0764,  0.7376, -6.4091],\n",
      "        [ 2.1581, -0.3089, -5.7944],\n",
      "        [ 2.2655, -0.9502, -5.3844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0449, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7052,  3.2569, -4.7588],\n",
      "        [-0.7986,  3.0786, -5.2315],\n",
      "        [ 0.0218,  2.3332, -5.8520]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3247, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8228, -0.9495, -5.2722],\n",
      "        [ 1.3273,  0.8897, -6.7615],\n",
      "        [-0.8290,  3.3625, -5.1072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1044, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5776,  2.8308, -5.1419],\n",
      "        [ 0.3369,  1.4935, -6.5853],\n",
      "        [ 3.4169, -1.6362, -4.7415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1322, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2044,  2.1366, -5.4317],\n",
      "        [ 1.6990,  0.5665, -6.5305],\n",
      "        [-0.8789,  2.8313, -4.6363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0206, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9656,  3.3614, -5.1470],\n",
      "        [ 2.8180, -0.7880, -5.9268],\n",
      "        [-0.6890,  3.1533, -5.2825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1185, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6160,  3.1096, -5.1509],\n",
      "        [ 1.4574,  0.4494, -6.4932],\n",
      "        [-0.8429,  3.0713, -4.7707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1089, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1279,  3.5016, -5.1322],\n",
      "        [-1.1054,  3.2367, -4.9538],\n",
      "        [ 0.5710,  1.6087, -6.4853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1170,  3.3935, -5.1251],\n",
      "        [-0.8554,  3.3289, -5.1122],\n",
      "        [-0.8560,  3.3754, -5.3013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0164, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8639,  3.2224, -5.2408],\n",
      "        [ 2.9840, -1.3997, -4.9716],\n",
      "        [-0.6953,  3.2355, -4.9434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4685, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7048,  0.5310, -6.4180],\n",
      "        [ 0.8533,  0.9245, -5.7135],\n",
      "        [ 1.5513,  0.8589, -6.6693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2054,  3.5308, -5.2266],\n",
      "        [-0.9559,  3.3246, -4.7783],\n",
      "        [-0.7785,  3.0683, -5.2197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5340, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0554,  3.4865, -5.1723],\n",
      "        [-1.0168,  3.4651, -5.4151],\n",
      "        [ 2.0173, -0.2617, -5.4993]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8331,  2.8098, -5.0950],\n",
      "        [-1.0409,  3.3105, -4.8620],\n",
      "        [ 3.2089, -1.4223, -4.1598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3217, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5033,  2.9664, -5.8964],\n",
      "        [ 1.6154,  0.1442, -6.3749],\n",
      "        [ 0.9967,  1.0636, -6.5350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7112,  3.2875, -4.5722],\n",
      "        [-0.9257,  3.3597, -5.0949],\n",
      "        [-0.7388,  3.0428, -4.7626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1662, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9591,  3.0359, -5.0765],\n",
      "        [-0.7662,  3.1129, -5.2263],\n",
      "        [ 1.4240,  0.8836, -6.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0555, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7672,  3.2393, -5.2805],\n",
      "        [-1.0348,  3.2234, -5.1276],\n",
      "        [ 2.0821,  0.1388, -6.4611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0367, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6661,  2.9033, -5.1582],\n",
      "        [ 3.0846, -1.5785, -4.6215],\n",
      "        [ 2.2970, -0.3006, -6.0350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8705,  2.9819, -4.8441],\n",
      "        [ 3.0003, -1.4119, -4.7349],\n",
      "        [-0.8242,  2.9589, -5.2129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9863, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3934,  2.4754, -5.7126],\n",
      "        [-0.8873,  3.1718, -4.8689],\n",
      "        [-0.9814,  3.0923, -4.9434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9655, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1307, -1.5241, -4.6885],\n",
      "        [-0.3162,  2.4951, -5.4179],\n",
      "        [ 2.9597, -1.1418, -5.0411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0428,  2.3219, -5.8208],\n",
      "        [-0.4539,  3.0619, -4.7796],\n",
      "        [ 2.8406, -1.1728, -5.2690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1830, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2416,  2.6059, -5.1273],\n",
      "        [ 1.6509,  0.5206, -6.5448],\n",
      "        [ 1.8171,  0.3737, -6.4458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0395, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6031,  2.8389, -4.8868],\n",
      "        [ 2.5016, -0.4087, -6.1209],\n",
      "        [ 2.7663, -0.6206, -5.5218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0498, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4957,  2.3426, -5.4149],\n",
      "        [-0.4059,  2.6819, -4.8029],\n",
      "        [-0.4235,  2.6195, -4.9853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3636, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4480,  2.7244, -4.9884],\n",
      "        [ 0.8341,  1.3664, -5.9516],\n",
      "        [-0.2200,  2.6666, -5.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0233, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3186, -1.8501, -4.3455],\n",
      "        [-0.3333,  2.4854, -5.0490],\n",
      "        [ 3.4532, -1.8897, -4.3791]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0160, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1968, -1.9877, -4.4442],\n",
      "        [ 2.6243, -0.6465, -5.5893],\n",
      "        [ 3.6031, -1.9077, -4.2393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7673, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8679, -0.3121, -6.3024],\n",
      "        [ 3.3584, -1.6254, -4.7090],\n",
      "        [ 3.1481, -1.8163, -4.6285]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3709, -1.4338, -4.8231],\n",
      "        [ 3.1040, -1.5783, -4.4858],\n",
      "        [ 3.4497, -1.7504, -4.4700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0811, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4272, -1.9457, -4.6949],\n",
      "        [ 0.4010,  1.9323, -5.3480],\n",
      "        [-0.3555,  2.7995, -5.2853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9396, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1776, -1.2456, -5.2826],\n",
      "        [ 2.3269, -0.4068, -6.0492],\n",
      "        [ 3.1391, -1.5055, -4.9145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0805, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0061, -1.2679, -5.3273],\n",
      "        [ 3.3622, -1.6247, -4.8454],\n",
      "        [ 0.3688,  1.7710, -5.7022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0654, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1631,  2.2954, -5.3311],\n",
      "        [ 2.9460, -1.1445, -5.5897],\n",
      "        [ 0.0422,  2.3320, -5.2655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3324,  2.4570, -5.2995],\n",
      "        [-0.1823,  2.7216, -5.1486],\n",
      "        [ 3.2486, -1.7822, -4.6014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6509, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1733, -1.4896, -4.4829],\n",
      "        [ 2.0068,  0.2263, -6.6101],\n",
      "        [ 3.4138, -1.6822, -4.3681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9284, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4401,  2.7482, -5.1148],\n",
      "        [ 2.2288, -0.4399, -6.2647],\n",
      "        [ 3.2081, -1.6128, -4.9227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1602, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4760, -1.3437, -5.6175],\n",
      "        [ 2.2620, -0.5511, -5.5308],\n",
      "        [ 0.5504,  1.2621, -5.9937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7056, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4482,  2.8598, -5.3905],\n",
      "        [ 3.2530, -1.2927, -5.0572],\n",
      "        [ 1.8408, -0.0936, -6.0434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2329, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0375,  0.0202, -6.3007],\n",
      "        [ 1.6018,  0.2720, -6.3170],\n",
      "        [ 1.9528,  0.0881, -6.4978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3794, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7420,  3.1557, -4.9619],\n",
      "        [ 1.0714,  1.4176, -6.3605],\n",
      "        [ 1.6618,  0.3397, -6.3457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2664, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8112,  1.1574, -6.4253],\n",
      "        [ 3.1732, -1.4623, -4.7197],\n",
      "        [ 1.8901,  0.6483, -6.4325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1015, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8480,  0.2336, -6.2988],\n",
      "        [ 2.7450, -1.1026, -5.5334],\n",
      "        [ 0.0253,  2.2658, -5.7609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4620, -0.6401, -6.2873],\n",
      "        [-0.4014,  2.9682, -5.1712],\n",
      "        [ 2.8037, -0.8529, -5.5060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0270, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5002,  0.7990, -6.8122],\n",
      "        [ 2.8516, -0.9327, -5.6458],\n",
      "        [ 1.7675, -0.0337, -6.6815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3357, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2830,  1.0685, -6.8279],\n",
      "        [-1.0760,  3.1976, -4.6069],\n",
      "        [ 1.7280,  0.1424, -6.3981]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1607, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6993,  1.3427, -6.7012],\n",
      "        [-0.5416,  3.0593, -5.1755],\n",
      "        [-0.4749,  2.9451, -5.8576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1536, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1406, -0.0083, -6.2305],\n",
      "        [ 0.5838,  1.5555, -6.8535],\n",
      "        [-0.7429,  2.7858, -5.1723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4590, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8673,  1.2497, -6.6037],\n",
      "        [-0.3900,  2.9754, -5.2313],\n",
      "        [ 1.1897,  0.9465, -6.8783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3328, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3073,  1.8728, -6.2794],\n",
      "        [ 2.3732, -0.2002, -5.5865],\n",
      "        [ 1.0768,  0.9964, -6.4385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2523, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8797,  3.2119, -5.1707],\n",
      "        [ 1.5473,  1.0036, -6.4698],\n",
      "        [ 1.5333,  0.4107, -6.5545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0196, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9749, -1.2447, -4.5413],\n",
      "        [-0.7382,  3.0042, -5.0787],\n",
      "        [-0.7783,  3.1342, -5.2454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1865, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2936,  2.3080, -6.3344],\n",
      "        [-0.8955,  3.1594, -5.4048],\n",
      "        [ 0.7481,  1.2584, -6.6865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0856, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7216, -0.6286, -5.4337],\n",
      "        [-1.0361,  3.3971, -4.9364],\n",
      "        [ 0.2998,  1.7537, -6.8323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2373, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8812,  3.2355, -5.2414],\n",
      "        [ 1.1585,  1.1074, -6.6989],\n",
      "        [ 2.8927, -0.6965, -5.5419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0260, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9253,  3.1704, -5.4073],\n",
      "        [-0.9239,  3.5496, -4.8667],\n",
      "        [ 2.4666, -0.5121, -5.8084]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4425, -0.1286, -5.7215],\n",
      "        [-0.6006,  2.9064, -5.5424],\n",
      "        [-0.6512,  3.2104, -5.6899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0952, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8666,  3.1354, -4.8974],\n",
      "        [ 1.9991,  0.4684, -6.6528],\n",
      "        [-0.2371,  2.3702, -6.4686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1903, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4429,  2.8533, -5.7283],\n",
      "        [ 1.2447,  0.8712, -6.3736],\n",
      "        [-1.2264,  3.3576, -4.8707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4786, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7984,  3.3474, -5.4939],\n",
      "        [ 0.6060,  1.7074, -6.5865],\n",
      "        [-0.6082,  2.8392, -5.6781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4526, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0364,  3.3112, -4.7675],\n",
      "        [ 1.3767,  0.8365, -6.6466],\n",
      "        [ 0.9156,  1.2685, -6.7099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3051, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8848,  3.3504, -4.6231],\n",
      "        [-1.1044,  3.1297, -4.3755],\n",
      "        [ 0.9356,  1.2891, -6.6716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6148, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9353,  3.2709, -5.4206],\n",
      "        [ 1.1613,  1.3077, -6.2102],\n",
      "        [ 0.6127,  1.2474, -6.8130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6868, -1.9202, -2.4411],\n",
      "        [-1.1552,  3.3028, -4.9212],\n",
      "        [ 2.8550, -1.8482, -2.6341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1127, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6523,  2.5977, -6.4074],\n",
      "        [ 2.6722, -0.5915, -4.6198],\n",
      "        [-1.0162,  3.4468, -5.0850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2144, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6857, -0.4701, -5.6666],\n",
      "        [ 1.3321,  1.1192, -6.5455],\n",
      "        [ 3.2075, -1.6560, -3.7129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8497, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2756,  2.4784, -6.1174],\n",
      "        [ 2.8287, -1.8084, -2.8532],\n",
      "        [ 0.0289,  2.4148, -6.2195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2957,  3.5130, -4.6582],\n",
      "        [ 2.7744, -1.7982, -2.2800],\n",
      "        [ 2.0358,  0.2892, -5.9375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6935, -0.5944, -4.6777],\n",
      "        [ 3.2937, -1.9260, -4.0249],\n",
      "        [-1.0700,  3.3721, -4.6167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4683, -1.6373, -2.1840],\n",
      "        [ 3.1053, -1.9621, -2.9056],\n",
      "        [ 2.8206, -2.0968, -2.4477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4043,  2.8912, -6.0333],\n",
      "        [ 3.0268, -1.2270, -4.4238],\n",
      "        [ 2.6825, -2.4755, -1.6871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9189, -1.9678, -2.4211],\n",
      "        [-1.1239,  3.2725, -4.9393],\n",
      "        [ 3.0474, -1.9603, -2.7993]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2447, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6897, -0.9936, -4.5147],\n",
      "        [-0.9958,  3.4685, -4.9987],\n",
      "        [ 2.7203, -1.7949, -3.2407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0140, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9658,  3.2070, -5.0837],\n",
      "        [-0.7556,  3.2084, -5.3116],\n",
      "        [ 3.0960, -2.2837, -2.7948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9184, -1.5764, -3.3967],\n",
      "        [-0.9454,  3.3348, -5.1260],\n",
      "        [ 3.1774, -2.1727, -3.0249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8744, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2822, -0.2392, -4.6476],\n",
      "        [ 3.0442, -2.0185, -3.0024],\n",
      "        [ 2.6153, -2.1765, -2.3851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6144, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7400,  1.2337, -6.6088],\n",
      "        [ 0.5410,  1.5501, -6.6092],\n",
      "        [ 1.3542,  0.7268, -6.3176]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7258, -1.8462, -2.8475],\n",
      "        [-0.9907,  3.4413, -4.9338],\n",
      "        [ 2.9054, -1.8941, -3.2925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0211, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6328, -1.6128, -2.3912],\n",
      "        [-0.7990,  3.1519, -5.0705],\n",
      "        [ 2.6260, -1.2948, -2.9558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2000, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3100,  1.0290, -6.5612],\n",
      "        [ 2.8888, -1.6378, -2.9588],\n",
      "        [ 2.5341, -1.3138, -3.3641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8492, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1133,  3.4602, -4.7170],\n",
      "        [ 2.1376, -0.9038, -3.2813],\n",
      "        [ 2.3117, -0.0866, -4.5710]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2008, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3853, -1.3341, -2.5396],\n",
      "        [ 1.4433,  0.6990, -6.5123],\n",
      "        [ 1.9413,  0.3294, -5.3991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0835, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7740,  0.2123, -5.9938],\n",
      "        [ 2.2821, -1.3854, -2.8729],\n",
      "        [ 2.5223, -1.1639, -2.9122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0989, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3002,  1.8616, -6.3157],\n",
      "        [ 2.1927, -0.6652, -3.5246],\n",
      "        [ 2.4639, -0.5770, -5.1325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2929, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9878,  1.3196, -6.7164],\n",
      "        [ 1.8093,  0.8016, -6.4262],\n",
      "        [ 2.5462, -1.1750, -3.4768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9179, -0.5604, -3.6905],\n",
      "        [ 1.6115,  0.9402, -6.0162],\n",
      "        [-1.0574,  3.3519, -4.9698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8212, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0685,  0.8266, -6.7020],\n",
      "        [ 1.5988,  0.6153, -6.5403],\n",
      "        [ 0.5711,  1.9024, -6.6788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8121, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5627,  0.5002, -6.5750],\n",
      "        [ 1.7666, -0.0325, -4.1429],\n",
      "        [ 1.2601,  0.8466, -6.6898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0776, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8667,  0.3583, -4.8690],\n",
      "        [-0.8294,  3.0495, -5.0063],\n",
      "        [-1.0750,  3.4337, -4.9825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2689, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4311,  0.6803, -6.2330],\n",
      "        [ 0.7316,  1.4285, -6.1520],\n",
      "        [-0.9707,  3.2313, -4.9448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0738,  3.4313, -4.3511],\n",
      "        [-0.9351,  3.1196, -5.2919],\n",
      "        [-0.9323,  3.2642, -5.8459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4266, -0.8982, -3.9405],\n",
      "        [-0.2120,  2.4174, -5.9585],\n",
      "        [-0.8902,  3.3744, -5.0674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0499, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9240, -0.5153, -4.2789],\n",
      "        [ 2.3471, -0.6222, -4.4982],\n",
      "        [-1.1790,  3.1904, -4.2992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0574, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1247,  0.2895, -5.0757],\n",
      "        [-1.1314,  3.3835, -4.3110],\n",
      "        [-1.3407,  3.1052, -4.2576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1423, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5658,  0.8560, -6.8138],\n",
      "        [-1.1466,  3.4210, -4.4916],\n",
      "        [-0.8671,  3.2856, -4.3553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0731, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1161, -0.0500, -4.8008],\n",
      "        [ 2.1856, -0.1638, -4.4068],\n",
      "        [ 3.0667, -0.9874, -4.7964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0161, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2405,  3.3285, -4.0606],\n",
      "        [-1.0985,  3.3215, -5.3038],\n",
      "        [ 2.7123, -0.9692, -4.8819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2061, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4591,  1.7228, -6.8150],\n",
      "        [ 1.5337,  0.6920, -6.1963],\n",
      "        [ 3.2126, -1.3655, -5.0986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2840, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3720,  0.4824, -6.5150],\n",
      "        [ 1.3984,  0.8047, -6.4468],\n",
      "        [-0.3246,  2.3365, -6.2354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0293, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9857,  3.4447, -4.7925],\n",
      "        [-1.1213,  3.4318, -4.3550],\n",
      "        [ 2.4996, -0.2064, -5.3539]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4800, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4286,  1.5632, -6.1747],\n",
      "        [-1.3088,  3.1233, -4.4985],\n",
      "        [ 3.1659, -1.1278, -4.9666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1585, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4999,  0.6769, -6.5895],\n",
      "        [ 3.2560, -1.0315, -5.1178],\n",
      "        [ 2.1256, -0.1561, -6.2426]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0308, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4916,  2.8403, -5.7212],\n",
      "        [ 3.0505, -1.3480, -5.3631],\n",
      "        [ 2.6680, -0.4254, -5.3137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1088, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9743, -1.1157, -5.4317],\n",
      "        [ 1.7936,  0.4536, -6.7207],\n",
      "        [-0.0857,  2.4431, -5.7642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4554, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0597, -1.2649, -5.1453],\n",
      "        [-0.7210,  3.2086, -5.5829],\n",
      "        [-1.4314,  3.3672, -4.3343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1563, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4947,  3.4010, -4.1049],\n",
      "        [ 1.2794,  0.7067, -6.7620],\n",
      "        [-1.0349,  3.2786, -5.3023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4822, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1683,  3.6300, -4.6633],\n",
      "        [ 1.6497,  0.5240, -6.5403],\n",
      "        [-0.5123,  2.9424, -5.5223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1619, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8090, -0.7792, -5.9837],\n",
      "        [ 1.0959,  0.5318, -6.5605],\n",
      "        [-1.2558,  3.6486, -4.5916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0273, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3547, -0.4219, -6.3317],\n",
      "        [-1.2697,  3.4705, -4.1578],\n",
      "        [-0.8937,  3.5455, -4.5102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0813, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4582, -0.7145, -5.5779],\n",
      "        [-0.9597,  3.2635, -4.4437],\n",
      "        [ 1.9900,  0.4101, -6.4309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1803,  3.5134, -4.3484],\n",
      "        [-1.5085,  3.6734, -4.0206],\n",
      "        [ 1.4359,  0.5905, -6.7889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4410, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7679,  3.0291, -5.2892],\n",
      "        [ 1.8435,  0.1666, -6.4375],\n",
      "        [ 1.4831,  0.7451, -6.5969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0551, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2372,  3.5236, -4.4255],\n",
      "        [ 2.2501, -0.4145, -5.7324],\n",
      "        [ 1.9920, -0.3894, -5.4282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3419, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0717,  0.7997, -6.4602],\n",
      "        [-1.3001,  3.2628, -4.1474],\n",
      "        [ 0.1905,  1.8410, -5.9210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8385, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1661, -0.0909, -6.3347],\n",
      "        [ 2.3542, -0.5275, -6.4815],\n",
      "        [ 2.1139, -0.0961, -6.6631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1127, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2164, -0.3290, -5.2260],\n",
      "        [ 2.1317,  0.2563, -6.3508],\n",
      "        [ 2.2448,  0.1770, -6.4105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0594, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1847, -0.2838, -5.5780],\n",
      "        [ 2.0333, -0.3897, -5.1836],\n",
      "        [-1.0845,  3.4839, -4.2108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1325, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6970,  0.3783, -6.5387],\n",
      "        [ 2.0358, -0.6714, -4.4523],\n",
      "        [ 2.0320, -0.2855, -6.0397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3813, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9688,  3.4310, -5.2186],\n",
      "        [ 0.9049,  1.5084, -6.6947],\n",
      "        [ 1.9380, -0.4135, -5.1835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2187, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0383, -0.1559, -5.5479],\n",
      "        [-1.6901,  3.5039, -3.9717],\n",
      "        [ 1.0981,  0.7726, -6.8086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0317, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2219,  3.3079, -3.9193],\n",
      "        [-1.4597,  3.2409, -3.7811],\n",
      "        [ 2.0478, -0.5297, -5.3391]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7986, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1500,  2.1271, -6.1554],\n",
      "        [-1.2417,  3.4328, -4.3868],\n",
      "        [-1.4418,  3.1721, -3.5437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0076, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4136,  3.5976, -4.1178],\n",
      "        [-1.5165,  3.3563, -4.1825],\n",
      "        [-1.3923,  3.5316, -4.6650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9550,  3.2934, -5.2727],\n",
      "        [ 2.2702, -0.8618, -4.7248],\n",
      "        [-1.2677,  3.4535, -3.9600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1198, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3542,  3.5163, -4.4396],\n",
      "        [ 1.6115,  0.2497, -6.6524],\n",
      "        [ 2.0083, -0.0263, -6.0516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0204, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4266, -0.8400, -5.6351],\n",
      "        [-1.2008,  3.5092, -4.8714],\n",
      "        [-0.9203,  3.3303, -5.3934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4275, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4206, -1.0323, -1.0339],\n",
      "        [ 2.2686, -0.0583, -5.8809],\n",
      "        [ 1.4340,  0.8453, -6.3439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1478, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4583,  1.5424, -5.2845],\n",
      "        [-0.9804,  3.4627, -5.2080],\n",
      "        [ 0.0202,  1.9224, -6.1294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4329, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3457, -0.6672, -5.2202],\n",
      "        [ 2.5276, -0.5441, -5.7656],\n",
      "        [ 1.4551,  0.6069, -6.9055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0576, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3945, -0.4856, -5.7765],\n",
      "        [ 2.3817, -0.3861, -5.8378],\n",
      "        [-0.3112,  2.5332, -5.7915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0472, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2463,  2.5266, -5.7690],\n",
      "        [-1.3671,  3.2991, -4.5552],\n",
      "        [ 2.2472, -0.3664, -5.6487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1874, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1701,  0.8207, -6.7156],\n",
      "        [-1.1418,  3.4187, -4.7188],\n",
      "        [-0.9365,  3.0922, -5.7899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0554, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9706,  3.0943, -4.5974],\n",
      "        [ 2.3908, -0.2619, -5.9114],\n",
      "        [ 2.2300, -0.2538, -6.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1845, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6849,  0.3296, -6.6155],\n",
      "        [-0.2342,  2.5491, -5.9325],\n",
      "        [ 0.8724,  1.3566, -6.6523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0678, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1768, -0.5020, -5.6348],\n",
      "        [-0.0386,  2.2199, -6.1357],\n",
      "        [ 2.6421, -0.6417, -6.0179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0962, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2004,  2.0123, -6.2493],\n",
      "        [-1.3403,  3.2179, -4.5395],\n",
      "        [ 1.7414,  0.0776, -6.8093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1209, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1835,  2.4753, -6.3558],\n",
      "        [ 1.9109,  0.1601, -6.7142],\n",
      "        [-0.0260,  2.1677, -5.6296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2387,  3.4293, -4.6686],\n",
      "        [ 2.0584, -0.4148, -6.5776],\n",
      "        [ 2.6903, -0.5098, -5.7483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0894, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7495,  0.1126, -6.3864],\n",
      "        [-0.6471,  2.6077, -5.5679],\n",
      "        [ 2.6198, -0.3150, -6.2364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0636, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2328,  2.2621, -5.7946],\n",
      "        [ 2.5610, -0.6284, -5.5147],\n",
      "        [ 2.3102, -0.3074, -6.0871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0496, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7805, -0.2876, -6.2826],\n",
      "        [ 2.5286, -0.3494, -5.9927],\n",
      "        [ 2.5738, -0.4393, -5.7747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2515, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7531,  1.3926, -5.9576],\n",
      "        [ 2.6821, -0.5140, -5.9936],\n",
      "        [ 0.3544,  1.4445, -6.1202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2159,  3.5090, -4.3702],\n",
      "        [ 2.5314, -0.8686, -5.4908],\n",
      "        [-0.8580,  3.1024, -5.1801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0731, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7814, -0.6712, -6.0338],\n",
      "        [ 0.0865,  1.7979, -5.8486],\n",
      "        [ 2.7609, -1.0792, -5.1378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5086, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3151,  0.8129, -6.7419],\n",
      "        [ 2.6819, -0.8518, -5.5495],\n",
      "        [ 0.6488,  1.0292, -6.9690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3112, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4613,  1.5026, -6.2900],\n",
      "        [ 1.2931,  0.7469, -7.1089],\n",
      "        [ 0.1049,  1.7671, -6.3845]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2374, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9675,  1.1941, -6.8467],\n",
      "        [-0.0943,  2.1374, -5.7683],\n",
      "        [ 2.7086, -1.0492, -5.4144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1786, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1057,  3.2563, -4.6991],\n",
      "        [ 0.6905,  1.1411, -6.3560],\n",
      "        [ 2.7221, -0.7994, -5.3603]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0363, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4839,  3.3483, -4.1632],\n",
      "        [ 2.2053, -0.1336, -6.5539],\n",
      "        [-1.5205,  3.3259, -4.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0304, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4049, -0.9421, -5.3509],\n",
      "        [ 2.6462, -0.8659, -5.3709],\n",
      "        [ 2.7953, -0.8317, -5.9582]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8071, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3595,  2.5253, -5.6698],\n",
      "        [-0.0630,  2.1337, -5.9910],\n",
      "        [ 2.1379, -0.0129, -6.4785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1591, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5795,  1.1660, -6.2738],\n",
      "        [-0.4917,  3.0969, -5.4104],\n",
      "        [-1.5790,  3.4623, -3.9308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3408, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9415,  1.2876, -6.7440],\n",
      "        [ 1.8728,  0.5350, -6.6230],\n",
      "        [ 1.5836,  0.3426, -6.7930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0357, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7499,  2.5715, -5.7195],\n",
      "        [ 2.5914, -0.3953, -6.1691],\n",
      "        [ 2.7692, -1.0526, -5.5013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2243, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1677,  2.0484, -6.1691],\n",
      "        [ 2.9006, -0.9483, -5.4280],\n",
      "        [ 1.2811,  0.8718, -6.5028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5056, -1.0445, -5.4830],\n",
      "        [ 2.3066, -0.2851, -6.3755],\n",
      "        [-1.5826,  3.7563, -3.9272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0375, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1606, -0.0976, -6.1649],\n",
      "        [-1.6722,  3.7377, -4.1384],\n",
      "        [-1.3324,  3.5266, -4.8162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6137, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7161, -0.4773, -6.2470],\n",
      "        [ 0.1926,  1.8048, -6.3238],\n",
      "        [-1.6373,  3.4565, -4.2261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8917, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7313,  3.5805, -4.0040],\n",
      "        [ 0.1254,  1.8687, -6.2837],\n",
      "        [ 1.0745,  1.2131, -7.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0377, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5651,  3.6913, -3.6704],\n",
      "        [-0.1440,  2.0929, -5.9714],\n",
      "        [-1.6355,  3.6330, -4.2093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1034, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0396,  2.8395, -5.6762],\n",
      "        [ 2.9533, -0.9509, -5.7157],\n",
      "        [ 1.7369,  0.5628, -6.8215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0135, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7008,  2.8641, -5.5237],\n",
      "        [-1.6531,  3.8987, -4.2894],\n",
      "        [-1.2661,  3.5434, -5.0887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6897, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1540,  2.0381, -5.9246],\n",
      "        [ 2.7297, -1.1567, -5.0519],\n",
      "        [ 2.8573, -0.9423, -5.4037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1816, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3648,  3.4993, -4.8497],\n",
      "        [ 2.9599, -1.3404, -4.8486],\n",
      "        [ 1.1863,  0.8105, -6.6120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9635, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0862, -0.2961, -6.4530],\n",
      "        [ 2.3561, -0.3117, -6.5129],\n",
      "        [ 2.4100, -0.2607, -6.7775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6682,  2.8410, -5.7193],\n",
      "        [-1.3479,  3.1047, -4.8419],\n",
      "        [ 2.8090, -1.0512, -5.3185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4536,  3.6268, -4.3661],\n",
      "        [-1.4522,  3.5346, -4.1071],\n",
      "        [-1.1137,  3.5494, -5.0065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0903, -0.9790, -5.3780],\n",
      "        [-1.5129,  3.6546, -4.3500],\n",
      "        [ 2.9535, -1.5064, -5.6292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1165, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1408,  3.3314, -4.4641],\n",
      "        [ 2.1263, -0.1008, -6.5217],\n",
      "        [ 1.9245,  0.5966, -6.8030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6415,  3.7913, -3.8621],\n",
      "        [ 2.7786, -0.8921, -6.1513],\n",
      "        [-1.3728,  3.8588, -4.5354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0733, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6178,  3.4176, -4.6225],\n",
      "        [ 2.7047, -0.8973, -5.7229],\n",
      "        [ 1.6406,  0.0517, -6.4702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4518,  3.6184, -4.7594],\n",
      "        [ 2.7582, -0.9327, -5.6067],\n",
      "        [-1.6173,  3.6825, -3.4494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7445, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2657,  0.8686, -6.5933],\n",
      "        [ 1.2235,  0.3497, -6.9254],\n",
      "        [ 2.1752, -0.0871, -6.3668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3999, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9500, -1.1276, -5.3097],\n",
      "        [-1.1572,  3.3636, -4.9695],\n",
      "        [ 2.2189, -0.0997, -6.8650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1837, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7832, -1.2800, -5.4856],\n",
      "        [ 1.0686,  1.6001, -6.5910],\n",
      "        [ 2.4110, -0.1941, -6.2771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0660, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0843,  1.8379, -5.9658],\n",
      "        [ 2.6393, -0.3053, -6.0228],\n",
      "        [-1.2728,  3.3697, -5.2621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1687, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3824,  3.7011, -4.0943],\n",
      "        [ 0.8718,  1.3421, -6.4441],\n",
      "        [-0.8118,  3.4742, -4.8733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3105, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0641,  1.5935, -6.5946],\n",
      "        [ 2.2579,  0.1950, -6.9947],\n",
      "        [ 1.3363,  1.1737, -6.8707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3577,  3.5146, -4.0980],\n",
      "        [ 2.2786, -0.2643, -7.0265],\n",
      "        [ 2.5032, -0.7534, -6.1328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0350, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0267,  3.4052, -5.1345],\n",
      "        [ 2.2152, -0.1841, -6.6215],\n",
      "        [-1.6267,  3.6728, -3.3253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5035,  3.6339, -3.2828],\n",
      "        [ 2.3582, -0.4892, -5.8170],\n",
      "        [ 2.5531, -0.7993, -5.6002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2285, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9249,  1.3297, -6.4632],\n",
      "        [ 2.3665, -0.3174, -6.3945],\n",
      "        [ 2.0347, -0.1392, -6.6154]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0747, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8522,  0.1464, -7.0026],\n",
      "        [ 2.4476, -0.4769, -6.2413],\n",
      "        [-1.5312,  3.9166, -3.9085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1993, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0375, -0.0353, -6.7921],\n",
      "        [ 2.7034, -0.4494, -6.2691],\n",
      "        [ 1.4169,  0.8158, -6.6733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0417, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5101, -0.7272, -5.7669],\n",
      "        [-1.5167,  3.9360, -4.4280],\n",
      "        [ 2.2263, -0.2398, -6.0929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0377, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0699, -0.5267, -6.1731],\n",
      "        [-1.6179,  3.8199, -3.6465],\n",
      "        [ 2.7602, -0.5478, -5.7733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0565, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3154, -0.1019, -6.3873],\n",
      "        [ 2.6414, -0.4757, -6.4464],\n",
      "        [ 2.6505, -0.5401, -5.9245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0333, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8642,  2.8928, -5.1094],\n",
      "        [-1.7098,  3.7788, -3.3867],\n",
      "        [ 2.2539, -0.3504, -6.4883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1238, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6547,  3.5393, -3.0374],\n",
      "        [ 1.6156,  0.7645, -6.6116],\n",
      "        [-1.4714,  3.4480, -2.9997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2095, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0361,  0.8810, -7.0078],\n",
      "        [-1.6795,  3.7636, -4.1736],\n",
      "        [-1.7757,  3.6945, -3.2632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3027, -0.5235, -6.8116],\n",
      "        [-1.6610,  3.7289, -4.0603],\n",
      "        [-1.7750,  3.5221, -2.8191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5699, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8726,  3.7801, -2.5460],\n",
      "        [ 1.6444,  0.1910, -6.3467],\n",
      "        [ 2.6178, -0.5658, -6.0108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5434, -0.6180, -6.2892],\n",
      "        [ 2.6660, -0.8197, -5.7154],\n",
      "        [ 2.9216, -0.6387, -6.1495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0899, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8274, -0.0213, -6.7968],\n",
      "        [-0.1528,  2.3685, -6.0286],\n",
      "        [ 2.5660, -0.4964, -6.1761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2177, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9215,  3.5289, -2.5864],\n",
      "        [ 1.0826,  1.2469, -6.6751],\n",
      "        [ 2.7594, -0.6711, -5.5224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1655, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7561,  1.2190, -6.4994],\n",
      "        [-1.7007,  3.8358, -3.2091],\n",
      "        [-1.8644,  3.9993, -3.7662]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5593,  3.9108, -3.7953],\n",
      "        [-1.7045,  3.9729, -4.1484],\n",
      "        [-0.0928,  2.2827, -6.1099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0736, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0759,  1.8524, -6.1682],\n",
      "        [ 2.5421, -0.9613, -5.9626],\n",
      "        [ 2.8598, -0.5008, -6.6628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8110, -1.1444, -5.4598],\n",
      "        [-1.6403,  3.7863, -3.9179],\n",
      "        [-1.6804,  3.8961, -4.1053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0926, -0.6823, -6.1033],\n",
      "        [-1.8652,  3.6171, -3.0058],\n",
      "        [-1.7302,  3.8364, -3.6317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5276, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3414,  0.6703, -6.6616],\n",
      "        [-1.6991,  3.9768, -3.5615],\n",
      "        [ 0.4079,  1.1997, -6.2955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7809,  3.7760, -3.4758],\n",
      "        [-1.2264,  3.4661, -4.8351],\n",
      "        [ 3.0322, -0.7246, -6.4780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1198, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6538, -0.3934, -5.9811],\n",
      "        [ 3.0055, -0.5028, -6.2171],\n",
      "        [ 1.5812,  0.4636, -6.4754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3865, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8423,  3.8472, -3.5154],\n",
      "        [ 0.5419,  1.2598, -5.3988],\n",
      "        [ 2.5515, -0.6622, -6.4029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0526, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9072, -0.1259, -6.3333],\n",
      "        [-1.9506,  3.6600, -2.9727],\n",
      "        [ 2.9346, -0.5791, -6.2264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4136, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4038,  0.5458, -6.5379],\n",
      "        [ 0.7437,  1.0739, -6.6156],\n",
      "        [ 3.0073, -1.1883, -5.6707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8188, -1.1350, -5.8530],\n",
      "        [ 2.8924, -0.1588, -6.3118],\n",
      "        [-1.8586,  3.8537, -3.7438]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0220, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9060, -1.0447, -5.8708],\n",
      "        [ 3.2951, -1.0473, -5.9546],\n",
      "        [ 2.6321, -0.7427, -5.9998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7632,  3.5999, -2.8239],\n",
      "        [ 2.7576, -0.9162, -5.9113],\n",
      "        [-1.8628,  3.7591, -3.8636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1483, -0.8497, -6.0576],\n",
      "        [-2.0138,  3.9008, -3.1549],\n",
      "        [-1.0434,  3.2991, -5.2714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5843, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2126,  0.0845, -6.6708],\n",
      "        [ 3.0617, -0.8891, -5.9410],\n",
      "        [ 1.0740,  1.1207, -6.5036]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0680, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6841,  2.8658, -5.2617],\n",
      "        [ 1.9140,  0.1925, -6.2832],\n",
      "        [-1.1193,  3.4402, -5.4546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0408, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0915,  2.2100, -5.8948],\n",
      "        [-1.5155,  3.9721, -3.7845],\n",
      "        [ 3.0029, -0.7973, -6.1685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0745, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2390,  2.5587, -5.9699],\n",
      "        [ 0.1519,  1.9960, -5.7816],\n",
      "        [ 2.7374, -1.3473, -5.7352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2019, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0378,  1.2397, -6.7307],\n",
      "        [-1.8663,  3.6888, -3.6187],\n",
      "        [-1.5948,  4.1698, -3.2922]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0652, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0110,  2.2513, -5.9560],\n",
      "        [-0.2043,  2.5808, -6.2243],\n",
      "        [ 2.6958, -0.6644, -6.2377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8379, -0.9029, -6.4186],\n",
      "        [-1.5987,  3.9935, -3.3490],\n",
      "        [ 3.1277, -0.8775, -6.1811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1148, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5820, -0.6863, -6.5155],\n",
      "        [ 2.8822, -1.0325, -6.1764],\n",
      "        [ 2.8951, -1.0741, -5.7622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4715, -0.2615, -6.4493],\n",
      "        [-1.4970,  3.9398, -3.0807],\n",
      "        [ 2.8752, -1.3185, -6.0102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8732, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0260, -0.7770, -6.2580],\n",
      "        [ 0.8003,  1.2847, -6.8154],\n",
      "        [ 1.3180,  1.0616, -6.3986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0501, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6292,  3.8778, -3.7298],\n",
      "        [-1.0091,  3.2780, -5.2888],\n",
      "        [ 0.0522,  2.0147, -5.8274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0481, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7603,  3.7412, -3.5306],\n",
      "        [ 2.8178, -0.9278, -6.0423],\n",
      "        [ 2.1187,  0.0229, -6.3237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1190, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5083, -0.4426, -6.1939],\n",
      "        [ 1.6257,  0.5842, -6.7867],\n",
      "        [-1.9656,  4.0307, -3.0865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2922, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8951,  3.9175, -2.9823],\n",
      "        [ 0.0539,  1.5260, -6.1243],\n",
      "        [ 1.2299,  1.1737, -7.0567]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0388, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3007, -0.5445, -6.1205],\n",
      "        [-1.6602,  3.9341, -3.2297],\n",
      "        [ 2.3093, -0.5652, -6.3231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0516, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3150, -0.6242, -6.4135],\n",
      "        [ 2.3416, -0.6298, -5.8866],\n",
      "        [ 2.2141, -0.7060, -6.2202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0741, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7423,  3.8672, -3.6250],\n",
      "        [ 2.4429, -0.5796, -6.2786],\n",
      "        [ 2.0332,  0.3495, -7.0341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0686, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2035, -0.6066, -6.3244],\n",
      "        [ 2.5718, -0.2798, -6.5276],\n",
      "        [ 1.9736, -0.3795, -6.3788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0187, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9959,  4.0061, -3.1654],\n",
      "        [-0.6151,  2.6641, -5.5596],\n",
      "        [-1.0523,  3.1155, -5.4510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0030,  3.5418, -2.7732],\n",
      "        [-1.8846,  3.7681, -3.7450],\n",
      "        [-1.6776,  3.7950, -3.0367]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0459, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5890, -0.3386, -6.3286],\n",
      "        [-1.8807,  3.8951, -2.7304],\n",
      "        [ 2.2712, -0.2026, -6.6049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4313,  3.4464, -4.1643],\n",
      "        [-2.0832,  3.6217, -2.8441],\n",
      "        [-1.5437,  3.9715, -3.2054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0396, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3492, -0.5931, -6.2405],\n",
      "        [-2.0582,  3.7490, -2.5744],\n",
      "        [ 2.2728, -0.4732, -6.3792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0685, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5057, -0.5967, -5.9504],\n",
      "        [-1.7681,  3.8303, -3.3859],\n",
      "        [ 0.1113,  1.8860, -6.2728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0303, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1496, -0.3172, -6.6184],\n",
      "        [-1.7005,  3.8469, -3.3366],\n",
      "        [-1.5727,  3.9332, -3.4661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0761, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1225, -0.1327, -6.4140],\n",
      "        [ 2.4766, -0.4960, -6.4662],\n",
      "        [ 2.2925, -0.2177, -6.3942]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8258, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0438,  2.2381, -6.2387],\n",
      "        [-1.8557,  4.1232, -2.7961],\n",
      "        [ 2.0172, -0.2968, -6.1792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0387, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1336, -0.0290, -6.6979],\n",
      "        [-1.7761,  4.0554, -3.7997],\n",
      "        [-2.0037,  3.9623, -2.7958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0235, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9907,  3.5928, -3.0652],\n",
      "        [ 2.5977, -0.1577, -6.4677],\n",
      "        [-1.8452,  3.8920, -3.4551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0290, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9130,  3.9286, -2.9343],\n",
      "        [-1.0995,  3.5091, -5.0395],\n",
      "        [ 2.3162, -0.2669, -6.7923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0558, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8408, -0.1740, -6.6646],\n",
      "        [ 2.6445, -0.6378, -6.5103],\n",
      "        [-1.8006,  3.7578, -2.8941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0227, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6703,  2.8581, -5.8111],\n",
      "        [ 2.6563, -0.6932, -6.0246],\n",
      "        [-1.7014,  3.7932, -4.0587]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1183, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5266,  0.1309, -6.5313],\n",
      "        [ 2.1115, -0.1330, -6.2619],\n",
      "        [ 2.6053, -0.8143, -6.0173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6621, -0.8516, -6.0491],\n",
      "        [ 2.5082, -0.6092, -6.5289],\n",
      "        [ 2.3737, -0.5232, -6.3142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0279, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7679, -0.6263, -6.2810],\n",
      "        [ 2.6887, -0.3537, -6.3628],\n",
      "        [-1.4462,  4.1478, -4.3440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9440, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6851, -0.8553, -6.2748],\n",
      "        [ 2.3439, -0.0713, -6.5990],\n",
      "        [ 0.2923,  1.3352, -6.1880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9020, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9912, -0.0926, -6.4075],\n",
      "        [ 0.8968,  1.3630, -6.4908],\n",
      "        [-0.9157,  3.1418, -5.2308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5317, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5624,  0.5092, -6.8932],\n",
      "        [ 2.7644, -0.9463, -5.8892],\n",
      "        [-0.3282,  2.8488, -5.6996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6519, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7654,  0.0593, -6.3589],\n",
      "        [ 2.8582, -0.6796, -5.9483],\n",
      "        [ 2.5006, -0.4018, -6.2071]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2516, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2080,  3.3034, -5.1141],\n",
      "        [ 2.1485, -0.2105, -6.4265],\n",
      "        [ 1.5790,  0.6056, -6.9443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1323, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7286,  3.9109, -3.4182],\n",
      "        [ 2.7166, -0.6166, -6.2888],\n",
      "        [ 1.4489,  0.6038, -6.8442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6561, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5384, -0.7269, -6.0968],\n",
      "        [ 2.5230, -0.5190, -6.4470],\n",
      "        [ 1.7864,  0.0677, -6.5621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1274, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3321e+00,  3.7286e+00, -4.6728e+00],\n",
      "        [ 4.0221e-01,  1.5975e+00, -6.4940e+00],\n",
      "        [ 2.1492e+00,  4.6924e-03, -6.3953e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4916, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1550,  1.5589, -6.2030],\n",
      "        [-0.4001,  2.3884, -6.0570],\n",
      "        [ 1.5156,  0.6817, -6.4035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6309, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7235,  0.4816, -6.8846],\n",
      "        [ 0.3378,  1.2929, -6.3343],\n",
      "        [ 2.1237, -0.4895, -6.0294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1609, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2194,  3.5473, -5.1595],\n",
      "        [ 0.5336,  1.5738, -6.2479],\n",
      "        [ 1.9264,  0.2468, -6.3205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0416,  0.2410, -6.8301],\n",
      "        [ 0.6899,  0.9484, -6.6773],\n",
      "        [ 1.7093,  0.4354, -6.8116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8868, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1635,  2.2544, -6.2763],\n",
      "        [ 0.3657,  1.6999, -6.1522],\n",
      "        [ 1.6711,  0.2603, -6.2838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1617, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4554,  3.9380, -4.5150],\n",
      "        [ 1.3983,  0.8503, -7.1137],\n",
      "        [-0.8370,  2.8837, -5.6739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2397, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7223,  0.6879, -6.8542],\n",
      "        [ 1.4604,  0.4875, -6.5520],\n",
      "        [-0.1182,  2.2023, -5.9760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7110, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8678,  0.0227, -6.5813],\n",
      "        [ 2.0361,  0.1071, -6.3694],\n",
      "        [-1.5408,  3.7778, -4.6227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1555, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2575, -0.0749, -6.3729],\n",
      "        [ 0.3074,  1.8144, -6.2218],\n",
      "        [ 1.8631,  0.1974, -6.5886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2440, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6821,  0.4254, -6.5595],\n",
      "        [-0.1156,  2.2811, -6.4474],\n",
      "        [ 0.7193,  1.4476, -6.4132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5321, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4317,  2.5457, -6.1642],\n",
      "        [ 0.2813,  1.5760, -6.3314],\n",
      "        [-1.0972,  3.5985, -5.2526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2342, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0089,  0.5621, -6.8029],\n",
      "        [ 1.9666, -0.2223, -6.1663],\n",
      "        [ 2.2582,  0.0193, -6.4472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3760, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3401,  1.6602, -6.5018],\n",
      "        [ 1.1285,  1.0649, -6.9064],\n",
      "        [ 1.8506,  0.4928, -6.5765]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6622, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7357,  0.2416, -6.5111],\n",
      "        [ 2.1969,  0.1774, -6.9639],\n",
      "        [ 0.0083,  1.7283, -6.0323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1155, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9360, -0.2234, -6.4984],\n",
      "        [ 1.8272,  0.1214, -6.4002],\n",
      "        [-0.3294,  2.2960, -6.1259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3414, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4592,  0.6945, -6.8912],\n",
      "        [ 1.2986,  0.7227, -6.8493],\n",
      "        [ 1.7752,  0.2408, -6.3106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9619, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1300,  0.7506, -6.5373],\n",
      "        [ 0.3337,  1.2678, -6.7058],\n",
      "        [ 0.2220,  1.6623, -6.5067]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1146, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7919,  2.9417, -6.0155],\n",
      "        [-0.7634,  2.8763, -5.9295],\n",
      "        [ 0.2291,  1.3044, -6.3745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1229, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2231, -0.1968, -6.3216],\n",
      "        [-0.3045,  1.9284, -6.1491],\n",
      "        [ 1.9398,  0.3229, -6.6247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0680, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1214, -0.2133, -6.1761],\n",
      "        [-0.1910,  2.3733, -6.2563],\n",
      "        [-0.6716,  2.6104, -5.5132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6796, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1993,  1.5075, -6.1707],\n",
      "        [ 2.4391, -0.5128, -5.7730],\n",
      "        [ 1.4451,  0.8516, -6.7859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1747, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7849,  1.3419, -6.4762],\n",
      "        [-0.6947,  2.6553, -5.8669],\n",
      "        [ 2.4733, -0.8304, -5.8032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0481, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5923, -0.6216, -5.3510],\n",
      "        [ 2.2836, -0.0172, -6.2350],\n",
      "        [-1.1774,  3.5579, -5.2133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0576, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3649,  1.9945, -6.0202],\n",
      "        [ 2.5808, -0.7089, -5.5410],\n",
      "        [ 2.5250, -0.5544, -5.6760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1042, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4687, -1.0204, -5.2153],\n",
      "        [ 1.8573,  0.2539, -6.6328],\n",
      "        [-0.1624,  2.1084, -6.1690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2355, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4614,  2.4574, -5.6151],\n",
      "        [ 1.1961,  0.7572, -6.5484],\n",
      "        [ 2.1315,  0.3490, -6.6502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2194, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4395,  2.0234, -5.6727],\n",
      "        [ 1.2422,  0.9353, -6.3400],\n",
      "        [ 2.5737, -1.1498, -5.0315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1003, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3340,  1.5079, -6.5934],\n",
      "        [ 2.4460, -1.2578, -3.9727],\n",
      "        [-1.4400,  3.7935, -4.7601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0426, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6767,  3.7774, -4.3545],\n",
      "        [ 2.2240, -0.4403, -5.9396],\n",
      "        [-0.5608,  2.3071, -5.9650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6843,  2.4577, -5.7903],\n",
      "        [ 2.7041, -1.0884, -4.5197],\n",
      "        [ 2.8714, -1.1672, -4.7568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3951, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1580,  1.9311, -6.1544],\n",
      "        [-0.2149,  2.3368, -5.6493],\n",
      "        [ 0.4839,  0.9488, -6.3596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1470, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1891,  1.9753, -6.3252],\n",
      "        [ 0.3389,  1.5115, -6.7020],\n",
      "        [ 2.7744, -1.3975, -4.3637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1167, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5378, -1.3700, -3.6757],\n",
      "        [ 2.7500, -1.3433, -3.7729],\n",
      "        [ 0.6138,  1.6259, -6.8664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0762, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3941,  2.5943, -6.1963],\n",
      "        [-0.7905,  3.0559, -5.4706],\n",
      "        [ 0.1171,  1.8825, -6.6092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0739, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1681,  1.7069, -6.5274],\n",
      "        [ 2.8392, -1.3971, -3.9518],\n",
      "        [-1.0222,  3.4605, -5.1165]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0222, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0249,  3.0727, -5.4377],\n",
      "        [ 2.5599, -1.1199, -4.6221],\n",
      "        [ 2.4773, -1.2832, -4.2155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1341, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0192, -0.0749, -6.4856],\n",
      "        [ 0.2676,  1.4249, -6.7290],\n",
      "        [ 2.9912, -1.4846, -3.8892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6077, -1.7191, -3.2995],\n",
      "        [ 2.6091, -0.7025, -6.0875],\n",
      "        [-1.9503,  3.8721, -3.4977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0506,  3.7540, -2.4626],\n",
      "        [ 3.0171, -1.5404, -3.6786],\n",
      "        [ 2.8195, -1.6319, -3.5900]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0445, -1.3788, -5.1963],\n",
      "        [ 2.7613, -1.1066, -4.6079],\n",
      "        [-1.8211,  3.9889, -3.4819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9284, -1.5902, -3.5438],\n",
      "        [-1.2425,  3.5645, -4.4173],\n",
      "        [-1.3395,  3.3298, -4.9647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0658, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8914, -1.6581, -3.3405],\n",
      "        [ 2.7343, -2.0232, -3.1617],\n",
      "        [ 1.8581,  0.1935, -6.4875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9335,  3.6836, -2.3467],\n",
      "        [ 2.9117, -1.7473, -3.3560],\n",
      "        [ 2.9443, -1.2376, -4.9959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0072, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7698,  3.6285, -4.2038],\n",
      "        [-1.7746,  3.9251, -3.5973],\n",
      "        [ 2.8629, -1.5665, -4.0784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8203,  3.6964, -3.8419],\n",
      "        [ 2.8940, -1.8046, -3.0621],\n",
      "        [ 2.7272, -0.6434, -5.6978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1791, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7842,  1.1707, -6.4769],\n",
      "        [-2.0445,  3.5530, -1.7185],\n",
      "        [-1.3178,  3.3279, -4.9289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0013, -1.8191, -3.6280],\n",
      "        [ 3.1104, -2.0428, -3.7300],\n",
      "        [ 2.7951, -1.6659, -2.6977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7646, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8886, -2.0297, -3.6579],\n",
      "        [ 3.3537, -1.9173, -4.2682],\n",
      "        [ 3.0423, -1.8804, -3.6150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9430, -1.9508, -3.7891],\n",
      "        [-1.5463,  3.9869, -3.8332],\n",
      "        [ 2.8964, -1.7668, -4.1479]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0166, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7227, -1.3115, -5.2673],\n",
      "        [ 2.8883, -0.9196, -5.4725],\n",
      "        [-1.9889,  3.5936, -1.5098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0219, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8001,  2.9328, -5.0482],\n",
      "        [ 2.6925, -1.3039, -4.2091],\n",
      "        [-0.8307,  2.9548, -5.4184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6857, -0.4568, -6.2529],\n",
      "        [ 2.7140, -1.6899, -3.5952],\n",
      "        [-0.9503,  3.0947, -5.1123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8518, -1.4893, -4.5073],\n",
      "        [ 2.7851, -1.4874, -4.1310],\n",
      "        [ 2.6824, -1.8555, -4.1692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9023, -1.2968, -4.5261],\n",
      "        [-1.7084,  3.8685, -4.0716],\n",
      "        [ 2.7897, -1.3725, -4.4344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0062, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9674,  3.8204, -2.4301],\n",
      "        [-1.6107,  3.9426, -3.2322],\n",
      "        [-1.1478,  3.5625, -5.1068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0422, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8934,  4.0363, -3.1112],\n",
      "        [ 2.5001, -1.2707, -4.0185],\n",
      "        [-0.4775,  1.7891, -6.2003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7105,  3.9685, -3.8868],\n",
      "        [-1.9762,  3.8402, -3.5104],\n",
      "        [ 2.8079, -1.3362, -4.5225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0229, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6772, -1.4938, -3.9348],\n",
      "        [ 2.8305, -1.2239, -4.6645],\n",
      "        [ 2.6025, -0.7557, -5.8451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8385, -1.2805, -4.5788],\n",
      "        [ 2.7507, -1.5566, -4.5236],\n",
      "        [-2.0438,  4.0818, -3.3825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9061, -1.2511, -4.9121],\n",
      "        [-1.9909,  4.1366, -3.3377],\n",
      "        [ 2.9312, -1.0364, -4.8536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0072, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8764, -1.3580, -4.5948],\n",
      "        [-1.9410,  4.0618, -3.4350],\n",
      "        [-1.9380,  3.9889, -3.0493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0088, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8388, -1.1668, -5.2109],\n",
      "        [-1.7673,  3.8834, -3.8478],\n",
      "        [-1.9518,  3.9451, -2.7130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0433,  4.1514, -3.0110],\n",
      "        [-1.8027,  4.0295, -3.5836],\n",
      "        [ 2.8343, -1.0499, -4.9911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9548,  4.0511, -3.4373],\n",
      "        [ 2.9700, -1.1736, -5.0365],\n",
      "        [ 2.7065, -1.5273, -4.7292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0479, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5740,  2.4664, -5.8968],\n",
      "        [ 2.7077, -0.8818, -4.6195],\n",
      "        [ 2.7628, -0.7823, -6.2466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0547, -1.2561, -4.7681],\n",
      "        [-2.1701,  4.1459, -3.3725],\n",
      "        [-1.9180,  3.8930, -3.4054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2530, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4272,  1.6733, -6.4206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 2/3, Loss_per_epoch: 243.49595302413218\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0102, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8605, -1.5483, -4.3930],\n",
      "        [ 2.8611, -1.3545, -5.2263],\n",
      "        [-1.8559,  4.1459, -3.9330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7345, -1.0760, -4.8734],\n",
      "        [-1.2337,  3.3396, -4.9832],\n",
      "        [ 2.9412, -1.4202, -5.0522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9889, -1.1790, -4.9629],\n",
      "        [-1.7742,  3.8336, -3.7724],\n",
      "        [ 2.9998, -1.4827, -4.9605]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8463, -1.2471, -5.1197],\n",
      "        [-1.8181,  4.0338, -3.6817],\n",
      "        [ 2.8067, -1.2735, -4.8861]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9180, -1.4757, -4.4576],\n",
      "        [-1.7824,  3.9387, -3.5318],\n",
      "        [-1.7971,  4.0491, -4.2742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7718, -1.3263, -5.5055],\n",
      "        [-2.0320,  4.1926, -3.6343],\n",
      "        [ 3.0090, -1.3263, -4.8808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8475, -1.6353, -4.9394],\n",
      "        [-1.8494,  4.0079, -4.3183],\n",
      "        [-1.8368,  4.2384, -4.2880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9074,  4.1249, -3.6338],\n",
      "        [ 2.8610, -1.5215, -4.8149],\n",
      "        [ 3.1175, -1.3637, -5.1418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2082,  0.1416, -6.5969],\n",
      "        [-1.8001,  4.2206, -3.5376],\n",
      "        [-1.5655,  3.8298, -4.7054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7036,  4.0929, -4.5971],\n",
      "        [-1.6987,  4.1797, -4.5258],\n",
      "        [ 2.8887, -1.6257, -5.0072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9669, -1.6765, -5.1141],\n",
      "        [ 0.3757,  1.7510, -6.8279],\n",
      "        [ 3.1680, -1.4574, -4.9170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8667,  3.8300, -3.9239],\n",
      "        [ 2.8996, -1.1915, -5.5504],\n",
      "        [ 2.8594, -1.7463, -4.5305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8867, -1.5330, -4.9115],\n",
      "        [ 2.9341, -1.6849, -4.9769],\n",
      "        [ 2.7617, -1.3938, -4.8726]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4231,  3.7427, -4.7514],\n",
      "        [-1.6827,  3.9757, -4.2234],\n",
      "        [ 3.0690, -1.1384, -5.1948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1957, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5786,  3.8455, -4.4344],\n",
      "        [ 1.3271,  1.0652, -7.1514],\n",
      "        [ 3.0866, -1.4072, -4.7126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0141, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3584,  3.4709, -5.0283],\n",
      "        [-0.9779,  3.0185, -5.2305],\n",
      "        [-1.0937,  3.0744, -5.3760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3429, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3447,  0.7818, -7.0294],\n",
      "        [ 2.7441, -1.6641, -4.8376],\n",
      "        [-2.0293,  4.2582, -3.7676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1904, -1.4007, -5.4038],\n",
      "        [-0.3522,  2.6702, -5.6112],\n",
      "        [ 3.0389, -1.6666, -5.0216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0068, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3726,  3.6520, -4.7345],\n",
      "        [-1.6582,  4.0244, -4.0138],\n",
      "        [-1.3523,  3.2713, -5.2072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0794, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0870,  1.7573, -5.9301],\n",
      "        [-0.0818,  2.3621, -6.4492],\n",
      "        [ 3.1019, -1.8241, -4.7678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7483,  4.1863, -3.8779],\n",
      "        [-1.8196,  4.0160, -3.8415],\n",
      "        [ 3.3145, -1.5620, -4.7877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4549, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5372,  1.5901, -6.5265],\n",
      "        [-1.8060,  4.2311, -3.8204],\n",
      "        [ 2.9352, -1.7773, -4.5410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4231, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9682, -1.1877, -5.7102],\n",
      "        [ 3.0904, -1.5881, -4.4202],\n",
      "        [ 0.0499,  2.4408, -6.2248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0076, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0442,  3.1618, -4.8016],\n",
      "        [-1.6897,  4.2784, -3.9982],\n",
      "        [-1.5271,  3.8239, -4.8339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0347, -1.5760, -5.1472],\n",
      "        [ 3.1267, -1.7612, -5.0079],\n",
      "        [-1.5386,  3.7728, -4.4835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0063, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7269,  3.9522, -4.1828],\n",
      "        [-1.8156,  4.1498, -4.1150],\n",
      "        [ 3.0307, -1.3667, -5.5642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2070,  3.3356, -5.2886],\n",
      "        [-1.4445,  3.5298, -5.0411],\n",
      "        [ 2.8398, -1.4330, -5.6055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9701, -1.2104, -5.8454],\n",
      "        [ 3.0358, -1.3596, -5.1190],\n",
      "        [-1.8912,  3.9862, -3.4840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1730, -1.3707, -5.2735],\n",
      "        [ 3.0407, -1.5854, -5.0536],\n",
      "        [-1.8567,  4.1120, -3.7598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6192,  2.9251, -5.3589],\n",
      "        [ 2.9518, -0.7759, -6.1394],\n",
      "        [-1.2499,  3.2740, -4.8169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0426,  4.2547, -3.0299],\n",
      "        [ 2.7391, -1.4723, -5.3043],\n",
      "        [ 2.8706, -1.2983, -5.3604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7966, -1.5454, -5.2456],\n",
      "        [-1.7101,  3.8501, -4.4729],\n",
      "        [-1.9724,  3.9723, -3.2429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5298,  3.6652, -5.0218],\n",
      "        [-2.0244,  3.8969, -2.8042],\n",
      "        [ 2.8076, -1.5570, -5.1577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7559,  3.9857, -4.1822],\n",
      "        [-1.9878,  4.0423, -3.2817],\n",
      "        [ 2.9895, -1.4875, -5.3124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0781, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8521, -1.1961, -5.2906],\n",
      "        [ 0.0519,  1.7404, -6.2962],\n",
      "        [ 2.6101, -0.4279, -6.2712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1069, -0.2853, -6.5012],\n",
      "        [ 2.8724, -1.4549, -5.2065],\n",
      "        [-2.0439,  3.9239, -2.9600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1034, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5730,  0.4697, -6.4144],\n",
      "        [-2.0320,  4.2691, -3.2761],\n",
      "        [ 2.7024, -1.1620, -5.5842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9440,  3.8918, -3.5372],\n",
      "        [ 2.8677, -1.2783, -5.5130],\n",
      "        [ 2.8346, -1.4976, -5.3115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4926,  2.7188, -5.8142],\n",
      "        [-1.7927,  4.0218, -2.9652],\n",
      "        [ 3.0629, -1.3577, -5.3998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2227, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0240,  0.9475, -6.0907],\n",
      "        [-1.9931,  4.1570, -3.0255],\n",
      "        [ 3.1042, -1.6163, -5.3422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0758,  3.9531, -2.9918],\n",
      "        [ 1.9901, -0.2146, -6.2584],\n",
      "        [ 2.8520, -1.4751, -5.0608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0178, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9169,  4.0196, -3.7618],\n",
      "        [ 2.5873, -0.4523, -6.4522],\n",
      "        [-1.8444,  4.1162, -3.0456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8063,  4.1844, -4.2118],\n",
      "        [ 2.9125, -1.6601, -5.0921],\n",
      "        [-1.5413,  3.8312, -4.0023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8903, -1.4196, -5.2763],\n",
      "        [-1.7879,  4.0467, -3.7144],\n",
      "        [ 2.9561, -1.7523, -5.1412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6821, -1.0855, -6.2330],\n",
      "        [-1.2907,  3.4438, -5.1482],\n",
      "        [-1.5558,  3.9077, -4.5293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8390,  2.7994, -5.6753],\n",
      "        [-2.1405,  4.2568, -3.2662],\n",
      "        [-1.9690,  4.1822, -3.4642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3242, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1636,  0.6876, -6.0676],\n",
      "        [ 3.0410, -1.5599, -5.0348],\n",
      "        [-1.9593,  4.2124, -3.1764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2571, -1.7798, -5.3402],\n",
      "        [ 3.1886, -1.6220, -5.2579],\n",
      "        [-0.3730,  2.6476, -5.8357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0875,  4.2955, -3.2725],\n",
      "        [-1.8229,  4.1026, -3.1764],\n",
      "        [-0.4565,  2.7258, -6.1007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2823, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0521, -1.5046, -5.1776],\n",
      "        [ 3.0794, -1.5961, -4.9997],\n",
      "        [ 1.0291,  0.7782, -6.5761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9345, -1.6394, -5.1532],\n",
      "        [ 2.9856, -1.7854, -5.4452],\n",
      "        [ 2.7856, -1.1128, -6.0949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6138, -1.1130, -5.7341],\n",
      "        [ 3.0247, -1.5150, -4.9092],\n",
      "        [ 3.0889, -1.6422, -4.9714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0825, -1.6531, -5.1905],\n",
      "        [ 2.9296, -0.9271, -5.5898],\n",
      "        [-1.9268,  4.0552, -2.6786]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2477, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9650,  0.8951, -6.8166],\n",
      "        [-1.8493,  4.2001, -3.0220],\n",
      "        [ 3.0656, -1.4499, -5.1232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4216, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9331,  3.9831, -3.1674],\n",
      "        [ 1.2691,  0.3567, -5.9438],\n",
      "        [ 2.9121, -1.6431, -5.2032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0196, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7950,  4.0555, -3.7696],\n",
      "        [ 2.3573, -0.5637, -6.4028],\n",
      "        [-1.7529,  4.2055, -3.7707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0704,  4.1802, -2.3913],\n",
      "        [ 3.1002, -1.8245, -5.2879],\n",
      "        [-2.0620,  4.1224, -3.3609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9315, -1.9897, -5.0785],\n",
      "        [ 2.7531, -1.7176, -4.4369],\n",
      "        [ 3.0475, -1.5480, -5.1674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1597, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1152,  1.7882, -6.4620],\n",
      "        [ 3.0149, -1.6229, -5.1699],\n",
      "        [ 1.5146,  0.4511, -6.2497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6089, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0488,  4.5163, -3.2119],\n",
      "        [ 2.8449, -1.6984, -4.9295],\n",
      "        [ 0.2180,  1.8535, -6.2968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1555,  4.2478, -2.8479],\n",
      "        [ 3.0572, -1.8403, -4.8101],\n",
      "        [-2.0067,  4.0032, -2.2575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0023, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7386,  4.1824, -3.6988],\n",
      "        [-2.0480,  4.3886, -3.4564],\n",
      "        [-2.1952,  4.4707, -2.8513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0313, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1996,  2.3096, -6.1955],\n",
      "        [ 3.0988, -1.6317, -4.9676],\n",
      "        [-1.4655,  3.5974, -4.9183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7166, -0.9192, -6.2864],\n",
      "        [-2.0328,  4.0807, -3.5556],\n",
      "        [-2.0880,  4.2922, -2.7551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0599, -2.0689, -4.4026],\n",
      "        [ 2.9095, -2.1879, -3.9045],\n",
      "        [-0.2710,  2.4395, -6.0907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8677, -1.9908, -4.4843],\n",
      "        [-1.1131,  3.1745, -5.7149],\n",
      "        [ 2.9861, -1.6203, -5.0409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6059, -1.7929, -5.1986],\n",
      "        [ 2.9641, -2.1404, -3.9785],\n",
      "        [ 3.1279, -1.8194, -5.1956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0019, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0515,  4.3107, -2.8213],\n",
      "        [-2.0877,  4.6466, -3.8928],\n",
      "        [-1.8854,  4.5981, -4.4328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1406, -2.2266, -4.3240],\n",
      "        [-1.7001,  4.1625, -4.7558],\n",
      "        [ 3.0109, -1.7213, -4.6951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0206, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1445, -0.9790, -5.7192],\n",
      "        [ 2.7659, -0.4360, -6.4209],\n",
      "        [ 3.2891, -1.9239, -4.8470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0033, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0331,  4.3295, -3.5425],\n",
      "        [-2.0374,  4.3256, -3.8206],\n",
      "        [ 3.1574, -2.0955, -4.2308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0941, -1.9420, -4.2088],\n",
      "        [ 3.2360, -1.6908, -5.1060],\n",
      "        [ 2.8584, -1.2081, -5.9584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1755, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1501, -2.3603, -4.0096],\n",
      "        [ 0.6908,  1.0750, -6.4188],\n",
      "        [-2.0096,  4.3707, -4.1141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2002,  4.3502, -3.3567],\n",
      "        [ 3.0534, -2.1008, -4.3674],\n",
      "        [-2.1606,  4.2781, -2.7472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8044,  4.1219, -4.5105],\n",
      "        [ 2.9729, -1.9091, -4.0196],\n",
      "        [-1.7545,  4.1191, -4.3482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0317, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1522,  3.3810, -5.5144],\n",
      "        [-0.2342,  2.2982, -6.3005],\n",
      "        [ 3.2888, -1.6452, -4.7258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7038,  2.3528, -5.7571],\n",
      "        [-2.3515,  4.3094, -3.0875],\n",
      "        [ 3.0569, -2.2089, -4.2465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2240, -1.9002, -4.5391],\n",
      "        [-0.7236,  2.4499, -5.9608],\n",
      "        [ 3.1444, -2.3475, -3.6625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1622, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1042,  4.4607, -3.6783],\n",
      "        [ 0.8416,  1.3193, -6.3531],\n",
      "        [-2.1458,  4.3431, -3.3240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0220, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3569, -0.4356, -5.9536],\n",
      "        [-1.7317,  4.2862, -4.5319],\n",
      "        [-1.7549,  3.9300, -4.3875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.9800, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2484,  4.4770, -3.0823],\n",
      "        [ 3.1344, -2.7951, -2.3036],\n",
      "        [-2.0999,  4.3588, -4.2170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1516, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1852,  0.6072, -6.0718],\n",
      "        [ 3.2960, -2.0086, -3.9113],\n",
      "        [-2.0577,  4.2111, -2.2794]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9621, -1.8669, -4.0933],\n",
      "        [-1.8399,  4.2025, -4.3770],\n",
      "        [-2.0142,  4.1861, -3.0080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1548, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7311, -2.1426, -2.7897],\n",
      "        [ 1.3143,  0.7420, -6.4714],\n",
      "        [-1.4466,  3.8667, -4.8081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9842,  4.2794, -3.5650],\n",
      "        [ 2.5044, -1.9556, -3.1644],\n",
      "        [-1.2247,  3.3952, -4.9770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0396, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8136,  4.2490, -3.1837],\n",
      "        [ 2.0129, -2.1542, -1.3791],\n",
      "        [ 2.3228, -0.3335, -6.0975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0150, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2652, -1.9141, -2.1364],\n",
      "        [ 2.5363, -2.0382, -2.5740],\n",
      "        [-1.9617,  4.6125, -3.7039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5757, -1.5680, -3.1601],\n",
      "        [-1.9214,  4.1316, -3.7661],\n",
      "        [ 2.4239, -1.7163, -2.7758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0194, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4315, -1.7478, -2.9811],\n",
      "        [-1.8513,  4.1201, -4.0664],\n",
      "        [ 2.1631, -1.5595, -2.2491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0228, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2942, -1.5210, -2.8590],\n",
      "        [-1.8420,  4.1164, -2.9254],\n",
      "        [ 2.0129, -1.5745, -2.5320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9934,  4.1371, -3.3842],\n",
      "        [-2.0257,  4.0760, -2.4537],\n",
      "        [ 2.8329, -1.4506, -4.8437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0625,  4.1284, -3.8663],\n",
      "        [ 2.9337, -1.2502, -4.6439],\n",
      "        [ 2.7601, -1.3469, -4.0884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5589, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4307, -0.4143, -6.3847],\n",
      "        [ 1.4535,  0.0780, -6.3885],\n",
      "        [ 2.9006, -1.0753, -5.0965]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1399,  3.2334, -5.6279],\n",
      "        [-2.0380,  4.3124, -3.7147],\n",
      "        [ 2.4880, -1.3829, -3.8905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9812,  4.4809, -3.2265],\n",
      "        [ 2.5826, -1.3949, -4.0825],\n",
      "        [-1.0127,  3.2716, -5.6318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0115, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0702, -1.3537, -4.7058],\n",
      "        [-1.9806,  4.5033, -3.1014],\n",
      "        [ 2.7806, -1.1478, -4.5262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0238, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4432, -0.8169, -6.2083],\n",
      "        [ 2.9268, -1.0453, -5.8612],\n",
      "        [ 2.9039, -1.3221, -5.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0857, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2579,  2.4056, -5.8874],\n",
      "        [-2.0404,  4.1806, -3.5567],\n",
      "        [ 1.7901,  0.2083, -6.5034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0027, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0705,  4.0187, -3.1555],\n",
      "        [-1.9186,  4.1987, -3.4453],\n",
      "        [-1.7477,  4.3969, -4.4228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9063, -1.0986, -5.9780],\n",
      "        [ 2.9378, -1.1479, -5.3068],\n",
      "        [-2.0177,  4.2041, -3.4046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2543, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9272,  4.4460, -3.5740],\n",
      "        [ 2.9316, -1.2760, -5.0933],\n",
      "        [ 0.8435,  0.9452, -6.1987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4033,  3.3350, -4.9832],\n",
      "        [ 2.8169, -0.8380, -4.8634],\n",
      "        [-1.9018,  4.2660, -3.5353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8581, -1.3598, -5.2920],\n",
      "        [-1.9229,  4.1598, -2.5714],\n",
      "        [ 3.0870, -1.0592, -6.0148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0766, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8016,  0.1408, -6.3530],\n",
      "        [ 3.0140, -0.9431, -5.9829],\n",
      "        [ 2.6124, -0.6814, -5.9304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1246,  4.2053, -4.0037],\n",
      "        [ 2.7421, -1.3739, -5.1080],\n",
      "        [-1.9870,  4.3995, -3.6590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0224, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0473, -1.2544, -5.5509],\n",
      "        [ 3.0809, -1.1997, -5.4804],\n",
      "        [-0.4835,  2.7272, -5.8028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0092, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0424, -1.1780, -5.7794],\n",
      "        [-2.1938,  4.0865, -3.3208],\n",
      "        [ 3.0652, -1.5169, -5.7166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0138, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1488,  4.0480, -3.3513],\n",
      "        [ 2.9868, -1.1300, -5.4771],\n",
      "        [ 2.8068, -0.9840, -6.0589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8386, -1.1220, -5.3115],\n",
      "        [-0.8641,  2.8999, -5.5961],\n",
      "        [-2.1385,  4.3158, -3.6702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0237, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8537,  4.3732, -3.6233],\n",
      "        [ 2.4610, -0.2155, -6.7700],\n",
      "        [-1.9709,  4.3776, -3.8344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6520, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8793,  0.0992, -6.5189],\n",
      "        [ 2.9463, -1.1006, -5.3810],\n",
      "        [-2.1148,  4.2666, -3.0416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0729,  4.4719, -3.5020],\n",
      "        [ 2.8256, -1.1994, -5.6308],\n",
      "        [-2.0455,  4.3509, -3.2296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1418, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3667,  0.6034, -6.6896],\n",
      "        [ 2.8885, -0.9940, -6.3686],\n",
      "        [ 2.6561, -1.1482, -6.2000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0083, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8369,  4.0919, -4.2074],\n",
      "        [ 2.9573, -0.9812, -5.7519],\n",
      "        [-1.8347,  4.2716, -3.8696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0521, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1036,  0.2299, -6.1675],\n",
      "        [-2.1832,  4.4871, -3.6312],\n",
      "        [ 2.9762, -1.4825, -5.4033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0523, -1.3431, -5.4936],\n",
      "        [ 3.1101, -1.4715, -5.4768],\n",
      "        [ 2.8476, -1.1196, -5.9034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0769, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8691, -1.4001, -5.2793],\n",
      "        [-1.6064,  3.5396, -5.0797],\n",
      "        [ 1.5557,  0.1025, -6.3350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1557,  4.3156, -3.0237],\n",
      "        [-2.1938,  4.5377, -3.0811],\n",
      "        [ 2.0128, -0.0428, -6.1895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6379, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1445,  1.8769, -6.1122],\n",
      "        [ 2.8142, -1.3982, -5.6588],\n",
      "        [-1.8523,  4.1764, -3.0733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8927, -1.7306, -5.5278],\n",
      "        [ 2.9123, -1.4321, -5.4889],\n",
      "        [-1.8502,  4.4509, -4.1125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7656, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0249, -0.1382, -6.1871],\n",
      "        [ 3.0129, -1.4699, -5.7486],\n",
      "        [ 2.9682, -1.3732, -5.7529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3471, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9041,  3.0913, -5.3606],\n",
      "        [ 1.1537,  0.5970, -5.8396],\n",
      "        [ 3.1197, -1.2632, -5.4550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0882, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1402,  4.1507, -3.1303],\n",
      "        [ 3.1130, -1.3908, -5.4779],\n",
      "        [ 0.2253,  1.4830, -5.8271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0844, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3058,  1.6362, -5.9146],\n",
      "        [ 3.1399, -1.4255, -5.4849],\n",
      "        [ 3.2131, -1.6807, -5.1002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0065, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3747,  3.8507, -5.1833],\n",
      "        [-1.2509,  3.1834, -5.3162],\n",
      "        [-2.0077,  4.3903, -3.4193]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0589, -1.2109, -5.4863],\n",
      "        [ 3.3780, -1.6831, -5.4153],\n",
      "        [-1.6914,  3.8479, -4.6785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9981,  4.2731, -4.1212],\n",
      "        [-1.6139,  3.7659, -4.9645],\n",
      "        [ 2.7470, -1.2491, -5.2407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2831, -1.5648, -5.3767],\n",
      "        [ 3.0982, -1.5434, -5.4360],\n",
      "        [-1.4046,  3.6636, -4.8617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3462, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5046,  3.7592, -4.9535],\n",
      "        [-1.6892,  3.9291, -4.7860],\n",
      "        [ 2.8536, -1.1579, -5.7238]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0568, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4332,  3.6597, -5.0691],\n",
      "        [ 3.0523, -1.2394, -5.7208],\n",
      "        [ 0.0426,  1.8621, -6.2234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5176,  2.7669, -5.5302],\n",
      "        [ 2.9502, -1.5069, -5.6810],\n",
      "        [ 2.9017, -1.3149, -5.5854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9175, -0.8669, -5.8946],\n",
      "        [-1.9386,  4.3988, -3.7676],\n",
      "        [-2.0389,  4.2937, -3.6271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6094, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5114,  3.7002, -4.7053],\n",
      "        [ 1.7581,  0.1484, -6.2143],\n",
      "        [ 2.6652, -0.8268, -6.1520]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1427, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3987,  1.1915, -4.8179],\n",
      "        [ 2.7481, -1.1688, -5.8827],\n",
      "        [ 2.8093, -0.5832, -5.9046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0257, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5741, -0.3818, -6.3358],\n",
      "        [-1.4537,  3.7809, -5.2581],\n",
      "        [-0.9927,  2.8829, -5.9044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1816, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6205,  0.3853, -5.6463],\n",
      "        [ 2.4493, -0.7010, -6.4379],\n",
      "        [ 0.1228,  1.4002, -5.2650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3786, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6400, -0.6656, -6.2989],\n",
      "        [-1.7156,  4.0733, -4.3770],\n",
      "        [ 0.4667,  1.1553, -5.5054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6880, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6513, -0.4011, -6.3456],\n",
      "        [ 2.8345, -0.5739, -6.2304],\n",
      "        [-0.0866,  1.7499, -5.5263]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5040, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5382,  0.6023, -6.3086],\n",
      "        [ 0.4532,  1.2132, -4.9622],\n",
      "        [ 2.6301, -0.6892, -6.1811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5643, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5518,  1.4142, -5.5620],\n",
      "        [ 1.6063,  0.5141, -5.6228],\n",
      "        [ 2.0220,  0.4448, -6.8292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1861, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8760, -0.7551, -6.1781],\n",
      "        [ 0.6999,  1.1168, -5.2488],\n",
      "        [-0.8332,  2.8635, -5.8309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5856, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6003,  2.4553, -5.9776],\n",
      "        [ 1.5733,  0.5079, -6.3809],\n",
      "        [ 0.3650,  1.5000, -5.7018]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0726, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0298,  1.8183, -5.9940],\n",
      "        [-0.9610,  3.1076, -5.5616],\n",
      "        [ 2.5697, -0.3265, -6.1972]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8592, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2076,  2.0402, -6.3017],\n",
      "        [-0.2431,  2.1978, -6.0547],\n",
      "        [ 2.0971, -0.1470, -6.3711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0307, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8629, -0.7002, -6.2469],\n",
      "        [ 2.3238, -0.4457, -6.1982],\n",
      "        [-1.9604,  3.9854, -3.8133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0414, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4881, -0.6694, -6.6413],\n",
      "        [-0.1755,  2.3339, -5.9315],\n",
      "        [-1.5865,  4.0324, -3.8653]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0595,  3.8638, -3.4796],\n",
      "        [ 2.8962, -0.8183, -6.2870],\n",
      "        [ 2.8913, -0.9612, -6.0545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3062, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9267,  0.7017, -5.7723],\n",
      "        [-0.5535,  2.0733, -6.5817],\n",
      "        [ 2.6099, -0.6983, -6.2116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2167, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4100, -0.4355, -6.5768],\n",
      "        [-1.0111,  3.4175, -5.2620],\n",
      "        [ 0.8046,  0.5645, -5.2964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1031, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1920, -0.4029, -6.3875],\n",
      "        [ 0.1329,  1.6698, -6.2570],\n",
      "        [ 2.5691, -0.5834, -6.4967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2144, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4956,  0.2839, -6.4985],\n",
      "        [ 2.5081, -0.1367, -6.6192],\n",
      "        [ 1.4526,  0.4533, -6.4501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1739, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4148, -0.2353, -6.3085],\n",
      "        [ 2.5163, -0.5750, -6.7947],\n",
      "        [ 0.9463,  0.2609, -5.7789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2026, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9695,  3.9874, -3.1760],\n",
      "        [-1.8672,  3.8787, -3.6469],\n",
      "        [ 0.8433,  0.6476, -5.8506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7120, -0.5722, -6.1494],\n",
      "        [-1.8050,  4.1116, -2.6049],\n",
      "        [-1.2177,  3.2560, -5.4410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7437,  3.9582, -3.4202],\n",
      "        [ 2.2865, -0.4871, -6.1926],\n",
      "        [-1.9334,  4.1332, -3.7109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6522, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8031,  0.0770, -6.3567],\n",
      "        [ 2.8747, -0.5282, -6.0883],\n",
      "        [ 2.5840, -0.8004, -5.7351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7433,  4.1914, -3.5926],\n",
      "        [ 2.7313, -0.9765, -6.5834],\n",
      "        [ 3.0749, -1.2501, -5.8503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0236, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7518,  3.9754, -2.6117],\n",
      "        [ 2.8979, -1.1520, -6.2018],\n",
      "        [ 2.4798, -0.5189, -6.1217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0388, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0227,  3.9614, -2.7813],\n",
      "        [-1.8412,  4.2890, -3.4168],\n",
      "        [ 2.0176, -0.1339, -6.6299]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6860, -0.8038, -6.6161],\n",
      "        [-1.9123,  4.1798, -3.5887],\n",
      "        [-2.0564,  4.0905, -3.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0031, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7764,  4.0546, -3.8103],\n",
      "        [-1.8562,  4.1992, -3.3088],\n",
      "        [-1.9104,  4.1746, -3.0288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0673, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1030,  1.7757, -6.3656],\n",
      "        [ 2.6708, -0.9374, -6.2309],\n",
      "        [-1.9570,  4.1595, -3.4212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0827,  4.0270, -3.2517],\n",
      "        [ 2.5496, -0.7836, -6.3997],\n",
      "        [-1.8107,  4.3158, -3.3720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4591, -0.7064, -6.4373],\n",
      "        [-1.8882,  4.1466, -2.8787],\n",
      "        [ 2.7826, -0.7911, -6.4418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0619, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0727,  4.2420, -3.5388],\n",
      "        [ 1.6170, -0.0354, -6.7201],\n",
      "        [-1.2758,  3.5962, -4.5289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3577, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9451,  0.9375, -6.4251],\n",
      "        [ 2.3253, -0.6956, -6.3372],\n",
      "        [ 1.4355,  0.4879, -6.2647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0386, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1366, -0.1893, -6.2842],\n",
      "        [ 2.9896, -0.8932, -6.1198],\n",
      "        [-1.9566,  4.4006, -4.1553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7326, -1.0776, -6.0961],\n",
      "        [-1.9649,  4.0257, -3.0417],\n",
      "        [-1.9148,  4.2002, -3.2826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1339, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8939, -0.9951, -5.8961],\n",
      "        [ 1.0703,  0.2956, -6.2190],\n",
      "        [-2.1256,  4.4101, -3.3785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7885, -0.7337, -6.0082],\n",
      "        [ 2.8341, -1.3221, -5.9388],\n",
      "        [-1.9061,  4.2350, -4.0640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0031, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8723,  4.2833, -4.4210],\n",
      "        [-1.8820,  4.3508, -4.1066],\n",
      "        [-1.5731,  3.8645, -3.9817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0107, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8951,  3.9954, -2.7651],\n",
      "        [ 2.6890, -1.3889, -6.2819],\n",
      "        [ 3.0626, -1.4343, -5.9989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9230, -1.4088, -5.8099],\n",
      "        [-1.8706,  4.1499, -4.2987],\n",
      "        [ 2.8570, -1.1844, -5.5761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3450, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9155,  4.1646, -3.0284],\n",
      "        [-2.0474,  4.1108, -3.8389],\n",
      "        [ 0.7573,  1.3443, -6.5374]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8081, -1.0540, -6.2982],\n",
      "        [-1.8589,  4.1551, -3.9834],\n",
      "        [ 2.5713, -1.1563, -6.4214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9976, -1.0532, -5.9776],\n",
      "        [-1.9694,  4.2219, -3.7066],\n",
      "        [-1.7525,  4.2609, -4.1056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0486, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0869,  2.1500, -6.2886],\n",
      "        [ 2.6581, -0.8679, -6.2919],\n",
      "        [ 3.1129, -1.0781, -6.0928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1295,  4.1498, -3.5863],\n",
      "        [ 2.9554, -1.0395, -6.1926],\n",
      "        [-1.9912,  4.3639, -4.2496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3184, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7155,  4.3479, -3.9760],\n",
      "        [ 3.0842, -1.4030, -5.8747],\n",
      "        [ 1.1507,  0.7047, -6.0740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0043, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2906, -1.4848, -5.6115],\n",
      "        [-1.8396,  4.4585, -3.8278],\n",
      "        [-1.9771,  4.3430, -3.2701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3012, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8211,  1.1956, -6.8914],\n",
      "        [-1.9541,  4.3124, -4.0982],\n",
      "        [-1.9121,  3.9392, -3.4656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0110, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0830,  4.0782, -2.1266],\n",
      "        [ 2.7482, -1.4615, -5.4699],\n",
      "        [ 2.9403, -1.3362, -5.9385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0381, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2898, -1.5582, -5.6114],\n",
      "        [ 3.2167, -1.6746, -5.6416],\n",
      "        [ 2.2487, -0.0168, -6.5338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0210, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9810, -1.3000, -6.0280],\n",
      "        [ 2.6085, -0.6440, -5.9121],\n",
      "        [ 2.9959, -1.5230, -6.1065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1646, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7108,  4.2585, -3.9317],\n",
      "        [ 3.1679, -1.6720, -5.6197],\n",
      "        [ 0.8414,  1.3192, -6.1625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7476,  4.2952, -3.8724],\n",
      "        [-1.8137,  4.2836, -4.3616],\n",
      "        [-0.9263,  3.0504, -5.3864]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2414, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0025, -1.3644, -5.6324],\n",
      "        [ 3.0543, -1.7644, -5.8251],\n",
      "        [ 0.8360,  0.8172, -6.1455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0070, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1450, -1.6290, -5.8751],\n",
      "        [ 3.0929, -1.5013, -5.3311],\n",
      "        [-2.0002,  4.2582, -3.7621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0308, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6364, -1.1295, -5.9754],\n",
      "        [-1.9941,  4.2787, -3.7198],\n",
      "        [ 2.1858, -0.4857, -6.2905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9648, -1.4870, -5.5379],\n",
      "        [-2.0253,  4.1003, -3.8883],\n",
      "        [-1.3656,  3.8468, -4.4012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9020,  4.0501, -4.1779],\n",
      "        [-2.0444,  4.3640, -4.0168],\n",
      "        [-1.0644,  3.2364, -5.0956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0044, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7850,  4.3544, -3.8003],\n",
      "        [-2.1101,  4.3360, -3.4806],\n",
      "        [ 3.3380, -1.4074, -5.9645]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7239, -1.5941, -5.1043],\n",
      "        [-1.9618,  4.4090, -4.0922],\n",
      "        [-1.9991,  4.4278, -3.5804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0145, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7881, -0.9969, -6.1178],\n",
      "        [-1.9476,  4.1315, -3.7599],\n",
      "        [ 2.8248, -1.1846, -5.7669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0092, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0506, -1.4333, -5.7928],\n",
      "        [-1.9703,  4.6090, -3.6124],\n",
      "        [ 3.0432, -1.1906, -6.0066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0843, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7460,  0.1062, -6.2323],\n",
      "        [ 3.1299, -1.4593, -5.6223],\n",
      "        [ 2.4273, -0.2767, -6.9549]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0323, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8709, -0.4692, -6.1758],\n",
      "        [-2.0757,  4.6307, -3.4862],\n",
      "        [-1.9846,  4.1650, -2.7930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0022, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8849,  4.5445, -3.9746],\n",
      "        [-1.7967,  4.3377, -3.6873],\n",
      "        [-1.9838,  4.2631, -3.8262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0673, -1.2120, -6.0784],\n",
      "        [-1.9774,  4.1584, -2.1810],\n",
      "        [-2.2746,  4.3783, -2.5134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0719,  4.4186, -2.5078],\n",
      "        [-2.0914,  4.2691, -2.6336],\n",
      "        [ 3.1376, -1.5720, -5.0975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2615, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9088,  4.4403, -3.9401],\n",
      "        [ 0.8591,  0.7049, -5.9661],\n",
      "        [ 3.2137, -1.5254, -6.1649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0022, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0662,  4.3747, -3.1754],\n",
      "        [-1.8839,  4.5626, -2.7850],\n",
      "        [-1.9343,  4.3529, -3.9291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0022, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0599,  4.0885, -3.7125],\n",
      "        [-2.1662,  4.4736, -2.4111],\n",
      "        [-2.3107,  4.3422, -3.4713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0040, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8988,  4.5077, -3.7887],\n",
      "        [-2.0841,  4.6660, -3.6800],\n",
      "        [ 2.9994, -1.7599, -5.0915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0196, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8568, -0.9583, -6.4435],\n",
      "        [ 3.4869, -1.7455, -5.4931],\n",
      "        [ 2.6153, -0.8363, -6.4162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0341, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0082,  4.4220, -4.3128],\n",
      "        [-2.0583,  4.4806, -3.8755],\n",
      "        [ 1.9352, -0.3315, -6.6398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3421, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1909, -1.5298, -5.2078],\n",
      "        [ 1.9608,  0.1833, -7.0142],\n",
      "        [ 0.5046,  0.8155, -6.4472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2266, -1.7167, -5.4262],\n",
      "        [ 3.2870, -1.5758, -5.3009],\n",
      "        [ 2.7823, -1.1092, -5.9293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0074, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0833, -1.9022, -5.2996],\n",
      "        [-1.8731,  4.4025, -3.9034],\n",
      "        [ 3.0152, -1.3308, -5.8229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2730, -1.7630, -5.5194],\n",
      "        [ 2.9380, -1.8148, -5.1186],\n",
      "        [-2.0874,  4.3571, -3.1094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0050, -1.6696, -5.3513],\n",
      "        [-1.7405,  4.1778, -3.7981],\n",
      "        [-1.9867,  4.3869, -3.4124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0046, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1736,  4.4013, -3.4452],\n",
      "        [-2.2099,  4.1917, -3.9668],\n",
      "        [ 3.0844, -1.5403, -5.2314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0426, -1.4967, -5.4960],\n",
      "        [ 3.1550, -1.7200, -5.5579],\n",
      "        [-1.9847,  4.6066, -4.0854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0041, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1511,  4.2996, -3.5875],\n",
      "        [-2.0860,  4.4721, -3.2810],\n",
      "        [ 3.0814, -1.7319, -5.0487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4937, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6425,  0.4234, -6.6435],\n",
      "        [-1.9922,  4.5967, -4.2287],\n",
      "        [-2.3246,  4.5154, -3.8987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2886, -1.7609, -4.5797],\n",
      "        [-2.1219,  4.4366, -2.8169],\n",
      "        [-1.3908,  3.7503, -4.7220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1932, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9223,  0.6637, -5.8893],\n",
      "        [ 3.3180, -1.9772, -5.4562],\n",
      "        [-2.1991,  4.5442, -3.4073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0052, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9606,  4.2313, -4.2986],\n",
      "        [ 2.8879, -1.5658, -6.0670],\n",
      "        [-2.1983,  4.5492, -2.8623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1164, -1.8604, -4.6986],\n",
      "        [ 3.0594, -1.7763, -5.1740],\n",
      "        [ 3.2790, -1.7832, -5.6467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2064, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0594,  2.2083, -6.6549],\n",
      "        [ 3.4529, -1.7419, -4.7950],\n",
      "        [ 0.7644,  1.1610, -6.0732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6678,  4.0312, -4.1895],\n",
      "        [-2.3139,  4.2544, -3.2373],\n",
      "        [ 3.1388, -1.8339, -5.4233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0046, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1394, -1.9890, -4.8155],\n",
      "        [ 3.3936, -1.9699, -4.9134],\n",
      "        [-2.1560,  4.3020, -2.6466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2585, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9925,  0.8435, -5.9675],\n",
      "        [-1.5290,  4.2817, -4.3631],\n",
      "        [-2.1237,  4.6996, -3.1694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0092, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0498, -1.2666, -5.8379],\n",
      "        [ 3.0571, -1.9647, -5.2015],\n",
      "        [ 3.0431, -1.9186, -4.9416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1655,  4.3938, -2.7153],\n",
      "        [ 3.0324, -2.0031, -4.9134],\n",
      "        [ 2.9153, -1.0418, -6.2323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1185, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3905,  0.5107, -6.2401],\n",
      "        [ 3.1377, -1.9490, -4.8113],\n",
      "        [-2.1027,  4.5075, -3.6088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9891, -1.9307, -4.0823],\n",
      "        [-2.1416,  4.8672, -4.1621],\n",
      "        [-1.8407,  4.7617, -3.5754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9415,  4.0161, -5.1782],\n",
      "        [ 3.3612, -1.9532, -4.8382],\n",
      "        [ 3.3676, -2.0060, -5.0446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7254, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4138, -1.6669, -5.3784],\n",
      "        [ 1.7806, -0.2647, -6.0014],\n",
      "        [-2.2867,  4.2011, -2.5005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0046, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0589,  4.3173, -2.5003],\n",
      "        [ 3.1575, -2.1448, -5.0818],\n",
      "        [ 3.2721, -1.9169, -5.2833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0029, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1482,  4.4711, -4.1149],\n",
      "        [-2.2760,  4.5403, -3.7514],\n",
      "        [ 3.0812, -2.1154, -4.6419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0037, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2152,  4.4308, -2.8750],\n",
      "        [ 3.1272, -1.7381, -5.0820],\n",
      "        [-2.1304,  4.7533, -3.9995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8201,  4.4920, -3.9371],\n",
      "        [ 2.9370, -1.6891, -4.8609],\n",
      "        [ 3.2662, -1.9741, -5.3203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0064, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3254, -2.1242, -4.7808],\n",
      "        [-0.9709,  3.4018, -5.0477],\n",
      "        [-1.9460,  4.4708, -4.3465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2735, -1.6097, -4.8439],\n",
      "        [ 3.1391, -1.8578, -4.6289],\n",
      "        [-2.0505,  4.4079, -3.6399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0588,  4.3404, -3.5458],\n",
      "        [ 3.0904, -1.3351, -5.1202],\n",
      "        [-2.2281,  4.7029, -3.8735]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0015, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1682,  4.6792, -3.5274],\n",
      "        [-2.1699,  4.4242, -3.4829],\n",
      "        [-2.0490,  4.5878, -3.6315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0014, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0388,  4.4559, -3.6489],\n",
      "        [-2.1740,  4.7032, -3.7896],\n",
      "        [-2.2202,  4.6820, -3.6725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0336, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2960,  4.6068, -4.1651],\n",
      "        [ 3.1825, -1.9644, -4.7094],\n",
      "        [-0.0829,  2.2440, -5.7970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4003, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0713, -1.0706, -6.3750],\n",
      "        [ 3.0657, -2.0053, -4.7399],\n",
      "        [ 2.4877, -0.8125, -6.1663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0026, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2796, -2.0839, -4.8922],\n",
      "        [-2.0302,  4.5572, -3.7594],\n",
      "        [-2.1204,  4.5998, -4.2015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0267, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9658, -1.3653, -5.7186],\n",
      "        [-2.1772,  4.4165, -3.7858],\n",
      "        [ 2.2929, -0.4078, -6.0545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0927, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5094,  0.3169, -6.1254],\n",
      "        [ 2.8314, -1.6478, -5.6685],\n",
      "        [-2.0386,  4.7990, -3.6421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1183, -2.0982, -5.0670],\n",
      "        [ 2.8745, -1.1487, -6.2970],\n",
      "        [ 2.9804, -1.6659, -5.5935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0039, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2745,  4.8198, -3.5591],\n",
      "        [-1.8890,  4.5371, -4.1021],\n",
      "        [ 3.2231, -1.5124, -5.2865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0544, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8121, -1.8928, -5.3660],\n",
      "        [-1.9196,  4.1956, -4.2015],\n",
      "        [ 1.8704,  0.0569, -6.1628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6594, -0.8448, -6.4276],\n",
      "        [ 2.3925, -0.7464, -6.1111],\n",
      "        [-1.9948,  4.2990, -2.3736]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4112, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1433,  2.1157, -6.3088],\n",
      "        [-2.1517,  4.5052, -3.2773],\n",
      "        [-1.0072,  3.0774, -5.6841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3896, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1711,  4.4503, -3.4071],\n",
      "        [ 0.4812,  1.2628, -6.2069],\n",
      "        [ 3.0946, -1.7360, -5.2310]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0790, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9753, -1.3838, -5.8693],\n",
      "        [ 1.7046,  0.3147, -6.2642],\n",
      "        [-2.2153,  4.4347, -4.0105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3177, -1.6938, -5.4213],\n",
      "        [ 2.8018, -1.3793, -5.9037],\n",
      "        [ 2.9850, -1.4896, -5.7321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1032, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7146, -1.3282, -5.7327],\n",
      "        [ 0.4062,  1.4968, -6.1203],\n",
      "        [-1.8968,  4.4382, -4.3713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9942, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4877, -0.4242, -6.5707],\n",
      "        [ 3.0148, -1.6619, -5.8366],\n",
      "        [ 3.1792, -1.6164, -5.5494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8560, -1.1856, -6.3464],\n",
      "        [-0.1827,  2.2493, -6.3226],\n",
      "        [ 2.8701, -0.9178, -6.5098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0701, -1.6332, -5.4821],\n",
      "        [ 3.0908, -1.6957, -5.2809],\n",
      "        [ 3.1999, -1.5350, -5.7397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4047, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4654, -0.7269, -6.7241],\n",
      "        [ 2.7406, -1.3031, -5.8689],\n",
      "        [ 1.3568,  0.5789, -6.2350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4490, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8323,  1.0356, -6.5385],\n",
      "        [ 0.7014,  0.8423, -6.1576],\n",
      "        [ 0.1248,  2.1465, -6.4493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1159, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3458,  1.3740, -6.5651],\n",
      "        [-1.6353,  4.1105, -5.1994],\n",
      "        [-0.5701,  2.6753, -6.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1383, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3065,  1.4524, -6.3107],\n",
      "        [-0.1255,  2.2691, -6.3871],\n",
      "        [ 2.5599, -0.3898, -6.2531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0346, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4001,  3.5640, -5.4787],\n",
      "        [ 2.6514, -0.5180, -6.7233],\n",
      "        [-0.4441,  2.4251, -6.3902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5651, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9948, -1.0682, -5.8712],\n",
      "        [ 1.8438,  0.3777, -6.4454],\n",
      "        [-1.5388,  3.9584, -5.7288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0373, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9332, -1.4509, -5.6974],\n",
      "        [ 2.6613, -1.0564, -5.9284],\n",
      "        [-0.2639,  2.2888, -6.2914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1678, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3767,  1.3241, -6.6048],\n",
      "        [-0.8232,  3.0359, -6.1745],\n",
      "        [ 2.0404,  0.2499, -6.7210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0438, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7421e+00,  4.2517e+00, -5.0971e+00],\n",
      "        [ 2.9326e+00, -1.3401e+00, -5.8283e+00],\n",
      "        [ 2.1018e+00, -6.0763e-03, -6.6215e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5859, -0.6514, -6.2106],\n",
      "        [ 2.5646, -0.6649, -6.4079],\n",
      "        [-0.5881,  2.5250, -6.6405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1133, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9393, -0.2670, -6.3974],\n",
      "        [ 0.3551,  1.7068, -6.3149],\n",
      "        [-1.6867,  3.6932, -5.2757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5594, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2478,  3.2956, -5.6222],\n",
      "        [ 2.2266, -0.5612, -6.2775],\n",
      "        [ 0.3080,  1.6914, -6.0662]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4595, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6219,  4.0075, -5.1975],\n",
      "        [-0.6150,  2.7347, -6.2141],\n",
      "        [ 0.5887,  1.6252, -6.2756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1164, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5108,  3.9978, -4.9116],\n",
      "        [-0.0067,  1.8394, -6.6096],\n",
      "        [ 1.6519,  0.1336, -6.2309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0554, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1436, -0.4827, -6.2452],\n",
      "        [ 2.4299, -0.6144, -6.4634],\n",
      "        [-0.5722,  2.4139, -6.4306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0144, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3812,  4.1928, -5.2434],\n",
      "        [-0.7843,  2.5475, -5.9501],\n",
      "        [-1.6699,  3.8724, -4.9799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4937,  2.5029, -6.6036],\n",
      "        [-1.6474,  4.3534, -5.1280],\n",
      "        [-1.7269,  4.2324, -4.8574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2077, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6096,  0.9510, -5.7456],\n",
      "        [ 2.8695, -1.0784, -6.2677],\n",
      "        [-0.3951,  2.2861, -6.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9993,  4.2405, -4.7024],\n",
      "        [-1.6120,  4.1795, -4.9976],\n",
      "        [ 2.7588, -1.5889, -5.7564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0465, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8290,  4.2937, -4.9255],\n",
      "        [-0.0825,  1.9762, -6.4226],\n",
      "        [-0.9642,  3.1182, -5.7266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9094,  3.3494, -5.7436],\n",
      "        [ 2.6494, -1.3111, -6.1157],\n",
      "        [ 2.9182, -1.3859, -5.3937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6443, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9645,  4.1636, -4.7351],\n",
      "        [ 0.4745,  1.2097, -5.9609],\n",
      "        [ 0.3064,  1.6026, -6.0214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1883, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8852, -0.2611, -6.1076],\n",
      "        [ 2.8067, -1.1094, -5.7604],\n",
      "        [ 0.5464,  1.1575, -5.7449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0653, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8409,  0.2744, -6.2945],\n",
      "        [-1.7521,  4.2683, -4.8823],\n",
      "        [-1.7221,  3.9853, -5.0624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5219, -1.0907, -5.9416],\n",
      "        [ 3.0871, -1.5306, -5.3722],\n",
      "        [ 2.8260, -1.4857, -5.5766]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4420, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4533,  1.4122, -6.3844],\n",
      "        [ 2.4020, -1.0113, -6.2604],\n",
      "        [ 3.0265, -1.6316, -5.2577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0026, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9529,  4.3226, -4.8045],\n",
      "        [-1.7012,  4.1659, -4.8191],\n",
      "        [-1.7421,  4.1544, -4.7811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0881, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8813, -1.2402, -5.9986],\n",
      "        [ 0.1581,  1.4354, -6.3069],\n",
      "        [-1.8478,  4.5012, -4.4583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0540, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9420,  4.4951, -4.5647],\n",
      "        [-0.0728,  1.7661, -6.5813],\n",
      "        [ 2.9443, -1.4549, -5.3196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0293, -1.5169, -5.3577],\n",
      "        [ 3.0771, -1.5637, -5.3324],\n",
      "        [ 2.8566, -0.5806, -6.8729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1186, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4400,  1.3488, -6.1417],\n",
      "        [ 3.0829, -1.6712, -5.1987],\n",
      "        [ 2.8699, -2.0029, -4.9599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0789, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5910,  0.0951, -6.2145],\n",
      "        [ 2.6368, -0.7915, -6.0455],\n",
      "        [-1.9033,  4.3494, -4.7961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0032, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7735,  4.0917, -4.8252],\n",
      "        [-1.6391,  4.4352, -5.1003],\n",
      "        [-1.5609,  3.9586, -5.0498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0028, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0477,  4.5883, -4.6821],\n",
      "        [-2.0308,  4.3191, -4.4188],\n",
      "        [ 3.1807, -2.2073, -4.5763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0218, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7429,  3.9193, -4.8861],\n",
      "        [ 2.3672, -0.5447, -6.2982],\n",
      "        [ 3.0129, -1.7485, -5.2183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1489, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2572, -1.6951, -4.9335],\n",
      "        [ 3.0921, -2.1822, -4.9428],\n",
      "        [ 0.7862,  1.3971, -6.3184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1958, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1041,  4.5271, -4.8030],\n",
      "        [ 0.8611,  0.6103, -5.5688],\n",
      "        [ 2.9019, -1.8209, -4.6259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1713, -2.0472, -4.4699],\n",
      "        [-1.3039,  3.9983, -5.3970],\n",
      "        [-2.1800,  4.4196, -4.1760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0027, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2157, -2.0560, -4.7939],\n",
      "        [-2.1495,  4.4950, -4.2848],\n",
      "        [-2.0676,  4.6544, -4.4909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0416, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1816, -1.9714, -4.9935],\n",
      "        [ 0.0734,  2.1615, -6.6035],\n",
      "        [-2.0832,  4.3146, -4.1309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9070,  4.5069, -4.4416],\n",
      "        [ 3.3864, -1.8346, -4.8568],\n",
      "        [ 3.1461, -1.9312, -4.5337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0021, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9557,  4.4806, -4.4506],\n",
      "        [-1.7615,  4.1502, -4.7322],\n",
      "        [-2.0109,  4.4942, -4.4490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9225, -1.9888, -4.7716],\n",
      "        [-0.6160,  2.8501, -6.5060],\n",
      "        [-1.5025,  3.7596, -5.4065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2664, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7076, -2.0750, -4.2091],\n",
      "        [ 0.7699,  0.9431, -6.3854],\n",
      "        [ 3.0043, -2.2065, -4.6273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0025, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3677,  4.4019, -4.9022],\n",
      "        [-1.8555,  4.4307, -4.8261],\n",
      "        [-1.9599,  4.2168, -4.3072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0032, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0196,  4.2649, -4.7968],\n",
      "        [-2.0848,  4.3470, -4.6219],\n",
      "        [ 3.1569, -2.0476, -4.7892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2290, -2.1809, -4.7561],\n",
      "        [ 3.1674, -1.9772, -4.9075],\n",
      "        [ 3.0966, -2.0184, -4.7106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8898, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0621,  4.7540, -4.0775],\n",
      "        [ 2.0949, -0.4989, -6.0701],\n",
      "        [-1.9180,  4.4047, -4.3912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1479, -1.9086, -4.8458],\n",
      "        [ 2.6398, -1.1439, -6.1307],\n",
      "        [-1.8111,  4.1927, -5.4122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2690, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9574,  4.3822, -4.7596],\n",
      "        [ 2.6004, -0.9009, -6.5698],\n",
      "        [-0.8170,  2.9349, -6.3536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0891, -1.3649, -5.1345],\n",
      "        [ 2.8324, -2.0455, -4.5101],\n",
      "        [-0.7652,  3.1331, -5.9573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1074, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8192, -1.2629, -5.6073],\n",
      "        [-2.0346,  4.2765, -4.3448],\n",
      "        [ 1.5233,  0.4851, -6.5697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9614, -2.1924, -4.8842],\n",
      "        [ 2.9633, -1.9345, -4.7806],\n",
      "        [-2.2673,  4.5195, -4.6632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8461,  4.2723, -4.9683],\n",
      "        [-2.0886,  4.4438, -4.6588],\n",
      "        [ 3.0343, -2.0071, -4.5435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8403,  4.4152, -4.6136],\n",
      "        [ 3.0864, -1.6503, -5.0827],\n",
      "        [-2.1112,  4.5191, -4.2910]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0574, -0.4947, -6.1384],\n",
      "        [ 3.0145, -1.2110, -5.8249],\n",
      "        [ 2.4698, -0.8855, -6.2210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1065, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7166,  0.2818, -6.1900],\n",
      "        [-1.2375,  3.3845, -5.6139],\n",
      "        [ 2.2608, -0.0400, -6.1851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2113, -1.9397, -5.1409],\n",
      "        [ 3.4209, -2.1952, -5.0507],\n",
      "        [-1.3728,  3.8275, -5.5889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0799, -2.0213, -4.4727],\n",
      "        [ 3.1425, -1.8005, -5.1720],\n",
      "        [-1.8873,  4.2853, -4.8820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1583, -1.7152, -4.9664],\n",
      "        [-1.4365,  3.9576, -4.9677],\n",
      "        [-1.1595,  3.1600, -5.7337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.2771, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0517,  0.1166, -5.9862],\n",
      "        [ 2.9217, -2.0154, -4.8810],\n",
      "        [ 0.8680,  1.0211, -6.3498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4854, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1376, -0.0402, -5.9495],\n",
      "        [ 3.3252, -1.7925, -4.7479],\n",
      "        [-1.4567,  4.3351, -4.7836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9109, -1.6933, -5.1725],\n",
      "        [ 2.9186, -1.9156, -5.1030],\n",
      "        [ 2.9782, -1.9296, -5.0734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1059, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3823,  1.4246, -6.4036],\n",
      "        [ 2.9457, -2.0290, -4.9390],\n",
      "        [ 2.8394, -2.0429, -4.9673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2521, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8893,  2.9319, -5.9078],\n",
      "        [ 0.7773,  0.7034, -6.1455],\n",
      "        [-1.6884,  4.0550, -4.8585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7936, -1.4979, -5.3172],\n",
      "        [ 2.8113, -1.3921, -5.1382],\n",
      "        [ 2.7266, -1.3608, -5.1468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0052, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8843,  4.5120, -4.6842],\n",
      "        [-1.6420,  3.7461, -5.2723],\n",
      "        [ 3.0588, -1.6720, -4.9888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8710,  4.2647, -4.7625],\n",
      "        [ 2.6743, -1.3588, -5.2523],\n",
      "        [ 2.7583, -1.5891, -5.2983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6484, -1.6837, -5.1597],\n",
      "        [ 2.7592, -1.2259, -5.1859],\n",
      "        [ 2.9552, -1.2447, -5.5992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1883, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5249, -0.9366, -6.0626],\n",
      "        [ 1.2284,  0.8393, -5.6571],\n",
      "        [ 3.0657, -1.0955, -5.5006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8586,  2.9240, -5.8956],\n",
      "        [-0.8209,  3.0368, -6.3573],\n",
      "        [ 2.5062, -0.9276, -5.6936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2198, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6389, -1.1755, -5.8013],\n",
      "        [-1.5697,  4.0159, -4.9898],\n",
      "        [ 0.9000,  1.0242, -6.1414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6127, -1.0145, -5.6574],\n",
      "        [-1.9329,  4.1818, -4.8391],\n",
      "        [ 2.4948, -1.1313, -5.9247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2297, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9223,  0.8607, -5.7909],\n",
      "        [-1.2619,  3.4016, -5.9204],\n",
      "        [ 2.9122, -1.2172, -5.3585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4147, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3407,  1.2135, -5.8712],\n",
      "        [-1.3983,  3.8503, -5.7744],\n",
      "        [-0.9479,  3.1679, -5.9076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0171, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5458,  3.5962, -5.4649],\n",
      "        [ 2.3884, -1.1126, -5.6428],\n",
      "        [ 2.8026, -1.3926, -5.4873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5066, -1.1568, -5.5423],\n",
      "        [ 2.5054, -1.3386, -5.5230],\n",
      "        [ 2.8069, -1.2328, -5.3396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1037, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0727,  1.9962, -6.3278],\n",
      "        [ 0.0791,  1.8804, -6.5503],\n",
      "        [ 2.5834, -1.2633, -5.4793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0323, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4374,  2.4383, -6.2803],\n",
      "        [ 2.6238, -1.2031, -5.7214],\n",
      "        [ 2.6814, -1.2352, -5.3903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1371, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5589,  2.5108, -6.0428],\n",
      "        [ 0.2350,  1.3703, -6.6525],\n",
      "        [-0.3202,  2.0831, -6.4305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1044, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6209,  2.5954, -6.0260],\n",
      "        [ 0.2050,  1.5081, -6.3349],\n",
      "        [ 2.5386, -0.8666, -5.5298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3216, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7291,  4.0929, -4.9236],\n",
      "        [-1.3506,  3.6721, -5.5427],\n",
      "        [ 0.4176,  0.8855, -5.6379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7161, -1.4236, -5.4004],\n",
      "        [ 2.7091, -1.3701, -5.1057],\n",
      "        [ 2.8358, -1.2190, -5.6344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0492, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0791,  1.9944, -5.9013],\n",
      "        [-1.1923,  3.7248, -5.5395],\n",
      "        [-1.8677,  4.1449, -4.5991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3907, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6273,  2.4203, -6.3981],\n",
      "        [-0.9075,  3.1925, -5.8017],\n",
      "        [ 0.5586,  1.2666, -5.7943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7724, -1.1957, -5.1793],\n",
      "        [ 2.7443, -1.2616, -5.7010],\n",
      "        [ 2.7035, -1.3393, -5.0631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2225, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6720,  0.8174, -6.0807],\n",
      "        [ 2.7917, -1.3605, -5.6665],\n",
      "        [ 2.4517, -1.1225, -5.3354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0521, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7483,  4.0876, -4.6242],\n",
      "        [ 2.4012, -0.9246, -5.5165],\n",
      "        [-0.0787,  2.0038, -6.4690]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8212,  3.9685, -4.7093],\n",
      "        [-1.9443,  4.5362, -4.4655],\n",
      "        [-0.7058,  2.5789, -6.3156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2078, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1952,  1.6752, -6.5084],\n",
      "        [ 1.5428, -0.1969, -5.9828],\n",
      "        [ 1.4896,  0.2553, -6.1209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0465, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1926,  1.8457, -6.3252],\n",
      "        [ 2.8563, -1.4020, -5.1847],\n",
      "        [-1.8202,  4.2362, -4.5566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7643, -1.3828, -5.6574],\n",
      "        [-1.8290,  4.6347, -4.4038],\n",
      "        [ 2.7480, -1.2262, -5.4107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2086, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9825,  0.7252, -5.6636],\n",
      "        [-0.4104,  2.5970, -5.9761],\n",
      "        [-1.5143,  4.0205, -5.0425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0142, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8113, -1.5690, -5.4937],\n",
      "        [ 2.9738, -1.5371, -5.4847],\n",
      "        [ 2.5892, -1.3836, -5.8904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7006, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4737, -1.3616, -5.7941],\n",
      "        [ 0.3898,  1.2104, -5.3348],\n",
      "        [-0.2182,  2.5959, -6.4085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0724, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9728,  4.5051, -4.4605],\n",
      "        [ 1.7913,  0.2836, -6.8828],\n",
      "        [ 2.7695, -1.4079, -5.5818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8261,  4.2804, -4.6248],\n",
      "        [ 2.5673, -1.2501, -5.3712],\n",
      "        [ 2.6049, -1.3349, -5.7549]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0350, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1352,  2.4963, -6.6478],\n",
      "        [ 2.7139, -1.1142, -6.0015],\n",
      "        [ 2.9170, -1.3903, -5.5761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0982, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9317, -1.2723, -5.5661],\n",
      "        [ 2.6843, -0.9989, -5.6827],\n",
      "        [ 1.5294,  0.2900, -6.1615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1636, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4648,  1.0362, -5.9685],\n",
      "        [ 2.6801, -1.1700, -5.3396],\n",
      "        [ 2.5705, -1.2805, -5.6681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0198, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7052, -1.2229, -5.7238],\n",
      "        [ 2.6887, -1.1841, -5.5669],\n",
      "        [ 2.6521, -1.3256, -5.4801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8976,  4.5470, -4.1559],\n",
      "        [-0.9056,  3.1527, -5.6782],\n",
      "        [-1.3980,  3.6185, -5.3546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0164, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7662, -1.3461, -5.7132],\n",
      "        [ 2.8682, -1.2243, -5.7693],\n",
      "        [ 2.6680, -1.4640, -5.7103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3403, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6493,  4.3007, -4.6582],\n",
      "        [ 2.7512, -1.1715, -5.7384],\n",
      "        [-0.0828,  2.4606, -6.5872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9968, -1.2210, -5.6798],\n",
      "        [ 2.6516, -1.1836, -5.9373],\n",
      "        [-1.9769,  4.1226, -4.3039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0308, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3588, -0.8782, -5.9512],\n",
      "        [-0.6797,  2.7475, -6.0495],\n",
      "        [ 2.5951, -1.2468, -5.8840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0229, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7324, -0.5375, -6.3168],\n",
      "        [ 2.4998, -1.1168, -5.6134],\n",
      "        [-1.5751,  3.8378, -5.2266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1885, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5567,  1.0311, -5.6857],\n",
      "        [ 2.6601, -0.6642, -6.6077],\n",
      "        [ 2.4227, -0.6477, -5.8637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1006, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4566,  3.7921, -4.9950],\n",
      "        [-1.6843,  4.1141, -5.0347],\n",
      "        [ 2.5031, -0.7521, -6.5234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0027, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4831,  4.0628, -5.1776],\n",
      "        [-1.9621,  4.4534, -3.9884],\n",
      "        [-1.9913,  4.2753, -3.9294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0566, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0962,  4.3152, -5.0002],\n",
      "        [ 0.2404,  2.1769, -6.1171],\n",
      "        [ 2.6430, -0.7578, -5.8312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0842, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2716, -0.6728, -6.2759],\n",
      "        [-1.9745,  4.3997, -3.7040],\n",
      "        [ 0.2769,  1.7918, -6.2363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0636, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2926, -0.4089, -6.2208],\n",
      "        [ 2.1870, -0.6794, -6.1110],\n",
      "        [ 2.0493, -0.5767, -5.8390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0427, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3461, -0.4677, -6.1095],\n",
      "        [ 2.4257, -0.5378, -6.1131],\n",
      "        [-0.8146,  3.1315, -6.2213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1116, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1795,  1.5590, -5.9592],\n",
      "        [-0.0900,  2.0804, -6.2206],\n",
      "        [-2.1099,  4.4001, -4.1814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5691,  3.5725, -5.3295],\n",
      "        [ 2.4203, -0.2885, -6.4778],\n",
      "        [-1.9698,  4.3456, -4.2589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0701, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6238,  4.2245, -3.9433],\n",
      "        [ 2.0321,  0.0923, -6.7573],\n",
      "        [ 2.1984, -0.3871, -6.3251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8323,  4.4650, -3.9055],\n",
      "        [-1.8887,  4.3683, -4.0532],\n",
      "        [ 2.3305, -0.6456, -6.1749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0657, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4763, -0.3034, -6.3206],\n",
      "        [ 2.1442, -0.3206, -6.1440],\n",
      "        [ 2.5574, -0.3199, -6.1990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0306, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0589,  2.9042, -6.0855],\n",
      "        [ 2.0919, -0.5330, -6.2129],\n",
      "        [-1.9604,  4.0607, -4.4617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0329, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8605, -0.5446, -5.8547],\n",
      "        [-2.0070,  4.5251, -4.0372],\n",
      "        [ 2.3779, -0.3393, -6.2421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3060, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0485, -0.3188, -6.2673],\n",
      "        [-2.0157,  4.5509, -4.2304],\n",
      "        [ 0.7469,  0.9970, -5.7538]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0365,  4.3230, -3.8555],\n",
      "        [-2.0134,  4.5520, -4.2261],\n",
      "        [ 2.1153,  0.0286, -6.3457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6856, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3392,  3.4499, -5.7898],\n",
      "        [ 2.2325,  0.1141, -6.6177],\n",
      "        [-0.2008,  1.5777, -6.1988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0294, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3714, -0.7445, -5.9875],\n",
      "        [-2.0852,  4.5360, -3.8869],\n",
      "        [ 2.5549, -0.5717, -5.9957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0735,  4.4940, -4.0582],\n",
      "        [ 2.6957, -0.6992, -6.3853],\n",
      "        [ 2.5169, -0.8294, -5.8834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6039, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3727, -0.8332, -6.3629],\n",
      "        [-1.8891,  4.2909, -3.9031],\n",
      "        [ 1.5457,  0.2174, -6.1451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0970, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8203, -0.0933, -5.9403],\n",
      "        [ 1.9834,  0.0091, -6.5657],\n",
      "        [ 2.8117, -0.9597, -6.5012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2566, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0923,  3.4821, -5.3837],\n",
      "        [ 2.5085, -0.9313, -5.9239],\n",
      "        [ 0.8094,  0.8759, -5.7490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0694, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0173,  0.0954, -6.3757],\n",
      "        [-2.1419,  4.5626, -4.1667],\n",
      "        [ 2.2714, -0.3549, -6.0544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6998, -0.7320, -5.8349],\n",
      "        [ 2.3968, -0.7033, -6.0669],\n",
      "        [ 2.3857, -0.6962, -6.0813]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1431, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3115, -0.3470, -6.0974],\n",
      "        [ 0.3035,  1.1614, -5.7478],\n",
      "        [-1.3963,  3.5601, -5.6834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0304, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3665, -0.5700, -6.0192],\n",
      "        [ 2.6628, -0.6070, -6.2715],\n",
      "        [-1.9469,  4.4968, -4.0582]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3444, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8022,  4.5194, -3.5187],\n",
      "        [ 1.0245,  0.4404, -5.6731],\n",
      "        [-1.9859,  4.0178, -3.3054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0409, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9801,  4.4503, -3.7983],\n",
      "        [-1.9785,  4.5762, -3.5894],\n",
      "        [-0.0617,  2.0077, -6.1786]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2558, -0.4590, -6.3408],\n",
      "        [-0.2584,  2.6383, -6.4288],\n",
      "        [-1.9767,  4.1577, -3.3394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0372, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9297,  4.4474, -3.6837],\n",
      "        [-1.7802,  4.2063, -3.4739],\n",
      "        [-0.2425,  1.9453, -6.1047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0305, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0401,  4.4564, -3.1411],\n",
      "        [-0.6842,  2.5331, -6.1192],\n",
      "        [ 2.5152, -0.4575, -6.2072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1853,  4.4547, -3.8400],\n",
      "        [ 2.6371, -0.4012, -6.4558],\n",
      "        [-1.8717,  4.2888, -4.2652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0718, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2687, -0.8852, -5.7891],\n",
      "        [ 1.8187, -0.0616, -6.1090],\n",
      "        [-0.6696,  2.7901, -5.9851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0458, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4668, -0.4415, -5.7535],\n",
      "        [-2.0591,  4.6243, -4.0535],\n",
      "        [ 2.0490, -0.4035, -6.5892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2430, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8984,  0.8970, -5.7970],\n",
      "        [ 2.4542, -0.9527, -5.8932],\n",
      "        [-2.0104,  4.3580, -4.6720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4981, -0.7024, -6.0081],\n",
      "        [ 2.4342, -0.5828, -6.2052],\n",
      "        [ 2.5019, -0.5746, -6.4931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0018, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0915,  4.4054, -4.1011],\n",
      "        [-1.8278,  4.6368, -4.0934],\n",
      "        [-1.8431,  4.4957, -3.9860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0865, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3102, -0.8889, -6.0159],\n",
      "        [ 2.5300, -1.0238, -6.1233],\n",
      "        [ 1.4656, -0.0975, -5.7693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6524, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1307,  0.5464, -5.1829],\n",
      "        [-2.1006,  4.6209, -4.1229],\n",
      "        [ 0.3378,  1.5998, -6.1088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1756, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2270,  4.5704, -3.3366],\n",
      "        [ 2.4708, -1.0938, -6.2051],\n",
      "        [ 2.7826, -0.6836, -6.1395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0277, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8750, -0.7905, -5.8570],\n",
      "        [-1.9294,  4.7167, -4.1836],\n",
      "        [ 2.3358, -0.5197, -5.8634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6946,  4.4175, -4.4433],\n",
      "        [-2.1108,  4.3746, -4.2466],\n",
      "        [ 2.3730, -0.6714, -6.0031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0016, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9238,  4.7384, -4.0218],\n",
      "        [-1.8955,  4.4245, -4.1610],\n",
      "        [-2.1729,  4.6455, -3.4357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9419,  4.6090, -3.6015],\n",
      "        [-0.7682,  2.7093, -6.0055],\n",
      "        [-1.4081,  3.9924, -4.8644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0366, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4207, -0.9241, -6.1494],\n",
      "        [ 2.4918, -0.5335, -6.2452],\n",
      "        [-1.9142,  4.3477, -4.1822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2008, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3159, -0.7001, -5.8991],\n",
      "        [ 1.0934,  0.6784, -6.0610],\n",
      "        [ 2.4956, -0.5502, -5.9543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0237, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1899,  4.5451, -4.0554],\n",
      "        [ 2.4677, -0.8336, -5.8910],\n",
      "        [ 2.5399, -0.8503, -6.3463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1699, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2842,  2.4169, -6.6440],\n",
      "        [ 0.4387,  1.0252, -5.6217],\n",
      "        [-2.1120,  4.4403, -4.0891]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8740,  3.8804, -5.1835],\n",
      "        [-1.9522,  4.6542, -4.0119],\n",
      "        [-1.2451,  3.4909, -6.4632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1769, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1647, -0.1879, -6.2455],\n",
      "        [-1.9918,  4.5460, -3.7908],\n",
      "        [ 0.6242,  1.2256, -5.4390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0536, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9067,  0.1313, -6.3659],\n",
      "        [-1.9210,  4.2968, -4.6005],\n",
      "        [-1.9081,  4.4076, -4.3115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0851, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3788, -0.7604, -6.2879],\n",
      "        [ 0.1851,  1.6368, -6.6213],\n",
      "        [-1.7982,  4.4233, -4.4976]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2774, -0.6294, -6.4392],\n",
      "        [ 2.1271, -0.0843, -6.5363],\n",
      "        [-0.9798,  3.2777, -5.8849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2241, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9593,  0.7992, -6.0528],\n",
      "        [ 2.3032, -0.6004, -6.5658],\n",
      "        [-2.0504,  4.3571, -3.7121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0275, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1398,  4.2768, -3.7995],\n",
      "        [-1.8512,  4.2556, -5.3310],\n",
      "        [ 2.1584, -0.3519, -6.2009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0658, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2906, -0.1374, -6.7368],\n",
      "        [ 2.2692, -0.2673, -6.6464],\n",
      "        [ 2.6206, -0.6746, -6.4122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0020, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6004,  4.2586, -4.9575],\n",
      "        [-1.9285,  4.5955, -3.9029],\n",
      "        [-2.0359,  4.7354, -4.0990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1783, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0377,  0.6816, -5.9451],\n",
      "        [-1.9438,  4.4984, -3.5968],\n",
      "        [-1.9459,  4.6393, -3.8344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1918, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5544,  1.0093, -6.3815],\n",
      "        [-1.9149,  4.4846, -3.9499],\n",
      "        [ 2.1444, -0.3214, -6.5746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0493, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0067,  4.5526, -4.5371],\n",
      "        [ 2.4625, -0.6500, -6.7527],\n",
      "        [ 2.1795, -0.0459, -6.9911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1708, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3497, -0.6622, -6.3717],\n",
      "        [-2.1013,  4.1851, -4.6641],\n",
      "        [ 1.1405,  0.6076, -6.0792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2823, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3550, -0.7652, -6.4933],\n",
      "        [ 2.5681, -0.5981, -6.4936],\n",
      "        [ 0.7820,  0.9141, -5.6787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0316, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7079, -0.5409, -6.3530],\n",
      "        [-2.0679,  4.5150, -3.9902],\n",
      "        [ 2.6055, -0.2691, -6.1381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2077, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2027,  4.3721, -3.3693],\n",
      "        [ 2.5295, -0.9844, -6.2685],\n",
      "        [ 2.4036, -0.1131, -6.2983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2902, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0960,  3.2010, -5.9588],\n",
      "        [-0.6379,  2.5359, -6.2061],\n",
      "        [ 0.7620,  0.9930, -5.9327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1000, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9704,  4.6425, -3.8220],\n",
      "        [ 2.4115, -0.7720, -6.1025],\n",
      "        [ 0.4059,  1.6317, -6.1563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5646, -0.4946, -6.2615],\n",
      "        [ 2.3601, -0.7732, -6.3959],\n",
      "        [ 2.6277, -0.7953, -6.3894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1453, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5609, -0.8250, -6.4028],\n",
      "        [-1.4799,  3.4002, -5.3593],\n",
      "        [-1.2078,  3.5111, -5.7477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3889, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1998,  0.5857, -6.2240],\n",
      "        [-0.4931,  2.2925, -6.3935],\n",
      "        [ 2.1432, -0.6474, -6.2473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0265, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6459, -0.4120, -6.6320],\n",
      "        [ 2.7114, -0.7141, -6.5880],\n",
      "        [-2.0434,  4.5847, -4.0453]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0346, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6057, -0.6473, -6.5760],\n",
      "        [-0.6406,  3.1491, -6.4230],\n",
      "        [ 2.6017, -0.5149, -6.6596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0373, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4449, -0.5210, -6.4172],\n",
      "        [-1.9360,  4.3333, -2.8295],\n",
      "        [ 2.3228, -0.4849, -6.5037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7997, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3960, -0.5146, -6.7037],\n",
      "        [ 2.2955, -0.3819, -6.8925],\n",
      "        [ 2.3068, -0.2179, -6.8037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0332, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3723,  0.0690, -6.6473],\n",
      "        [-2.0360,  4.2994, -3.7675],\n",
      "        [-1.8710,  4.2513, -4.9536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0263, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4604, -0.1214, -6.6649],\n",
      "        [-1.6823,  3.8779, -5.1077],\n",
      "        [-2.0176,  4.4075, -3.7160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0654, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2281, -0.3553, -7.0578],\n",
      "        [-2.2294,  4.4582, -2.8538],\n",
      "        [ 2.0935,  0.0453, -7.0275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0799, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3729,  0.2507, -6.6840],\n",
      "        [ 2.1298,  0.0653, -6.8414],\n",
      "        [-1.2456,  3.7406, -5.4909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1031, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2933,  0.0581, -6.7725],\n",
      "        [ 2.0495, -0.0917, -6.7099],\n",
      "        [ 2.1209, -0.1721, -7.2610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0304, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2913,  3.5370, -5.7691],\n",
      "        [-2.0343,  4.5539, -3.6391],\n",
      "        [ 2.4075, -0.0593, -6.6514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0353, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3600,  0.1239, -6.7520],\n",
      "        [-2.2261,  4.4011, -3.9315],\n",
      "        [-1.8492,  4.1511, -4.1843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0839, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7689,  0.3463, -7.0677],\n",
      "        [-2.3441,  4.5744, -3.6548],\n",
      "        [-0.5361,  2.8196, -6.4893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1521, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3000,  2.5656, -6.5563],\n",
      "        [ 1.9171,  0.2085, -7.1725],\n",
      "        [ 1.9244,  0.5921, -7.3275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0359, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3321,  3.5427, -5.3238],\n",
      "        [ 2.3953,  0.1158, -6.8964],\n",
      "        [-1.8018,  4.3404, -3.4481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0639, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1806,  4.4546, -2.7964],\n",
      "        [ 2.2714,  0.1761, -6.5289],\n",
      "        [ 2.2958, -0.2798, -6.8163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0741, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4075,  3.2343, -5.7850],\n",
      "        [ 2.4339,  0.1285, -6.9292],\n",
      "        [ 2.0133, -0.0689, -7.1593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2040, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6354,  1.2865, -5.8583],\n",
      "        [ 2.3116, -0.1452, -6.6867],\n",
      "        [ 2.1243, -0.0324, -6.9138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7394, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0441,  4.5673, -3.5754],\n",
      "        [ 2.2148, -0.3330, -6.4859],\n",
      "        [ 2.0529,  0.0368, -6.8223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3640, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0087, -0.1904, -6.3908],\n",
      "        [ 2.3488, -0.4471, -6.6716],\n",
      "        [ 1.2218,  0.7985, -6.4692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0634, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2640, -0.1298, -6.8180],\n",
      "        [-2.0300,  4.3447, -4.1617],\n",
      "        [ 2.1998, -0.0455, -6.8824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0858, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1347e+00, -2.8056e-01, -6.4242e+00],\n",
      "        [ 2.3888e+00, -2.3407e-03, -6.7175e+00],\n",
      "        [ 2.2644e+00, -1.7245e-01, -6.7026e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2691, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8056,  4.2796, -3.9852],\n",
      "        [ 0.9095,  0.8349, -5.7088],\n",
      "        [ 2.3267, -0.2588, -6.4472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0612, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9390,  4.5306, -3.8425],\n",
      "        [ 2.2472, -0.0702, -6.4901],\n",
      "        [ 2.2449, -0.1444, -6.9148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0023, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9607,  4.4304, -4.3487],\n",
      "        [-1.6947,  4.0375, -4.8643],\n",
      "        [-1.8324,  4.6849, -3.7996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0636, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7961,  4.3632, -4.0056],\n",
      "        [ 2.5256,  0.1150, -6.4946],\n",
      "        [ 2.3143,  0.0852, -6.9027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0018, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1594,  4.4626, -3.5517],\n",
      "        [-2.2115,  4.3903, -3.3838],\n",
      "        [-1.9883,  4.3182, -3.8271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0294, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2621, -0.1688, -6.6004],\n",
      "        [-1.8576,  4.5193, -3.3233],\n",
      "        [-1.7963,  4.6449, -4.5229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2479, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1046, -0.4402, -6.6137],\n",
      "        [ 2.1861, -0.4019, -6.7993],\n",
      "        [ 1.1336,  0.9266, -6.4425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2544, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6575,  3.0780, -6.4626],\n",
      "        [-1.9717,  4.5497, -3.7276],\n",
      "        [-1.9487,  4.3244, -3.8431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0020, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9751,  4.6543, -3.9410],\n",
      "        [-1.9057,  4.5238, -3.7438],\n",
      "        [-1.8062,  4.3154, -3.8454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0743, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3734, -0.5106, -6.4319],\n",
      "        [ 2.2710, -0.2735, -6.3986],\n",
      "        [ 1.9962, -0.3392, -6.2262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0380, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1973, -0.5595, -6.0921],\n",
      "        [-1.7841,  4.4661, -4.4900],\n",
      "        [ 2.4303, -0.5398, -6.5004]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0161, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1718,  4.7906, -3.9363],\n",
      "        [ 2.4925, -0.5759, -6.3327],\n",
      "        [-1.9070,  4.5707, -4.0339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9659, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4380, -0.3573, -6.4035],\n",
      "        [ 2.5439, -0.5862, -6.0870],\n",
      "        [ 2.3629, -0.3691, -6.2022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0481, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5951, -0.4132, -6.3075],\n",
      "        [ 2.2694, -0.0435, -6.7657],\n",
      "        [-1.9964,  4.6806, -4.4927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0238, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3442, -0.3055, -6.0483],\n",
      "        [-2.1531,  4.4923, -4.3385],\n",
      "        [-1.9299,  4.6373, -4.1486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8654,  4.4884, -4.4363],\n",
      "        [-1.7208,  4.0789, -4.9657],\n",
      "        [ 2.4680, -0.6019, -6.3268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0142, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8665,  4.4823, -4.6249],\n",
      "        [ 2.5943, -0.6232, -6.2134],\n",
      "        [-2.1118,  4.6967, -4.0141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4444,  3.9590, -4.9629],\n",
      "        [ 2.1731, -0.5332, -6.1675],\n",
      "        [-1.3962,  3.9980, -5.1185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0221, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4014, -0.3739, -6.1713],\n",
      "        [-1.7726,  4.1326, -4.4454],\n",
      "        [-1.7473,  4.2118, -5.0516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0576, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5792, -0.4521, -6.7820],\n",
      "        [ 2.4463, -0.3335, -6.4524],\n",
      "        [ 2.5572, -0.1417, -6.3923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8710,  4.4242, -4.3123],\n",
      "        [ 2.6750, -0.4891, -6.2992],\n",
      "        [ 2.3309, -0.5540, -5.9439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7191, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8043,  4.4784, -4.1280],\n",
      "        [ 2.5240, -0.5397, -6.3641],\n",
      "        [ 2.0837,  0.1038, -7.0362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0471, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3251, -0.5836, -6.6634],\n",
      "        [ 2.5083, -0.5741, -6.5754],\n",
      "        [ 2.6085, -0.5167, -6.3189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4643, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5949, -0.5882, -6.3780],\n",
      "        [ 2.3975, -0.6910, -6.3015],\n",
      "        [ 0.3685,  1.3601, -6.7489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1653, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4680,  0.6992, -6.7251],\n",
      "        [ 2.4315, -0.4795, -5.9201],\n",
      "        [ 2.4140, -0.3455, -6.4807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0319, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6278, -0.5663, -6.3569],\n",
      "        [ 2.4677, -0.4310, -6.6559],\n",
      "        [-2.0842,  4.3443, -4.3835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0742, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0527, -0.3506, -6.7779],\n",
      "        [-1.0831,  3.6850, -5.7642],\n",
      "        [ 0.1409,  2.1375, -7.1196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1510, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5530, -0.2493, -6.3179],\n",
      "        [ 2.4574, -0.3617, -6.4393],\n",
      "        [ 0.3898,  1.3095, -6.6715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1105, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5887, -0.7045, -6.5824],\n",
      "        [-1.7761,  4.5822, -4.6200],\n",
      "        [ 0.4631,  1.5402, -7.0499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0693, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4653,  3.7533, -5.1069],\n",
      "        [ 0.1368,  1.8962, -6.7941],\n",
      "        [ 2.4182, -0.6990, -6.4567]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0798, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7611,  4.6255, -4.7289],\n",
      "        [-0.9667,  3.1731, -6.1458],\n",
      "        [ 1.8668,  0.4731, -7.0059]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3858, -0.4661, -6.6736],\n",
      "        [-1.9219,  4.5766, -4.2332],\n",
      "        [-1.8437,  4.8587, -4.6692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0376, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7382,  2.7946, -6.2826],\n",
      "        [ 2.5760, -0.5469, -6.3333],\n",
      "        [ 2.6091, -0.5743, -6.1572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0024, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0059,  4.3476, -4.5557],\n",
      "        [-1.6332,  4.1938, -5.1023],\n",
      "        [-1.9662,  4.2076, -4.0375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0429, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0778,  2.5355, -7.2327],\n",
      "        [-2.1181,  4.5086, -3.8571],\n",
      "        [ 2.1287, -0.7211, -6.6208]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5820, -0.7394, -6.0766],\n",
      "        [-1.9641,  4.5926, -4.2380],\n",
      "        [-1.7394,  4.4385, -4.8889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1831, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9494,  4.5294, -4.3154],\n",
      "        [-1.9848,  4.5161, -4.4434],\n",
      "        [ 1.1423,  0.8216, -6.3145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0192, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5655, -0.8550, -6.3196],\n",
      "        [-2.0631,  4.6297, -4.5578],\n",
      "        [ 2.7058, -1.0211, -5.8509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0417, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9343,  4.7879, -4.0172],\n",
      "        [-2.2543,  4.6560, -4.2122],\n",
      "        [-0.0563,  1.9812, -7.1048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1164,  4.8089, -3.8902],\n",
      "        [ 2.5931, -0.9983, -6.0085],\n",
      "        [ 2.2485, -0.3949, -6.6292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0258, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0345,  4.7093, -3.7364],\n",
      "        [ 2.6961, -1.0923, -6.0895],\n",
      "        [ 2.3048, -0.6024, -6.3098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2406, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9907,  0.9911, -5.6799],\n",
      "        [-0.6813,  2.9217, -6.6384],\n",
      "        [-1.9780,  4.7489, -4.3604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0243, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8399,  4.2534, -5.3125],\n",
      "        [ 2.5263, -0.7346, -6.0614],\n",
      "        [ 2.5157, -0.8888, -6.1884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0570, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3175,  2.1692, -6.8717],\n",
      "        [ 2.9531, -0.7769, -6.3493],\n",
      "        [-1.9678,  4.8049, -4.4029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4796, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0790,  4.6112, -4.2286],\n",
      "        [ 2.7083, -1.0122, -6.1613],\n",
      "        [ 1.4716,  0.3373, -6.7196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0734, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6459, -0.9490, -6.0384],\n",
      "        [ 2.9394, -0.8882, -6.1066],\n",
      "        [ 1.7867,  0.1072, -6.6039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9391, -0.7887, -5.7692],\n",
      "        [-2.1191,  4.4853, -3.7231],\n",
      "        [ 2.7178, -0.8963, -6.2115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0779, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5285, -1.4523, -5.9512],\n",
      "        [ 2.1709, -0.3574, -6.6482],\n",
      "        [ 1.8133, -0.0976, -6.5588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0813,  4.5466, -3.7081],\n",
      "        [ 2.7152, -0.9651, -5.7162],\n",
      "        [-2.0817,  4.7665, -3.5435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8353, -1.1216, -5.7566],\n",
      "        [-0.9200,  3.1580, -5.8510],\n",
      "        [-1.9885,  4.8594, -3.9779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8417, -1.3094, -6.3101],\n",
      "        [-2.1416,  4.6117, -4.4015],\n",
      "        [ 2.6815, -1.1950, -5.9330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0316, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4580, -0.7083, -6.1630],\n",
      "        [ 2.2196, -0.7185, -6.2996],\n",
      "        [-1.8399,  4.8014, -4.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6598, -1.1668, -6.1819],\n",
      "        [-2.1213,  4.7784, -3.7422],\n",
      "        [-2.0338,  4.8308, -3.7273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0151, grad_fn=<NllLossBackward0>), logits=tensor([[-2.1057,  4.5875, -3.8147],\n",
      "        [-2.0619,  4.8256, -4.4885],\n",
      "        [ 2.4185, -0.7157, -6.2672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0385,  4.5961, -4.0316],\n",
      "        [-2.2417,  4.9224, -4.0082],\n",
      "        [ 2.4916, -0.8917, -5.8336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6703, -1.0276, -5.9519],\n",
      "        [ 2.6785, -1.1558, -5.6762],\n",
      "        [ 2.9114, -1.3896, -5.5992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0017, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8763,  4.7598, -3.8297],\n",
      "        [-1.6559,  4.3392, -4.7497],\n",
      "        [-2.0988,  4.8136, -4.0151]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.3328, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2993,  4.5998, -3.3944],\n",
      "        [-0.0873,  2.2631, -6.6640],\n",
      "        [-1.4888,  3.5324, -6.0938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0157, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8527, -0.8803, -5.6255],\n",
      "        [-1.9412,  4.3356, -2.3661],\n",
      "        [ 2.6871, -1.2159, -5.7635]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0048,  4.0480, -2.0328],\n",
      "        [ 2.6705, -0.9801, -6.0731],\n",
      "        [ 2.6116, -1.1762, -6.0387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0262, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5829, -1.2544, -5.4651],\n",
      "        [ 2.8711, -1.0510, -5.8171],\n",
      "        [ 2.4410, -0.8373, -6.0499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0445, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6187, -0.8657, -5.9228],\n",
      "        [-1.8014,  3.1455, -0.3202],\n",
      "        [-1.7368,  2.8152, -0.0482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4685,  3.7188, -2.3289],\n",
      "        [-1.6732,  3.6761, -1.9036],\n",
      "        [-1.6806,  3.4317, -0.8816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5857, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4936,  4.4457, -4.3196],\n",
      "        [ 0.3804,  1.9192, -7.2312],\n",
      "        [ 2.7180, -1.1451, -5.4375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6017, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6772, -1.3249, -5.7465],\n",
      "        [ 0.9613,  0.9652, -6.6098],\n",
      "        [ 0.6052,  1.2931, -6.6907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3267, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3843,  4.0914, -4.6907],\n",
      "        [-1.3802,  4.1088, -4.1863],\n",
      "        [ 0.5523,  1.0475, -6.4858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1886, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9164, -1.5675, -5.1510],\n",
      "        [-1.3173,  4.4603, -5.4482],\n",
      "        [ 1.1238,  0.8153, -6.1114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1135,  4.2256, -5.1939],\n",
      "        [-1.2409,  3.9983, -5.1544],\n",
      "        [ 2.6675, -1.5907, -5.3614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9949, -1.5033, -5.1710],\n",
      "        [ 2.8279, -0.7172, -6.1666],\n",
      "        [ 2.7057, -1.4543, -5.4721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0594, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3925,  4.1159, -5.5157],\n",
      "        [ 2.8876, -1.6976, -5.6669],\n",
      "        [ 1.6045, -0.1233, -6.2945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9091,  4.1916, -5.8041],\n",
      "        [ 2.8065, -1.5124, -5.1366],\n",
      "        [-1.2443,  3.8611, -6.0187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8141, -1.2799, -5.7043],\n",
      "        [ 2.8664, -1.6472, -5.3008],\n",
      "        [-0.9467,  4.0183, -5.5602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0311,  3.8842, -5.7846],\n",
      "        [ 2.8169, -1.7053, -5.0801],\n",
      "        [ 2.9411, -1.5365, -5.3348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1987, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9985,  4.0934, -5.8690],\n",
      "        [ 1.2942,  1.0391, -7.2755],\n",
      "        [ 2.7138, -1.4251, -5.2618]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0884,  3.7088, -5.7615],\n",
      "        [-1.0844,  4.0322, -5.8890],\n",
      "        [-0.8902,  3.7928, -6.0782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2535, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1832,  4.0488, -5.6841],\n",
      "        [ 0.9746,  0.8735, -6.8935],\n",
      "        [ 2.9957, -1.6396, -5.3945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7763,  3.8341, -5.7556],\n",
      "        [ 3.1063, -1.2810, -5.5358],\n",
      "        [-1.2125,  3.9975, -6.3023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0495, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7997, -1.6340, -5.2400],\n",
      "        [-0.8998,  4.0816, -6.0097],\n",
      "        [ 2.0029,  0.0219, -6.5966]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0534, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0253,  1.8989, -6.7346],\n",
      "        [-1.0025,  3.9098, -6.2173],\n",
      "        [ 2.7127, -1.4048, -5.6032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0096, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9954, -1.7167, -4.9703],\n",
      "        [-0.7765,  4.0681, -6.5380],\n",
      "        [ 2.8672, -1.5934, -5.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7810, -1.6867, -5.2990],\n",
      "        [ 3.0487, -1.9162, -5.4008],\n",
      "        [-0.9326,  3.7769, -6.5471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9729,  3.9799, -6.2955],\n",
      "        [ 2.7517, -1.7893, -4.9456],\n",
      "        [ 2.8719, -1.5589, -5.1214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2725, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7085,  3.8854, -6.5191],\n",
      "        [ 1.0666,  1.2634, -7.3078],\n",
      "        [ 3.0333, -1.5086, -5.1006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9231, -1.5570, -5.5311],\n",
      "        [ 2.5048, -1.4424, -5.6906],\n",
      "        [-1.0716,  4.0858, -5.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0765, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7709, -1.5681, -5.1134],\n",
      "        [ 2.3330,  0.0775, -6.5911],\n",
      "        [ 2.2077,  0.1137, -6.7400]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4288, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8367, -1.8912, -5.1195],\n",
      "        [ 2.9084, -1.8916, -5.4110],\n",
      "        [ 2.7231, -1.5314, -5.3216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0835,  3.8404, -6.3322],\n",
      "        [-1.0174,  4.0115, -5.9933],\n",
      "        [ 2.9538, -1.7438, -5.4400]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8621,  4.0064, -6.1792],\n",
      "        [ 2.8969, -1.8621, -5.3302],\n",
      "        [ 3.2456, -1.8252, -4.9979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0717, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7462, -1.8258, -5.1531],\n",
      "        [-1.1203,  4.0156, -6.1527],\n",
      "        [ 0.3770,  1.8928, -6.9687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8070, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8589,  3.4963, -6.7298],\n",
      "        [-0.9408,  4.0132, -6.2895],\n",
      "        [ 2.1011, -0.2051, -6.7098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0471, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2012,  0.0563, -7.1885],\n",
      "        [ 2.7448, -1.1594, -5.8469],\n",
      "        [ 2.8282, -1.7548, -5.4150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9212,  3.7093, -6.0389],\n",
      "        [-1.0173,  3.9509, -6.1036],\n",
      "        [ 2.7128, -1.4934, -5.4782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1812, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9811,  1.3741, -6.7860],\n",
      "        [-0.8380,  3.2586, -6.1600],\n",
      "        [ 2.8382, -1.6886, -5.3976]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7659,  3.7409, -6.0728],\n",
      "        [ 2.9321, -1.5842, -5.4869],\n",
      "        [ 2.6605, -1.2753, -6.0697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9520,  3.7953, -5.8042],\n",
      "        [-1.0270,  3.9205, -5.9577],\n",
      "        [ 2.8784, -1.6625, -5.5517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8801,  3.9084, -6.2863],\n",
      "        [-0.9835,  4.0238, -5.7675],\n",
      "        [-0.9017,  3.8804, -6.0100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8119, -1.7162, -5.6184],\n",
      "        [ 2.7553, -1.3004, -5.1542],\n",
      "        [-1.1604,  3.8233, -6.1349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0151, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8365, -0.8070, -6.1722],\n",
      "        [-0.9082,  3.7927, -5.5369],\n",
      "        [ 3.0186, -1.5791, -5.6985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2401, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9161, -1.4924, -5.6069],\n",
      "        [ 2.7775, -1.4248, -5.9946],\n",
      "        [ 1.0078,  1.0083, -7.3996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7536,  3.7120, -5.3894],\n",
      "        [-1.0720,  3.9870, -6.1038],\n",
      "        [-0.9418,  3.9911, -5.8994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8998,  4.0161, -6.0282],\n",
      "        [-0.9393,  3.8424, -5.9709],\n",
      "        [-0.9864,  4.1284, -5.9841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1823, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0488,  4.0107, -6.2667],\n",
      "        [ 2.7292, -1.6170, -5.6237],\n",
      "        [ 1.2069,  0.8424, -7.3070]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5076, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4663,  1.5544, -6.7479],\n",
      "        [ 2.0189,  0.1127, -6.1357],\n",
      "        [-1.0334,  4.1982, -6.3281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0207,  4.1604, -5.8981],\n",
      "        [-0.8988,  3.9868, -6.0206],\n",
      "        [ 2.7872, -1.2891, -5.5097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1075, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8270, -0.9810, -6.2319],\n",
      "        [ 1.7469,  0.1074, -6.4291],\n",
      "        [ 2.1380,  0.1031, -6.6824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3801, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2510,  0.5364, -6.1954],\n",
      "        [-0.9399,  3.9024, -5.7264],\n",
      "        [ 2.7073, -1.2498, -5.7481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8922, -1.3703, -5.6382],\n",
      "        [ 2.4510, -1.1731, -5.8360],\n",
      "        [ 2.8620, -1.2759, -6.2647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4573, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3044,  3.8781, -5.8835],\n",
      "        [ 2.8991, -1.2864, -5.3533],\n",
      "        [ 2.9825, -1.3554, -6.0840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0512,  3.9571, -6.0679],\n",
      "        [ 2.7870, -1.5408, -5.6815],\n",
      "        [ 2.6505, -1.2092, -5.8097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0442,  3.7448, -5.9829],\n",
      "        [-1.1506,  3.7840, -5.8471],\n",
      "        [ 2.6191, -1.4176, -5.8684]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1876,  3.9820, -6.0989],\n",
      "        [-1.1453,  4.1228, -6.0399],\n",
      "        [ 2.9413, -1.0528, -5.9794]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6958, -0.8534, -6.1722],\n",
      "        [ 2.2487, -0.4597, -6.4521],\n",
      "        [-1.3076,  4.1472, -6.4398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1176,  3.8490, -5.9464],\n",
      "        [ 2.7992, -0.8083, -6.1423],\n",
      "        [-1.1397,  3.9312, -6.1670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0233, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6965, -0.8744, -6.6177],\n",
      "        [ 2.4623, -0.8765, -6.4534],\n",
      "        [-0.9041,  4.0493, -5.8843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1248, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9419,  3.9487, -5.5744],\n",
      "        [ 1.2575,  0.4253, -6.4276],\n",
      "        [-1.2014,  4.0352, -6.1554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0267, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0525, -1.2759, -6.0692],\n",
      "        [-0.8855,  3.8515, -6.1901],\n",
      "        [ 2.1329, -0.6863, -6.5665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1873, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4504,  3.9197, -6.1328],\n",
      "        [ 1.0029,  1.3401, -6.2492],\n",
      "        [ 2.7667, -1.2319, -5.9387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0088, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8233,  3.6331, -6.6210],\n",
      "        [-1.0202,  3.6246, -6.0766],\n",
      "        [-1.1500,  4.1177, -5.9514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1622, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1293,  3.8046, -6.0814],\n",
      "        [ 1.2928,  0.7638, -6.1025],\n",
      "        [-0.9106,  3.2411, -6.5288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0181, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6888, -0.7770, -5.8538],\n",
      "        [ 2.7941, -1.2168, -5.9825],\n",
      "        [-1.0533,  4.1983, -5.5224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8027,  3.1657, -6.5959],\n",
      "        [ 2.7941, -1.2646, -5.7991],\n",
      "        [ 2.9288, -1.0001, -5.7351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1015,  4.0766, -5.6917],\n",
      "        [ 2.6764, -1.3719, -5.4140],\n",
      "        [-1.2169,  3.8816, -6.1685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0575,  3.9215, -5.7592],\n",
      "        [-1.1460,  4.1516, -5.9349],\n",
      "        [-1.0368,  4.3506, -5.7217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2512,  4.0588, -5.9270],\n",
      "        [-1.1991,  4.1284, -5.7296],\n",
      "        [ 2.8639, -1.2852, -5.9163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3219, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6319,  3.2906, -6.6560],\n",
      "        [ 2.8186, -1.2493, -5.7603],\n",
      "        [-1.0693,  3.9900, -5.2728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7922, -1.0655, -5.9812],\n",
      "        [ 2.5605, -0.8544, -6.3306],\n",
      "        [ 2.8875, -0.9951, -5.9264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7818, -0.8027, -6.3993],\n",
      "        [ 2.8253, -1.3125, -5.5658],\n",
      "        [ 2.6677, -0.9920, -6.4170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8495, -1.2594, -5.8801],\n",
      "        [ 2.8441, -1.0833, -5.9065],\n",
      "        [ 2.6797, -1.4177, -6.0953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0529, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5239, -1.5536, -5.9977],\n",
      "        [-1.0348,  3.7260, -6.1354],\n",
      "        [ 0.1915,  2.1418, -6.9967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9324,  3.9435, -6.0975],\n",
      "        [-0.8723,  4.0256, -5.9387],\n",
      "        [ 2.9641, -1.0482, -5.6787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0065, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1474,  4.1490, -6.1266],\n",
      "        [-0.9761,  3.9524, -6.0934],\n",
      "        [-1.0371,  3.8767, -6.6026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9899,  3.9385, -6.2253],\n",
      "        [-1.1130,  3.7923, -6.1422],\n",
      "        [-0.8918,  3.9406, -6.3402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0149, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1350,  4.1010, -6.2087],\n",
      "        [ 2.7638, -1.2020, -5.8468],\n",
      "        [ 2.7231, -1.1586, -6.4131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0263, -1.1249, -5.8877],\n",
      "        [ 2.2169, -0.9143, -6.3235],\n",
      "        [-1.2032,  3.9963, -6.3604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0161, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8274, -1.5255, -5.7734],\n",
      "        [ 2.8224, -1.3335, -5.8676],\n",
      "        [ 2.9071, -1.0215, -5.8835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1308,  3.7270, -6.3080],\n",
      "        [-0.9716,  4.1077, -6.0733],\n",
      "        [ 2.9446, -0.9281, -5.8580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1980, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9085,  4.3133, -6.0786],\n",
      "        [-1.1984,  3.8785, -5.6994],\n",
      "        [ 2.7458, -0.8082, -6.1894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6817, -1.0331, -6.2954],\n",
      "        [-1.0313,  4.0156, -6.0377],\n",
      "        [ 3.0208, -1.1511, -5.9849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0217, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6379,  2.7206, -6.6138],\n",
      "        [ 2.7828, -1.3324, -6.2784],\n",
      "        [-0.5583,  3.6682, -5.9767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0367, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3549, -0.4188, -6.2296],\n",
      "        [-0.4781,  3.4268, -6.6994],\n",
      "        [-0.5702,  2.9406, -6.7768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7759, -1.0436, -5.9368],\n",
      "        [ 2.8687, -1.0375, -6.1088],\n",
      "        [-1.0869,  3.6800, -6.4089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2605, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8586,  0.9413, -6.6072],\n",
      "        [ 2.7216, -0.5128, -6.2977],\n",
      "        [-1.0693,  3.8754, -5.7748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0280, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5378, -0.8341, -6.0176],\n",
      "        [-0.4193,  2.9159, -6.7592],\n",
      "        [-0.6121,  3.5850, -6.4194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0251, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6463, -1.1147, -5.8879],\n",
      "        [ 2.5190, -1.0155, -5.8279],\n",
      "        [ 2.7983, -0.9680, -5.8246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0238, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7687, -1.0707, -6.2122],\n",
      "        [-0.5035,  3.0128, -6.8355],\n",
      "        [ 2.7264, -1.1500, -6.0255]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8434, -0.6720, -6.4349],\n",
      "        [ 2.9122, -1.0661, -6.3930],\n",
      "        [-1.1898,  3.9581, -5.9176]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0275, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7536, -1.1673, -5.5960],\n",
      "        [-0.3137,  3.1152, -6.5134],\n",
      "        [ 2.4399, -1.0377, -5.8606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0499, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5690, -1.0359, -6.2837],\n",
      "        [ 2.6805, -0.8783, -5.9801],\n",
      "        [ 2.2198, -0.0928, -6.8826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0737, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8994, -1.1590, -6.2963],\n",
      "        [ 0.2724,  1.9406, -7.2949],\n",
      "        [ 2.6397, -0.8181, -6.1637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0264, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6231, -0.6182, -6.3261],\n",
      "        [-0.8010,  3.6159, -6.6379],\n",
      "        [ 2.6262, -0.9151, -5.9836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8732,  3.9927, -6.0666],\n",
      "        [ 3.0028, -1.3397, -5.7761],\n",
      "        [ 2.8838, -1.1293, -6.0442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1580, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7002, -1.3362, -6.0005],\n",
      "        [ 0.1764,  2.2564, -6.9238],\n",
      "        [ 0.5281,  1.4375, -7.4502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0187, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7757, -1.2287, -6.3767],\n",
      "        [-0.9741,  3.3740, -6.2400],\n",
      "        [-0.5059,  3.1735, -6.3956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1035, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3811,  4.1731, -5.8038],\n",
      "        [ 2.8101, -0.7136, -6.4559],\n",
      "        [ 0.4768,  1.6177, -6.7425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7899,  3.7045, -6.2350],\n",
      "        [-1.0761,  3.9805, -5.9852],\n",
      "        [ 2.7385, -1.4207, -6.2364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0877,  3.9273, -5.9581],\n",
      "        [-1.2353,  4.2003, -5.9462],\n",
      "        [ 2.7507, -0.9434, -6.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0898,  4.1875, -5.6345],\n",
      "        [ 2.4974, -1.0815, -6.7103],\n",
      "        [-1.1668,  4.2309, -5.5659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0190, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1975,  3.8486, -5.7085],\n",
      "        [ 2.6900, -0.9096, -6.1201],\n",
      "        [ 2.6344, -1.1110, -6.0022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9678, -1.4277, -6.0857],\n",
      "        [-1.2214,  3.9643, -5.6199],\n",
      "        [ 2.9363, -1.1068, -5.8821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5994, -0.9700, -6.3151],\n",
      "        [ 2.7153, -1.2273, -6.0674],\n",
      "        [ 2.6833, -1.5002, -5.8315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0194, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7776, -1.2638, -6.3206],\n",
      "        [ 2.8739, -1.0588, -5.9492],\n",
      "        [ 2.8301, -1.0196, -6.3237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9939, -1.2743, -6.0530],\n",
      "        [-1.3708,  3.8875, -5.5600],\n",
      "        [ 2.8698, -1.3597, -6.0638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0214,  4.0124, -5.7942],\n",
      "        [-1.0150,  3.8500, -6.0624],\n",
      "        [ 2.7439, -1.3227, -6.1426]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0931,  4.3192, -5.4990],\n",
      "        [-0.9477,  3.8772, -6.2607],\n",
      "        [ 2.7706, -1.3874, -5.8123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0212,  3.8357, -4.9862],\n",
      "        [-1.0018,  4.3470, -5.4232],\n",
      "        [-0.2904,  2.6657, -6.8432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0234, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9206,  3.5827, -6.1721],\n",
      "        [ 2.5229, -0.7000, -6.5092],\n",
      "        [-0.7864,  3.1206, -6.4476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0902, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1781,  4.3549, -5.2947],\n",
      "        [ 1.8112,  0.5433, -6.5659],\n",
      "        [ 2.5844, -1.4099, -5.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0459, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1897,  2.3074, -6.8714],\n",
      "        [ 2.9078, -1.0152, -6.0767],\n",
      "        [-1.2961,  4.1673, -6.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4191, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0377,  0.8633, -7.1483],\n",
      "        [ 2.9735, -1.3153, -5.8116],\n",
      "        [ 1.2637,  0.7234, -6.4927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8836, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3538,  4.2851, -5.0557],\n",
      "        [-1.3038,  4.0141, -5.0895],\n",
      "        [-1.4715,  4.2405, -5.4550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1207,  4.1108, -5.6148],\n",
      "        [-1.0942,  4.1236, -5.5875],\n",
      "        [-1.1464,  4.5888, -5.2275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1295, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0066, -1.5377, -5.7295],\n",
      "        [ 2.7591, -0.5638, -6.4834],\n",
      "        [ 2.9098, -1.0270, -6.4214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6263, -0.7740, -6.2315],\n",
      "        [-1.0453,  4.1026, -5.4405],\n",
      "        [-0.1599,  2.1017, -6.8724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7877, -1.3637, -5.8617],\n",
      "        [ 2.8967, -0.9125, -6.1078],\n",
      "        [-1.0424,  3.9864, -5.2944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9371,  3.7770, -5.9159],\n",
      "        [-0.3895,  3.2517, -6.6678],\n",
      "        [-0.7305,  3.9337, -4.4405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7605, -1.3687, -6.0037],\n",
      "        [ 2.6429, -0.9537, -6.3011],\n",
      "        [-0.8210,  3.8283, -4.6808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2268, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8151,  3.5608, -3.9370],\n",
      "        [ 2.9660, -1.3664, -5.6479],\n",
      "        [ 1.4223,  1.3424, -7.2807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0277, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7552, -1.1352, -5.8618],\n",
      "        [ 2.4525, -0.5884, -6.4535],\n",
      "        [-0.6258,  3.5272, -4.3660]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0321, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6219,  3.5528, -4.7760],\n",
      "        [ 2.7117, -1.2136, -5.9272],\n",
      "        [ 2.5466, -0.2182, -6.8153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0543, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8231, -1.0879, -6.5730],\n",
      "        [ 0.1161,  2.1532, -7.0247],\n",
      "        [ 2.7213, -1.1791, -5.6134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0171, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6381,  3.4041, -4.0315],\n",
      "        [ 2.9326, -0.9318, -6.4044],\n",
      "        [-0.5666,  3.8268, -4.3798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0589, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1599,  0.3239, -6.8058],\n",
      "        [-0.8586,  3.5604, -4.0626],\n",
      "        [ 3.0400, -1.0786, -6.0025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5230,  3.5616, -3.9030],\n",
      "        [-0.5720,  3.3285, -3.8774],\n",
      "        [-0.4805,  3.5925, -4.5080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0152, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4557,  3.6625, -4.3829],\n",
      "        [-0.6705,  3.4651, -3.8195],\n",
      "        [-0.7275,  3.6407, -6.4145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0222, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0531, -0.8963, -6.4166],\n",
      "        [ 3.0604, -0.8907, -6.5902],\n",
      "        [ 2.6159, -0.9398, -6.5558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0151, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4870,  3.6460, -3.9180],\n",
      "        [ 2.6832, -1.3979, -6.1876],\n",
      "        [-0.6446,  3.7814, -5.2128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9706,  3.5135, -6.4505],\n",
      "        [ 2.8164, -1.1121, -6.3487],\n",
      "        [ 2.9994, -1.2969, -6.0839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7433,  3.7062, -4.2524],\n",
      "        [-0.4857,  3.4706, -3.7959],\n",
      "        [-0.4505,  3.4196, -4.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6018,  3.3871, -3.8351],\n",
      "        [-0.7723,  3.5772, -3.4945],\n",
      "        [-0.4877,  3.6073, -4.3292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0297, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3662, -0.5992, -6.4613],\n",
      "        [ 2.7275, -0.9738, -6.2385],\n",
      "        [ 2.8907, -1.3787, -5.8243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0290, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5903,  3.7418, -4.5373],\n",
      "        [-0.3018,  2.6736, -6.5451],\n",
      "        [ 2.6621, -1.0742, -5.7450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0253, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2777,  3.3545, -6.7491],\n",
      "        [ 2.3656, -0.8711, -6.6318],\n",
      "        [-0.6028,  3.9044, -4.7176]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1659, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6587,  0.3540, -6.7926],\n",
      "        [-0.8091,  3.6793, -4.1143],\n",
      "        [ 1.5671,  0.2884, -6.1266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8893, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7606, -1.1698, -6.0008],\n",
      "        [ 2.8242, -1.4869, -6.1670],\n",
      "        [-0.0308,  2.5296, -6.7720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0285, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6038,  4.2137, -5.4813],\n",
      "        [-0.5819,  3.5545, -4.3743],\n",
      "        [ 2.1910, -0.5727, -6.7355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0700, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5755,  3.5639, -4.8119],\n",
      "        [ 2.9572, -1.4807, -6.0594],\n",
      "        [ 0.5394,  2.1514, -6.7431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0409, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7104, -0.8210, -6.1910],\n",
      "        [-0.5260,  3.5441, -4.9056],\n",
      "        [-0.0195,  2.5138, -6.8132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1697,  2.8840, -6.6381],\n",
      "        [ 2.9751, -1.4371, -6.1057],\n",
      "        [ 2.8453, -1.4384, -5.9833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1562, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3258,  0.7320, -6.8923],\n",
      "        [ 3.2091, -1.3156, -5.9472],\n",
      "        [ 2.9752, -1.0431, -6.2814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1747, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4614,  1.7360, -7.2886],\n",
      "        [ 2.1053,  0.1598, -7.0158],\n",
      "        [ 0.2718,  2.1377, -6.9966]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7407,  3.7565, -4.9931],\n",
      "        [-0.7422,  3.6823, -6.3231],\n",
      "        [ 2.8923, -0.9832, -6.0882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0847, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5872,  0.1655, -6.5200],\n",
      "        [ 2.6611, -0.9389, -6.2814],\n",
      "        [ 3.0396, -1.5069, -5.5857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0148, -1.5532, -6.2803],\n",
      "        [-0.6451,  3.9004, -5.2708],\n",
      "        [-0.5623,  3.6240, -5.3648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3853, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9324,  1.6675, -7.2204],\n",
      "        [ 1.0250,  1.1254, -6.7730],\n",
      "        [-0.7223,  3.2244, -6.1218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4188, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0225, -1.5169, -5.7759],\n",
      "        [ 1.5026,  0.7665, -7.2291],\n",
      "        [ 0.2355,  2.3132, -7.2104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6458,  3.8602, -5.0197],\n",
      "        [-0.7230,  3.7369, -6.4839],\n",
      "        [ 2.9467, -1.3617, -6.0697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7814, -1.2778, -6.3344],\n",
      "        [-0.7670,  3.9025, -5.6575],\n",
      "        [-0.9095,  3.8411, -4.8825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0110, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0550, -1.2455, -5.7923],\n",
      "        [-0.6722,  3.8949, -4.9571],\n",
      "        [ 3.2290, -1.5020, -5.6732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0132, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7153,  4.0725, -5.4895],\n",
      "        [ 2.8703, -1.0819, -6.5878],\n",
      "        [ 2.8743, -1.5366, -5.8499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7761, -0.9947, -6.4519],\n",
      "        [-0.9344,  4.1064, -5.5121],\n",
      "        [ 2.9548, -1.1491, -6.2032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9342, -1.4190, -6.0735],\n",
      "        [-0.6761,  4.3020, -5.3041],\n",
      "        [ 3.1041, -1.6972, -5.5042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5026, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9767, -1.2255, -6.3388],\n",
      "        [ 0.3388,  1.4647, -6.7667],\n",
      "        [ 3.2070, -1.4516, -5.7013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5070, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8601, -1.4270, -5.9813],\n",
      "        [-0.8375,  4.1977, -5.3418],\n",
      "        [ 0.6661,  1.9143, -6.6987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2679, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9255,  3.8373, -5.0029],\n",
      "        [-0.6797,  4.0835, -5.1210],\n",
      "        [ 1.1907,  1.0121, -7.0712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0130, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8671, -1.3220, -5.8845],\n",
      "        [ 2.9295, -1.4676, -5.7054],\n",
      "        [-0.8227,  3.6471, -6.1424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3080, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1041, -0.1144, -6.6457],\n",
      "        [ 0.9062,  1.1362, -6.8029],\n",
      "        [-1.0504,  4.1488, -5.9503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8994,  4.0301, -5.2888],\n",
      "        [ 2.8587, -1.3829, -5.8402],\n",
      "        [-0.9440,  4.0425, -6.0319]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9814, -1.4083, -5.5353],\n",
      "        [ 3.0533, -1.3251, -5.7874],\n",
      "        [ 2.8848, -1.2564, -6.2313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1240, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7865, -0.8602, -6.2258],\n",
      "        [ 1.4942,  0.5849, -6.7529],\n",
      "        [-0.8843,  4.0277, -5.4353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0354, -1.5952, -5.5765],\n",
      "        [ 2.7720, -1.6682, -5.6232],\n",
      "        [ 3.1670, -1.3924, -6.0104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8047,  4.1920, -5.4897],\n",
      "        [-0.7919,  4.2650, -5.2986],\n",
      "        [ 2.6404, -0.9324, -5.8829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0295, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8662,  4.2348, -4.8807],\n",
      "        [ 2.2823, -0.3129, -6.6514],\n",
      "        [ 3.0110, -1.5840, -5.6865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5460, -0.8907, -6.6222],\n",
      "        [-1.1345,  4.1577, -5.2813],\n",
      "        [ 3.0324, -1.1396, -6.0952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8866, -1.3444, -5.9490],\n",
      "        [ 3.0461, -1.5485, -5.8095],\n",
      "        [-0.7999,  3.9853, -4.6957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2189, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9812, -1.5065, -6.1579],\n",
      "        [ 1.0445,  1.3594, -7.2500],\n",
      "        [ 2.1558, -0.1263, -6.9572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0227, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7813, -1.5294, -5.8096],\n",
      "        [ 2.9029, -1.5856, -5.6241],\n",
      "        [-0.4431,  2.6753, -6.8093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1060, -1.6175, -6.0367],\n",
      "        [-0.7892,  4.1933, -5.2744],\n",
      "        [-1.0343,  4.3060, -5.7133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0425, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9290,  4.1816, -5.7318],\n",
      "        [ 3.1158, -1.5132, -6.1419],\n",
      "        [ 2.0998, -0.0386, -6.7521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6068,  4.0146, -5.2570],\n",
      "        [ 2.8794, -1.2578, -5.8592],\n",
      "        [-0.9795,  4.1418, -5.3638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8412, -1.5840, -5.8386],\n",
      "        [ 2.3106, -0.6670, -6.5555],\n",
      "        [ 2.7183, -1.1193, -6.4262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9657, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0358,  4.1248, -4.9275],\n",
      "        [ 2.8460, -1.0863, -5.9796],\n",
      "        [ 2.3057, -0.5076, -6.7506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0135, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0864,  4.2722, -5.1134],\n",
      "        [ 2.8642, -1.1948, -5.9302],\n",
      "        [ 2.9049, -1.0841, -6.6156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6491, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5745, -1.6788, -5.8314],\n",
      "        [-0.7574,  4.1544, -5.1234],\n",
      "        [ 2.9717, -1.3231, -6.0215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7207, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0675,  0.0571, -6.8881],\n",
      "        [-0.8005,  4.1548, -5.4021],\n",
      "        [ 2.7886, -1.1780, -6.3496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4375,  3.3817, -6.6350],\n",
      "        [ 2.9865, -1.5356, -6.0311],\n",
      "        [-0.9166,  4.0936, -5.0906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5659, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8309, -1.0879, -6.1031],\n",
      "        [ 1.9142, -0.3322, -6.8336],\n",
      "        [ 0.2624,  1.6080, -6.8347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0199, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6566,  3.8667, -4.6057],\n",
      "        [ 2.5617, -0.7461, -6.4089],\n",
      "        [-0.4544,  3.9070, -5.6376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0213, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7024, -1.0648, -6.1149],\n",
      "        [ 2.5612, -1.0498, -5.9920],\n",
      "        [-0.4796,  3.7969, -5.1186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1366, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0007,  0.0114, -6.4881],\n",
      "        [ 2.0200,  0.5443, -7.7457],\n",
      "        [ 2.4078, -0.1405, -6.8945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5676, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8313,  0.1517, -7.4348],\n",
      "        [ 0.6038,  1.8398, -6.8189],\n",
      "        [ 2.4523, -0.7376, -6.5584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1131, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7449,  0.0379, -6.2250],\n",
      "        [ 2.6129, -1.1644, -6.3961],\n",
      "        [ 2.2402,  0.4149, -7.0809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2162,  3.4177, -5.1436],\n",
      "        [ 2.6174, -0.8370, -6.6892],\n",
      "        [-0.3527,  3.5234, -4.8326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0649, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2809,  3.3571, -4.6131],\n",
      "        [ 0.1822,  2.0414, -7.0427],\n",
      "        [-0.2445,  3.4984, -5.0663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0243, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0562,  3.3158, -4.7953],\n",
      "        [-0.4138,  3.4694, -4.7950],\n",
      "        [-0.4342,  3.6009, -3.8285]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0506, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0305, -1.4705, -5.6675],\n",
      "        [ 2.2067,  0.1132, -6.6111],\n",
      "        [-0.4262,  3.3006, -4.3854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0447, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8050, -1.2252, -5.9764],\n",
      "        [ 2.8342, -1.1809, -6.2662],\n",
      "        [ 0.1356,  2.4069, -6.7048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3414,  3.3381, -4.5029],\n",
      "        [-0.0372,  3.0546, -4.6334],\n",
      "        [ 2.6316, -0.6801, -6.5496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9946, -1.3685, -6.4297],\n",
      "        [ 2.7233, -1.0667, -6.4533],\n",
      "        [ 2.5057, -0.7692, -6.6129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0235, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2327,  3.3656, -5.0649],\n",
      "        [-0.2523,  3.3682, -4.8921],\n",
      "        [ 2.8252, -1.2660, -6.0875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9609, -1.5497, -5.6294],\n",
      "        [-0.1106,  3.3645, -5.3980],\n",
      "        [-0.3231,  3.5436, -4.4902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0271, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9500, -1.4622, -5.6891],\n",
      "        [ 2.4424, -0.6312, -6.1458],\n",
      "        [ 2.8188, -0.9121, -6.2270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2045, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1589,  3.3717, -6.5541],\n",
      "        [-0.3560,  3.8491, -5.3490],\n",
      "        [ 2.5990, -0.6252, -6.7867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2618,  3.1338, -5.7744],\n",
      "        [ 2.7212, -1.6185, -5.7139],\n",
      "        [ 2.9907, -1.7239, -5.7267]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0279, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7918, -1.2508, -6.6367],\n",
      "        [ 2.2480, -0.7355, -6.3636],\n",
      "        [ 2.9648, -1.1254, -6.0335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2250, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8369,  0.9909, -6.7597],\n",
      "        [-0.0255,  3.4944, -4.9109],\n",
      "        [-0.1164,  3.5149, -5.1800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8127, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0162,  3.4843, -5.2429],\n",
      "        [ 2.0648, -0.2182, -6.8363],\n",
      "        [-0.1685,  3.4355, -6.2823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7628, -1.4786, -5.9883],\n",
      "        [-0.2391,  3.2222, -4.9315],\n",
      "        [-0.2168,  2.9546, -6.9208]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4626, -0.7334, -6.6761],\n",
      "        [ 2.9495, -1.5086, -5.7989],\n",
      "        [ 1.9369, -0.0456, -6.5720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0612, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0815,  2.6924, -6.4238],\n",
      "        [ 2.8493, -1.1635, -5.9592],\n",
      "        [-0.0209,  2.2904, -6.7719]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0218, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5266,  3.3321, -5.2774],\n",
      "        [ 2.8577, -1.4615, -6.0708],\n",
      "        [-0.2154,  3.2557, -4.9693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0359, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9276, -0.9913, -6.3310],\n",
      "        [-0.0752,  2.8470, -6.6202],\n",
      "        [-0.1934,  3.1345, -6.2981]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7273, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5784,  1.5676, -6.7105],\n",
      "        [ 1.2774,  0.8859, -6.4798],\n",
      "        [ 1.3168,  0.8443, -6.1418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6407, -1.2867, -6.3266],\n",
      "        [ 2.7135, -1.5616, -5.6975],\n",
      "        [-0.0854,  3.2467, -5.8810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1997, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1736,  1.5399, -6.9131],\n",
      "        [ 1.6174,  0.5944, -7.2053],\n",
      "        [-0.1176,  2.5911, -6.5390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3532, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8116, -1.4753, -5.9351],\n",
      "        [ 0.7150,  1.3038, -7.0331],\n",
      "        [ 2.8725, -1.2965, -6.2905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1055, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0541e-01,  2.5967e+00, -6.9198e+00],\n",
      "        [ 2.6090e+00, -6.8630e-01, -6.3221e+00],\n",
      "        [ 1.5072e+00,  3.5306e-04, -6.6399e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1355, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2186,  0.4126, -6.4693],\n",
      "        [ 3.1000, -1.3226, -5.3937],\n",
      "        [-0.3110,  3.3706, -6.2426]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2241, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2311, -0.4310, -6.5272],\n",
      "        [ 1.0621,  0.8246, -6.4362],\n",
      "        [ 2.7191, -1.0516, -5.7767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0565,  2.3138, -7.0277],\n",
      "        [ 2.6375, -0.9032, -6.4443],\n",
      "        [ 3.0013, -1.7537, -5.7971]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5844, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2471,  2.8292, -6.4302],\n",
      "        [ 3.1129, -1.5659, -5.6935],\n",
      "        [ 1.7485,  0.2522, -6.1350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2222,  3.3719, -6.5354],\n",
      "        [ 3.1572, -1.5714, -5.8000],\n",
      "        [ 2.8952, -1.4077, -6.1780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1552, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0975, -1.9128, -5.4016],\n",
      "        [ 2.5817, -0.8336, -6.1146],\n",
      "        [ 3.0538, -1.4822, -5.4590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3252,  3.3860, -5.5163],\n",
      "        [ 3.0327, -1.5604, -5.7971],\n",
      "        [ 2.6997, -1.5730, -5.9363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8477, -1.1733, -5.5746],\n",
      "        [ 2.8956, -1.3717, -5.8709],\n",
      "        [-0.4203,  3.4375, -5.1831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6976, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0915,  2.5132, -6.9518],\n",
      "        [ 1.4308,  0.3549, -6.3539],\n",
      "        [ 0.9599,  1.0735, -6.6383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3306, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5574,  3.6733, -5.4212],\n",
      "        [ 1.3418,  0.8567, -6.0245],\n",
      "        [ 2.8600, -1.5597, -6.0550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3445, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1615, -1.6499, -5.9687],\n",
      "        [ 0.2450,  2.5013, -6.2754],\n",
      "        [ 1.2289,  0.8086, -5.9448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0843, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1266,  1.9464, -6.7970],\n",
      "        [ 2.5631, -0.5397, -6.3487],\n",
      "        [-0.0563,  2.7550, -6.7295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0217, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4396,  3.3268, -4.9512],\n",
      "        [-0.5021,  3.1159, -6.6948],\n",
      "        [ 2.8868, -1.2936, -6.0441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0349, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4966,  3.6338, -5.2138],\n",
      "        [ 2.3818, -0.2801, -6.3804],\n",
      "        [-0.3220,  3.5448, -5.1126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0181, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4913,  3.7557, -5.8590],\n",
      "        [ 2.8172, -1.0995, -6.4870],\n",
      "        [-0.4913,  3.3999, -5.7674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1412, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0108,  0.2349, -6.6749],\n",
      "        [ 1.8241,  0.0693, -6.3468],\n",
      "        [ 1.9750, -0.2037, -6.4405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2512, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3506, -0.4222, -5.9924],\n",
      "        [-0.4284,  3.2050, -5.9945],\n",
      "        [ 0.7342,  0.7902, -5.5068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1713, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5960, -1.1141, -6.2363],\n",
      "        [ 0.7865,  1.3949, -5.8737],\n",
      "        [-0.1310,  2.7523, -6.2872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1181, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4293,  3.5011, -5.0748],\n",
      "        [ 2.3839, -0.5135, -6.1563],\n",
      "        [ 1.5494,  0.4210, -6.0177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5895,  3.2797, -6.3434],\n",
      "        [-0.6392,  3.7228, -5.0406],\n",
      "        [-0.4996,  3.6418, -4.7640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8198, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2134,  2.1267, -6.2823],\n",
      "        [ 2.7193, -1.5347, -5.7525],\n",
      "        [-0.6355,  3.7241, -4.5480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5407, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7301,  0.9839, -5.7386],\n",
      "        [ 2.6281, -0.7966, -6.1367],\n",
      "        [ 0.8519,  0.9822, -5.5894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2743, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6787,  0.8303, -5.8642],\n",
      "        [ 2.4477, -1.0941, -6.2954],\n",
      "        [ 2.5587, -1.2698, -5.8460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0199, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7448,  3.7206, -4.7916],\n",
      "        [ 2.5925, -0.8929, -5.9702],\n",
      "        [ 2.7626, -1.2736, -6.3839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0370, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4968,  3.3302, -4.3131],\n",
      "        [-0.6629,  3.7300, -4.8307],\n",
      "        [ 2.2886, -0.2439, -6.3531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8298, -1.0812, -6.1654],\n",
      "        [-0.7319,  3.6078, -4.9755],\n",
      "        [-0.5920,  3.6457, -5.9084]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0734, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7731, -1.0300, -5.5378],\n",
      "        [ 2.7219, -1.1661, -6.1730],\n",
      "        [ 1.8157,  0.1748, -6.4388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2916, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5476,  3.9837, -5.5997],\n",
      "        [ 0.9928,  1.2895, -6.2448],\n",
      "        [ 2.9414, -1.5611, -5.8673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1756, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7352,  3.8977, -5.0031],\n",
      "        [ 1.4489,  0.6271, -6.6760],\n",
      "        [ 0.0694,  1.8750, -6.5207]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4463, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9142, -1.0587, -5.6141],\n",
      "        [ 2.8096, -1.4730, -5.4500],\n",
      "        [ 2.6635, -1.0776, -5.9093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0142, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7098,  3.4537, -6.5135],\n",
      "        [-0.6873,  3.5078, -6.2893],\n",
      "        [ 2.8579, -1.5767, -5.7829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0283, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4069,  3.6771, -4.9466],\n",
      "        [ 2.6863, -1.3159, -5.1721],\n",
      "        [-0.2782,  2.7039, -6.6553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2113, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8266, -1.2884, -5.9452],\n",
      "        [ 0.3889,  1.0818, -6.0099],\n",
      "        [ 0.3539,  1.8006, -6.9183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8375, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6833, -0.8060, -6.1801],\n",
      "        [ 2.2011, -0.1099, -6.6230],\n",
      "        [ 2.4248, -0.6047, -6.4723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6228, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8951,  0.2404, -6.4556],\n",
      "        [-0.6949,  3.7680, -5.8786],\n",
      "        [ 2.5241, -1.0833, -5.8460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0193, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5655, -1.0941, -5.9674],\n",
      "        [ 2.8054, -1.0908, -6.0673],\n",
      "        [-0.4565,  3.9698, -5.4615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0317, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4538, -0.5164, -6.5250],\n",
      "        [-0.5467,  3.8037, -6.3646],\n",
      "        [ 2.4960, -0.9334, -5.8677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0179, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6327, -0.7332, -6.3263],\n",
      "        [-0.8559,  4.0951, -5.4903],\n",
      "        [-0.7445,  3.6322, -5.5073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0192, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8373,  3.5173, -5.5077],\n",
      "        [-0.5301,  3.7946, -5.1042],\n",
      "        [ 2.5056, -0.9458, -6.2574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0302, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8445,  3.8573, -5.8270],\n",
      "        [ 2.6434, -0.7512, -6.1872],\n",
      "        [ 2.4279, -0.5790, -6.4634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6788,  3.7781, -5.2089],\n",
      "        [-0.7024,  3.6266, -5.6897],\n",
      "        [-0.3884,  2.8673, -6.7633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4681,  3.6999, -5.1836],\n",
      "        [-0.4914,  3.8470, -5.3855],\n",
      "        [-0.5612,  3.8215, -5.8223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0681, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5935,  3.9379, -5.9147],\n",
      "        [ 1.9784,  0.0124, -6.5353],\n",
      "        [ 2.2240, -0.5223, -5.9707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3861, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9707,  0.8588, -6.2175],\n",
      "        [ 1.2327,  0.8262, -6.6744],\n",
      "        [-0.8598,  3.9273, -5.0713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0976, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7816, -0.0112, -6.7343],\n",
      "        [ 2.2549, -0.1881, -6.4244],\n",
      "        [-0.2946,  2.5775, -6.6520]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0260, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3037, -0.4930, -6.6178],\n",
      "        [-0.8795,  3.8952, -5.1477],\n",
      "        [-0.7706,  3.8423, -5.0701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9448, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5723, -0.5637, -6.1440],\n",
      "        [-0.3740,  2.2764, -6.8478],\n",
      "        [ 2.1295, -0.4517, -6.5754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4083, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6247,  0.2319, -6.4453],\n",
      "        [ 2.1913, -0.8214, -6.2265],\n",
      "        [ 1.4220,  0.9535, -7.2859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1737, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2855,  0.7433, -7.3033],\n",
      "        [-0.7394,  3.8464, -6.2739],\n",
      "        [ 2.2647, -0.6573, -6.3432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0374, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7644,  3.8404, -5.1948],\n",
      "        [ 2.3985, -0.4999, -6.4446],\n",
      "        [ 2.2096, -0.7983, -6.1484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1215, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9514,  3.8745, -4.6162],\n",
      "        [-0.7787,  3.9613, -5.0379],\n",
      "        [ 1.3731,  0.4934, -6.4314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0735, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6574,  3.9212, -5.1481],\n",
      "        [ 1.8410,  0.2245, -6.6788],\n",
      "        [-0.5102,  3.0218, -6.4896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0544, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2719,  1.9636, -6.5702],\n",
      "        [ 1.1944,  1.2447, -7.4910],\n",
      "        [ 0.9604,  1.1930, -7.1838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0382, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5698, -0.5782, -6.2646],\n",
      "        [ 2.4721, -0.8218, -6.2876],\n",
      "        [ 2.6505, -0.6646, -6.3576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1071, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4617,  3.4235, -6.6676],\n",
      "        [ 1.4542,  0.3084, -7.0916],\n",
      "        [ 2.7055, -0.9850, -6.2959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4510, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3652,  1.5731, -6.9297],\n",
      "        [ 1.3958,  0.7303, -7.2594],\n",
      "        [-0.8201,  3.6906, -5.8239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0356, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8540,  3.7643, -5.6494],\n",
      "        [ 2.1748, -0.2276, -6.3964],\n",
      "        [-0.7007,  3.8989, -5.5846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0178, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6816, -1.1101, -5.7402],\n",
      "        [ 2.7745, -1.0003, -5.8644],\n",
      "        [-0.7825,  4.0526, -5.4101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0981, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4472, -0.7645, -6.4559],\n",
      "        [-0.6162,  3.6545, -5.5174],\n",
      "        [ 2.4857, -1.0409, -6.1359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8636,  3.9770, -5.0754],\n",
      "        [-0.6883,  3.7664, -5.6522],\n",
      "        [-0.8246,  3.7278, -4.7908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0256, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7228, -0.8561, -6.1867],\n",
      "        [ 2.8663, -1.0076, -6.4532],\n",
      "        [ 2.6767, -0.8696, -6.5964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6961,  3.8792, -5.2655],\n",
      "        [ 2.8094, -1.1755, -5.9361],\n",
      "        [-0.8343,  3.8821, -5.5747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4261, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4390,  0.5589, -7.1572],\n",
      "        [ 2.6046, -0.9641, -6.1007],\n",
      "        [ 2.7549, -0.9987, -6.0349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0779, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0761, -0.2515, -6.6486],\n",
      "        [ 2.3359, -0.4655, -6.2530],\n",
      "        [ 2.1291, -0.3405, -6.8205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1545, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5309, -1.0553, -6.1126],\n",
      "        [ 1.6822,  0.3453, -6.7811],\n",
      "        [ 2.0032,  0.5090, -7.5126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0262, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8495,  3.9227, -5.7982],\n",
      "        [ 2.3015, -0.4306, -6.7198],\n",
      "        [-0.7290,  4.2686, -4.8450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0885, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9070,  0.5147, -6.6761],\n",
      "        [-0.8111,  3.9125, -4.9402],\n",
      "        [ 2.6991, -0.6581, -6.6199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1648, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3799,  1.6520, -6.6916],\n",
      "        [ 1.5074,  0.1899, -6.5017],\n",
      "        [-0.7321,  3.9179, -5.6429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0828, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7718,  4.0008, -5.2363],\n",
      "        [-0.7431,  3.8681, -5.1402],\n",
      "        [ 1.5240,  0.1709, -7.3438]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0576, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6272,  3.9737, -5.1294],\n",
      "        [ 0.2178,  2.0165, -6.5733],\n",
      "        [-0.8468,  3.8435, -4.6959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4605, -0.8236, -5.9636],\n",
      "        [-0.8383,  3.7899, -5.2611],\n",
      "        [-0.8439,  3.7194, -5.6262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0198, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8467,  4.0679, -5.5833],\n",
      "        [ 2.4358, -0.6618, -6.5596],\n",
      "        [-0.9340,  3.9318, -5.4849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1562, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7183,  3.8978, -4.9682],\n",
      "        [ 2.1913, -0.5827, -6.2162],\n",
      "        [ 1.3279,  0.6114, -7.1948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6843, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8459,  3.8928, -4.7102],\n",
      "        [ 1.8972,  0.2272, -6.8413],\n",
      "        [ 0.3097,  2.0138, -6.8253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8365,  3.7564, -5.3722],\n",
      "        [-1.0767,  3.9766, -5.3479],\n",
      "        [-0.7492,  4.2060, -5.2321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0215, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6592, -0.9822, -6.0261],\n",
      "        [-0.7048,  3.4643, -6.0550],\n",
      "        [ 2.5275, -1.2381, -6.1058]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0174, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4934, -0.7889, -6.2450],\n",
      "        [-0.6557,  4.1671, -5.8840],\n",
      "        [-0.9767,  3.9710, -5.7013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0246, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5735, -1.0932, -6.1640],\n",
      "        [ 2.7251, -1.2634, -5.9709],\n",
      "        [ 2.5213, -0.9800, -5.8424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1580, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8919,  1.4738, -6.7534],\n",
      "        [-0.9181,  3.9215, -5.8116],\n",
      "        [ 2.7937, -1.0251, -6.3475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0568, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9936, -0.1578, -7.1083],\n",
      "        [ 2.3874, -0.5650, -6.7709],\n",
      "        [-0.7477,  3.9457, -6.0224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8982,  4.0754, -5.4541],\n",
      "        [-0.7984,  3.9267, -4.8068],\n",
      "        [-0.7357,  3.9105, -5.1531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1720,  2.0795, -6.7552],\n",
      "        [ 2.6711, -0.8464, -6.3377],\n",
      "        [ 2.6899, -1.3148, -6.0250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8074, -1.3166, -6.0160],\n",
      "        [ 2.4035, -0.7781, -6.1312],\n",
      "        [-1.0726,  3.9807, -5.7458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8960,  3.7245, -4.6756],\n",
      "        [-0.9275,  3.8994, -5.0438],\n",
      "        [-0.9382,  4.2762, -5.2994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6155, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9604, -1.2673, -5.6294],\n",
      "        [ 0.1793,  1.9069, -6.9026],\n",
      "        [ 1.7381,  0.2792, -6.8235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4947, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5648,  0.4155, -6.8844],\n",
      "        [ 2.6356, -0.7558, -6.3100],\n",
      "        [ 2.6008, -1.0399, -6.0712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6293, -1.0056, -6.3934],\n",
      "        [-0.7803,  3.9973, -5.0868],\n",
      "        [-0.8483,  3.9467, -5.7530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0350, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6473, -1.0232, -6.0241],\n",
      "        [-0.9299,  3.9602, -4.9536],\n",
      "        [-0.3526,  2.2416, -6.8749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1274, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0934, -0.4025, -6.6911],\n",
      "        [-0.4251,  2.7412, -6.9895],\n",
      "        [ 1.5661,  0.3575, -7.1802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8708,  4.3364, -5.1929],\n",
      "        [ 2.7592, -0.8638, -5.8027],\n",
      "        [ 2.7811, -1.4617, -5.3445]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0398, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  2.7625, -6.6773],\n",
      "        [ 2.1140, -0.5827, -6.1074],\n",
      "        [-0.6712,  3.8853, -6.6239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0380, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8023,  3.9913, -5.2981],\n",
      "        [-0.8411,  4.3464, -5.1132],\n",
      "        [ 2.1184, -0.1363, -6.6408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9225, -1.1640, -5.5922],\n",
      "        [-0.9445,  3.6876, -6.0323],\n",
      "        [-1.1419,  4.1400, -4.8990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0283, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7846,  4.0086, -5.1516],\n",
      "        [ 2.3526, -0.2822, -6.5298],\n",
      "        [-0.9082,  4.0419, -5.0217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5544, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7867, -0.9097, -5.9449],\n",
      "        [ 2.9170, -1.6467, -5.5764],\n",
      "        [-0.2132,  2.5001, -6.5357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2252, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0313,  0.9386, -6.7256],\n",
      "        [-1.0646,  4.1034, -4.8589],\n",
      "        [ 2.3979, -1.4316, -5.6393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2709, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8850,  1.0586, -6.7258],\n",
      "        [ 2.7298, -1.1443, -5.6693],\n",
      "        [-0.9004,  3.9366, -5.0390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8805, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9531,  4.1235, -4.3430],\n",
      "        [-0.0958,  2.4409, -6.5753],\n",
      "        [ 2.5251, -1.2832, -5.9348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1450, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2764, -0.3094, -6.8515],\n",
      "        [ 1.5185,  0.6621, -6.9030],\n",
      "        [-0.8047,  4.0266, -5.0762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3180, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7529,  3.6256, -6.2227],\n",
      "        [ 0.7260,  1.1436, -6.7644],\n",
      "        [ 2.8561, -1.1803, -5.7856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3692, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6014, -1.3985, -5.4216],\n",
      "        [ 0.8315,  1.0395, -6.7448],\n",
      "        [ 0.6619,  1.1095, -7.2775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5912, -1.5112, -5.5920],\n",
      "        [ 2.7152, -1.0083, -6.3144],\n",
      "        [ 2.5182, -1.2029, -6.1115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4608, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1151,  0.4392, -6.3761],\n",
      "        [ 1.2598,  0.7858, -6.9883],\n",
      "        [ 2.8788, -1.4972, -5.7254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0236, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4258, -0.7514, -6.6038],\n",
      "        [ 2.6918, -1.0428, -6.0291],\n",
      "        [-0.9759,  4.1376, -4.9150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2106, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6176,  1.2825, -7.0728],\n",
      "        [ 0.3214,  1.7736, -6.8955],\n",
      "        [-1.0818,  4.0224, -4.4255]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8266, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7995, -1.5332, -5.7274],\n",
      "        [ 1.8352, -0.1601, -6.7840],\n",
      "        [ 1.6564,  0.7642, -7.2222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6105, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2857,  1.4023, -6.7914],\n",
      "        [-0.3922,  3.2935, -7.0312],\n",
      "        [ 0.1841,  1.4609, -6.2177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8918, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1648, -0.4141, -6.7840],\n",
      "        [-1.2178,  3.9209, -4.8074],\n",
      "        [ 2.6656, -1.3929, -6.2682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4892, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5962, -0.7997, -5.9897],\n",
      "        [ 1.3773,  0.2256, -6.8155],\n",
      "        [-0.8981,  3.9382, -5.4871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0453, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1440,  2.3873, -6.7756],\n",
      "        [ 2.7118, -0.8772, -6.1386],\n",
      "        [-1.0341,  3.8746, -4.7876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8106, -0.9276, -5.7358],\n",
      "        [-1.1491,  3.9574, -5.0217],\n",
      "        [ 2.1389, -0.2082, -6.7161]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0197,  3.8139, -5.6554],\n",
      "        [ 2.2051, -0.5710, -6.4972],\n",
      "        [ 2.5928, -0.9198, -5.9356]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4163, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9936,  3.9866, -5.6659],\n",
      "        [ 1.3571,  0.5138, -6.4964],\n",
      "        [-0.6314,  2.5554, -6.4139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0552, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8659,  4.2871, -4.6019],\n",
      "        [ 2.5728, -0.3538, -6.5114],\n",
      "        [ 2.0525, -0.1268, -6.5762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0860, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9207,  4.0851, -5.6382],\n",
      "        [-1.1101,  4.0578, -5.3162],\n",
      "        [ 1.5292,  0.2490, -6.2531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1752, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2393,  2.9528, -6.5374],\n",
      "        [ 2.2375, -0.1914, -6.5663],\n",
      "        [ 1.4339,  0.7255, -6.7968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0542, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5158, -0.2588, -6.5312],\n",
      "        [-0.9026,  3.9329, -6.1502],\n",
      "        [ 2.2358, -0.0822, -6.8367]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0352, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9764,  3.9987, -5.0409],\n",
      "        [ 2.5302, -0.8756, -6.0986],\n",
      "        [ 2.4165, -0.2712, -6.6943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0223, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8801,  3.7267, -6.4758],\n",
      "        [ 2.6212, -0.3434, -6.3496],\n",
      "        [-1.1028,  3.9451, -4.8590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4038, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6447,  1.4479, -6.7656],\n",
      "        [ 2.6220, -0.8166, -5.9910],\n",
      "        [-0.9710,  4.1707, -4.5448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9430, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1445,  3.9639, -3.8523],\n",
      "        [-0.1946,  2.5610, -6.5314],\n",
      "        [-1.0573,  4.2522, -4.1599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1189, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3466, -0.7426, -5.9112],\n",
      "        [ 2.1377, -0.2292, -6.6910],\n",
      "        [ 1.5878,  0.1963, -6.6958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0666, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0813,  3.9502, -5.7291],\n",
      "        [ 2.0072,  0.2544, -6.4844],\n",
      "        [-0.4530,  2.9347, -6.6809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0237, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3715, -0.4705, -6.5898],\n",
      "        [-1.0763,  3.8724, -4.9799],\n",
      "        [-0.7262,  4.2248, -4.6838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1730, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1449,  1.9429, -6.7580],\n",
      "        [ 0.6395,  1.5292, -6.5310],\n",
      "        [ 1.3412,  0.4138, -7.0026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0397, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4064, -0.6158, -6.2701],\n",
      "        [ 2.4395, -0.5355, -6.6818],\n",
      "        [ 2.7539, -1.0917, -5.4120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4410, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4227,  1.8370, -7.0266],\n",
      "        [ 0.8027,  1.2212, -6.8173],\n",
      "        [ 1.9452,  0.3259, -7.0475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1504, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5739,  0.3553, -7.1874],\n",
      "        [ 2.3374, -0.7190, -6.0796],\n",
      "        [ 1.8689,  0.0158, -6.2474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0166, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8017,  3.6940, -5.9280],\n",
      "        [ 2.7186, -0.9019, -5.7882],\n",
      "        [-0.5532,  3.8543, -5.9935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0696, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8552, -0.9775, -5.5692],\n",
      "        [-0.5802,  3.5094, -6.3239],\n",
      "        [ 1.8005,  0.1177, -6.8849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3966, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6096,  0.2622, -7.3759],\n",
      "        [ 0.2787,  2.0451, -7.0211],\n",
      "        [ 0.9983,  0.7942, -6.5120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0943, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2973,  1.7903, -6.9528],\n",
      "        [-0.1531,  2.7921, -6.5611],\n",
      "        [ 2.5179, -1.0234, -6.0431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0270, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4208,  2.7868, -6.7763],\n",
      "        [ 2.7911, -1.2531, -5.4489],\n",
      "        [ 2.6685, -1.0752, -6.2264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3395, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2719,  3.1028, -6.7484],\n",
      "        [ 0.1586,  2.4718, -6.8086],\n",
      "        [ 1.1360,  0.7743, -6.8083]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0202, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6767, -1.0937, -5.9031],\n",
      "        [ 2.8280, -0.9726, -6.1003],\n",
      "        [ 2.9333, -1.2522, -5.5668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8223, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0422,  0.9126, -6.8596],\n",
      "        [ 1.7464,  0.2712, -7.3200],\n",
      "        [ 2.5962, -1.0694, -6.1167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2654, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8000,  1.1822, -7.1459],\n",
      "        [ 0.1516,  1.4464, -6.6886],\n",
      "        [-0.4011,  2.9769, -6.7054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5660, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3179,  3.0324, -6.5571],\n",
      "        [-0.7665,  3.6753, -5.4140],\n",
      "        [ 1.6936,  0.2550, -7.0196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3546, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0406,  2.0715, -7.1629],\n",
      "        [ 0.5862,  1.0214, -6.7556],\n",
      "        [ 2.7781, -1.4091, -6.4621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2794, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7293, -1.3623, -5.7795],\n",
      "        [-0.9382,  3.9001, -5.3503],\n",
      "        [ 0.7026,  0.9299, -7.1240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6736, -1.2378, -5.5134],\n",
      "        [-0.8488,  4.1221, -4.9377],\n",
      "        [-0.7736,  3.6753, -5.4483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6821,  4.0861, -4.7824],\n",
      "        [ 2.8566, -1.1476, -5.6658],\n",
      "        [ 2.8540, -1.4312, -5.8913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2347, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5946, -0.9910, -6.2080],\n",
      "        [ 1.0476,  1.1008, -6.4878],\n",
      "        [-0.7515,  3.9261, -4.9924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0842, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8595,  3.9263, -4.4843],\n",
      "        [-0.8036,  3.8652, -5.2613],\n",
      "        [ 1.5464,  0.2157, -6.8262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0735, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9054,  3.8122, -5.3558],\n",
      "        [ 1.5969,  0.0579, -6.8843],\n",
      "        [-0.8025,  3.2784, -5.7470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9987, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9019, -0.9575, -5.9663],\n",
      "        [ 2.1946, -0.7195, -6.5002],\n",
      "        [-0.9013,  3.9534, -4.1046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2296, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7815,  0.8523, -6.5261],\n",
      "        [-0.7156,  3.8784, -4.9017],\n",
      "        [ 2.8225, -1.0930, -5.3744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0479, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9333, -1.0365, -5.9840],\n",
      "        [ 1.9503, -0.1915, -6.6182],\n",
      "        [ 2.7887, -1.5205, -5.4629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0134,  3.8705, -4.0088],\n",
      "        [ 2.9607, -1.3013, -6.0862],\n",
      "        [ 2.8399, -1.1273, -6.2177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0323, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6611, -1.1514, -5.3907],\n",
      "        [-0.0421,  2.8045, -6.6698],\n",
      "        [ 2.7006, -1.2977, -5.8617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2190, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1125,  3.5716, -4.9381],\n",
      "        [ 0.8255,  0.6975, -6.8239],\n",
      "        [ 2.8277, -1.3045, -5.5614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1889, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7447,  3.2780, -5.8704],\n",
      "        [ 0.8990,  1.2338, -6.9073],\n",
      "        [-0.8741,  3.8882, -4.1654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3423, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8974,  3.9159, -4.2200],\n",
      "        [-0.8226,  4.0710, -3.9211],\n",
      "        [ 0.8203,  1.3784, -6.7657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0909, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6686,  0.3779, -6.9053],\n",
      "        [ 2.8463, -0.9988, -5.9257],\n",
      "        [-0.9133,  3.8998, -4.7741]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2998, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2818,  1.5313, -6.6513],\n",
      "        [-0.9030,  3.9186, -3.6213],\n",
      "        [ 0.8422,  0.9552, -6.7751]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1485, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6052, -0.6974, -6.2690],\n",
      "        [ 1.4244,  0.6678, -7.0701],\n",
      "        [ 2.6810, -1.0369, -5.7831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0070,  3.8263, -3.6220],\n",
      "        [-1.1759,  4.0360, -3.9250],\n",
      "        [ 2.8313, -1.3616, -5.8539]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8292, -1.5459, -5.2853],\n",
      "        [-1.1550,  3.9985, -3.6453],\n",
      "        [-0.7911,  4.1833, -3.8939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1424, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3439, -0.4968, -6.3317],\n",
      "        [ 1.3965,  0.5662, -6.6225],\n",
      "        [-0.9812,  3.8858, -3.6907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0876, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3868,  1.6527, -6.4353],\n",
      "        [-0.9242,  4.1991, -3.9887],\n",
      "        [-0.9453,  3.9107, -4.1020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0262, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2371, -0.6217, -6.4270],\n",
      "        [ 2.7083, -1.3818, -5.1549],\n",
      "        [-1.1059,  4.1150, -3.8945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0166, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0686,  4.1644, -3.7612],\n",
      "        [-1.2307,  4.0810, -4.4653],\n",
      "        [ 2.4753, -0.7488, -6.4639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2581, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7402, -1.1552, -5.7447],\n",
      "        [ 0.8242,  0.9096, -5.9310],\n",
      "        [ 2.7641, -1.3542, -5.0992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0102, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9261, -1.3251, -5.6842],\n",
      "        [-0.8717,  3.7521, -6.0753],\n",
      "        [-1.0109,  4.1058, -3.6445]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6773, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7400,  0.1575, -5.8618],\n",
      "        [-0.0172,  1.6270, -6.7556],\n",
      "        [ 2.6768, -1.0675, -6.3342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0413, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2735, -0.6142, -6.4170],\n",
      "        [ 2.4965, -0.8972, -6.3387],\n",
      "        [ 2.4816, -0.8196, -6.3275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0455,  4.3023, -3.5335],\n",
      "        [ 2.5438, -0.5216, -6.6620],\n",
      "        [ 3.0106, -1.4584, -5.1448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4544, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7503,  0.9660, -6.6523],\n",
      "        [ 2.9583, -1.5225, -5.2635],\n",
      "        [ 1.1113,  0.7878, -6.5021]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0630, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2071,  4.2567, -4.1380],\n",
      "        [-0.3733,  2.7460, -6.4522],\n",
      "        [ 1.7597, -0.1281, -6.2113]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7914, -1.4731, -5.1718],\n",
      "        [ 3.0457, -1.6005, -5.5697],\n",
      "        [-1.0173,  3.8485, -4.8903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0477, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9140, -0.1166, -6.1039],\n",
      "        [ 2.8199, -1.7253, -4.9799],\n",
      "        [ 3.1743, -1.6056, -5.3837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0263, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7862, -1.3645, -5.4222],\n",
      "        [ 2.8550, -1.3516, -5.7750],\n",
      "        [-0.3731,  2.6402, -6.6029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2665, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0742,  4.0499, -4.0960],\n",
      "        [ 0.6787,  0.9697, -6.9533],\n",
      "        [ 1.6335,  0.3039, -6.6316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9922, -1.4647, -5.7001],\n",
      "        [-1.1211,  4.2517, -3.7309],\n",
      "        [ 3.0814, -1.8174, -5.3243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3231, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0104,  3.9432, -5.0443],\n",
      "        [-0.9649,  4.0679, -5.5136],\n",
      "        [ 1.1346,  0.6652, -6.2533]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4968, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2563,  0.7811, -5.8964],\n",
      "        [-1.2113,  4.2185, -4.0695],\n",
      "        [ 2.7170, -0.7796, -6.3413]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1204, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9027, -1.4520, -5.4920],\n",
      "        [-1.1656,  4.1979, -4.4911],\n",
      "        [ 0.5307,  1.4238, -6.9570]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2051, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9088,  1.1944, -6.7120],\n",
      "        [ 2.2166, -0.8565, -6.0098],\n",
      "        [ 3.1653, -1.5547, -5.3648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1079, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3766,  4.3281, -3.5282],\n",
      "        [ 1.4354,  0.4432, -6.5707],\n",
      "        [-1.2111,  4.2363, -4.3610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3880, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6684,  0.5220, -7.1123],\n",
      "        [ 2.9572, -1.5550, -5.4266],\n",
      "        [ 0.5452,  0.8837, -6.8502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0223, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8756, -1.4261, -5.5618],\n",
      "        [-0.0374,  2.9856, -6.7665],\n",
      "        [-1.0883,  4.1389, -3.8222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1321, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5317,  1.3884, -7.0080],\n",
      "        [ 2.8165, -1.3330, -5.5406],\n",
      "        [ 2.5936, -1.0300, -6.5407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8272, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3813, -0.6300, -6.4058],\n",
      "        [ 3.1857, -1.5243, -6.0807],\n",
      "        [-0.2731,  2.0583, -6.4755]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0426,  3.9017, -5.9514],\n",
      "        [-1.2536,  4.3982, -4.0092],\n",
      "        [-1.1392,  4.3466, -4.1160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8789, -1.5919, -6.2893],\n",
      "        [-0.7556,  3.2611, -6.4692],\n",
      "        [-1.0542,  3.9997, -5.1154]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0144, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8874, -1.1969, -5.9599],\n",
      "        [-1.2215,  4.3318, -4.6768],\n",
      "        [ 2.6233, -1.1713, -6.1550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3685, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0396,  0.0214, -6.6854],\n",
      "        [ 0.7096,  1.2137, -6.4556],\n",
      "        [-1.2834,  4.3851, -4.8485]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0226, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8031, -1.1849, -6.0687],\n",
      "        [-1.0612,  4.3095, -5.4430],\n",
      "        [ 2.3827, -0.7068, -6.3963]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6198, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1531,  4.4914, -5.1751],\n",
      "        [ 1.7402,  0.0865, -7.3064],\n",
      "        [ 2.6320, -0.9764, -6.0819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0895, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0383,  2.2827, -6.6605],\n",
      "        [ 3.0537, -1.5181, -5.6204],\n",
      "        [ 0.2093,  1.9815, -6.8872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3518, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1297,  0.8746, -6.7288],\n",
      "        [ 1.8288, -0.1075, -6.5413],\n",
      "        [ 0.4117,  1.2943, -6.8122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4934, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1754,  1.7012, -6.5270],\n",
      "        [ 2.2362, -0.3814, -7.0040],\n",
      "        [ 1.6798,  0.3118, -6.7094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1277, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7478, -1.6262, -6.0782],\n",
      "        [ 0.5443,  1.3980, -6.8398],\n",
      "        [ 2.9466, -1.2215, -6.3002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8772, -1.5889, -5.6442],\n",
      "        [-0.7797,  3.7651, -6.0995],\n",
      "        [ 2.8192, -1.2464, -5.7104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2992, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7341, -1.3350, -5.8194],\n",
      "        [ 0.9947,  1.0196, -6.8508],\n",
      "        [ 0.0309,  1.6888, -6.6431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1452, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0056,  4.2171, -5.4882],\n",
      "        [-0.9410,  3.8879, -5.8528],\n",
      "        [ 0.3963,  1.0405, -7.1386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3263, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2307,  0.7393, -7.4556],\n",
      "        [-1.1115,  4.0907, -5.2783],\n",
      "        [-1.1353,  4.2967, -5.0009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2384, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3554,  0.8181, -6.9236],\n",
      "        [ 2.5707, -1.6604, -5.6726],\n",
      "        [ 0.2902,  1.5952, -6.8824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2934, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0034, -1.3015, -6.3098],\n",
      "        [ 0.7617,  1.0556, -6.3409],\n",
      "        [ 2.7560, -1.4091, -5.7843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0340, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2936,  2.8780, -6.7739],\n",
      "        [ 2.2615, -0.6475, -6.1009],\n",
      "        [-1.0265,  3.8855, -5.7486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0796, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4578, -0.8010, -6.6562],\n",
      "        [ 2.8307, -1.2951, -6.0380],\n",
      "        [ 0.2527,  1.8483, -6.6557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3374, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2728, -0.6624, -6.5416],\n",
      "        [ 1.8637, -0.1282, -6.4345],\n",
      "        [ 1.0200,  0.7592, -6.9649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3158, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3302,  1.0005, -7.4739],\n",
      "        [ 2.7491, -0.6728, -6.7047],\n",
      "        [ 2.6226, -0.4911, -6.6671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1144, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2797,  0.3291, -6.4149],\n",
      "        [-1.1425,  4.2112, -6.0554],\n",
      "        [ 2.9401, -1.5443, -5.5805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2445, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5528,  3.1921, -6.3291],\n",
      "        [ 0.8095,  0.8325, -6.8934],\n",
      "        [-1.1989,  4.0676, -5.1363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1524, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8192, -1.5222, -5.3902],\n",
      "        [ 2.3174, -0.2038, -6.3730],\n",
      "        [ 1.4077,  0.5924, -6.9090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0383, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1330,  2.4848, -6.6342],\n",
      "        [ 2.5958, -1.2774, -6.5584],\n",
      "        [ 2.6543, -1.0867, -5.8966]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9161, -1.5739, -5.4853],\n",
      "        [-1.1012,  4.3172, -5.0785],\n",
      "        [ 2.8626, -1.0519, -5.8108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1344, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9412,  3.7849, -6.3112],\n",
      "        [ 1.5942,  0.2546, -6.7077],\n",
      "        [ 0.0574,  1.7990, -6.8489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1102, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2698,  4.3804, -4.0647],\n",
      "        [ 1.4435,  0.4272, -6.7334],\n",
      "        [ 2.6522, -1.3808, -5.3552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2098, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2246,  1.8688, -6.4280],\n",
      "        [ 1.3263,  0.4047, -6.3502],\n",
      "        [ 1.8323, -0.2547, -6.5417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4937, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6281,  0.4446, -6.6554],\n",
      "        [ 2.8308, -1.3150, -6.0703],\n",
      "        [ 2.5114, -1.7719, -4.6301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9776, -1.6239, -5.4109],\n",
      "        [ 2.9067, -1.7198, -5.2170],\n",
      "        [-1.0407,  4.1060, -4.4976]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0151, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6184, -0.9941, -5.9982],\n",
      "        [ 2.8855, -1.3890, -5.2643],\n",
      "        [-1.1952,  4.2520, -5.1306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1148, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1991,  4.4278, -4.6337],\n",
      "        [-0.0200,  2.3395, -6.4261],\n",
      "        [ 1.6994,  0.4406, -6.5673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3561,  4.4044, -4.9185],\n",
      "        [-1.3481,  4.5998, -4.8504],\n",
      "        [ 2.8823, -1.0990, -5.9000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6693,  2.9947, -6.5236],\n",
      "        [-1.1813,  4.3827, -4.3169],\n",
      "        [-1.2204,  4.5524, -4.3663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2339,  4.6463, -4.6464],\n",
      "        [ 2.4866, -1.0221, -6.3792],\n",
      "        [-1.3913,  4.3763, -4.5230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0658, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3927,  4.4190, -4.0520],\n",
      "        [ 0.1666,  1.7244, -6.8636],\n",
      "        [-1.1789,  4.6989, -4.7787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0274, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5006,  4.5069, -4.1847],\n",
      "        [ 2.1443, -0.3797, -6.6642],\n",
      "        [-1.4349,  4.6481, -4.6699]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0549, -1.6405, -5.2124],\n",
      "        [-1.3197,  4.5197, -4.5060],\n",
      "        [-1.3191,  4.6454, -5.0145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2299, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9974, -1.8774, -4.9371],\n",
      "        [ 2.9319, -1.4598, -5.0686],\n",
      "        [ 0.8886,  0.9381, -6.7275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0360, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5878, -2.2017, -3.8618],\n",
      "        [-1.3680,  4.4376, -4.3694],\n",
      "        [-0.0694,  2.2371, -6.7047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0063, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8109, -1.5738, -5.6001],\n",
      "        [-1.3111,  4.5940, -4.3632],\n",
      "        [-1.1951,  4.4868, -4.8350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2402, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9709,  1.0000, -6.7356],\n",
      "        [-1.3401,  4.5578, -4.4449],\n",
      "        [ 2.9198, -1.7592, -4.9267]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1402, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4077,  0.4470, -6.5251],\n",
      "        [ 2.0870, -0.3197, -6.5370],\n",
      "        [ 2.7797, -1.9201, -4.2348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1184, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9121, -1.4725, -5.7645],\n",
      "        [ 2.8760, -1.9392, -4.0908],\n",
      "        [ 0.5050,  1.4318, -6.6838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8018, -1.8282, -4.8563],\n",
      "        [ 2.7360, -1.7834, -4.3657],\n",
      "        [-1.4829,  4.6061, -4.0818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4823,  4.3989, -4.0986],\n",
      "        [ 3.0270, -1.9165, -4.7272],\n",
      "        [-1.5166,  4.7095, -4.5716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1027, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4512,  4.3109, -3.9531],\n",
      "        [ 1.5267,  0.0490, -6.2894],\n",
      "        [-0.0921,  2.1738, -6.7297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1959, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8877, -1.7268, -5.0166],\n",
      "        [ 0.7392,  1.1375, -6.7293],\n",
      "        [ 2.1881, -0.5404, -6.4291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1517, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4299,  0.8489, -6.9279],\n",
      "        [-1.3037,  4.5116, -3.9978],\n",
      "        [ 2.8704, -2.1055, -4.7068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0053, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1563, -1.9475, -4.9688],\n",
      "        [ 3.1661, -2.0569, -4.6323],\n",
      "        [-1.1961,  4.3773, -5.6990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6407,  4.2843, -4.0677],\n",
      "        [ 2.7732, -1.9987, -4.3524],\n",
      "        [-1.3888,  4.4658, -4.1059]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3943,  4.6354, -5.1680],\n",
      "        [-1.5480,  4.5627, -4.0974],\n",
      "        [ 3.1899, -2.0057, -4.3935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0757, -1.4150, -5.5339],\n",
      "        [ 3.0043, -1.8636, -4.4149],\n",
      "        [ 3.0748, -1.7923, -5.5756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8048,  4.2002, -3.5212],\n",
      "        [ 2.8722, -2.0272, -4.3809],\n",
      "        [-1.4234,  4.3714, -4.5753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5951,  4.8061, -4.4904],\n",
      "        [-1.5769,  4.6302, -3.6857],\n",
      "        [ 3.0873, -1.9789, -4.4669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0150, -1.7094, -5.4843],\n",
      "        [ 3.0144, -2.1912, -4.3264],\n",
      "        [-0.8714,  3.1890, -6.5503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0359, -1.9483, -4.7168],\n",
      "        [-0.3992,  2.1425, -6.3449],\n",
      "        [-1.5002,  4.7426, -4.1132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9325, -1.9375, -4.6481],\n",
      "        [-1.5959,  4.6935, -3.8801],\n",
      "        [-1.1699,  4.3253, -3.8147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9223, -2.0804, -4.3216],\n",
      "        [ 2.7481, -2.2363, -4.0552],\n",
      "        [ 2.9677, -2.2861, -4.2469]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0037, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3730,  4.6749, -4.5033],\n",
      "        [-1.3176,  4.4578, -5.3752],\n",
      "        [ 3.1588, -2.0786, -4.9670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2950, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2043,  0.8593, -6.3070],\n",
      "        [-1.6753,  4.8035, -3.6655],\n",
      "        [-1.3359,  4.7055, -4.2501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9730, -1.6878, -5.5947],\n",
      "        [-1.5541,  4.7937, -3.9921],\n",
      "        [-1.6208,  4.6518, -4.2694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0441, -2.3289, -4.6677],\n",
      "        [ 2.9707, -1.8834, -4.5297],\n",
      "        [ 3.0199, -2.0605, -4.7383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0833, -2.0026, -4.6730],\n",
      "        [-1.4825,  4.5795, -4.4428],\n",
      "        [-1.7050,  4.7021, -3.7268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9451, -1.8234, -5.1401],\n",
      "        [-1.7665,  4.4209, -4.0838],\n",
      "        [ 3.1062, -2.2058, -3.9564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5654, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1807,  1.6617, -6.7768],\n",
      "        [ 2.9129, -1.9558, -4.6661],\n",
      "        [-1.5678,  4.7382, -4.0897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0163, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0407, -2.3874, -4.2999],\n",
      "        [-1.7127,  4.6322, -4.0002],\n",
      "        [-0.4848,  2.6686, -6.5612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1783,  0.4886, -5.9962],\n",
      "        [ 2.1086, -0.1394, -6.9462],\n",
      "        [-1.7625,  4.7244, -4.1303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0034, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4111,  4.5057, -4.0407],\n",
      "        [ 3.1069, -2.2824, -4.0760],\n",
      "        [-1.4520,  4.7825, -4.3038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0030, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4310,  4.6700, -4.5976],\n",
      "        [ 3.0996, -2.4470, -4.0920],\n",
      "        [-1.6142,  4.6218, -4.4273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0825, -2.0052, -5.5788],\n",
      "        [ 3.1581, -2.4689, -4.6089],\n",
      "        [-0.9312,  3.2248, -5.1750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1711, -2.1899, -4.1384],\n",
      "        [-0.8295,  3.0481, -6.1869],\n",
      "        [ 3.2279, -2.3483, -4.3561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0644, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6948,  0.1127, -7.1809],\n",
      "        [ 3.3174, -2.2808, -4.3794],\n",
      "        [-1.5665,  4.7989, -3.7907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(3.8448, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1643,  3.6139, -5.9648],\n",
      "        [-1.6709,  4.6816, -4.2272],\n",
      "        [ 1.3991,  0.6682, -6.8297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2394, -2.1686, -4.7395],\n",
      "        [ 3.0646, -2.1691, -4.8176],\n",
      "        [ 2.9302, -2.3371, -4.6296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2693, -1.9000, -5.2561],\n",
      "        [ 3.0179, -2.2840, -3.7920],\n",
      "        [-1.1769,  4.2561, -4.8759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4437, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5966,  0.5825, -7.1425],\n",
      "        [-1.4737,  4.5141, -5.2013],\n",
      "        [ 3.0955, -2.3666, -4.3124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6752, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2765,  0.4770, -6.0366],\n",
      "        [ 1.2109,  0.9226, -6.3577],\n",
      "        [ 2.9420, -2.3302, -3.6987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6503, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8258,  0.1801, -6.1634],\n",
      "        [ 3.0701, -2.0690, -4.3892],\n",
      "        [-0.0312,  2.0113, -6.6268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1279, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6767,  3.2475, -6.8434],\n",
      "        [-0.3651,  3.1288, -6.4765],\n",
      "        [ 0.4432,  1.3686, -7.4215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2956, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1575,  1.0038, -7.0327],\n",
      "        [ 2.8760, -2.2542, -4.7112],\n",
      "        [ 0.0902,  2.2702, -7.0951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0068, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0395, -1.7869, -5.0280],\n",
      "        [ 3.2098, -2.1588, -4.2568],\n",
      "        [ 3.0242, -2.0517, -4.4393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0483, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6659e-03,  2.1446e+00, -7.1058e+00],\n",
      "        [ 3.0256e+00, -2.0486e+00, -4.3623e+00],\n",
      "        [-5.6154e-01,  3.0450e+00, -6.8754e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0188, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0421, -2.0655, -4.4581],\n",
      "        [ 2.6623, -1.4170, -6.0678],\n",
      "        [-0.6407,  2.7637, -6.5366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7246,  3.6308, -6.7680],\n",
      "        [ 2.9484, -1.9883, -4.7380],\n",
      "        [-0.1426,  2.2978, -6.6128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1941, -2.0670, -5.1205],\n",
      "        [-0.8979,  3.9430, -5.9825],\n",
      "        [-0.4914,  2.6660, -7.0313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2136, -1.9753, -4.2336],\n",
      "        [ 2.9383, -1.9484, -4.4427],\n",
      "        [ 2.8478, -2.2496, -4.2436]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0245, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2608,  2.5855, -7.0105],\n",
      "        [ 2.7684, -2.1684, -4.1621],\n",
      "        [ 2.9045, -1.9105, -4.1052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.8135, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8449,  1.2026, -6.7911],\n",
      "        [ 3.1367, -1.6735, -5.1036],\n",
      "        [ 2.3979, -0.6469, -6.0044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8786, -1.8867, -4.8890],\n",
      "        [-0.6543,  3.1820, -7.0114],\n",
      "        [ 2.8540, -2.1834, -4.4379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0097, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1272, -1.5631, -5.1474],\n",
      "        [-0.6518,  3.7712, -5.9850],\n",
      "        [ 3.0278, -1.8761, -4.9824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7156, -1.5233, -5.6696],\n",
      "        [ 3.1077, -1.5580, -5.3436],\n",
      "        [-0.6698,  3.3925, -6.6588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0166, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3770,  3.2274, -6.8150],\n",
      "        [ 2.9129, -1.5239, -5.4596],\n",
      "        [ 3.0228, -1.5039, -5.7376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2006, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9031, -0.8984, -5.9087],\n",
      "        [ 1.2529,  0.5963, -6.7310],\n",
      "        [ 0.2628,  2.0051, -7.0863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5739, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3706,  2.8277, -6.7755],\n",
      "        [-0.4897,  2.1409, -7.1599],\n",
      "        [ 0.1700,  1.5591, -6.8332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0195, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4472,  3.4507, -6.9934],\n",
      "        [ 2.7409, -0.8634, -5.8996],\n",
      "        [-0.7030,  3.7639, -6.4803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6066, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3819,  2.3382, -6.9140],\n",
      "        [ 0.4492,  1.3892, -6.8754],\n",
      "        [ 1.3507,  0.8811, -6.3505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1951, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5439,  0.8857, -7.1532],\n",
      "        [-0.6008,  3.7625, -6.4229],\n",
      "        [ 1.9522,  0.1660, -6.6813]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0244, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4752, -0.6061, -6.5690],\n",
      "        [-0.7202,  3.8022, -6.4896],\n",
      "        [-0.5587,  3.4817, -6.3081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5670, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3960, -1.1466, -6.0840],\n",
      "        [-0.5917,  3.2941, -6.5907],\n",
      "        [ 0.2898,  1.7289, -6.8832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1639, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4706,  0.4667, -6.9421],\n",
      "        [ 2.4107, -0.6973, -6.4889],\n",
      "        [ 0.1149,  2.0472, -7.0155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4524, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8249, -1.1908, -6.2515],\n",
      "        [ 2.1137, -0.1787, -6.5728],\n",
      "        [ 0.5336,  1.4358, -7.2051]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3822, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6748, -0.9786, -6.3152],\n",
      "        [-0.5111,  3.5530, -6.5440],\n",
      "        [ 1.3347,  0.6342, -6.1086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1849,  2.0762, -6.8358],\n",
      "        [-0.4611,  4.0044, -6.1005],\n",
      "        [-0.6620,  3.6234, -5.7149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3240,  3.4079, -6.6940],\n",
      "        [ 2.3796, -0.7363, -6.5153],\n",
      "        [-0.5570,  3.3785, -6.4993]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2463, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6210,  2.7985, -6.7425],\n",
      "        [-0.4741,  3.5073, -6.2053],\n",
      "        [ 0.8404,  0.8292, -6.2866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4711, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1074,  0.1289, -7.0710],\n",
      "        [ 0.4343,  1.2928, -6.6202],\n",
      "        [ 2.3016, -0.3011, -6.6353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1864, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5781,  3.3842, -6.6319],\n",
      "        [ 1.1657,  0.6554, -6.0068],\n",
      "        [ 2.2884, -0.3430, -6.5584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0484, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8397, -1.3134, -5.8078],\n",
      "        [-0.2416,  1.9121, -6.8042],\n",
      "        [-0.4505,  3.4736, -6.6664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0310, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4942,  2.3513, -6.7662],\n",
      "        [-0.3321,  3.4672, -6.4635],\n",
      "        [ 2.8885, -1.3625, -5.9195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3282, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1964,  1.5643, -7.1481],\n",
      "        [ 0.6324,  1.3436, -7.2253],\n",
      "        [ 0.4315,  1.2747, -6.8035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0216, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8399, -1.0908, -5.8350],\n",
      "        [-0.6401,  3.7566, -5.5260],\n",
      "        [ 2.6296, -0.7676, -6.9907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1442, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2090,  2.1566, -6.7372],\n",
      "        [ 0.3804,  1.3382, -6.3878],\n",
      "        [-0.5026,  3.5256, -5.9941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0229, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4186,  3.0228, -6.3308],\n",
      "        [ 2.6622, -1.1136, -6.4074],\n",
      "        [ 3.0927, -1.1663, -5.6595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2007, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5460, -0.4564, -6.7839],\n",
      "        [-0.6290,  3.3004, -6.6562],\n",
      "        [ 0.8331,  1.1818, -6.6835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6184, -0.9733, -6.3124],\n",
      "        [-0.5644,  3.7466, -5.8395],\n",
      "        [-0.6281,  3.7574, -5.8869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2421, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9962,  0.7838, -6.9309],\n",
      "        [ 2.0397, -0.1607, -6.3013],\n",
      "        [ 2.5117, -1.0474, -6.5090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4459,  3.6217, -5.7930],\n",
      "        [-0.5636,  3.7972, -5.7835],\n",
      "        [ 2.6575, -1.3135, -6.0655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0258, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9323, -1.4368, -6.5860],\n",
      "        [-0.4662,  3.7665, -5.8791],\n",
      "        [ 2.4571, -0.5077, -6.7210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0407, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5360, -1.1284, -6.0949],\n",
      "        [-0.5334,  3.6796, -5.4527],\n",
      "        [ 2.0919, -0.3741, -6.5002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0183, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8415, -0.9562, -5.9207],\n",
      "        [-0.7934,  3.5661, -5.9918],\n",
      "        [ 2.7879, -1.1341, -6.0905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8035, -1.5719, -6.0074],\n",
      "        [ 2.6945, -1.4983, -5.6873],\n",
      "        [ 2.6877, -1.3168, -5.7359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0697, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8473,  0.1195, -6.6427],\n",
      "        [-0.5618,  2.9597, -6.9517],\n",
      "        [ 2.9420, -1.1790, -5.5522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0608, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1596,  1.9531, -6.5816],\n",
      "        [-0.5851,  3.5719, -5.3494],\n",
      "        [-0.6791,  3.6855, -5.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2273, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4997, -0.8196, -6.4706],\n",
      "        [ 2.8252, -1.2548, -5.9205],\n",
      "        [ 0.9557,  1.0887, -6.2289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0194, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4698,  2.7792, -6.6486],\n",
      "        [-0.6694,  3.8524, -5.6014],\n",
      "        [-0.9352,  3.7521, -5.8242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1684, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5997,  3.9102, -5.1526],\n",
      "        [ 0.6797,  1.1550, -6.7706],\n",
      "        [-0.7469,  3.8132, -5.3767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1104, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1260, -1.5901, -5.6329],\n",
      "        [ 0.2230,  1.2360, -6.7111],\n",
      "        [ 2.7874, -1.6530, -5.3442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6680, -1.5794, -5.3595],\n",
      "        [ 3.0681, -1.6562, -5.4055],\n",
      "        [ 2.9024, -1.7216, -5.5205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9000,  3.8957, -5.9282],\n",
      "        [ 2.9396, -1.4962, -5.5519],\n",
      "        [-0.8254,  3.7793, -5.7557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0092, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0050, -1.6943, -5.5240],\n",
      "        [-0.7327,  3.9671, -5.0268],\n",
      "        [-0.8034,  3.8892, -5.1146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0141, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8190,  4.1117, -5.2758],\n",
      "        [ 3.0909, -1.4063, -5.8117],\n",
      "        [ 2.4398, -0.5341, -6.6139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0533, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9023,  3.9573, -4.8863],\n",
      "        [-0.6276,  3.9895, -5.0081],\n",
      "        [ 0.0200,  1.9015, -6.9185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2820, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8919,  4.0540, -4.9515],\n",
      "        [-0.8484,  4.0828, -5.3362],\n",
      "        [ 0.6562,  0.9158, -6.6545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9654, -1.5760, -5.5722],\n",
      "        [-0.5053,  2.9776, -6.5617],\n",
      "        [ 2.7749, -1.5796, -5.8422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7275,  3.7584, -4.7668],\n",
      "        [-0.8295,  3.5795, -5.8903],\n",
      "        [-0.8513,  3.8058, -4.7181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1489, -1.3985, -5.5408],\n",
      "        [ 2.6311, -1.0724, -6.5928],\n",
      "        [-0.8615,  3.8695, -5.0027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0275, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6851,  3.9330, -5.0514],\n",
      "        [ 2.7649, -1.6215, -5.1048],\n",
      "        [ 2.2720, -0.5188, -6.3689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9041,  3.9177, -5.3319],\n",
      "        [-0.9247,  3.9598, -4.9709],\n",
      "        [-0.7494,  4.1448, -5.4995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0659, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6304, -0.1310, -6.4099],\n",
      "        [-0.7061,  3.1560, -6.5811],\n",
      "        [ 2.7693, -1.2459, -5.7661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7419, -1.3185, -5.0896],\n",
      "        [-0.9955,  3.7950, -5.8434],\n",
      "        [-0.6741,  4.0137, -5.9173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2226, -1.4162, -6.0420],\n",
      "        [ 2.9834, -1.4106, -5.4297],\n",
      "        [-0.6579,  2.7800, -6.4534]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0272, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7596, -1.3044, -5.6950],\n",
      "        [ 2.9788, -1.4831, -5.7833],\n",
      "        [-0.3640,  2.5529, -6.7152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5848, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0611,  1.6074, -6.9228],\n",
      "        [-0.6877,  3.9444, -4.9138],\n",
      "        [-1.1057,  4.2328, -5.4171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0761, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0491,  3.8568, -5.4027],\n",
      "        [-0.7808,  3.1113, -6.3316],\n",
      "        [ 0.1591,  1.6651, -6.8617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4437,  2.8300, -6.5977],\n",
      "        [ 3.1198, -1.7552, -5.7336],\n",
      "        [ 2.8087, -1.7484, -5.1387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0525, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2186,  2.3153, -7.0188],\n",
      "        [ 2.2253, -0.3616, -6.6474],\n",
      "        [-0.7639,  4.0221, -4.6904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0245, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4589, -0.6926, -6.8166],\n",
      "        [ 2.7775, -1.1615, -5.8261],\n",
      "        [ 2.9339, -1.5075, -5.3669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0383, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6276, -1.1191, -6.4879],\n",
      "        [-0.8724,  4.2410, -5.4893],\n",
      "        [-0.2917,  2.1256, -6.7897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0258, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4329, -0.8651, -6.6790],\n",
      "        [ 2.5965, -0.8045, -6.3205],\n",
      "        [-0.9901,  3.8335, -4.7184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1801, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0747,  4.1110, -4.8782],\n",
      "        [-0.5253,  2.4196, -6.6262],\n",
      "        [ 1.2028,  0.7258, -6.6273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9456,  4.0665, -5.0135],\n",
      "        [-1.0880,  3.9145, -5.2735],\n",
      "        [ 3.0264, -1.4905, -5.8572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0190, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6121,  2.8624, -6.7241],\n",
      "        [ 3.0887, -1.3448, -5.6673],\n",
      "        [ 2.9611, -1.2831, -5.9530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2083, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9226, -1.1869, -5.6684],\n",
      "        [ 1.2481,  1.0276, -6.8462],\n",
      "        [ 2.7424, -1.2007, -6.4862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0217, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9188,  3.8714, -4.8998],\n",
      "        [ 2.8249, -1.3413, -6.6842],\n",
      "        [-0.5459,  2.6212, -6.6816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0135, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4468,  3.3488, -6.6472],\n",
      "        [ 2.6949, -1.7180, -4.9857],\n",
      "        [-0.9528,  4.2219, -5.7696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0083, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1217, -1.4413, -5.6739],\n",
      "        [ 2.9749, -1.8860, -4.9910],\n",
      "        [-1.1069,  3.9800, -4.6728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1159, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4392,  0.5035, -6.5386],\n",
      "        [-0.8593,  4.0451, -5.4148],\n",
      "        [ 3.0738, -1.6422, -5.2800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0144, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9319, -1.7283, -4.9592],\n",
      "        [-0.9555,  4.0728, -4.6855],\n",
      "        [-0.8149,  2.8024, -6.6002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0962, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8610, -1.4164, -5.8999],\n",
      "        [ 3.0330, -1.6438, -5.7134],\n",
      "        [ 0.5160,  1.7085, -6.7916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7664, -1.7055, -4.7654],\n",
      "        [-1.1310,  4.2516, -5.1899],\n",
      "        [ 3.1310, -1.7442, -5.1209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6095, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0216, -1.9985, -5.4412],\n",
      "        [-0.3756,  3.1440, -6.3488],\n",
      "        [ 2.9019, -1.8819, -5.1963]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8441, -1.7493, -4.8234],\n",
      "        [ 2.7591, -1.7649, -4.8723],\n",
      "        [ 2.7783, -1.0759, -6.4557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1339, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4139,  1.1500, -6.5813],\n",
      "        [-1.1217,  4.3083, -5.2058],\n",
      "        [-1.0241,  4.1780, -5.0248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0769, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0426,  4.2723, -4.9539],\n",
      "        [-1.0012,  3.9417, -4.4414],\n",
      "        [ 0.0910,  1.5026, -6.6785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0682, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0281,  1.9222, -7.0084],\n",
      "        [-0.1806,  2.6122, -6.7259],\n",
      "        [-1.2510,  4.1295, -5.4849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8174, -1.1665, -5.2749],\n",
      "        [ 2.9387, -1.6039, -5.1318],\n",
      "        [-0.9808,  4.0898, -4.7997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8755, -1.6021, -4.8494],\n",
      "        [ 2.8206, -1.6900, -5.1409],\n",
      "        [ 2.8617, -1.4978, -5.0557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9864,  3.6382, -6.0551],\n",
      "        [-0.9649,  4.0848, -4.9152],\n",
      "        [-1.0336,  3.9757, -5.8839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0900, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6471,  2.6889, -6.5631],\n",
      "        [ 3.0288, -1.5362, -5.3946],\n",
      "        [ 0.1523,  1.5336, -6.6967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0293, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9421,  4.1397, -4.7828],\n",
      "        [-0.7770,  3.0512, -6.7149],\n",
      "        [ 2.3184, -0.4680, -5.9477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1111,  4.2149, -4.9249],\n",
      "        [-1.1786,  4.1820, -5.0040],\n",
      "        [ 2.6214, -1.5259, -4.8004]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5663, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5079, -1.1845, -4.3201],\n",
      "        [-1.0927,  4.2433, -4.5941],\n",
      "        [ 1.7973,  0.3382, -6.5834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0139, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8506, -1.5947, -4.8776],\n",
      "        [ 2.6339, -1.3768, -4.9310],\n",
      "        [ 2.9368, -1.5740, -5.0478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6892, -1.5208, -4.8787],\n",
      "        [-1.3287,  4.3005, -5.1763],\n",
      "        [-1.0301,  4.1770, -4.4897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2807, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5147,  0.4005, -7.1534],\n",
      "        [ 2.8674, -1.2219, -5.2045],\n",
      "        [ 1.1951,  0.8637, -7.5004]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0108, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7408, -1.4327, -5.5591],\n",
      "        [-1.0585,  4.2483, -4.9481],\n",
      "        [ 3.0318, -1.4138, -5.6654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0654, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9135,  0.3347, -7.0271],\n",
      "        [-1.2140,  4.1200, -4.6733],\n",
      "        [-1.3585,  4.2573, -4.6907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2130, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8862,  0.7556, -6.4861],\n",
      "        [-1.1170,  4.1708, -4.7244],\n",
      "        [-1.1962,  4.5051, -5.0928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0491, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8882, -1.3048, -5.3921],\n",
      "        [ 0.0381,  2.1181, -6.5227],\n",
      "        [ 2.9747, -1.2929, -5.2844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4102, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9612, -1.1554, -5.0619],\n",
      "        [ 3.0402, -1.1487, -5.2917],\n",
      "        [ 0.5443,  1.3837, -6.2629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0438, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9928, -1.4854, -4.9714],\n",
      "        [-1.1390,  4.3317, -4.9887],\n",
      "        [ 1.9018, -0.2017, -5.8856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2825,  4.0429, -4.8192],\n",
      "        [ 2.4913, -0.4894, -6.4620],\n",
      "        [-1.0715,  3.5805, -6.2530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0207, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1986,  4.1539, -4.8697],\n",
      "        [-0.9394,  3.4130, -6.4023],\n",
      "        [ 2.4201, -0.6722, -6.6161]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2678,  4.3580, -5.3482],\n",
      "        [-1.2303,  4.1540, -4.5134],\n",
      "        [-0.9701,  4.1798, -4.8535]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2674, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0856, -1.6567, -5.0754],\n",
      "        [ 1.0353,  0.8703, -6.5909],\n",
      "        [ 2.8169, -1.4591, -5.5579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0249, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4090, -0.6384, -6.8043],\n",
      "        [ 3.0151, -1.0432, -5.9287],\n",
      "        [-1.1969,  3.3285, -5.9835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1214, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3281,  4.1275, -4.9194],\n",
      "        [ 2.9875, -1.4649, -5.2690],\n",
      "        [ 0.4781,  1.3556, -6.8028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0041, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4196,  4.1207, -4.9649],\n",
      "        [-1.1583,  4.2153, -4.8640],\n",
      "        [-1.3288,  4.3583, -5.3258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8755, -1.4432, -5.8245],\n",
      "        [-1.3365,  4.2508, -5.2049],\n",
      "        [ 2.7481, -1.3487, -5.7517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0033, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4649,  4.3229, -5.3451],\n",
      "        [-1.3757,  4.2003, -5.5735],\n",
      "        [-1.4498,  4.4008, -4.9896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0510, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9991,  4.3339, -4.7929],\n",
      "        [-1.4533,  4.2276, -5.2987],\n",
      "        [ 1.9223,  0.0597, -6.8954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5399, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0520, -1.5244, -5.8788],\n",
      "        [ 2.7217, -1.1462, -6.0513],\n",
      "        [ 2.9874, -1.4397, -5.2676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0722, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1805,  1.6918, -6.7562],\n",
      "        [ 2.9455, -1.5429, -5.0698],\n",
      "        [-1.1019,  4.1217, -5.3591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0308, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3551, -0.2310, -7.0827],\n",
      "        [-1.2214,  4.0901, -5.1915],\n",
      "        [ 3.0065, -1.2108, -5.9170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4253,  4.3130, -5.1669],\n",
      "        [ 2.9875, -1.5479, -6.1344],\n",
      "        [-1.3377,  4.1449, -4.9270]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0039, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3337,  4.2961, -5.2989],\n",
      "        [-1.2320,  4.3096, -4.9261],\n",
      "        [-1.2896,  4.2610, -5.2878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9309, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3773, -0.3233, -6.6812],\n",
      "        [-1.2725,  4.2497, -5.7170],\n",
      "        [ 2.7741, -0.9879, -6.6220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4731, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8609, -1.5377, -6.2146],\n",
      "        [-1.1376,  4.2482, -5.3959],\n",
      "        [-1.1855,  4.4174, -5.0506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0955, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2329,  4.2616, -5.3765],\n",
      "        [ 1.6094,  0.4006, -6.7230],\n",
      "        [ 2.8437, -1.0277, -6.0136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0218, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3178, -0.4925, -6.6801],\n",
      "        [-1.3411,  4.2807, -5.1094],\n",
      "        [-1.3752,  4.3861, -5.2868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8868, -1.3263, -5.9211],\n",
      "        [ 2.6046, -0.9840, -6.4519],\n",
      "        [-0.9316,  3.1491, -6.4266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2271, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1277,  4.0381, -4.7263],\n",
      "        [-1.1362,  4.3994, -4.7479],\n",
      "        [ 0.8154,  0.8602, -6.5781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0363, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2804, -0.2668, -6.6827],\n",
      "        [ 2.8267, -0.7045, -6.5968],\n",
      "        [-1.3070,  4.1445, -5.1869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0509, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5885, -0.9059, -6.6219],\n",
      "        [-1.5269,  4.1205, -5.3287],\n",
      "        [ 1.9590, -0.1110, -7.0193]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5316, -0.8826, -6.7717],\n",
      "        [ 2.6751, -0.5464, -7.0460],\n",
      "        [-1.1899,  4.3895, -5.3159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0173, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4247, -0.7155, -7.1218],\n",
      "        [-1.4747,  4.2148, -5.0192],\n",
      "        [-1.1127,  4.0341, -5.1110]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3398, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4078, -0.4333, -6.4461],\n",
      "        [-1.1298,  4.1452, -5.6284],\n",
      "        [ 0.6960,  1.1685, -6.4131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2031, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2230,  0.7527, -6.7227],\n",
      "        [ 2.3506, -0.1796, -7.0908],\n",
      "        [ 2.3991, -0.6388, -6.7276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7594, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4468, -0.5941, -7.0341],\n",
      "        [ 2.4905, -0.7107, -6.7003],\n",
      "        [-1.4755,  3.7103, -6.0158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4262, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4443,  1.2643, -7.2466],\n",
      "        [-1.2173,  4.1269, -5.7479],\n",
      "        [ 2.1275, -0.2521, -6.5039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5556, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1624,  4.0725, -5.5031],\n",
      "        [ 1.6925,  0.2921, -7.0859],\n",
      "        [ 2.6993, -0.4868, -6.3881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2988, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7421,  0.9119, -6.8508],\n",
      "        [ 2.6686, -0.6359, -6.9404],\n",
      "        [ 2.2040, -0.3053, -7.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0315, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5066, -0.7480, -6.8530],\n",
      "        [-0.8303,  3.9820, -5.7848],\n",
      "        [ 2.5405, -0.4648, -6.3510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7575, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2125,  0.6519, -6.7104],\n",
      "        [ 2.7779, -0.3821, -6.9348],\n",
      "        [ 1.8488,  0.2547, -7.3726]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1162, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3229,  3.8345, -5.9732],\n",
      "        [ 2.6660, -0.6276, -6.6616],\n",
      "        [ 1.4916,  0.4646, -6.7903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0250, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5982, -0.5420, -6.6942],\n",
      "        [-1.0482,  3.6229, -6.2434],\n",
      "        [ 2.8841, -0.8706, -6.4318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5161, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5206, -0.4750, -6.6443],\n",
      "        [ 1.5375,  0.3187, -7.1626],\n",
      "        [-0.6095,  3.2181, -6.3125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0449, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5007, -0.3692, -6.9993],\n",
      "        [-0.8835,  2.7939, -6.3788],\n",
      "        [ 2.4163, -0.4720, -6.3984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2293, -0.5682, -6.9273],\n",
      "        [ 2.7218, -0.9203, -6.5247],\n",
      "        [ 2.4748, -0.7453, -6.4720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0243, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6850,  3.8378, -6.1333],\n",
      "        [-0.4689,  2.7674, -6.4718],\n",
      "        [-0.6127,  3.1368, -6.2441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0263, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7470,  3.4462, -6.0333],\n",
      "        [ 2.7067, -0.8999, -6.7606],\n",
      "        [-0.5684,  2.7162, -6.4316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0701, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5498,  2.8957, -6.3350],\n",
      "        [ 1.7859,  0.0485, -7.0702],\n",
      "        [-1.0244,  3.0780, -6.5781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3525, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6730, -0.7521, -6.4615],\n",
      "        [-0.5884,  3.0713, -6.7620],\n",
      "        [ 1.2870,  0.7462, -6.7867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0836, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7991,  3.5728, -5.9922],\n",
      "        [-0.8573,  3.3779, -6.5572],\n",
      "        [ 0.2474,  1.6313, -6.6510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0336, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6309,  2.7071, -6.3993],\n",
      "        [-0.5690,  2.7433, -6.2280],\n",
      "        [ 2.7309, -0.7691, -6.5648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0495, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2764,  2.3923, -6.4140],\n",
      "        [ 2.5304, -0.7679, -6.6873],\n",
      "        [ 2.3529, -0.7326, -6.1253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0453, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4816,  2.2980, -6.6986],\n",
      "        [-0.8366,  3.1566, -6.5216],\n",
      "        [-0.3581,  2.4751, -6.4313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0522, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6142, -0.6992, -6.6099],\n",
      "        [ 2.2144, -0.4237, -6.8124],\n",
      "        [-0.5692,  2.3703, -6.6289]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0434, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8125, -0.4754, -6.7484],\n",
      "        [ 2.5272, -0.6539, -6.9672],\n",
      "        [ 2.2861, -0.6307, -6.6828]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0518, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2098,  2.0549, -6.4812],\n",
      "        [ 2.6892, -0.4473, -6.6917],\n",
      "        [-0.7066,  3.5630, -6.3832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3202, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5694, -0.9191, -6.7339],\n",
      "        [ 0.6802,  1.0897, -7.0180],\n",
      "        [-0.6716,  3.7998, -6.2075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0204, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6598,  3.2279, -6.1644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 3/3, Loss_per_epoch: 195.5581829113653\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_load:\n",
    "        inputs, attention_mask, label = batch\n",
    "        optim.zero_grad()\n",
    "\n",
    "        if label is not None:\n",
    "            print(\"Working\")\n",
    "            batch_output= model(inputs, attention_mask=attention_mask, labels=label)\n",
    "            loss = batch_output.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(\"Outputs:\", batch_output)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss_per_epoch: {total_loss}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')                  "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fadd1ea9c9a50e6"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9d6a82f7b677b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T08:26:22.246914380Z",
     "start_time": "2023-12-02T08:26:19.854402762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model= BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "save_model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95fccc529bc375ac"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working: 1/200\n",
      "Working: 4/200\n",
      "Working: 7/200\n",
      "Working: 10/200\n",
      "Working: 13/200\n",
      "Working: 16/200\n",
      "Working: 19/200\n",
      "Working: 22/200\n",
      "Working: 25/200\n",
      "Working: 28/200\n",
      "Working: 31/200\n",
      "Working: 34/200\n",
      "Working: 37/200\n",
      "Working: 40/200\n",
      "Working: 43/200\n",
      "Working: 46/200\n",
      "Working: 49/200\n",
      "Working: 52/200\n",
      "Working: 55/200\n",
      "Working: 58/200\n",
      "Working: 61/200\n",
      "Working: 64/200\n",
      "Working: 67/200\n",
      "Working: 70/200\n",
      "Working: 73/200\n",
      "Working: 76/200\n",
      "Working: 79/200\n",
      "Working: 82/200\n",
      "Working: 85/200\n",
      "Working: 88/200\n",
      "Working: 91/200\n",
      "Working: 94/200\n",
      "Working: 97/200\n",
      "Working: 100/200\n",
      "Working: 103/200\n",
      "Working: 106/200\n",
      "Working: 109/200\n",
      "Working: 112/200\n",
      "Working: 115/200\n",
      "Working: 118/200\n",
      "Working: 121/200\n",
      "Working: 124/200\n",
      "Working: 127/200\n",
      "Working: 130/200\n",
      "Working: 133/200\n",
      "Working: 136/200\n",
      "Working: 139/200\n",
      "Working: 142/200\n",
      "Working: 145/200\n",
      "Working: 148/200\n",
      "Working: 151/200\n",
      "Working: 154/200\n",
      "Working: 157/200\n",
      "Working: 160/200\n",
      "Working: 163/200\n",
      "Working: 166/200\n",
      "Working: 169/200\n",
      "Working: 172/200\n",
      "Working: 175/200\n",
      "Working: 178/200\n",
      "Working: 181/200\n",
      "Working: 184/200\n",
      "Working: 187/200\n",
      "Working: 190/200\n",
      "Working: 193/200\n",
      "Working: 196/200\n",
      "Working: 199/200\n",
      "Working: 202/200\n",
      "Working: 205/200\n",
      "Working: 208/200\n",
      "Working: 211/200\n",
      "Working: 214/200\n",
      "Working: 217/200\n",
      "Working: 220/200\n",
      "Working: 223/200\n",
      "Working: 226/200\n",
      "Working: 229/200\n",
      "Working: 232/200\n",
      "Working: 235/200\n",
      "Working: 238/200\n",
      "Working: 241/200\n",
      "Working: 244/200\n",
      "Working: 247/200\n",
      "Working: 250/200\n",
      "Working: 253/200\n",
      "Working: 256/200\n",
      "Working: 259/200\n",
      "Working: 262/200\n",
      "Working: 265/200\n",
      "Working: 268/200\n",
      "Working: 271/200\n",
      "Working: 274/200\n",
      "Working: 277/200\n",
      "Working: 280/200\n",
      "Working: 283/200\n",
      "Working: 286/200\n",
      "Working: 289/200\n",
      "Working: 292/200\n",
      "Working: 295/200\n",
      "Working: 298/200\n",
      "Working: 301/200\n",
      "Working: 304/200\n",
      "Working: 307/200\n",
      "Working: 310/200\n",
      "Working: 313/200\n",
      "Working: 316/200\n",
      "Working: 319/200\n",
      "Working: 322/200\n",
      "Working: 325/200\n",
      "Working: 328/200\n",
      "Working: 331/200\n",
      "Working: 334/200\n",
      "Working: 337/200\n",
      "Working: 340/200\n",
      "Working: 343/200\n",
      "Working: 346/200\n",
      "Working: 349/200\n",
      "Working: 352/200\n",
      "Working: 355/200\n",
      "Working: 358/200\n",
      "Working: 361/200\n",
      "Working: 364/200\n",
      "Working: 367/200\n",
      "Working: 370/200\n",
      "Working: 373/200\n",
      "Working: 376/200\n",
      "Working: 379/200\n",
      "Working: 382/200\n",
      "Working: 385/200\n",
      "Working: 388/200\n",
      "Working: 391/200\n",
      "Working: 394/200\n",
      "Working: 397/200\n",
      "Working: 400/200\n",
      "Working: 403/200\n",
      "Working: 406/200\n",
      "Working: 409/200\n",
      "Working: 412/200\n",
      "Working: 415/200\n",
      "Working: 418/200\n",
      "Working: 421/200\n",
      "Working: 424/200\n",
      "Working: 427/200\n",
      "Working: 430/200\n",
      "Working: 433/200\n",
      "Working: 436/200\n",
      "Working: 439/200\n",
      "Working: 442/200\n",
      "Working: 445/200\n",
      "Working: 448/200\n",
      "Working: 451/200\n",
      "Working: 454/200\n",
      "Working: 457/200\n",
      "Working: 460/200\n",
      "Working: 463/200\n",
      "Working: 466/200\n",
      "Working: 469/200\n",
      "Working: 472/200\n",
      "Working: 475/200\n",
      "Working: 478/200\n",
      "Working: 481/200\n",
      "Working: 484/200\n",
      "Working: 487/200\n",
      "Working: 490/200\n",
      "Working: 493/200\n",
      "Working: 496/200\n",
      "Working: 499/200\n",
      "Working: 502/200\n",
      "Working: 505/200\n",
      "Working: 508/200\n",
      "Working: 511/200\n",
      "Working: 514/200\n",
      "Working: 517/200\n",
      "Working: 520/200\n",
      "Working: 523/200\n",
      "Working: 526/200\n",
      "Working: 529/200\n",
      "Working: 532/200\n",
      "Working: 535/200\n",
      "Working: 538/200\n",
      "Working: 541/200\n",
      "Working: 544/200\n",
      "Working: 547/200\n",
      "Working: 550/200\n",
      "Working: 553/200\n",
      "Working: 556/200\n",
      "Working: 559/200\n",
      "Working: 562/200\n",
      "Working: 565/200\n",
      "Working: 568/200\n",
      "Working: 571/200\n",
      "Working: 574/200\n",
      "Working: 577/200\n",
      "Working: 580/200\n",
      "Working: 583/200\n",
      "Working: 586/200\n",
      "Working: 589/200\n",
      "Working: 592/200\n",
      "Working: 595/200\n",
      "Working: 598/200\n",
      "Accuracy: 86.33%\n",
      "Precision: 87.7979\n",
      "Recall: 86.3333\n",
      "F1 Score: 86.3036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "save_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    actual_pred = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in test_load:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        print(f\"Working: {total + 1}/{len(test_load)}\")\n",
    "\n",
    "        test_predictions = save_model(inputs, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(test_predictions.logits, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        actual_pred += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = actual_pred / total\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# printing Accuracy,precision,recall,F! score\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.4f}\")\n",
    "print(f\"Recall: {recall*100:.4f}\")\n",
    "print(f\"F1 Score: {f1*100:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T08:27:51.202004254Z",
     "start_time": "2023-12-02T08:26:27.895022424Z"
    }
   },
   "id": "f47646cfcec0c03"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1eb5e5a923dedfa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:38:34.008853043Z",
     "start_time": "2023-12-01T22:38:33.954660236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "chunk= 100\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        csv= \"chunk.csv\"\n",
    "        x.to_csv(csv, index=False)\n",
    "\n",
    "    df2= pd.read_csv(\"chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Roberta for unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1565d0b43bc1e0d6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403ce9ec799a8e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:38:37.572607013Z",
     "start_time": "2023-12-01T22:38:37.505913472Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "text_data = df2['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba29db46d8d0ead"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text_encode = tokenizer(text_data, padding=True, truncation=True, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:38:51.116194939Z",
     "start_time": "2023-12-01T22:38:46.895998019Z"
    }
   },
   "id": "8eb7908ee75a4743"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecff625e6772ccc1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**text_encode)\n",
    "    embeddings_data = outputs.last_hidden_state.mean(dim=1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:39:52.742598806Z",
     "start_time": "2023-12-01T22:38:55.194041920Z"
    }
   },
   "id": "a213b623a760f9bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting and finding PCA components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ca2f7c6350d09e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(embeddings_data)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x=range(1, len(pca.explained_variance_ratio_) + 1), y=np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Variance')\n",
    "plt.xlabel('No. of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:39:59.198206802Z",
     "start_time": "2023-12-01T22:39:58.809263694Z"
    }
   },
   "id": "2c50a57aa693d044"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using elbow method to decide the number of components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b0f77f3b2a4bf6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cumulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "elbow_index = np.argmax(np.diff(cumulative_var) < 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:03.963809164Z",
     "start_time": "2023-12-01T22:40:03.945367535Z"
    }
   },
   "id": "66a63a969eb68e70"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components: 14\n"
     ]
    }
   ],
   "source": [
    "components = elbow_index + 1\n",
    "print(f\"Number of Components: {components}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:04.926633582Z",
     "start_time": "2023-12-01T22:40:04.904568021Z"
    }
   },
   "id": "73e9d25ca5c33d76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying PCA with 14 components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88d932ca8052edc3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=14)\n",
    "embeddings_pca = pca.fit_transform(embeddings_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ed9bb820218da83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2533f3082e46c7ad"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:14.573391192Z",
     "start_time": "2023-12-01T22:40:14.529877451Z"
    }
   },
   "id": "97704e7e86756524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustering using Kmeans"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2701cb471eb56eb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2)  # Positive and Negative\n",
    "clusters = kmeans.fit_predict(embeddings_scaled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:18.548031366Z",
     "start_time": "2023-12-01T22:40:18.209639047Z"
    }
   },
   "id": "4d1465988634079e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 text  Cluster_pred\n0   If you decide to eat here, just be aware it is...             1\n1   I've taken a lot of spin classes over the year...             0\n2   Family diner. Had the buffet. Eclectic assortm...             1\n3   Wow!  Yummy, different,  delicious.   Our favo...             1\n4   Cute interior and owner (?) gave us tour of up...             1\n..                                                ...           ...\n95  Had to wait until my third trip to NOLA to act...             0\n96  A GREAT EXPERIENCE!!!!!!!!!  I was a completel...             0\n97  Wow! I never thought my sons phone could be re...             0\n98  Service and management terrible... After messi...             1\n99  I have been to a number of dog friendly hotels...             0\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Cluster_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Had to wait until my third trip to NOLA to act...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>A GREAT EXPERIENCE!!!!!!!!!  I was a completel...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>Wow! I never thought my sons phone could be re...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>Service and management terrible... After messi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>I have been to a number of dog friendly hotels...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Cluster_pred'] = clusters\n",
    "# Print the DataFrame with predicted clusters\n",
    "df2[['text', 'Cluster_pred']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:20.857605024Z",
     "start_time": "2023-12-01T22:40:20.841555607Z"
    }
   },
   "id": "fa15a0d26455aae4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(df2['Cluster_pred'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:24.730441208Z",
     "start_time": "2023-12-01T22:40:24.694103355Z"
    }
   },
   "id": "56256995928f5dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def sentiment(stars):  #Ashken\n",
    "    if 0 <= stars <3:\n",
    "        return 0 #negative\n",
    "    elif 3<= stars <= 5:\n",
    "        return 1 #positive\n",
    "\n",
    "df2['sentiment_label'] = df2['stars'].apply(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:27.629385845Z",
     "start_time": "2023-12-01T22:40:27.611385876Z"
    }
   },
   "id": "dd4542a60146d7d8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                 review_id                 user_id             business_id  \\\n0   KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n1   BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n2   saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n3   AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n4   Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n..                     ...                     ...                     ...   \n95  QS7CuOtFLuS3dnwKHRtSYQ  PBnEwGVCBL0N-bET6ZI6kQ  m5-FtgWRd4qA7j0vaOXiXQ   \n96  4PHFo_GRG4FEk1q4X7xQVQ  jbsCBG0A-3wVDjrKar-0Wg  X63jIMRHYBvKKQDuJTRiQQ   \n97  1c6sgLe07HAhipebsQ1wgA  ZRXvbrutBBULaFS6T9NCwA  HnhxO2cpa15AHI1456Pl6Q   \n98  PPgbLBvi34A6m7bKJfTwhw  3TL6HZ1JrKcNTvGDWKlrow  GyC36Pn0Q1-qHnqXys6yFg   \n99  gImS1dtA_TixEouDfp2o4g  xE7AXFF9wVaN6id6OCtH3Q  D5V0Fawd6ODVgqCY8xngsw   \n\n    stars  useful  funny  cool  \\\n0       3       0      0     0   \n1       5       1      0     1   \n2       3       0      0     0   \n3       5       1      0     1   \n4       4       1      0     1   \n..    ...     ...    ...   ...   \n95      5       0      0     0   \n96      5       2      0     1   \n97      5       0      1     0   \n98      1       0      0     0   \n99      4       1      0     2   \n\n                                                 text                 date  \\\n0   If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n1   I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n2   Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n3   Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n4   Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n..                                                ...                  ...   \n95  Had to wait until my third trip to NOLA to act...  2016-11-10 20:56:13   \n96  A GREAT EXPERIENCE!!!!!!!!!  I was a completel...  2014-10-11 13:55:05   \n97  Wow! I never thought my sons phone could be re...  2015-10-17 00:55:35   \n98  Service and management terrible... After messi...  2013-12-07 13:17:13   \n99  I have been to a number of dog friendly hotels...  2017-01-14 21:05:04   \n\n    Cluster_pred  sentiment_label  \n0              1                1  \n1              0                1  \n2              1                1  \n3              1                1  \n4              1                1  \n..           ...              ...  \n95             0                1  \n96             0                1  \n97             0                1  \n98             1                0  \n99             0                1  \n\n[100 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>Cluster_pred</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>QS7CuOtFLuS3dnwKHRtSYQ</td>\n      <td>PBnEwGVCBL0N-bET6ZI6kQ</td>\n      <td>m5-FtgWRd4qA7j0vaOXiXQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Had to wait until my third trip to NOLA to act...</td>\n      <td>2016-11-10 20:56:13</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>4PHFo_GRG4FEk1q4X7xQVQ</td>\n      <td>jbsCBG0A-3wVDjrKar-0Wg</td>\n      <td>X63jIMRHYBvKKQDuJTRiQQ</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A GREAT EXPERIENCE!!!!!!!!!  I was a completel...</td>\n      <td>2014-10-11 13:55:05</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>1c6sgLe07HAhipebsQ1wgA</td>\n      <td>ZRXvbrutBBULaFS6T9NCwA</td>\n      <td>HnhxO2cpa15AHI1456Pl6Q</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Wow! I never thought my sons phone could be re...</td>\n      <td>2015-10-17 00:55:35</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>PPgbLBvi34A6m7bKJfTwhw</td>\n      <td>3TL6HZ1JrKcNTvGDWKlrow</td>\n      <td>GyC36Pn0Q1-qHnqXys6yFg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Service and management terrible... After messi...</td>\n      <td>2013-12-07 13:17:13</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>gImS1dtA_TixEouDfp2o4g</td>\n      <td>xE7AXFF9wVaN6id6OCtH3Q</td>\n      <td>D5V0Fawd6ODVgqCY8xngsw</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>I have been to a number of dog friendly hotels...</td>\n      <td>2017-01-14 21:05:04</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:28.464913061Z",
     "start_time": "2023-12-01T22:40:28.428279802Z"
    }
   },
   "id": "b7503828e8e0835a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with labelled set"
   ],
   "id": "1deecd4647849e6f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.00%\n"
     ]
    }
   ],
   "source": [
    "true_labels = df2['sentiment_label'].tolist()\n",
    "\n",
    "# encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded1 = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(encoded1, clusters)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:31.656497724Z",
     "start_time": "2023-12-01T22:40:31.646218330Z"
    }
   },
   "id": "7dc06b36467afbc"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a confusion matrix\n",
    "conf_matrix = confusion_matrix(encoded1, clusters)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:35.061148967Z",
     "start_time": "2023-12-01T22:40:34.934835048Z"
    }
   },
   "id": "120a53a4bfd011ac"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.09      0.33      0.14        12\n",
      "    Positive       0.85      0.53      0.66        88\n",
      "\n",
      "    accuracy                           0.51       100\n",
      "   macro avg       0.47      0.43      0.40       100\n",
      "weighted avg       0.76      0.51      0.60       100\n"
     ]
    }
   ],
   "source": [
    "# Printing a classification report\n",
    "report = classification_report(encoded1, clusters, target_names=['Negative', 'Positive'])\n",
    "print(\"Report:\\n\", report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T22:40:37.830399720Z",
     "start_time": "2023-12-01T22:40:37.814003295Z"
    }
   },
   "id": "d34c22671d02725a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e33da6e2cace718"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
