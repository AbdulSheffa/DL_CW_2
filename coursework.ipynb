{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ],
   "id": "d6f6c751c626a782"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6d076706c93cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:40:27.903042664Z",
     "start_time": "2023-12-01T09:40:23.199614610Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 15:10:26.331082: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-01 15:10:26.445529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 15:10:26.445580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 15:10:26.459855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 15:10:26.493907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 15:10:27.299110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ],
   "id": "cbd7368cf0d05d61"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:10:57.499194558Z",
     "start_time": "2023-12-01T09:08:28.711294876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 700000\n",
    "json_file_path = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Read the JSON file in chunks\n",
    "chunks = pd.read_json(json_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "for i, df_chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    chunk_csv_path = f\"chunk_{i + 1}.csv\"\n",
    "    df_chunk.to_csv(chunk_csv_path, index=False)\n",
    "    df_chunk_from_csv = pd.read_csv(chunk_csv_path)"
   ],
   "id": "91ba0966af9aa095"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fff3cc6a84e894f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:11:46.409676497Z",
     "start_time": "2023-12-01T09:11:31.170230223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "# Using the first 100000\n",
    "chunk_size = 700000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Reading the JSON file in chunks\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        first_chunk_csv_path = \"first_chunk.csv\"\n",
    "        x.to_csv(first_chunk_csv_path, index=False)\n",
    "\n",
    "    df= pd.read_csv(\"first_chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n699995  8nPFAHUg2Maq20h9PxHCSg  fM-DKqmd9Lh08bApyY-dsg   \n699996  siyHQqTd-35kd9fEQkMskg  YYfgEK77PFL6-ErHHElHww   \n699997  iuBOjA0J7B0_yT_3FEgUJQ  7h7SkRAIJMtynNSGntyxbw   \n699998  2WPcCACe-NecwIKDIRRPrA  qMdjazC9quXk6AElD3i9yA   \n699999  Q_BdRCOkQRcVyF7JOaT5Lg  89aoy6WORObOOdU5yPQxoQ   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n699995  IHd8_cnLZe5oILE_oKy-5g      1       0      0     0   \n699996  DvkSkF83xDONjkNIoEPRnQ      5       4      1     0   \n699997  Y3ZCO17N1_T_Ms1JmswwzA      4       0      0     0   \n699998  f4vbnGoGo3eWorVekctVGQ      5       0      0     0   \n699999  L3QX19EWlkS-clS68Boydg      4       0      0     0   \n\n                                                     text                 date  \n0       If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n1       I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n2       Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n3       Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n4       Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n...                                                   ...                  ...  \n699995  Do not go here! The only redeeming quality of ...  2015-05-23 23:16:41  \n699996  This place is amazing. I'm very surprised seei...  2017-07-12 19:57:37  \n699997  Horchata was good, and the tortillas were grea...  2016-11-15 23:50:18  \n699998  Best omelette and grits both my girlfriend and...  2015-12-29 17:38:16  \n699999  Stopped in to grab an order of beignets becaus...  2013-06-06 21:03:14  \n\n[700000 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>699995</th>\n      <td>8nPFAHUg2Maq20h9PxHCSg</td>\n      <td>fM-DKqmd9Lh08bApyY-dsg</td>\n      <td>IHd8_cnLZe5oILE_oKy-5g</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Do not go here! The only redeeming quality of ...</td>\n      <td>2015-05-23 23:16:41</td>\n    </tr>\n    <tr>\n      <th>699996</th>\n      <td>siyHQqTd-35kd9fEQkMskg</td>\n      <td>YYfgEK77PFL6-ErHHElHww</td>\n      <td>DvkSkF83xDONjkNIoEPRnQ</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>This place is amazing. I'm very surprised seei...</td>\n      <td>2017-07-12 19:57:37</td>\n    </tr>\n    <tr>\n      <th>699997</th>\n      <td>iuBOjA0J7B0_yT_3FEgUJQ</td>\n      <td>7h7SkRAIJMtynNSGntyxbw</td>\n      <td>Y3ZCO17N1_T_Ms1JmswwzA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Horchata was good, and the tortillas were grea...</td>\n      <td>2016-11-15 23:50:18</td>\n    </tr>\n    <tr>\n      <th>699998</th>\n      <td>2WPcCACe-NecwIKDIRRPrA</td>\n      <td>qMdjazC9quXk6AElD3i9yA</td>\n      <td>f4vbnGoGo3eWorVekctVGQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Best omelette and grits both my girlfriend and...</td>\n      <td>2015-12-29 17:38:16</td>\n    </tr>\n    <tr>\n      <th>699999</th>\n      <td>Q_BdRCOkQRcVyF7JOaT5Lg</td>\n      <td>89aoy6WORObOOdU5yPQxoQ</td>\n      <td>L3QX19EWlkS-clS68Boydg</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Stopped in to grab an order of beignets becaus...</td>\n      <td>2013-06-06 21:03:14</td>\n    </tr>\n  </tbody>\n</table>\n<p>700000 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:11:47.423067899Z",
     "start_time": "2023-12-01T09:11:47.402497737Z"
    }
   },
   "id": "d36238e10f1cf45e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb5a9e38a21f4e4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "texts = df['text']\n",
    "labels = df['stars'].apply(lambda x: 1 if x > 3 else 0)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:04.329405231Z",
     "start_time": "2023-12-01T09:12:04.189822722Z"
    }
   },
   "id": "f46808944f15d9df"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n699995  8nPFAHUg2Maq20h9PxHCSg  fM-DKqmd9Lh08bApyY-dsg   \n699996  siyHQqTd-35kd9fEQkMskg  YYfgEK77PFL6-ErHHElHww   \n699997  iuBOjA0J7B0_yT_3FEgUJQ  7h7SkRAIJMtynNSGntyxbw   \n699998  2WPcCACe-NecwIKDIRRPrA  qMdjazC9quXk6AElD3i9yA   \n699999  Q_BdRCOkQRcVyF7JOaT5Lg  89aoy6WORObOOdU5yPQxoQ   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n699995  IHd8_cnLZe5oILE_oKy-5g      1       0      0     0   \n699996  DvkSkF83xDONjkNIoEPRnQ      5       4      1     0   \n699997  Y3ZCO17N1_T_Ms1JmswwzA      4       0      0     0   \n699998  f4vbnGoGo3eWorVekctVGQ      5       0      0     0   \n699999  L3QX19EWlkS-clS68Boydg      4       0      0     0   \n\n                                                     text                 date  \n0       If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n1       I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n2       Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n3       Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n4       Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n...                                                   ...                  ...  \n699995  Do not go here! The only redeeming quality of ...  2015-05-23 23:16:41  \n699996  This place is amazing. I'm very surprised seei...  2017-07-12 19:57:37  \n699997  Horchata was good, and the tortillas were grea...  2016-11-15 23:50:18  \n699998  Best omelette and grits both my girlfriend and...  2015-12-29 17:38:16  \n699999  Stopped in to grab an order of beignets becaus...  2013-06-06 21:03:14  \n\n[700000 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>699995</th>\n      <td>8nPFAHUg2Maq20h9PxHCSg</td>\n      <td>fM-DKqmd9Lh08bApyY-dsg</td>\n      <td>IHd8_cnLZe5oILE_oKy-5g</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Do not go here! The only redeeming quality of ...</td>\n      <td>2015-05-23 23:16:41</td>\n    </tr>\n    <tr>\n      <th>699996</th>\n      <td>siyHQqTd-35kd9fEQkMskg</td>\n      <td>YYfgEK77PFL6-ErHHElHww</td>\n      <td>DvkSkF83xDONjkNIoEPRnQ</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>This place is amazing. I'm very surprised seei...</td>\n      <td>2017-07-12 19:57:37</td>\n    </tr>\n    <tr>\n      <th>699997</th>\n      <td>iuBOjA0J7B0_yT_3FEgUJQ</td>\n      <td>7h7SkRAIJMtynNSGntyxbw</td>\n      <td>Y3ZCO17N1_T_Ms1JmswwzA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Horchata was good, and the tortillas were grea...</td>\n      <td>2016-11-15 23:50:18</td>\n    </tr>\n    <tr>\n      <th>699998</th>\n      <td>2WPcCACe-NecwIKDIRRPrA</td>\n      <td>qMdjazC9quXk6AElD3i9yA</td>\n      <td>f4vbnGoGo3eWorVekctVGQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Best omelette and grits both my girlfriend and...</td>\n      <td>2015-12-29 17:38:16</td>\n    </tr>\n    <tr>\n      <th>699999</th>\n      <td>Q_BdRCOkQRcVyF7JOaT5Lg</td>\n      <td>89aoy6WORObOOdU5yPQxoQ</td>\n      <td>L3QX19EWlkS-clS68Boydg</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Stopped in to grab an order of beignets becaus...</td>\n      <td>2013-06-06 21:03:14</td>\n    </tr>\n  </tbody>\n</table>\n<p>700000 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:05.886046764Z",
     "start_time": "2023-12-01T09:12:05.859749769Z"
    }
   },
   "id": "e6c1147a38f4e35e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['stars'].value_counts().sort_index().index, df['stars'].value_counts().sort_index(), color='purple')\n",
    "plt.title('Ratings')\n",
    "plt.xlabel('Stars')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:33.582533713Z",
     "start_time": "2023-12-01T09:12:33.335961745Z"
    }
   },
   "id": "15acafa680b135c3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['review_length'] = df['text'].apply(len)\n",
    "df['review_length'].hist(bins=50, color='red')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:37.923099510Z",
     "start_time": "2023-12-01T09:12:37.654811215Z"
    }
   },
   "id": "33ee94c6c65f7b87"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1800x600 with 3 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "df.boxplot(column='useful', by='stars', ax=axes[0])\n",
    "axes[0].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='funny', by='stars', ax=axes[1])\n",
    "axes[1].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='cool', by='stars', ax=axes[2])\n",
    "axes[2].set_title('Votes by Stars')\n",
    "\n",
    "plt.suptitle('Features by Stars')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:43.144021048Z",
     "start_time": "2023-12-01T09:12:41.859319008Z"
    }
   },
   "id": "423ce5ed6ece5ee1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b21f02aac3bbbe6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                     review_id                 user_id  \\\n0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n...                        ...                     ...   \n686552  t1GdeuIO_Mcw0E2oEU662w  7y3HeuIWJFoVRe-e9kAi3A   \n689115  Xxr0VMnm0RnXlr5RSMt9HQ  sb3ZzGzoUECo64EZd6DE9g   \n693901  zMgZCG75QDYYCvKKrGGYfw  Q4-WQT0Q3_X4ZtCwDdfH_g   \n695069  bqYN3sifORSPnMIInqYgdg  kLPT0qfgHY5ppF2NbdPvlg   \n696153  0fm6c5UH3YpiKA7T7nQx-w  iEirxmriN2h2NSBkMpMbUA   \n\n                   business_id  stars  useful  funny  cool  \\\n0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n...                        ...    ...     ...    ...   ...   \n686552  oDaamK_x-XhTOieP-qVvnw      5       1      0     1   \n689115  cBbvS4klOQd221pKQVMGMA      4       2      0     0   \n693901  2FR-xWttfR8qaODqcuExvw      1       1      1     0   \n695069  1q54Dq02nSFa9NFbu0Tbxw      1       3      0     0   \n696153  dbfm8H0KQRrxT_QSXrQZ-A      5       1      0     0   \n\n                                                     text  \\\n0       If you decide to eat here, just be aware it is...   \n1       I've taken a lot of spin classes over the year...   \n2       Family diner. Had the buffet. Eclectic assortm...   \n3       Wow!  Yummy, different,  delicious.   Our favo...   \n4       Cute interior and owner (?) gave us tour of up...   \n...                                                   ...   \n686552  Great selection of Mexican related food stuffs...   \n689115  Found some sweet deals on fruits!!! I came acr...   \n693901  I got one of those prepaid debit card \"Excepta...   \n695069  Absolute worst, most embarrassing experience o...   \n696153  By far the best chicken pot pie I've ever had ...   \n\n                       date  review_length  \n0       2018-07-07 22:09:11            513  \n1       2012-01-03 15:28:18            829  \n2       2014-02-05 20:30:30            339  \n3       2015-01-04 00:01:03            243  \n4       2017-01-14 20:54:15            534  \n...                     ...            ...  \n686552  2014-01-11 13:00:46            103  \n689115  2019-11-14 00:13:54            163  \n693901  2020-04-29 00:09:28            428  \n695069  2019-04-28 15:00:37            894  \n696153  2021-01-17 17:18:22            227  \n\n[14965 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>review_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>513</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>829</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>339</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>243</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>534</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>686552</th>\n      <td>t1GdeuIO_Mcw0E2oEU662w</td>\n      <td>7y3HeuIWJFoVRe-e9kAi3A</td>\n      <td>oDaamK_x-XhTOieP-qVvnw</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Great selection of Mexican related food stuffs...</td>\n      <td>2014-01-11 13:00:46</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>689115</th>\n      <td>Xxr0VMnm0RnXlr5RSMt9HQ</td>\n      <td>sb3ZzGzoUECo64EZd6DE9g</td>\n      <td>cBbvS4klOQd221pKQVMGMA</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Found some sweet deals on fruits!!! I came acr...</td>\n      <td>2019-11-14 00:13:54</td>\n      <td>163</td>\n    </tr>\n    <tr>\n      <th>693901</th>\n      <td>zMgZCG75QDYYCvKKrGGYfw</td>\n      <td>Q4-WQT0Q3_X4ZtCwDdfH_g</td>\n      <td>2FR-xWttfR8qaODqcuExvw</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>I got one of those prepaid debit card \"Excepta...</td>\n      <td>2020-04-29 00:09:28</td>\n      <td>428</td>\n    </tr>\n    <tr>\n      <th>695069</th>\n      <td>bqYN3sifORSPnMIInqYgdg</td>\n      <td>kLPT0qfgHY5ppF2NbdPvlg</td>\n      <td>1q54Dq02nSFa9NFbu0Tbxw</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Absolute worst, most embarrassing experience o...</td>\n      <td>2019-04-28 15:00:37</td>\n      <td>894</td>\n    </tr>\n    <tr>\n      <th>696153</th>\n      <td>0fm6c5UH3YpiKA7T7nQx-w</td>\n      <td>iEirxmriN2h2NSBkMpMbUA</td>\n      <td>dbfm8H0KQRrxT_QSXrQZ-A</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>By far the best chicken pot pie I've ever had ...</td>\n      <td>2021-01-17 17:18:22</td>\n      <td>227</td>\n    </tr>\n  </tbody>\n</table>\n<p>14965 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Drop duplicates\n",
    "df = df.drop_duplicates(subset=['review_id'])\n",
    "df = df.drop_duplicates(subset=['user_id'])\n",
    "df = df.drop_duplicates(subset=['business_id'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:53.201406552Z",
     "start_time": "2023-12-01T09:12:52.836224110Z"
    }
   },
   "id": "ec860b4d0dd4f343"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls:\n",
      " review_id        0\n",
      "user_id          0\n",
      "business_id      0\n",
      "stars            0\n",
      "useful           0\n",
      "funny            0\n",
      "cool             0\n",
      "text             0\n",
      "date             0\n",
      "review_length    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove Null values \n",
    "nulls= df.isnull().sum()\n",
    "print(\"Nulls:\\n\", nulls)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:12:56.764151988Z",
     "start_time": "2023-12-01T09:12:56.747942372Z"
    }
   },
   "id": "dcd81e9a795ee1c9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import re\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "##%%\n",
    "# Remove special characters \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:13:00.303914901Z",
     "start_time": "2023-12-01T09:13:00.195372024Z"
    }
   },
   "id": "8df19d2f709cd75b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:13:02.347290599Z",
     "start_time": "2023-12-01T09:13:02.327822374Z"
    }
   },
   "id": "79b8eee080ee2c19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7d0e87ac37b0cbe"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(texts)\n",
    "s = token.texts_to_sequences(texts)\n",
    "index = token.word_index\n",
    "maximumlength = 100  # Set your desired sequence length\n",
    "data = pad_sequences(s, maxlen=maximumlength)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:13:52.531514691Z",
     "start_time": "2023-12-01T09:13:07.670788700Z"
    }
   },
   "id": "a35ed24848ab6241"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "l_encode = LabelEncoder()\n",
    "labels = l_encode.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:14:09.493485408Z",
     "start_time": "2023-12-01T09:14:09.433405740Z"
    }
   },
   "id": "30ab1057be0184f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and Test Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f482dc523c794b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.18, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:14:14.035475596Z",
     "start_time": "2023-12-01T09:14:13.816224237Z"
    }
   },
   "id": "ff432a9e5bb8299e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff7cbf3af4d67126"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(index) + 1, 100, input_length=maximumlength))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:14:40.533389631Z",
     "start_time": "2023-12-01T09:14:40.275473657Z"
    }
   },
   "id": "7739b5302e7a0a65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414c95ba8fd89d27"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:14:45.246520389Z",
     "start_time": "2023-12-01T09:14:45.203029189Z"
    }
   },
   "id": "8434b5be39863fd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad8bd4ada616cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    " # Training with Data Generator\n",
    "def data_generator(data, labels, batch_size):\n",
    "    samples = len(data)\n",
    "    while True:\n",
    "        s_indices = np.arange(samples)\n",
    "        np.random.shuffle(s_indices)\n",
    "        for i in range(0,samples, batch_size):\n",
    "            b_indices = s_indices[i:i+batch_size]\n",
    "            yield data[b_indices], labels[b_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:15:17.558199957Z",
     "start_time": "2023-12-01T09:15:17.436019828Z"
    }
   },
   "id": "36795275f60ebf9f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " 2338/17393 [===>..........................] - ETA: 35:51 - loss: 0.2858 - accuracy: 0.8826"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m epochs\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(X_train) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m batch_size\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m          \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m          \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/keras/src/engine/training.py:1807\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1799\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1800\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1801\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1804\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1805\u001B[0m ):\n\u001B[1;32m   1806\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1807\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1809\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    829\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 832\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    835\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    865\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    867\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 868\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    872\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    874\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1319\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1321\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1322\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1323\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1325\u001B[0m     args,\n\u001B[1;32m   1326\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1327\u001B[0m     executing_eagerly)\n\u001B[1;32m   1328\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/context.py:1486\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1484\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1486\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1487\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1488\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1490\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1491\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1492\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1493\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1494\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1495\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1496\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1500\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1501\u001B[0m   )\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 33\n",
    "epochs= len(X_train) // batch_size\n",
    "# Training\n",
    "model.fit(data_generator(X_train, y_train, batch_size),\n",
    "          epochs=7,\n",
    "          steps_per_epoch=epochs,\n",
    "          validation_data=(X_test, y_test))       ######### Lizara"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:21:16.795210182Z",
     "start_time": "2023-12-01T09:15:40.353708757Z"
    }
   },
   "id": "ac7145e0c7846845"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 [==============================] - 36s 8ms/step - loss: 0.2269 - accuracy: 0.9234\n",
      "Accuracy: 0.9234285950660706\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(f\"Accuracy: {accuracy*100}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T03:27:03.961778066Z",
     "start_time": "2023-12-01T03:26:27.756053309Z"
    }
   },
   "id": "f51b6af5cb1ab248"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 [==============================] - 39s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "y_predicted_value = model.predict(X_test)\n",
    "y_binary_value = (y_predicted_value > 0.5).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T03:28:59.456473069Z",
     "start_time": "2023-12-01T03:28:19.325843326Z"
    }
   },
   "id": "1e49e0dbe798f3ec"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9338\n",
      "Recall: 0.9547\n",
      "F1 Score: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Metrics Calculation\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_binary_value, average='binary')\n",
    "\n",
    "print(f\"Precision score: {prec*100:.4f}\")\n",
    "print(f\"Recall score: {rec*100:.4f}\")\n",
    "print(f\"F1 Score: {f1*100:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T03:29:38.505619098Z",
     "start_time": "2023-12-01T03:29:38.464818473Z"
    }
   },
   "id": "5ed1941d4577fbe9"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHHCAYAAACcHAM1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk+UlEQVR4nO3de3zO9f/H8ce184yNYZs5ziGsREYzoa9ahlFKB4fksIg2pyGUkGKlFMth3w7fqCiHylcO0yIky2EOOVNIYpucZsOO1+8Pv11fV1tsXdfluuJ57/a53ezzeX3en/fnyuy11/vzfn8MRqPRiIiIiIgDc7J3B0RERERuRAmLiIiIODwlLCIiIuLwlLCIiIiIw1PCIiIiIg5PCYuIiIg4PCUsIiIi4vCUsIiIiIjDU8IiIiIiDk8Ji4j8pYkTJ2IwGOzdDRERJSwijmLu3LkYDAYMBgMbN24sctxoNFK9enUMBgOdOnUqdftTpkxh6dKlVuipiMjNp4RFxMF4eHiwYMGCIvvXr1/PiRMncHd3/1vt/p2EZdy4cVy+fPlvXU9ExJqUsIg4mI4dO7J48WLy8vLM9i9YsICQkBACAgJs3oesrCwAXFxc8PDwsPn1RERuRAmLiIPp3r07Z86cISkpybQvJyeHJUuW0KNHjyLxb731Fi1btqRixYp4enoSEhLCkiVLzGIMBgNZWVnMmzfPNOzUp08f4H/Pqezbt48ePXpQoUIFWrVqZXas0EcffYTBYOA///mPWftTpkzBYDCwcuVKa30MIiJmlLCIOJhatWoRFhbGZ599Ztq3atUqLly4QLdu3YrEz5gxg3vuuYdJkyYxZcoUXFxceOKJJ1ixYoUp5pNPPsHd3Z3WrVvzySef8Mknn/Dcc8+ZtfPEE09w6dIlpkyZQv/+/YvtW9++fenUqROxsbH89ttvAOzevZtXXnmFqKgoOnbsaI2PQESkCBd7d0BEiurRowdjx47l8uXLeHp6Mn/+fO6//34CAwOLxB46dAhPT0/T1zExMTRt2pS3336byMhIAJ5++mkGDhxI7dq1efrpp4u9ZuPGjYt9dubP3n//fe68806ioqJYvnw5vXv3JiAggLfffvtv3q2IyI2pwiLigJ588kkuX77M8uXLuXjxIsuXLy92OAgwS1bOnTvHhQsXaN26Ndu3by/VNQcOHFiiuICAAGbNmkVSUhKtW7dm586d/Oc//8Hb27tU1xMRKQ1VWEQcUOXKlQkPD2fBggVcunSJ/Px8Hn/88WJjly9fzmuvvcbOnTvJzs427S/t+ilBQUElju3WrRuffvopK1asYMCAATz44IOlupaISGkpYRFxUD169KB///6kpqbSoUMHypcvXyTm+++/5+GHH6ZNmzbMnj2bKlWq4OrqykcffVSi4Z1rXVupuZEzZ86wbds2APbt20dBQQFOTirYiojt6F8YEQf16KOP4uTkxI8//viXw0FffPEFHh4erF69mn79+tGhQwfCw8OLjbXmirXR0dFcvHiRuLg4Nm7cyPTp063WtohIcVRhEXFQZcuWZc6cORw7dozOnTsXG+Ps7IzBYCA/P9+079ixY8UuEOfl5cX58+ct7teSJUtYuHAh8fHxDB48mF27djFu3Dg6derEHXfcYXH7IiLFUYVFxIH17t2bCRMm/OVwTWRkJJcuXaJ9+/YkJCQwadIkQkNDqVu3bpHYkJAQvv32W95++20+//xzNm/eXOr+pKenM2jQINq2bUtMTAwAM2fOxNvbmz59+lBQUFDqNkVESkIJi8g/2AMPPMCHH35Iamoqw4YN47PPPuONN97g0UcfLRL79ttvExISwrhx4+jevTtz5swp9fUGDRpEdna2aQE5gIoVK/Lee++RnJzMW2+9ZfE9iYgUx2A0Go327oSIiIjI9ajCIiIiIg5PCYuIiIg4PCUsIiIi4vCUsIiIiIjDU8IiIiIiDk8Ji4iIiDg8JSwiIiK3qIsXLzJs2DBq1qyJp6cnLVu2ZOvWrabjRqOR8ePHU6VKFTw9PQkPD+fw4cNmbZw9e5aePXvi7e1N+fLliYqKIjMz0yzmp59+onXr1nh4eFC9enWmTp1apC+LFy+mQYMGeHh40KhRI1auXFmqe7kll+afvemYvbsg4pB6hdSwdxdEHE45d9v/7u55T4xV2rm8Y2ap4p999ln27NnDJ598QmBgIJ9++inh4eHs27ePqlWrMnXqVOLj45k3bx5BQUG8/PLLREREsG/fPjw8PADo2bMnp06dIikpidzcXPr27cuAAQNML1jNyMigXbt2hIeHk5CQwO7du+nXrx/ly5dnwIABAGzatInu3bsTFxdHp06dWLBgAV26dGH79u3cddddJbqXW3LhOCUsIsVTwiJS1K2asFy+fJly5crx3//+l8jISNP+kJAQOnTowKuvvkpgYCAjRoxg5MiRAFy4cAF/f3/mzp1Lt27d2L9/P8HBwWzdupVmzZoBkJiYSMeOHTlx4gSBgYHMmTOHl156idTUVNzc3AAYM2YMS5cu5cCBAwA89dRTZGVlsXz5clM/WrRoQZMmTUhISCjR/WhISERExNYMTlbZsrOzycjIMNuys7OLvWReXh75+fmmSkkhT09PNm7cyNGjR0lNTTV7w7uPjw+hoaEkJycDkJycTPny5U3JCkB4eDhOTk6m95ElJyfTpk0bU7ICEBERwcGDBzl37pwp5s9vko+IiDBdpySUsIiIiNiawWCVLS4uDh8fH7MtLi6u2EuWK1eOsLAwXn31VU6ePEl+fj6ffvopycnJnDp1itTUVAD8/f3NzvP39zcdS01Nxc/Pz+y4i4sLvr6+ZjHFtVF47HoxhcdLQgmLiIiIrVmpwjJ27FguXLhgto0dO/YvL/vJJ59gNBqpWrUq7u7uxMfH0717d5yc/nk//v95PRYREblNubu74+3tbba5u7v/ZXydOnVYv349mZmZ/Pbbb2zZsoXc3Fxq165NQEAAAGlpaWbnpKWlmY4FBASQnp5udjwvL4+zZ8+axRTXRuGx68UUHi8JJSwiIiK2ZqUhob/Ly8uLKlWqcO7cOVavXs0jjzxCUFAQAQEBrFmzxhSXkZHB5s2bCQsLAyAsLIzz58+TkpJiilm7di0FBQWEhoaaYjZs2EBubq4pJikpifr161OhQgVTzLXXKYwpvE5JKGERERGxNSsNCZXW6tWrSUxM5OjRoyQlJdG2bVsaNGhA3759MRgMDBs2jNdee41ly5axe/dunnnmGQIDA+nSpQsADRs2pH379vTv358tW7bwww8/EBMTQ7du3QgMDASgR48euLm5ERUVxd69e1m4cCEzZswgNjbW1I+hQ4eSmJjItGnTOHDgABMnTmTbtm3ExJR89tQtuQ6LiIiIYHrG5cSJE/j6+tK1a1cmT56Mq6srAC+88AJZWVkMGDCA8+fP06pVKxITE81mFs2fP5+YmBgefPBBnJyc6Nq1K/Hx8abjPj4+fPPNN0RHRxMSEkKlSpUYP368aQ0WgJYtW7JgwQLGjRvHiy++SL169Vi6dGmJ12ABrcMiclvROiwiRd2UdVhCR1mlncub37RKO/9EqrCIiIjY2t8YzhFz+gRFRETE4anCIiIiYmsWzPCRq5SwiIiI2JqGhCymT1BEREQcniosIiIitqYhIYspYREREbE1DQlZTAmLiIiIranCYjGlfCIiIuLwVGERERGxNQ0JWUwJi4iIiK0pYbGYPkERERFxeKqwiIiI2JqTHrq1lBIWERERW9OQkMX0CYqIiIjDU4VFRETE1rQOi8WUsIiIiNiahoQspk9QREREHJ4qLCIiIramISGLKWERERGxNQ0JWUwJi4iIiK2pwmIxpXwiIiLi8FRhERERsTUNCVlMCYuIiIitaUjIYkr5RERExOGpwiIiImJrGhKymBIWERERW9OQkMWU8omIiIjDU4VFRETE1jQkZDElLCIiIramhMVi+gRFRETE4SlhERERsTWDwTpbKeTn5/Pyyy8TFBSEp6cnderU4dVXX8VoNJpijEYj48ePp0qVKnh6ehIeHs7hw4fN2jl79iw9e/bE29ub8uXLExUVRWZmplnMTz/9ROvWrfHw8KB69epMnTq1SH8WL15MgwYN8PDwoFGjRqxcubJU96OERURExNYMTtbZSuGNN95gzpw5zJw5k/379/PGG28wdepU3n33XVPM1KlTiY+PJyEhgc2bN+Pl5UVERARXrlwxxfTs2ZO9e/eSlJTE8uXL2bBhAwMGDDAdz8jIoF27dtSsWZOUlBTefPNNJk6cyHvvvWeK2bRpE927dycqKoodO3bQpUsXunTpwp49e0r+ERqvTbVuEbM3HbN3F0QcUq+QGvbugojDKedu+9/dPbu8d+OgEri8dMCNg/5fp06d8Pf358MPPzTt69q1K56ennz66acYjUYCAwMZMWIEI0eOBODChQv4+/szd+5cunXrxv79+wkODmbr1q00a9YMgMTERDp27MiJEycIDAxkzpw5vPTSS6SmpuLm5gbAmDFjWLp0KQcOHADgqaeeIisri+XLl5v60qJFC5o0aUJCQkKJ7kcVFhERkVtQy5YtWbNmDYcOHQJg165dbNy4kQ4dOgBw9OhRUlNTCQ8PN53j4+NDaGgoycnJACQnJ1O+fHlTsgIQHh6Ok5MTmzdvNsW0adPGlKwAREREcPDgQc6dO2eKufY6hTGF1ykJzRISERGxNSvNEsrOziY7O9tsn7u7O+7u7kVix4wZQ0ZGBg0aNMDZ2Zn8/HwmT55Mz549AUhNTQXA39/f7Dx/f3/TsdTUVPz8/MyOu7i44OvraxYTFBRUpI3CYxUqVCA1NfW61ykJVVhERERszUoP3cbFxeHj42O2xcXFFXvJRYsWMX/+fBYsWMD27duZN28eb731FvPmzbvJN28dqrCIiIj8Q4wdO5bY2FizfcVVVwBGjRrFmDFj6NatGwCNGjXi119/JS4ujt69exMQEABAWloaVapUMZ2XlpZGkyZNAAgICCA9Pd2s3by8PM6ePWs6PyAggLS0NLOYwq9vFFN4vCRUYREREbExg8Fglc3d3R1vb2+z7a8SlkuXLuHkZP5j3tnZmYKCAgCCgoIICAhgzZo1puMZGRls3ryZsLAwAMLCwjh//jwpKSmmmLVr11JQUEBoaKgpZsOGDeTm5ppikpKSqF+/PhUqVDDFXHudwpjC65SEEhYREREbs1bCUhqdO3dm8uTJrFixgmPHjvHVV1/x9ttv8+ijj5r6NGzYMF577TWWLVvG7t27eeaZZwgMDKRLly4ANGzYkPbt29O/f3+2bNnCDz/8QExMDN26dSMwMBCAHj164ObmRlRUFHv37mXhwoXMmDHDrBI0dOhQEhMTmTZtGgcOHGDixIls27aNmJiYEt+PhoRERERuQe+++y4vv/wyzz//POnp6QQGBvLcc88xfvx4U8wLL7xAVlYWAwYM4Pz587Rq1YrExEQ8PDxMMfPnzycmJoYHH3wQJycnunbtSnx8vOm4j48P33zzDdHR0YSEhFCpUiXGjx9vtlZLy5YtWbBgAePGjePFF1+kXr16LF26lLvuuqvE96N1WERuI1qHRaSom7EOi9cTH1mlnazFfa3Szj+RKiwiIiI2VtrhHClKz7CIiIiIw1OFRURExMZUYbGcEhYREREbU8JiOSUsIiIiNqaExXJ6hkVEREQcniosIiIitqYCi8WUsIiIiNiYhoQspyEhERERcXiqsIiIiNiYKiyWU8IiIiJiY0pYLKchIREREXF4qrCIiIjYmCoslnOYCsv333/P008/TVhYGL///jsAn3zyCRs3brRzz0RERCxksNJ2G3OIhOWLL74gIiICT09PduzYQXZ2NgAXLlxgypQpdu6diIiI2JtDJCyvvfYaCQkJvP/++7i6upr233fffWzfvt2OPRMREbGcwWCwynY7c4hnWA4ePEibNm2K7Pfx8eH8+fM3v0MiIiJWdLsnG9bgEBWWgIAAfv755yL7N27cSO3ate3QIxEREetRhcVyDpGw9O/fn6FDh7J582YMBgMnT55k/vz5jBw5kkGDBtm7eyIiImJnDjEkNGbMGAoKCnjwwQe5dOkSbdq0wd3dnZEjRzJ48GB7d09ERMQyt3dxxCocImExGAy89NJLjBo1ip9//pnMzEyCg4MpW7asvbsmIiJisdt9OMcaHGJI6NNPP+XSpUu4ubkRHBzMvffeq2RFRERETBwiYRk+fDh+fn706NGDlStXkp+fb+8uiYiIWI0eurWcQyQsp06d4vPPP8dgMPDkk09SpUoVoqOj2bRpk727JiIiYjElLJZziITFxcWFTp06MX/+fNLT03nnnXc4duwYbdu2pU6dOvbunoiIiNiZQzx0e60yZcoQERHBuXPn+PXXX9m/f7+9uyQiImKR2706Yg0OUWEBuHTpEvPnz6djx45UrVqV6dOn8+ijj7J37157d01ERMQyevmhxRyiwtKtWzeWL19OmTJlePLJJ3n55ZcJCwuzd7dERETEQThEwuLs7MyiRYuIiIjA2dnZ3t0RERGxKg0JWc4hEpb58+fbuwsiIiI2o4TFcnZLWOLj4xkwYAAeHh7Ex8dfN3bIkCE3qVciIiLWp4TFcnZLWN555x169uyJh4cH77zzzl/GGQwGJSwiIiK3ObvNEjp69CgVK1Y0/fmvtiNHjtiriyIiItZhh1lCtWrVKnbxuejoaACuXLlCdHQ0FStWpGzZsnTt2pW0tDSzNo4fP05kZCRlypTBz8+PUaNGkZeXZxazbt06mjZtiru7O3Xr1mXu3LlF+jJr1ixq1aqFh4cHoaGhbNmypXQ3g4NMa540aRKXLl0qsv/y5ctMmjTJDj0SERGxHnusdLt161ZOnTpl2pKSkgB44okngKuvxfn6669ZvHgx69ev5+TJkzz22GOm8/Pz84mMjCQnJ4dNmzYxb9485s6dy/jx400xR48eJTIykrZt27Jz506GDRvGs88+y+rVq00xCxcuJDY2lgkTJrB9+3YaN25MREQE6enppfsMjUajsVRn2ICzszOnTp3Cz8/PbP+ZM2fw8/Mr9buFZm86ZsXeidw6eoXUsHcXRBxOOXfb/+5eY/Ayq7Rz/N2H//a5w4YNY/ny5Rw+fJiMjAwqV67MggULePzxxwE4cOAADRs2JDk5mRYtWrBq1So6derEyZMn8ff3ByAhIYHRo0dz+vRp3NzcGD16NCtWrGDPnj2m63Tr1o3z58+TmJgIQGhoKM2bN2fmzJkAFBQUUL16dQYPHsyYMWNK3H+HmCVkNBqLzRx37dqFr6+vHXp0e/pp7df89N0KLv5xtSToW7UmoQ/3pNbdzQHIunCWjQs/4Pje7eRcuUSFgOo079yNes1am7VzdNdmNi+bzx+/HcXF1Y2q9RvRechEAC5nZrD636/zx4mjXMm8iGc5H2rfE0bLx/vi7ullaiMvN4cty+ZzIHktly6co4yPL6EP9+TONhE358MQuYH0tDTenT6NTRs3cOXKFapVr8GEV6cQfOddRWKnvDqRLxcvJHbUGHr06g3Ayd9/54P3ZrNt82bOnPmDSpX96BjZmX4DnsPV1Q2AY0ePEvfaRI7+8guZmRepXNmPiI6RDBgYjYur6029X7GMtR66zc7OJjs722yfu7s77u7u1z0vJyeHTz/9lNjYWAwGAykpKeTm5hIeHm6KadCgATVq1DAlLMnJyTRq1MiUrABEREQwaNAg9u7dyz333ENycrJZG4Uxw4YNM103JSWFsWPHmo47OTkRHh5OcnJyqe7drglLhQoVTGWuO+64w+x/aH5+PpmZmQwcONCOPby9lPWtzH2P96O8f1XAyP4fkvg6fiI9XplFxaq1+Ob9N8m+lEnnoRPxLOvDwR+/Y9XsKfhMeBe/mnUBOLzte9bMnU7Lrn2p3rAJBfn5nPn9mOkaBoOB2veEEfZYHzzL+XA+/STrPpnJ2nkX6TDwf3+hV82ezKWM84T3HU55/0Cyzp/FAYqBIgBkZFwgqncPmjUPZcbs96hQwZffjv+Kt7d3kdjv1iSx56ddVP5TBfnY0SMYC4y8OP4VqtWowS+HDzP5lfFcvnyZYSNfAMDF1YXIzo/QoGEw5cqV49DBg0x+ZTzGAiPRQ4fflHsV67BWwhIXF8crr7xitm/ChAlMnDjxuuctXbqU8+fP06dPHwBSU1Nxc3OjfPnyZnH+/v6kpqaaYq5NVgqPFx67XkxGRgaXL1/m3Llz5OfnFxtz4MCBG97vteyasEyfPh2j0Ui/fv145ZVX8PHxMR1zc3OjVq1aWvH2JqrdpIXZ1y279uWn75Zz6pcDVKxai1M/76PtM4MJqN0AgHsf7sGOb74k/dhh/GrWpSA/nw0LEmj1ZH/uatPe1E7FqjVNf/bwKsfdD3Q2fe1dyZ+7H+hMyqrFpn3Hdm/lxMHd9J06F4+y3v8fF2CTexb5O+b95wP8/asw4dUppn1Vq1UrEpeelsabcZN5N+F9hsWY//LVslVrWrb6X3WyWrXq/HrsKF8s+tyUsFSrVp1q1aqbYqoEViVl2xZ2bE+x9i3JP8TYsWOJjY0123ej6grAhx9+SIcOHQgMDLRV12zOrglL795XS6NBQUG0bNkSV5U4HUZBQT6Ht35PXnY2Veo0BKBK3WAObVlP0N334l6mLIe2biAvN4dqDe4GIP3Xw2Se+wODwcCCCc+TdeEclWvUptWT/alUrVax18k8d4afU36gav27TfuO7PgR/6B6bFu1mAOb1uDq7kHtJi0Ie6w3Lm43/sYUsbUN676jRcv7GD1iGNu3baWyvz9PPNmNRx9/0hRTUFDA+BdH06tPP+rUrVeidjMzL+J9zS9uf/bb8V9J/mEjbR8M/8sYcUzWqrCUZPjnz3799Ve+/fZbvvzyS9O+gIAAcnJyOH/+vFmVJS0tjYCAAFPMn2fzFM4iujbmzzOL0tLS8Pb2xtPTE2dnZ5ydnYuNKWyjpBziGZb777/f9OcrV66Qk5Njdry4MqvYxh+/HWXR5GHk5ebg6u5JZMx4U4Wk4/MvsXL2FP49+AmcnJ1xcXOn0+AJ/z+EBBdOXy0Rbv7vp7TuNgDvSgFsT1zCF2+Monfch6ZqCcCqhDiO7EgmLyeboCYtCO/3v/J2xulTnDy0F2dXNzoNHs/lixl898lMLmdl0C5q5E38NESK9/uJ3/hi0ef07NWHvs8OYN/ePbz1xhRcXd3o9EgX4GoVxtnFmW49e5Wozd+O/8rCz+YzLHZUkWP9enXnwP595OTk8OjjTzIwWmtT/ePYcd24jz76CD8/PyIjI037QkJCcHV1Zc2aNXTt2hWAgwcPcvz4cdPIRlhYGJMnTyY9Pd00KSYpKQlvb2+Cg4NNMStXrjS7XlJSkqkNNzc3QkJCWLNmDV26dAGuJvNr1qwhJiamVPfhENOaL126RExMDH5+fnh5eVGhQgWz7Xqys7PJyMgw23Jzsq97jvy1ClWq0eOV2Tz1cjx3t+1E0gdvceb3XwFI/nIe2ZczeXTU63Qb/y73tOvKytmT+eO3owAYCwoAaN6pO/Watca/Vj0eihqBAQOHt35vdp023Z+j+8SZdB4ykQvpJ9nw2b9Nx4xGIxgMtB8whoDaDQhqfC9tug1g/w/fkqf/t+IACgqMNGgYTPTQ4TRoGMxjjz9Jl65P8MXizwHYv28vn8//hImvxpXoN+v0tDQGDxpA+EMRZlWaQlPefJtPF37Ba6+/xQ8b1vPJ3P9Y/Z7k1lRQUMBHH31E7969cXH5X43Cx8eHqKgoYmNj+e6770hJSaFv376EhYXRosXVxwPatWtHcHAwvXr1YteuXaxevZpx48YRHR1tqvIMHDiQI0eO8MILL3DgwAFmz57NokWLGD78f7+ExsbG8v777zNv3jz279/PoEGDyMrKom/fvqW6F4dIWEaNGsXatWuZM2cO7u7ufPDBB7zyyisEBgby8ccfX/fcuLg4fHx8zLZvPplzk3p+63F2caW8f1X8a9Xjvif6UalGEDuTlnI+/SS71izjoX6x1Ai+h8o16tCiy9P4B9Vj19qr0/W8yl+d0eUb+L+psy6ubnj7BXDxrPl8ey8fX3yr1KD2PWE80Hsou79bTtb5M6ZjZStUxL3M/2YN+QbWAKORi2f/sPVHIHJDlSpXIqh2HbN9QUG1SU09BcCOlG2cPXuGThEPEHrPXYTecxenTp5k+rSpdG7/oNl5p9PTGfhsb+5u3ISXJhS/7lRAQBVq16lL+46RxAyL5b2EWaVe7kHsyx7rsAB8++23HD9+nH79+hU59s4779CpUye6du1KmzZtCAgIMBs2cnZ2Zvny5Tg7OxMWFsbTTz/NM888Y7Y+WlBQECtWrCApKYnGjRszbdo0PvjgAyIi/jej86mnnuKtt95i/PjxNGnShJ07d5KYmFjkQdwbcYghoa+//pqPP/6Yf/3rX/Tt25fWrVtTt25datasyfz58+nZs+dfnlvcA0gfbT9l6y7fNowFRvLzcsn7/2l0BoN5jmswOMP/z97xq1UPZxdXzqWeoOodV6d25uflkfFHGuUqXucv5v+fn5+XC0CVendyeNv35Fy5jJuHJwDnUk9gMDhRzreSVe9P5O9o3KQpvx47Zrbv11+PUaXK1QcaO3Z+mHtbmE8YGDyoPx07PUznR/63MFd6WhoDn+1Ng4Z3MuHVKTg53fh3yIKCAvLy8q5WNPV2+38Me71LqF27dn85w9LDw4NZs2Yxa9asvzy/Zs2aRYZ8/uxf//oXO3bsuG5MTExMqYeA/swhEpazZ89Su3Zt4OrzKmfPngWgVatWDBo06LrnFvcAkqvbWdt09Bb3w+L/UOvu5pSrWJmcy5c5+ON3nDj4E11GTKZCler4+AWyZt4MWj/VH4+y3hzZvonj+7bz8NCr2ba7pxeN2kayeeknlPOtjHdFP1JWLQGgXvOrsyGO7trCpYxz+AfVx83DgzO//8rGRR9Qpd6dpplA9Vu0Zcuy+SR9OI0WXXpxJTODjYs+ILh1Oz10Kw6hR6/e9HumB/95/988FNGevbt389WSxbw04ep00/LlK1C+vPlwtouLCxUrVqJWUBBwNVl5LuoZqlQJZNiIFzh37n//blWqVBmAVSu+xsXFhbr17sDVzY39e/cwK/4d2kV00Dos/zB696HlHCJhqV27NkePHqVGjRo0aNCARYsWce+99/L1118XmSMutnPp4nlWv/8mly6cxc2zDJWqB9FlxGRq3hkCwCPDX+OHJR+ybMYEcq9cprx/IO2eHUlQ43tNbbR6sj9OTs6sfn8q+Tk5+NeuT9cX3sDDqxwALm5u7F2/ig2f/Zv8vFzK+VamTsh9NI98ytSGm4cnj46KY92ns/l80mA8vMpR7942tHysz039PET+yp13NeKtd+KZOeMdPvj3bAKrVmPEC2PoENn5xif/v80/buK348f57fhxOj70L7Nj237aD1wtyc/7zwcc//UYRiNUCazCk916mhafE7mdOMTS/O+88w7Ozs4MGTKEb7/9ls6dO2M0GsnNzeXtt99m6NChpWpPS/OLFE9L84sUdTOW5q83KtEq7Rx+s/2Ng25RDlFhufZp4vDwcA4cOEBKSgp169bl7rvvvs6ZIiIijk9DQpZziITlz2rWrEnNmjVvHCgiIiK3BYdIWOLj44vdbzAY8PDwoG7durRp0wZnPREvIiL/QPaaJXQrcYiE5Z133uH06dNcunTJtFDcuXPnKFOmDGXLliU9PZ3atWvz3XffUb169Ru0JiIi4liUr1jOIRaOmzJlCs2bN+fw4cOcOXOGM2fOcOjQIUJDQ5kxYwbHjx8nICDA7FkXERERuX04RIVl3LhxfPHFF9Sp87+VI+vWrctbb71F165dOXLkCFOnTjW970BEROSfxMlJJRZLOUTCcurUKfLy8orsz8vLIzX16gv1AgMDuXjx4s3umoiIiMU0JGQ5hxgSatu2Lc8995zZ0r47duxg0KBBPPDAAwDs3r2boP9fIVJERERuLw6RsHz44Yf4+voSEhJiWmq/WbNm+Pr68uGHHwJQtmxZpk2bZueeioiIlJ69Xn54K3GIIaGAgACSkpI4cOAAhw4dAqB+/frUr1/fFNO2bVt7dU9ERMQit3muYRUOkbAUql27NgaDgTp16uDi4lBdExER+dtu9+qINTjEkNClS5eIioqiTJky3HnnnRw/fhyAwYMH8/rrr9u5dyIiImJvDpGwjB07ll27drFu3To8PDxM+8PDw1m4cKEdeyYiImI5PcNiOYcYd1m6dCkLFy6kRYsWZv9D7rzzTn755Rc79kxERMRyt3muYRUOUWE5ffo0fn5+RfZnZWXd9hmliIiIOEjC0qxZM1asWGH6ujBJ+eCDDwgLC7NXt0RERKxCQ0KWc4ghoSlTptChQwf27dtHXl4eM2bMYN++fWzatIn169fbu3siIiIWuc1zDatwiApLq1at2LlzJ3l5eTRq1IhvvvkGPz8/kpOTCQkJsXf3RERExM4cosICUKdOHd5//317d0NERMTqbvfhHGuwa8Li5OR0w/+JBoOh2BcjioiI/FMoX7GcXROWr7766i+PJScnEx8fT0FBwU3skYiIiDgiuyYsjzzySJF9Bw8eZMyYMXz99df07NmTSZMm2aFnIiIi1qMhIcs5xEO3ACdPnqR///40atSIvLw8du7cybx586hZs6a9uyYiImIRg8E62+3M7gnLhQsXGD16NHXr1mXv3r2sWbOGr7/+mrvuusveXRMREbEKrcNiObsOCU2dOpU33niDgIAAPvvss2KHiERERETsmrCMGTMGT09P6taty7x585g3b16xcV9++eVN7pmIiIj13ObFEauwa8LyzDPP3PYlLhERufXpZ53l7JqwzJ07156XFxERkX8Ih1npVkRE5FalAovl7D5LSERE5FZnr1lCv//+O08//TQVK1bE09OTRo0asW3bNtNxo9HI+PHjqVKlCp6enoSHh3P48GGzNs6ePUvPnj3x9vamfPnyREVFkZmZaRbz008/0bp1azw8PKhevTpTp04t0pfFixfToEEDPDw8aNSoEStXrizVvShhERERuQWdO3eO++67D1dXV1atWsW+ffuYNm0aFSpUMMVMnTqV+Ph4EhIS2Lx5M15eXkRERHDlyhVTTM+ePdm7dy9JSUksX76cDRs2MGDAANPxjIwM2rVrR82aNUlJSeHNN99k4sSJvPfee6aYTZs20b17d6KiotixYwddunShS5cu7Nmzp8T3YzAajUYLPxOHM3vTMXt3QcQh9QqpYe8uiDiccu62/9291VvfW6WdjSNblzh2zJgx/PDDD3z/ffHXNhqNBAYGMmLECEaOHAlcXRvN39+fuXPn0q1bN/bv309wcDBbt26lWbNmACQmJtKxY0dOnDhBYGAgc+bM4aWXXiI1NRU3NzfTtZcuXcqBAwcAeOqpp8jKymL58uWm67do0YImTZqQkJBQovtRhUVERMTGrDUklJ2dTUZGhtmWnZ1d7DWXLVtGs2bNeOKJJ/Dz8+Oee+7h/fffNx0/evQoqamphIeHm/b5+PgQGhpKcnIycPW9fuXLlzclKwDh4eE4OTmxefNmU0ybNm1MyQpAREQEBw8e5Ny5c6aYa69TGFN4nZJQwiIiIvIPERcXh4+Pj9kWFxdXbOyRI0eYM2cO9erVY/Xq1QwaNIghQ4aY1jxLTU0FwN/f3+w8f39/07HU1FT8/PzMjru4uODr62sWU1wb117jr2IKj5eEZgmJiIjYmLXWYRk7diyxsbFm+9zd3YuNLSgooFmzZkyZMgWAe+65hz179pCQkEDv3r2t0p+bSRUWERERG7PWyw/d3d3x9vY22/4qYalSpQrBwcFm+xo2bMjx48cBCAgIACAtLc0sJi0tzXQsICCA9PR0s+N5eXmcPXvWLKa4Nq69xl/FFB4vCSUsIiIiNmaPac333XcfBw8eNNt36NAhatasCUBQUBABAQGsWbPGdDwjI4PNmzcTFhYGQFhYGOfPnyclJcUUs3btWgoKCggNDTXFbNiwgdzcXFNMUlIS9evXN81ICgsLM7tOYUzhdUpCCYuIiMgtaPjw4fz4449MmTKFn3/+mQULFvDee+8RHR0NXE2ihg0bxmuvvcayZcvYvXs3zzzzDIGBgXTp0gW4WpFp3749/fv3Z8uWLfzwww/ExMTQrVs3AgMDAejRowdubm5ERUWxd+9eFi5cyIwZM8yGroYOHUpiYiLTpk3jwIEDTJw4kW3bthETE1Pi+9EzLCIiIjZmj5VumzdvzldffcXYsWOZNGkSQUFBTJ8+nZ49e5piXnjhBbKyshgwYADnz5+nVatWJCYm4uHhYYqZP38+MTExPPjggzg5OdG1a1fi4+NNx318fPjmm2+Ijo4mJCSESpUqMX78eLO1Wlq2bMmCBQsYN24cL774IvXq1WPp0qXcddddJb4frcMichvROiwiRd2MdVgeiC/59N3rWTuk5EMotxoNCYmIiIjD05CQiIiIjenlh5ZTwiIiImJjTspYLKYhIREREXF4qrCIiIjYmAosllPCIiIiYmPWWpr/dqaERURExMaclK9YTM+wiIiIiMNThUVERMTGNCRkOSUsIiIiNqZ8xXIaEhIRERGHpwqLiIiIjRlQicVSSlhERERsTLOELKchIREREXF4qrCIiIjYmGYJWU4Ji4iIiI0pX7GchoRERETE4anCIiIiYmNOKrFYTAmLiIiIjSlfsZwSFhERERvTQ7eW0zMsIiIi4vBUYREREbExFVgsp4RFRETExvTQreU0JCQiIiIOTxUWERERG1N9xXJKWERERGxMs4QspyEhERERcXiqsIiIiNiYkwosFitRwrJs2bISN/jwww//7c6IiIjcijQkZLkSJSxdunQpUWMGg4H8/HxL+iMiIiJSRIkSloKCAlv3Q0RE5JalAovl9AyLiIiIjWlIyHJ/a5ZQVlYWK1euJCEhgfj4eLNNREREzDkZrLOVxsSJEzEYDGZbgwYNTMevXLlCdHQ0FStWpGzZsnTt2pW0tDSzNo4fP05kZCRlypTBz8+PUaNGkZeXZxazbt06mjZtiru7O3Xr1mXu3LlF+jJr1ixq1aqFh4cHoaGhbNmypXQ3w9+osOzYsYOOHTty6dIlsrKy8PX15Y8//jDdzJAhQ0rdCREREbG+O++8k2+//db0tYvL/37sDx8+nBUrVrB48WJ8fHyIiYnhscce44cffgAgPz+fyMhIAgIC2LRpE6dOneKZZ57B1dWVKVOmAHD06FEiIyMZOHAg8+fPZ82aNTz77LNUqVKFiIgIABYuXEhsbCwJCQmEhoYyffp0IiIiOHjwIH5+fiW+F4PRaDSW5ub/9a9/cccdd5CQkICPjw+7du3C1dWVp59+mqFDh/LYY4+VpjmbmL3pmL27IOKQeoXUsHcXRBxOOXfbL0nW9/PdVmnno26NShw7ceJEli5dys6dO4scu3DhApUrV2bBggU8/vjjABw4cICGDRuSnJxMixYtWLVqFZ06deLkyZP4+/sDkJCQwOjRozl9+jRubm6MHj2aFStWsGfPHlPb3bp14/z58yQmJgIQGhpK8+bNmTlzJnD1udjq1aszePBgxowZU+L7KfX/pZ07dzJixAicnJxwdnYmOzub6tWrM3XqVF588cXSNiciInLLM1hpy87OJiMjw2zLzs7+y+sePnyYwMBAateuTc+ePTl+/DgAKSkp5ObmEh4ebopt0KABNWrUIDk5GYDk5GQaNWpkSlYAIiIiyMjIYO/evaaYa9sojClsIycnh5SUFLMYJycnwsPDTTElVeqExdXVFSenq6f5+fmZbt7Hx4fffvuttM2JiIhICcXFxeHj42O2xcXFFRsbGhrK3LlzSUxMZM6cORw9epTWrVtz8eJFUlNTcXNzo3z58mbn+Pv7k5qaCkBqaqpZslJ4vPDY9WIyMjK4fPkyf/zxB/n5+cXGFLZRUqV+huWee+5h69at1KtXj/vvv5/x48fzxx9/8Mknn3DXXXeVtjkREZFbnpOVZgmNHTuW2NhYs33u7u7Fxnbo0MH057vvvpvQ0FBq1qzJokWL8PT0tEp/bqZSV1imTJlClSpVAJg8eTIVKlRg0KBBnD59mvfee8/qHRQREfmnMxiss7m7u+Pt7W22/VXC8mfly5fnjjvu4OeffyYgIICcnBzOnz9vFpOWlkZAQAAAAQEBRWYNFX59oxhvb288PT2pVKkSzs7OxcYUtlFSpU5YmjVrRtu2bYGrQ0KJiYlkZGSQkpJC48aNS9uciIiI3ASZmZn88ssvVKlShZCQEFxdXVmzZo3p+MGDBzl+/DhhYWEAhIWFsXv3btLT000xSUlJeHt7ExwcbIq5to3CmMI23NzcCAkJMYspKChgzZo1ppiS0sJxIiIiNmaPheNGjhxJ586dqVmzJidPnmTChAk4OzvTvXt3fHx8iIqKIjY2Fl9fX7y9vRk8eDBhYWG0aNECgHbt2hEcHEyvXr2YOnUqqampjBs3jujoaFNVZ+DAgcycOZMXXniBfv36sXbtWhYtWsSKFStM/YiNjaV37940a9aMe++9l+nTp5OVlUXfvn1LdT+lTliCgoKu+8EfOXKktE2KiIjc0uyx0O2JEyfo3r07Z86coXLlyrRq1Yoff/yRypUrA/DOO+/g5ORE165dyc7OJiIigtmzZ5vOd3Z2Zvny5QwaNIiwsDC8vLzo3bs3kyZNMsUEBQWxYsUKhg8fzowZM6hWrRoffPCBaQ0WgKeeeorTp08zfvx4UlNTadKkCYmJiUUexL2RUq/DMmPGDLOvc3Nz2bFjB4mJiYwaNapUc6ptReuwiBRP67CIFHUz1mF5bsleq7Tz78fvtEo7/0SlrrAMHTq02P2zZs1i27ZtFndIRETkVmOtWUK3M6ullR06dOCLL76wVnMiIiK3DGvNErqdWe2h2yVLluDr62ut5kRERG4Zeluz5f7WwnHXfvBGo5HU1FROnz5t9rCOiIiIiLWUOmF55JFHzBIWJycnKleuzL/+9S+z11bbU797a9m7CyIOqULzGHt3QcThXN4x0+bXsP1jvbe+UicsEydOtEE3REREbl0aErJcqZM+Z2dns1XvCp05cwZnZ2erdEpERETkWqWusPzVsi3Z2dm4ublZ3CEREZFbjZMKLBYrccISHx8PXC1rffDBB5QtW9Z0LD8/nw0bNjjMMywiIiKORAmL5UqcsLzzzjvA1QpLQkKC2fCPm5sbtWrVIiEhwfo9FBERkdteiROWo0ePAtC2bVu+/PJLKlSoYLNOiYiI3Er00K3lSv0My3fffWeLfoiIiNyyNCRkuVLPEuratStvvPFGkf1Tp07liSeesEqnRERERK5V6oRlw4YNdOzYscj+Dh06sGHDBqt0SkRE5FaidwlZrtRDQpmZmcVOX3Z1dSUjI8MqnRIREbmV6G3Nlit1haVRo0YsXLiwyP7PP/+c4OBgq3RKRETkVuJkpe12VuoKy8svv8xjjz3GL7/8wgMPPADAmjVrWLBgAUuWLLF6B0VERERKnbB07tyZpUuXMmXKFJYsWYKnpyeNGzdm7dq1+Pr62qKPIiIi/2gaEbJcqRMWgMjISCIjIwHIyMjgs88+Y+TIkaSkpJCfn2/VDoqIiPzT6RkWy/3tIbENGzbQu3dvAgMDmTZtGg888AA//vijNfsmIiIiApSywpKamsrcuXP58MMPycjI4MknnyQ7O5ulS5fqgVsREZG/oAKL5UpcYencuTP169fnp59+Yvr06Zw8eZJ3333Xln0TERG5JTgZrLPdzkpcYVm1ahVDhgxh0KBB1KtXz5Z9EhERETFT4grLxo0buXjxIiEhIYSGhjJz5kz++OMPW/ZNRETkluBkMFhlu52VOGFp0aIF77//PqdOneK5557j888/JzAwkIKCApKSkrh48aIt+ykiIvKPpaX5LVfqWUJeXl7069ePjRs3snv3bkaMGMHrr7+On58fDz/8sC36KCIiIrc5i1b6rV+/PlOnTuXEiRN89tln1uqTiIjILUUP3Vruby0c92fOzs506dKFLl26WKM5ERGRW4qB2zzbsAKrJCwiIiLy12736og13O4vfxQREZF/AFVYREREbEwVFsspYREREbExw+0+J9kKNCQkIiJyG3j99dcxGAwMGzbMtO/KlStER0dTsWJFypYtS9euXUlLSzM77/jx40RGRlKmTBn8/PwYNWoUeXl5ZjHr1q2jadOmuLu7U7duXebOnVvk+rNmzaJWrVp4eHgQGhrKli1bStV/JSwiIiI2Zu9pzVu3buXf//43d999t9n+4cOH8/XXX7N48WLWr1/PyZMneeyxx0zH8/PziYyMJCcnh02bNjFv3jzmzp3L+PHjTTFHjx4lMjKStm3bsnPnToYNG8azzz7L6tWrTTELFy4kNjaWCRMmsH37dho3bkxERATp6eklvgeD0Wg0/v2PwDFdybtxjMjtqELzGHt3QcThXN4x0+bXeHvDEau0E9umdqnPyczMpGnTpsyePZvXXnuNJk2aMH36dC5cuEDlypVZsGABjz/+OAAHDhygYcOGJCcn06JFC1atWkWnTp04efIk/v7+ACQkJDB69GhOnz6Nm5sbo0ePZsWKFezZs8d0zW7dunH+/HkSExMBCA0NpXnz5sycefWzLigooHr16gwePJgxY8aU6D5UYREREfmHyM7OJiMjw2zLzs6+7jnR0dFERkYSHh5utj8lJYXc3Fyz/Q0aNKBGjRokJycDkJycTKNGjUzJCkBERAQZGRns3bvXFPPntiMiIkxt5OTkkJKSYhbj5OREeHi4KaYklLCIiIjYmLVefhgXF4ePj4/ZFhcX95fX/fzzz9m+fXuxMampqbi5uVG+fHmz/f7+/qSmpppirk1WCo8XHrteTEZGBpcvX+aPP/4gPz+/2JjCNkpCs4RERERszFrTmseOHUtsbKzZPnd392Jjf/vtN4YOHUpSUhIeHh7W6YAdqcIiIiLyD+Hu7o63t7fZ9lcJS0pKCunp6TRt2hQXFxdcXFxYv3498fHxuLi44O/vT05ODufPnzc7Ly0tjYCAAAACAgKKzBoq/PpGMd7e3nh6elKpUiWcnZ2LjSlsoySUsIiIiNiYwWCdrTQefPBBdu/ezc6dO01bs2bN6Nmzp+nPrq6urFmzxnTOwYMHOX78OGFhYQCEhYWxe/dus9k8SUlJeHt7ExwcbIq5to3CmMI23NzcCAkJMYspKChgzZo1ppiS0JCQiIiIjTnZ4eWH5cqV46677jLb5+XlRcWKFU37o6KiiI2NxdfXF29vbwYPHkxYWBgtWrQAoF27dgQHB9OrVy+mTp1Kamoq48aNIzo62lTZGThwIDNnzuSFF16gX79+rF27lkWLFrFixQrTdWNjY+nduzfNmjXj3nvvZfr06WRlZdG3b98S348SFhERERtz1IVu33nnHZycnOjatSvZ2dlEREQwe/Zs03FnZ2eWL1/OoEGDCAsLw8vLi969ezNp0iRTTFBQECtWrGD48OHMmDGDatWq8cEHHxAREWGKeeqppzh9+jTjx48nNTWVJk2akJiYWORB3OvROiwitxGtwyJS1M1Yh2X2pmNWaef5lrWs0s4/kSosIiIiNqaXH1pOCYuIiIiNOTnqmNA/iGYJiYiIiMNThUVERMTGVGCxnBIWERERG9OQkOU0JCQiIiIOTxUWERERG1OBxXJKWERERGxMwxmW02coIiIiDk8VFhERERszaEzIYkpYREREbEzpiuWUsIiIiNiYpjVbTs+wiIiIiMNThUVERMTGVF+xnBIWERERG9OIkOU0JCQiIiIOTxUWERERG9O0ZsspYREREbExDWdYTp+hiIiIODxVWERERGxMQ0KWU8IiIiJiY0pXLKchIREREXF4qrCIiIjYmIaELKeERURExMY0nGE5JSwiIiI2pgqL5ZT0iYiIiMNThUVERMTGVF+xnBIWERERG9OIkOU0JCQiIiIOTxUWERERG3PSoJDFHKbC8v333/P0008TFhbG77//DsAnn3zCxo0b7dwzERERyxgM1tluZw6RsHzxxRdERETg6enJjh07yM7OBuDChQtMmTLFzr0TERH555kzZw5333033t7eeHt7ExYWxqpVq0zHr1y5QnR0NBUrVqRs2bJ07dqVtLQ0szaOHz9OZGQkZcqUwc/Pj1GjRpGXl2cWs27dOpo2bYq7uzt169Zl7ty5Rfoya9YsatWqhYeHB6GhoWzZsqXU9+MQCctrr71GQkIC77//Pq6urqb99913H9u3b7djz0RERCxnsNJ/pVGtWjVef/11UlJS2LZtGw888ACPPPIIe/fuBWD48OF8/fXXLF68mPXr13Py5Ekee+wx0/n5+flERkaSk5PDpk2bmDdvHnPnzmX8+PGmmKNHjxIZGUnbtm3ZuXMnw4YN49lnn2X16tWmmIULFxIbG8uECRPYvn07jRs3JiIigvT09NJ9hkaj0ViqM2ygTJky7Nu3j1q1alGuXDl27dpF7dq1OXLkCMHBwVy5cqVU7V3Ju3GMyO2oQvMYe3dBxOFc3jHT5tdYubd0P5z/Ssc7/Sw639fXlzfffJPHH3+cypUrs2DBAh5//HEADhw4QMOGDUlOTqZFixasWrWKTp06cfLkSfz9/QFISEhg9OjRnD59Gjc3N0aPHs2KFSvYs2eP6RrdunXj/PnzJCYmAhAaGkrz5s2ZOfPq51xQUED16tUZPHgwY8aMKXHfHaLCEhAQwM8//1xk/8aNG6ldu7YdeiQiInLryM/P5/PPPycrK4uwsDBSUlLIzc0lPDzcFNOgQQNq1KhBcnIyAMnJyTRq1MiUrABERESQkZFhqtIkJyebtVEYU9hGTk4OKSkpZjFOTk6Eh4ebYkrKIWYJ9e/fn6FDh/Kf//wHg8HAyZMnSU5OZuTIkbz88sv27p6IiIhFrDVLKDs72/ScZyF3d3fc3d2Ljd+9ezdhYWFcuXKFsmXL8tVXXxEcHMzOnTtxc3OjfPnyZvH+/v6kpqYCkJqaapasFB4vPHa9mIyMDC5fvsy5c+fIz88vNubAgQOluneHSFjGjBlDQUEBDz74IJcuXaJNmza4u7szcuRIBg8ebO/uiYiIWMRaM3zi4uJ45ZVXzPZNmDCBiRMnFhtfv359du7cyYULF1iyZAm9e/dm/fr11unMTeYQCYvBYOCll15i1KhR/Pzzz2RmZhIcHEzZsmXt3TURERGLWSthGTt2LLGxsWb7/qq6AuDm5kbdunUBCAkJYevWrcyYMYOnnnqKnJwczp8/b1ZlSUtLIyAgALj6uMafZ/MUziK6NubPM4vS0tLw9vbG09MTZ2dnnJ2di40pbKOkHOIZlk8//ZRLly7h5uZGcHAw9957r5IVERGRP3F3dzdNUy7crpew/FlBQQHZ2dmEhITg6urKmjVrTMcOHjzI8ePHCQsLAyAsLIzdu3ebzeZJSkrC29ub4OBgU8y1bRTGFLbh5uZGSEiIWUxBQQFr1qwxxZSUQyQsw4cPx8/Pjx49erBy5Ury8/Pt3SURERGrsce05rFjx7JhwwaOHTvG7t27GTt2LOvWraNnz574+PgQFRVFbGws3333HSkpKfTt25ewsDBatGgBQLt27QgODqZXr17s2rWL1atXM27cOKKjo01J0sCBAzly5AgvvPACBw4cYPbs2SxatIjhw4eb+hEbG8v777/PvHnz2L9/P4MGDSIrK4u+ffuW6n4cYkjo1KlTJCYm8tlnn/Hkk09SpkwZnnjiCXr27EnLli3t3T0RERGLONlhldr09HSeeeYZTp06hY+PD3fffTerV6/moYceAuCdd97BycmJrl27kp2dTUREBLNnzzad7+zszPLlyxk0aBBhYWF4eXnRu3dvJk2aZIoJCgpixYoVDB8+nBkzZlCtWjU++OADIiIiTDFPPfUUp0+fZvz48aSmptKkSRMSExOLPIh7Iw6xDsu1Ll26xFdffcWCBQv49ttvqVatGr/88kup2tA6LCLF0zosIkXdjHVY1hz4wyrtPNigklXa+SdyiArLtcqUKUNERATnzp3j119/Zf/+/fbukoiIiEVKO5wjRTnEMyxwtbIyf/58OnbsSNWqVZk+fTqPPvqoaXEaERGRfyq9/NByDlFh6datG8uXL6dMmTI8+eSTvPzyy6V+elhERERuXQ6RsDg7O7No0SIiIiJwdna2d3dERESsSkNClnOIhGX+/Pn27oKIiIjN2GOW0K3GbglLfHw8AwYMwMPDg/j4+OvGDhky5Cb1SkRERByR3aY1BwUFsW3bNipWrEhQUNBfxhkMBo4cOVKqtjWt2XY+fP894qdPo+fTz/DC2Je4cP48s2e9S/KmjaSeOkWFCr60fTCc6MFDKVeunOm8zT8mM+vdGRw+dBBPzzJ0fqQLg4cOx8Xlas48Z9a7JMwuOrXQw9OTzdt23qzbu+VpWvPfV7aMOxOe78TDDzSmcoWy7Dp4gpFTl5Cy77gp5uVBkfR9tCXly3mSvOsIQ6Ys5Jfjp83aad/qTl4c0IG76gVyJSePjSmHeTL2/SLX8/XxYsvCMVT1r0BA61FcyLxcJCascW2++WAoe385RYtur1v/pm8TN2Na8/eHzlmlndZ3VLBKO/9EdquwHD16tNg/i+Pas/snliz+nDvuqG/al346ndPp6cSOHE2dOnU5efJ3Xps0kdPp6UybfrVydvDAAaIH9ufZAQN5bcobpKen8dqkCRQUFDBi1GgAevfpxxNPdjO7Xv+oPtx1V6Obdn8i1zNnfA+C6wbSb9w8Tp2+QPeO97IiYTBNu77GydMXGNEnnOe730//8Z9w7PczjH++E1/Piuaerq+RnXP1t6guDzZh1svdmTDza9ZtOYSLixN31qlS7PUSJvRg9+GTVPUv/geUT1lPPni1F99tOYRfxXLFxojjuN1n+FiDQ0xrnjRpEpcuXSqy//Lly2Yr6on9XMrKYuzoUUx45TW8fXxM++vVu4O3Z7zLv9o+QPUaNQhtEcbgocNYv24teXlX/5FenbiSO+6oz8DnY6hRsybNmt/LsNhRLPxsPllZmQCU8fKiUuXKpu3MmTMc+eVnHu36uF3uV+RaHu6udHmwCS9NX8oP23/hyG9/MPnfK/nlt9P0f6I1ANE92vLG+6tZvm43ew6f5NmXP6ZKZR8ebtsYAGdnJ94a1ZUXpy/lgyUb+fl4OgeOpPJF0o4i1+v/RCt8ypVh+sdrihwr9O64bixM3Mbmn/QL3z+BwUrb7cwhEpZXXnmFzMzMIvsvXbpU5DXaYh9TXptEmzb30yLsxq9KyLyYSdmyZU3DPTk5Obj96eVcHh4eZGdns+8v1tn58ovF1KxVi6YhzSzvvIiFXJydcHFx5kpOrtn+K9m5tLynDrWqVqRKZR/Wbj5gOpaReYWte44RenctAO5pUJ2q/hUoKDCS/NlojnwzmaUzBxH8pwpLg9oBjO3fgWdf/piCguJH7Hs93IKgqhWZ/O9V1r1REQfmEAmL0WjEUEy9bNeuXfj6+l733OzsbDIyMsy27OxsW3X1trRq5Qr279/HkOEjbhh77txZ3kuYTdcnnjLta3lfK3bt3MGqFcvJz88nLS2Nf8+ZBcAfp08XaSM7O5uVy7/m0cdUXRHHkHkpmx93HWFs/w5UqeyDk5OBbh2bE3p3EAGVvAmo5A1A+tmLZueln7mIf8Wrx4KqXV1SfdzAjrzxwWq6Dk3gfMZlVr8/lAreZQBwc3VhXlwfXpy+lN9Si3/moU6Nyrw65GH6vvQx+fkFtrplsTIng8Eq2+3MrglLhQoV8PX1xWAwcMcdd+Dr62vafHx8eOihh3jyySev20ZcXBw+Pj5m25tvxN2kO7j1pZ46xdTXJxP3xps3fIV5ZmYmMYOeo3adOgx8/n8Pd7a8rxXDR7zAa5Mm0PyeRjwcGUGr1vcDYHAq+ldw7bdJXLqUxcOPPGrdmxGxQL9xH2MwwJFvJnNh83Siu9/PosRtf1kF+bPCHzZvfLCapWt2smP/bwyY8ClGjDz20D0AvDrkYQ4eTePzlVuLb8PJwLwpfXgtYSU/H0+3zo3JTaEhIcvZdR2W6dOnYzQa6devH6+88go+1zwb4ebmRq1atW644u3YsWOJjY0122d0vv4PVim5ffv2cvbMGbo98ZhpX35+PinbtvL5Z/PZumM3zs7OZGVl8vxzz+Ll5cU78bNwdXU1a+eZPn3p1bsPp0+n4+3tw8nffyd++jSqVatW5JpffrGY1vf/i4qVbt+XfInjOXriD9o9O4MyHm54l/Ug9Y8MPnm9L0d//4PUPzIA8PMtZ/ozgF/Fcvx08AQAp/64AMCBI6dMx3Ny8zh24gzVA65Wku9vfgd31Q3k0a1NAEyV5xPfvc4bH67m3U+/I+TOmjSuX413Rj8BXE1inJycuLh1Bp2en8X6rYds+0GI2IldE5bevXsDV6c4t2zZssgPuZJwd3cv8pu/pjVbT2iLFixZ+rXZvgkvjaVW7dr0jeqPs7MzmZmZDBoQhZubGzNmzvnLSozBYMDP7+rrxFetXE5AQBUaBt9pFnPixG9s3bKZGTPn2OaGRCx06UoOl67kUL6cJ+EtG/LS9P9y7PcznDp9gbah9fnp0O8AlPPyoPldtXh/8UYAduz/jSvZudSr5c+mnVeXanBxcaJGoC/HT50FoPvID/B0/9+/gyF31uS9V54mPGo6R347TUbWFUIen2zWnwFPtuZfze+gx6gPOfb7mZvxEcjfcbuXR6zAbglLRkYG3t5Xx3bvueceLl++zOXLRdcZAExxcvN5eZWlXr07zPZ5lilDeZ/y1Kt3B5mZmQzs348rVy4z5fU3ycrMJOv/H6Cu4OtretXC3P98wH2tWmNwcmJN0jf854P3efPt6UVexbD0yy+oVLkyrVq3uTk3KFJC4WENMRjg0LF06lSvzJThXTh0NI2PlyUDMGvBd4x+tj0/Hz/Nsd/PMOH5SE6dvsCy73YBcDHrCh8s2cjLAztyIvUcx0+dZXjvcAC+TNoOXK3iXKti+bIAHDiSalqHZd8vp8xiTp/N5EpOXpH94li0NL/l7JawVKhQgVOnTuHn50f58uWLfei28GHc/Px8O/RQSmL/vr3s/unqP8idOjxkdmzlN2uoWvXqkM/G7zfwwXsJ5OTkcEf9BsyYOcv0HEuhgoIClv33Kx7p8pjeKSUOx6esB5MGP0xV//KcvXCJ/67ZyYRZX5OXd/XB12lzv6WMpzszx3WnfDlPNu38hYejZ5vWYAEYO/0r8vIL+PC1Z/B0d2Xrnl/pMCCe8xeL/2VNRP7Hbivdrl+/nvvuuw8XFxfWr19/3dj777//usf/TENCIsXTSrciRd2MlW63HLlglXbure1z46BblN0qLNcmIaVNSERERP5JNCBkOYdYhyUxMZGNGzeavp41axZNmjShR48enDtnnfcviIiIyD+XQyQso0aNIiPj6lTA3bt3ExsbS8eOHTl69GiRKcsiIiL/OFqIxWJ2ndZc6OjRowQHBwPwxRdf0LlzZ6ZMmcL27dvp2LGjnXsnIiJiGc0SspxDVFjc3NxMLz/89ttvadeuHQC+vr6myouIiMg/lcFgne125hAVllatWhEbG8t9993Hli1bWLhwIQCHDh0qdiVUERERub04RIVl5syZuLi4sGTJEubMmUPVqlUBWLVqFe3bt7dz70RERCyjR1gsZ7d1WGxJ67CIFE/rsIgUdTPWYdn+q3Ueb2ha8/Zd+d0hhoTg6gv1li5dyv79+wG48847efjhh7XiqYiIiDhGwvLzzz/TsWNHfv/9d+rXrw9AXFwc1atXZ8WKFdSpU8fOPRQREfn7NEvIcg7xDMuQIUOoU6cOv/32G9u3b2f79u0cP36coKAghgwZYu/uiYiIWESzhCznEBWW9evX8+OPP+Lr62vaV7FiRV5//XXuu+8+O/ZMREREHIFDJCzu7u5cvHixyP7MzEzc3Nzs0CMRERHruc2LI1bhEENCnTp1YsCAAWzevBmj0YjRaOTHH39k4MCBPPzww/bunoiIiGU0r9liDpGwxMfHU7duXVq2bImHhwceHh7cd9991K1blxkzZti7eyIiImJndk1YCgoKeOONN4iMjOT333+nS5cuLF68mCVLlnDw4EG++uorfHx87NlFERERixms9F9pxMXF0bx5c8qVK4efnx9dunTh4MGDZjFXrlwhOjqaihUrUrZsWbp27UpaWppZzPHjx4mMjKRMmTL4+fkxatQo8vLMFzxbt24dTZs2xd3dnbp16zJ37twi/Zk1axa1atXCw8OD0NBQtmzZUqr7sWvCMnnyZF588UXKli1L1apVWblyJUuXLqVz587UrVvXnl0TERGxGnvMElq/fj3R0dH8+OOPJCUlkZubS7t27cjKyjLFDB8+nK+//prFixezfv16Tp48yWOPPWY6np+fT2RkJDk5OWzatIl58+Yxd+5cxo8fb4o5evQokZGRtG3blp07dzJs2DCeffZZVq9ebYpZuHAhsbGxTJgwge3bt9O4cWMiIiJIT08v+Wdoz5Vu69Wrx8iRI3nuueeAqy8+jIyM5PLlyzg5/f1cSivdihRPK92KFHUzVrrdcyLTKu3cVa3s3z739OnT+Pn5sX79etq0acOFCxeoXLkyCxYs4PHHHwfgwIEDNGzYkOTkZFq0aMGqVavo1KkTJ0+exN/fH4CEhARGjx7N6dOncXNzY/To0axYsYI9e/aYrtWtWzfOnz9PYmIiAKGhoTRv3pyZM69+1gUFBVSvXp3BgwczZsyYEvXfrhWW48eP07FjR9PX4eHhGAwGTp48acdeiYiI3HouXLgAYFpCJCUlhdzcXMLDw00xDRo0oEaNGiQnJwOQnJxMo0aNTMkKQEREBBkZGezdu9cUc20bhTGFbeTk5JCSkmIW4+TkRHh4uCmmJOw6rTkvLw8PDw+zfa6uruTm5tqpRyIiIjZgpRk+2dnZZGdnm+1zd3fH3d39uucVFBQwbNgw7rvvPu666y4AUlNTcXNzo3z58max/v7+pKammmKuTVYKjxceu15MRkYGly9f5ty5c+Tn5xcbc+DAgRLc9VV2TViMRiN9+vQx+6CvXLnCwIED8fLyMu378ssv7dE9ERERq7DW0vxxcXG88sorZvsmTJjAxIkTr3tedHQ0e/bsYePGjVbphz3YNWHp3bt3kX1PP/20HXoiIiLi+MaOHUtsbKzZvhtVV2JiYli+fDkbNmygWrVqpv0BAQHk5ORw/vx5sypLWloaAQEBppg/z+YpnEV0bcyfZxalpaXh7e2Np6cnzs7OODs7FxtT2EZJ2DVh+eijj+x5eRERkZvCWu8BKsnwTyGj0cjgwYP56quvWLduHUFBQWbHQ0JCcHV1Zc2aNXTt2hWAgwcPcvz4ccLCwgAICwtj8uTJpKen4+fnB0BSUhLe3t4EBwebYlauXGnWdlJSkqkNNzc3QkJCWLNmDV26dAGuDlGtWbOGmJiSTwRwiKX5RUREbmX2WKQ2OjqaBQsW8N///pdy5cqZnjnx8fHB09MTHx8foqKiiI2NxdfXF29vbwYPHkxYWBgtWrQAoF27dgQHB9OrVy+mTp1Kamoq48aNIzo62pQ4DRw4kJkzZ/LCCy/Qr18/1q5dy6JFi1ixYoWpL7GxsfTu3ZtmzZpx7733Mn36dLKysujbt2+J78eu05ptRdOaRYqnac0iRd2Mac37T2bdOKgEGgZ63Tjo/xn+oqzz0Ucf0adPH+Dqc6MjRozgs88+Izs7m4iICGbPnm02VPPrr78yaNAg1q1bh5eXF7179+b111/HxeV/NY9169YxfPhw9u3bR7Vq1Xj55ZdN1yg0c+ZM3nzzTVJTU2nSpAnx8fGEhoaW/H6UsIjcPpSwiBR1UxKWU1ZKWKqUPGG51WhISERExMasNUvoduYQLz8UERERuR5VWERERGzMWrOEbmdKWERERGxM+YrllLCIiIjYmjIWi+kZFhEREXF4qrCIiIjYmGYJWU4Ji4iIiI3poVvLaUhIREREHJ4qLCIiIjamAovllLCIiIjYmjIWi2lISERERByeKiwiIiI2pllCllPCIiIiYmOaJWQ5DQmJiIiIw1OFRURExMZUYLGcEhYRERFbU8ZiMSUsIiIiNqaHbi2nZ1hERETE4anCIiIiYmOaJWQ5JSwiIiI2pnzFchoSEhEREYenCouIiIiNaUjIckpYREREbE4Zi6U0JCQiIiIOTxUWERERG9OQkOWUsIiIiNiY8hXLaUhIREREHJ4qLCIiIjamISHLKWERERGxMb1LyHJKWERERGxN+YrF9AyLiIiIODwlLCIiIjZmsNJWWhs2bKBz584EBgZiMBhYunSp2XGj0cj48eOpUqUKnp6ehIeHc/jwYbOYs2fP0rNnT7y9vSlfvjxRUVFkZmaaxfz000+0bt0aDw8PqlevztSpU4v0ZfHixTRo0AAPDw8aNWrEypUrS3UvSlhERERszGCwzlZaWVlZNG7cmFmzZhV7fOrUqcTHx5OQkMDmzZvx8vIiIiKCK1eumGJ69uzJ3r17SUpKYvny5WzYsIEBAwaYjmdkZNCuXTtq1qxJSkoKb775JhMnTuS9994zxWzatInu3bsTFRXFjh076NKlC126dGHPnj0l/wyNRqOx9B+BY7uSZ+8eiDimCs1j7N0FEYdzecdMm18j/WKuVdrxK+f6t881GAx89dVXdOnSBbhaXQkMDGTEiBGMHDkSgAsXLuDv78/cuXPp1q0b+/fvJzg4mK1bt9KsWTMAEhMT6dixIydOnCAwMJA5c+bw0ksvkZqaipubGwBjxoxh6dKlHDhwAICnnnqKrKwsli9fbupPixYtaNKkCQkJCSXqvyosIiIiNmaw0n/Z2dlkZGSYbdnZ2X+rT0ePHiU1NZXw8HDTPh8fH0JDQ0lOTgYgOTmZ8uXLm5IVgPDwcJycnNi8ebMppk2bNqZkBSAiIoKDBw9y7tw5U8y11ymMKbxOSShhERERsTUrPcQSFxeHj4+P2RYXF/e3upSamgqAv7+/2X5/f3/TsdTUVPz8/MyOu7i44OvraxZTXBvXXuOvYgqPl4SmNYuIiPxDjB07ltjYWLN97u7udurNzaWERURExMastQyLu7u71RKUgIAAANLS0qhSpYppf1paGk2aNDHFpKenm52Xl5fH2bNnTecHBASQlpZmFlP49Y1iCo+XhIaEREREbMxes4SuJygoiICAANasWWPal5GRwebNmwkLCwMgLCyM8+fPk5KSYopZu3YtBQUFhIaGmmI2bNhAbu7/HixOSkqifv36VKhQwRRz7XUKYwqvUxJKWERERG5RmZmZ7Ny5k507dwJXH7TduXMnx48fx2AwMGzYMF577TWWLVvG7t27eeaZZwgMDDTNJGrYsCHt27enf//+bNmyhR9++IGYmBi6detGYGAgAD169MDNzY2oqCj27t3LwoULmTFjhtnQ1dChQ0lMTGTatGkcOHCAiRMnsm3bNmJiSj5zUdOaRW4jmtYsUtTNmNZ8NivfKu34ejmXKn7dunW0bdu2yP7evXszd+5cjEYjEyZM4L333uP8+fO0atWK2bNnc8cdd5hiz549S0xMDF9//TVOTk507dqV+Ph4ypYta4r56aefiI6OZuvWrVSqVInBgwczevRos2suXryYcePGcezYMerVq8fUqVPp2LFjie9FCYvIbUQJi0hRNyNhOXfJOglLhTKlS1huJRoSEhEREYenhEVEREQcnqY1i4iI2Ji1Z/jcjpSwiIiI2JjBaiux3L40JCQiIiIOTxUWERERG9OQkOWUsIiIiNiY8hXLaUhIREREHJ4qLCIiIramEovFlLCIiIjYmGYJWU5DQiIiIuLwVGERERGxMc0SspwSFhERERtTvmI5JSwiIiK2pozFYnqGRURERByeKiwiIiI2pllCllPCIiIiYmN66NZyGhISERERh2cwGo1Ge3dCbk3Z2dnExcUxduxY3N3d7d0dEYeh7w2R0lPCIjaTkZGBj48PFy5cwNvb297dEXEY+t4QKT0NCYmIiIjDU8IiIiIiDk8Ji4iIiDg8JSxiM+7u7kyYMEEPFYr8ib43REpPD92KiIiIw1OFRURERByeEhYRERFxeEpYRERExOEpYRGHUqtWLaZPn27vbojYxLp16zAYDJw/f/66cfo+EClKCcttpE+fPhgMBl5//XWz/UuXLsVwk9/MNXfuXMqXL19k/9atWxkwYMBN7YvInxV+rxgMBtzc3Khbty6TJk0iLy/PonZbtmzJqVOn8PHxAfR9IFIaSlhuMx4eHrzxxhucO3fO3l0pVuXKlSlTpoy9uyFC+/btOXXqFIcPH2bEiBFMnDiRN99806I23dzcCAgIuOEvCPo+EClKCcttJjw8nICAAOLi4v4yZuPGjbRu3RpPT0+qV6/OkCFDyMrKMh0/deoUkZGReHp6EhQUxIIFC4qUsN9++20aNWqEl5cX1atX5/nnnyczMxO4Whbv27cvFy5cMP0WO3HiRMC8FN6jRw+eeuops77l5uZSqVIlPv74YwAKCgqIi4sjKCgIT09PGjduzJIlS6zwScntzt3dnYCAAGrWrMmgQYMIDw9n2bJlnDt3jmeeeYYKFSpQpkwZOnTowOHDh03n/frrr3Tu3JkKFSrg5eXFnXfeycqVKwHzISF9H4iUjhKW24yzszNTpkzh3Xff5cSJE0WO//LLL7Rv356uXbvy008/sXDhQjZu3EhMTIwp5plnnuHkyZOsW7eOL774gvfee4/09HSzdpycnIiPj2fv3r3MmzePtWvX8sILLwBXy+LTp0/H29ubU6dOcerUKUaOHFmkLz179uTrr782JToAq1ev5tKlSzz66KMAxMXF8fHHH5OQkMDevXsZPnw4Tz/9NOvXr7fK5yVSyNPTk5ycHPr06cO2bdtYtmwZycnJGI1GOnbsSG5uLgDR0dFkZ2ezYcMGdu/ezRtvvEHZsmWLtKfvA5FSMspto3fv3sZHHnnEaDQajS1atDD269fPaDQajV999ZWx8K9CVFSUccCAAWbnff/990YnJyfj5cuXjfv37zcCxq1bt5qOHz582AgY33nnnb+89uLFi40VK1Y0ff3RRx8ZfXx8isTVrFnT1E5ubq6xUqVKxo8//th0vHv37sannnrKaDQajVeuXDGWKVPGuGnTJrM2oqKijN27d7/+hyFyHdd+rxQUFBiTkpKM7u7uxi5duhgB4w8//GCK/eOPP4yenp7GRYsWGY1Go7FRo0bGiRMnFtvud999ZwSM586dMxqN+j4QKQ0Xu2ZLYjdvvPEGDzzwQJHf6Hbt2sVPP/3E/PnzTfuMRiMFBQUcPXqUQ4cO4eLiQtOmTU3H69atS4UKFcza+fbbb4mLi+PAgQNkZGSQl5fHlStXuHTpUonH5l1cXHjyySeZP38+vXr1Iisri//+9798/vnnAPz8889cunSJhx56yOy8nJwc7rnnnlJ9HiJ/tnz5csqWLUtubi4FBQX06NGDxx57jOXLlxMaGmqKq1ixIvXr12f//v0ADBkyhEGDBvHNN98QHh5O165dufvuu/92P/R9IHKVEpbbVJs2bYiIiGDs2LH06dPHtD8zM5PnnnuOIUOGFDmnRo0aHDp06IZtHzt2jE6dOjFo0CAmT56Mr68vGzduJCoqipycnFI9TNizZ0/uv/9+0tPTSUpKwtPTk/bt25v6CrBixQqqVq1qdp7e0SKWatu2LXPmzMHNzY3AwEBcXFxYtmzZDc979tlniYiIYMWKFXzzzTfExcUxbdo0Bg8e/Lf7ou8DESUst7XXX3+dJk2aUL9+fdO+pk2bsm/fPurWrVvsOfXr1ycvL48dO3YQEhICXP0N79pZRykpKRQUFDBt2jScnK4+JrVo0SKzdtzc3MjPz79hH1u2bEn16tVZuHAhq1at4oknnsDV1RWA4OBg3N3dOX78OPfff3/pbl7kBry8vIp8HzRs2JC8vDw2b95My5YtAThz5gwHDx4kODjYFFe9enUGDhzIwIEDGTt2LO+//36xCYu+D0RKTgnLbaxRo0b07NmT+Ph4077Ro0fTokULYmJiePbZZ/Hy8mLfvn0kJSUxc+ZMGjRoQHh4OAMGDGDOnDm4uroyYsQIPD09TVM169atS25uLu+++y6dO3fmhx9+ICEhwezatWrVIjMzkzVr1tC4cWPKlCnzl5WXHj16kJCQwKFDh/juu+9M+8uVK8fIkSMZPnw4BQUFtGrVigsXLvDDDz/g7e1N7969bfCpye2sXr16PPLII/Tv359///vflCtXjjFjxlC1alUeeeQRAIYNG0aHDh244447OHfuHN999x0NGzYstj19H4iUgr0fopGb59oHCQsdPXrU6ObmZrz2r8KWLVuMDz30kLFs2bJGLy8v4913322cPHmy6fjJkyeNHTp0MLq7uxtr1qxpXLBggdHPz8+YkJBginn77beNVapUMXp6ehojIiKMH3/8sdnDhkaj0Thw4EBjxYoVjYBxwoQJRqPR/GHDQvv27TMCxpo1axoLCgrMjhUUFBinT59urF+/vtHV1dVYuXJlY0REhHH9+vWWfVhyWyvue6XQ2bNnjb169TL6+PiY/n4fOnTIdDwmJsZYp04do7u7u7Fy5crGXr16Gf/44w+j0Vj0oVujUd8HIiVlMBqNRjvmS3ILOHHiBNWrV+fbb7/lwQcftHd3RETkFqSERUpt7dq1ZGZm0qhRI06dOsULL7zA77//zqFDh0zj6iIiItakZ1ik1HJzc3nxxRc5cuQI5cqVo2XLlsyfP1/JioiI2IwqLCIiIuLwtDS/iIiIODwlLCIiIuLwlLCIiIiIw1PCIiIiIg5PCYvILahPnz506dLF9PW//vUvhg0bdtP7sW7dOgwGA+fPn7/p1xaRW4sSFpGbqE+fPhgMBgwGA25ubtStW5dJkyaRl5dn0+t++eWXvPrqqyWKVZIhIo5I67CI3GTt27fno48+Ijs7m5UrVxIdHY2rqytjx441i8vJycHNzc0q1/T19bVKOyIi9qIKi8hN5u7uTkBAADVr1mTQoEGEh4ezbNky0zDO5MmTCQwMNL1F+7fffuPJJ5+kfPny+Pr68sgjj3Ds2DFTe/n5+cTGxlK+fHkqVqzICy+8wJ+XV/rzkFB2djajR4+mevXquLu7U7duXT788EOOHTtG27ZtAahQoQIGg4E+ffoAUFBQQFxcHEFBQXh6etK4cWOWLFlidp2VK1dyxx134OnpSdu2bc36KSJiCSUsInbm6elJTk4OAGvWrOHgwYMkJSWxfPlycnNziYiIoFy5cnz//ff88MMPlC1blvbt25vOmTZtGnPnzuU///kPGzdu5OzZs3z11VfXveYzzzzDZ599Rnx8PPv37+ff//43ZcuWpXr16nzxxRcAHDx4kFOnTjFjxgwA4uLi+Pjjj0lISGDv3r0MHz6cp59+mvXr1wNXE6vHHnuMzp07s3PnTp599lnGjBljq49NRG43dnzxosht59q3ABcUFBiTkpKM7u7uxpEjRxp79+5t9Pf3N2ZnZ5viP/nkE2P9+vXN3s6bnZ1t9PT0NK5evdpoNBqNVapUMU6dOtV0PDc311itWjWztw3ff//9xqFDhxqNRqPx4MGDRsCYlJRUbB+Le6PwlStXjGXKlDFu2rTJLDYqKsrYvXt3o9FoNI4dO9YYHBxsdnz06NFF2hIR+Tv0DIvITbZ8+XLKli1Lbm4uBQUF9OjRg4kTJxIdHU2jRo3MnlvZtWsXP//8M+XKlTNr48qVK/zyyy9cuHCBU6dOERoaajrm4uJCs2bNigwLFdq5cyfOzs7cf//9Je7zzz//zKVLl3jooYfM9ufk5HDPPfcAsH//frN+AISFhZX4GiIi16OEReQma9u2LXPmzMHNzY3AwEBcXP73bejl5WUWm5mZSUhICPPnzy/STuXKlf/W9T09PUt9TmZmJgArVqygatWqZsfc3d3/Vj9EREpDCYvITebl5UXdunVLFNu0aVMWLlyIn58f3t7excZUqVKFzZs306ZNGwDy8vJISUmhadOmxcY3atSIgoIC1q9fT3h4eJHjhRWe/Px8077g4GDc3d05fvz4X1ZmGjZsyLJly8z2/fjjjze+SRGREtBDtyIOrGfPnlSqVIlHHnmE77//nqNHj7Ju3TqGDBnCiRMnABg6dCivv/46S5cu5cCBAzz//PPXXUOlVq1a9O7dm379+rF06VJTm4sWLQKgZs2aGAwGli9fzunTp8nMzKRcuXKMHDmS4cOHM2/ePH755Re2b9/Ou+++y7x58wAYOHAghw8fZtSoURw8eJAFCxYwd+5cW39EInKbUMIi4sDKlCnDhg0bqFGjBo899hgNGzYkKiqKK1eumCouI0aMoFevXvTu3ZuwsDDKlSvHo48+et1258yZw+OPP87zzz9PgwYN6N+/P1lZWQBUrVqVV155hTFjxuDv709MTAwAr776Ki+//DJxcXE0bNiQ9u3bs2LFCoKCggCoUaMGX3zxBUuXLqVx48YkJCQwZcoUG346InI7MRj/6sk8EREREQehCouIiIg4PCUsIiIi4vCUsIiIiIjDU8IiIiIiDk8Ji4iIiDg8JSwiIiLi8JSwiIiIiMNTwiIiIiIOTwmLiIiIODwlLCIiIuLwlLCIiIiIw1PCIiIiIg7v/wDMwMRK58t5/wAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_binary_value)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T03:29:46.735267778Z",
     "start_time": "2023-12-01T03:29:46.586637963Z"
    }
   },
   "id": "73c1fc91228cce3e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(y_test, y_predicted_value, alpha=0.5)\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T03:29:54.371805422Z",
     "start_time": "2023-12-01T03:29:54.068191214Z"
    }
   },
   "id": "e5de6df9a058348c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bert Super-vised\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "360c532733d75665"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f46aef73a3182928",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:01:57.910208862Z",
     "start_time": "2023-12-01T10:01:55.445878221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "chunk= 100000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "chunks2 = pd.read_json(file, lines=True, chunksize=chunk)\n",
    "for i, x in enumerate(chunks2):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "    if i == 0:\n",
    "        csv = \"chunk.csv\"\n",
    "        x.to_csv(csv, index=False)\n",
    "    df1= pd.read_csv(\"chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def sentiment(stars):  \n",
    "    if 0 <= stars < 3:\n",
    "        return 0 #negative\n",
    "    elif 3 <= stars <= 5:\n",
    "        return 1 #normal\n",
    "    else:\n",
    "        return 'undefined'  \n",
    "\n",
    "df1['sentiment_label'] = df1['stars'].apply(sentiment)\n",
    "le = LabelEncoder()\n",
    "df1['encoded_sentiment'] = le.fit_transform(df1['sentiment_label'])\n",
    "\n",
    "# Extract labels\n",
    "y = df1['encoded_sentiment'].values\n",
    "df1.to_csv('preprocessed_data1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:02:29.655618341Z",
     "start_time": "2023-12-01T10:02:28.633531278Z"
    }
   },
   "id": "744e9395667a6e15"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f70b30861e00a4ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:02:31.956719987Z",
     "start_time": "2023-12-01T10:02:31.941429223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    review_id                 user_id             business_id  \\\n0      KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n1      BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n2      saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n3      AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n4      Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n...                       ...                     ...                     ...   \n99995  pAEbIxvr6ebx2bHc1XvguA  SMH5CeiLvKx61lKwtLZ_PA  lV0k3BnslFRkuWD_kbKd0Q   \n99996  xH1AoE-4nf2ECGQJRjO4_g  2clTdtp-BjphxLjN83CpUA  G0xz3kyRhRi6oZl7KfR0pA   \n99997  GatIbXTz-WDru5emONUSIg  MRrN6DH3QGCFcDv5RENYVg  C4lZdhasjZVQyDlOiXY1sA   \n99998  6NfkodAdhvI89xONXuBC3A  rnNQzeKJbvqVCsYsL10mkQ  dChRGpit9fM_kZK5pafNyA   \n99999  sJ1BMq7lkKgOWEFx3n6ZRw  _BcWyKQL16ndpBdggh2kNA  hMcgO98QaOFmQVTfCUeGzw   \n\n       stars  useful  funny  cool  \\\n0          3       0      0     0   \n1          5       1      0     1   \n2          3       0      0     0   \n3          5       1      0     1   \n4          4       1      0     1   \n...      ...     ...    ...   ...   \n99995      4       0      0     0   \n99996      1       1      0     0   \n99997      4       0      0     0   \n99998      2       0      0     0   \n99999      5       0      0     0   \n\n                                                    text                 date  \\\n0      If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n1      I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n2      Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n3      Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n4      Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n...                                                  ...                  ...   \n99995  Came here for lunch with a group. They were bu...  2018-05-30 22:28:56   \n99996  The equipment is so old and so felty! I just u...  2015-04-05 23:31:52   \n99997  This is one of my favorite Mexican restaurants...  2016-06-04 00:59:15   \n99998  Came here for brunch - had an omlette ($19 + t...  2018-06-11 12:45:08   \n99999  Came in for my 5-6 month prophy and saw Kara -...  2013-06-06 10:10:33   \n\n       sentiment_label  encoded_sentiment  \n0                    1                  1  \n1                    1                  1  \n2                    1                  1  \n3                    1                  1  \n4                    1                  1  \n...                ...                ...  \n99995                1                  1  \n99996                0                  0  \n99997                1                  1  \n99998                0                  0  \n99999                1                  1  \n\n[100000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>sentiment_label</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>pAEbIxvr6ebx2bHc1XvguA</td>\n      <td>SMH5CeiLvKx61lKwtLZ_PA</td>\n      <td>lV0k3BnslFRkuWD_kbKd0Q</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came here for lunch with a group. They were bu...</td>\n      <td>2018-05-30 22:28:56</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>xH1AoE-4nf2ECGQJRjO4_g</td>\n      <td>2clTdtp-BjphxLjN83CpUA</td>\n      <td>G0xz3kyRhRi6oZl7KfR0pA</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>The equipment is so old and so felty! I just u...</td>\n      <td>2015-04-05 23:31:52</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>GatIbXTz-WDru5emONUSIg</td>\n      <td>MRrN6DH3QGCFcDv5RENYVg</td>\n      <td>C4lZdhasjZVQyDlOiXY1sA</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>This is one of my favorite Mexican restaurants...</td>\n      <td>2016-06-04 00:59:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>6NfkodAdhvI89xONXuBC3A</td>\n      <td>rnNQzeKJbvqVCsYsL10mkQ</td>\n      <td>dChRGpit9fM_kZK5pafNyA</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came here for brunch - had an omlette ($19 + t...</td>\n      <td>2018-06-11 12:45:08</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>sJ1BMq7lkKgOWEFx3n6ZRw</td>\n      <td>_BcWyKQL16ndpBdggh2kNA</td>\n      <td>hMcgO98QaOFmQVTfCUeGzw</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Came in for my 5-6 month prophy and saw Kara -...</td>\n      <td>2013-06-06 10:10:33</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "df1['sentiment_label'].value_counts().sort_index().plot(kind='bar', color=['red', 'yellow', 'green'])\n",
    "plt.title('Sentiment Labels')\n",
    "plt.xlabel('Sentiment Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:03:11.076273694Z",
     "start_time": "2023-12-01T10:03:10.975738704Z"
    }
   },
   "id": "3b3a2f14f5ae398e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting divided parts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cbd632a3aacc089"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:03:26.468217020Z",
     "start_time": "2023-12-01T10:03:26.423274181Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = df1[df1['sentiment_label'] == 1].sample(n=2000, random_state=42)\n",
    "negative_reviews= df1[df1['sentiment_label'] == 0].sample(n=2000, random_state=42)\n",
    "\n",
    "# Combine the selected samples\n",
    "new_data = pd.concat([positive_reviews, negative_reviews], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "56c63405a6148b1b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:03:28.809136811Z",
     "start_time": "2023-12-01T10:03:28.781467298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                   review_id                 user_id             business_id  \\\n0     LP2B6aJarFIJ7I6BJ1cD9A  QY1R3IciPr-IkKeTFsqVOA  Di6uZDhcwnLsgM66Z4fNfw   \n1     mHShP4zIYe1D2S-EBJOGIw  xFj_h72vn0th96ETHQUY7A  rZceWBN1i0QMmmXJcJLfxw   \n2     2Ok8SYfDBlYi8kckknqEUA  huoocXS_i6g65qESKMo7gQ  4NidY2tw42l6iobbtai_kA   \n3     3YGWryyrTbDH0fiCn4OHBQ  Q3E9uqEA0kcgkkjju6h1KQ  wzIN0IqcNOnUjwuzRoG9AA   \n4     ZYbM8Y34-okwn--e5kiO1g  FtZ2rivohSn-KIj_8138yQ  USekrAG0-4tJUs9V2gBBOQ   \n...                      ...                     ...                     ...   \n3995  WpMd2rE9H3Z_Fp38i9OMFw  yjGTXXUjzpR8Q25ZKcKwXw  8usO-H5uFTzlISfGZN8rSg   \n3996  xDKP0idPe3I2zGZTocGa4A  -kMtxnQmE_S3_U1yZ2cVtQ  eaV07HGOcyb27XobHVl8LQ   \n3997  KmVnM35HQR7sEun610gsag  OZ-T-UTOXUXva4RJ8j61oQ  qcLkiAqlxx0oTqZMSC_y6Q   \n3998  P_NZRZPHP3k-HnYacdUtwQ  KGdV17WL4x2w52VK81D_Iw  SRb3xScVOeYfhZP4U8LMdA   \n3999  ioSOnTOAOzSQ_snexu8msw  I9v7jlsljI9NLnrsqIO4qQ  o9Gru-QFoxGK26FSWRCLXA   \n\n      stars  useful  funny  cool  \\\n0         5       0      0     0   \n1         3       6      1     1   \n2         3       0      0     0   \n3         5       0      0     0   \n4         5       0      0     0   \n...     ...     ...    ...   ...   \n3995      2       4      1     0   \n3996      1       3      0     1   \n3997      1       4      1     0   \n3998      2       0      0     0   \n3999      1       0      0     0   \n\n                                                   text                 date  \\\n0     The type of bar a vigilante would go to after ...  2018-03-05 00:04:30   \n1     This place is OK the therapist my son has is w...  2015-11-09 19:21:43   \n2     Really prompt service even though we were 30 m...  2018-01-01 02:41:07   \n3     I got the pick 3! Had the stuffed pepper , jam...  2018-07-28 23:09:42   \n4     Fantastic!  \\n\\nStaff was amazingly friendly. ...  2016-07-27 22:12:07   \n...                                                 ...                  ...   \n3995  The food was good, but that was the only thing...  2015-07-28 20:36:17   \n3996  So basically, you are going to be having a stu...  2012-09-10 19:52:09   \n3997  Absolutely the worst hair cut I have ever seen...  2015-12-23 16:45:43   \n3998  Unfortunately, the other reviews said it all. ...  2015-07-29 00:28:15   \n3999  What a zoo!  I hate renting from this facility...  2006-10-28 19:30:50   \n\n      sentiment_label  encoded_sentiment  \n0                   1                  1  \n1                   1                  1  \n2                   1                  1  \n3                   1                  1  \n4                   1                  1  \n...               ...                ...  \n3995                0                  0  \n3996                0                  0  \n3997                0                  0  \n3998                0                  0  \n3999                0                  0  \n\n[4000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>sentiment_label</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LP2B6aJarFIJ7I6BJ1cD9A</td>\n      <td>QY1R3IciPr-IkKeTFsqVOA</td>\n      <td>Di6uZDhcwnLsgM66Z4fNfw</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>The type of bar a vigilante would go to after ...</td>\n      <td>2018-03-05 00:04:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mHShP4zIYe1D2S-EBJOGIw</td>\n      <td>xFj_h72vn0th96ETHQUY7A</td>\n      <td>rZceWBN1i0QMmmXJcJLfxw</td>\n      <td>3</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>This place is OK the therapist my son has is w...</td>\n      <td>2015-11-09 19:21:43</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2Ok8SYfDBlYi8kckknqEUA</td>\n      <td>huoocXS_i6g65qESKMo7gQ</td>\n      <td>4NidY2tw42l6iobbtai_kA</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Really prompt service even though we were 30 m...</td>\n      <td>2018-01-01 02:41:07</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3YGWryyrTbDH0fiCn4OHBQ</td>\n      <td>Q3E9uqEA0kcgkkjju6h1KQ</td>\n      <td>wzIN0IqcNOnUjwuzRoG9AA</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>I got the pick 3! Had the stuffed pepper , jam...</td>\n      <td>2018-07-28 23:09:42</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ZYbM8Y34-okwn--e5kiO1g</td>\n      <td>FtZ2rivohSn-KIj_8138yQ</td>\n      <td>USekrAG0-4tJUs9V2gBBOQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Fantastic!  \\n\\nStaff was amazingly friendly. ...</td>\n      <td>2016-07-27 22:12:07</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>WpMd2rE9H3Z_Fp38i9OMFw</td>\n      <td>yjGTXXUjzpR8Q25ZKcKwXw</td>\n      <td>8usO-H5uFTzlISfGZN8rSg</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>The food was good, but that was the only thing...</td>\n      <td>2015-07-28 20:36:17</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>xDKP0idPe3I2zGZTocGa4A</td>\n      <td>-kMtxnQmE_S3_U1yZ2cVtQ</td>\n      <td>eaV07HGOcyb27XobHVl8LQ</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>So basically, you are going to be having a stu...</td>\n      <td>2012-09-10 19:52:09</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>KmVnM35HQR7sEun610gsag</td>\n      <td>OZ-T-UTOXUXva4RJ8j61oQ</td>\n      <td>qcLkiAqlxx0oTqZMSC_y6Q</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Absolutely the worst hair cut I have ever seen...</td>\n      <td>2015-12-23 16:45:43</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>P_NZRZPHP3k-HnYacdUtwQ</td>\n      <td>KGdV17WL4x2w52VK81D_Iw</td>\n      <td>SRb3xScVOeYfhZP4U8LMdA</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unfortunately, the other reviews said it all. ...</td>\n      <td>2015-07-29 00:28:15</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>ioSOnTOAOzSQ_snexu8msw</td>\n      <td>I9v7jlsljI9NLnrsqIO4qQ</td>\n      <td>o9Gru-QFoxGK26FSWRCLXA</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>What a zoo!  I hate renting from this facility...</td>\n      <td>2006-10-28 19:30:50</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load model and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83e734884c57f29"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dff61dffb6be7e22",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:03:56.316286043Z",
     "start_time": "2023-12-01T10:03:54.133226675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification \n",
    "model_name = \"bert-base-uncased\"\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f725cbd7499c1a33"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T10:04:58.586090145Z",
     "start_time": "2023-12-01T10:04:53.745750466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts (reviews)\n",
    "import torch\n",
    "\n",
    "tokenized_vals = []\n",
    "\n",
    "for text in new_data['text']:\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Append individually\n",
    "    tokenized_vals.append({\n",
    "        'input_ids': tokens['input_ids'],\n",
    "        'attention_mask': tokens['attention_mask']\n",
    "    })\n",
    "\n",
    "# Extracting input tensors separately\n",
    "input_ids = torch.cat([entry['input_ids'] for entry in tokenized_vals], dim=0)\n",
    "attention_mask = torch.cat([entry['attention_mask'] for entry in tokenized_vals], dim=0)\n"
   ],
   "id": "c40fe859315047eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train and test split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89000442e487914f"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "label = new_data['encoded_sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:05:12.512720878Z",
     "start_time": "2023-12-01T10:05:12.467227025Z"
    }
   },
   "id": "7f8363980f8ba3c6"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T10:05:37.837444957Z",
     "start_time": "2023-12-01T10:05:37.819811406Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_id, X_test_id, X_train_m, X_test_m, y_train, y_test = train_test_split(\n",
    "    input_ids.numpy(),\n",
    "    attention_mask.numpy(),\n",
    "    label,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "c53659815d380684"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8393e04480e9c9ed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:05:39.577733790Z",
     "start_time": "2023-12-01T10:05:39.528602311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3400, 128) (600, 128) (3400, 128) (600, 128) (3400,) (600,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_id.shape, X_test_id.shape, X_train_m.shape, X_test_m.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:21.002275392Z",
     "start_time": "2023-12-01T10:09:20.953329391Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[114], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m X_train_mask\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(X_train_m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m      5\u001B[0m X_test_mask\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(X_test_m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m----> 6\u001B[0m y_train1\u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m \n\u001B[1;32m      7\u001B[0m y_test1\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(y_test\u001B[38;5;241m.\u001B[39mvalues, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)  \n",
      "\u001B[0;31mTypeError\u001B[0m: an integer is required (got type builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "# PyTorch tensors \n",
    "X_train_ids= torch.tensor(X_train_id, dtype=torch.long)\n",
    "X_test_ids= torch.tensor(X_test_id, dtype=torch.long)\n",
    "X_train_mask= torch.tensor(X_train_m, dtype=torch.long)\n",
    "X_test_mask= torch.tensor(X_test_m, dtype=torch.long)\n",
    "y_train= torch.tensor(y_train.values, dtype=torch.long) \n",
    "y_test= torch.tensor(y_test.values, dtype=torch.long)  "
   ],
   "id": "89b874ad21459b1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414b8b3f3170c940"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8aea60188055875",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:06:41.186846139Z",
     "start_time": "2023-12-01T10:06:41.142989698Z"
    }
   },
   "outputs": [],
   "source": [
    "criter = torch.nn.CrossEntropyLoss()   \n",
    "optim= torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bbb21d72ff741",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "790920d77b296858",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:11:50.047884828Z",
     "start_time": "2023-12-01T10:11:50.028654099Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 3\n",
    "\n",
    "train_data = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_data = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_load= DataLoader(test_data,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b2c73f3627ce1996",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:37:46.440643535Z",
     "start_time": "2023-12-01T10:11:51.027751253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2711, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4997,  1.9424, -1.1834],\n",
      "        [ 1.4153,  0.6377, -1.3267],\n",
      "        [ 0.4253,  1.6966, -1.8704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4567, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6663,  0.3402, -1.7503],\n",
      "        [ 1.3673,  0.5620, -1.5273],\n",
      "        [ 1.2695,  1.2556, -2.1849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2809, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6192,  2.0520, -0.8048],\n",
      "        [ 1.7078,  0.5453, -1.3906],\n",
      "        [ 1.3929,  0.6489, -1.7489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2784, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5907,  0.3293, -1.4386],\n",
      "        [ 1.5399,  0.2238, -1.6134],\n",
      "        [ 1.7444,  0.4839, -1.5456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2915, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4020,  0.3762, -1.6529],\n",
      "        [ 1.7168,  0.1607, -1.5430],\n",
      "        [ 1.6433,  0.5414, -1.7695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2785, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7050,  0.2207, -1.4804],\n",
      "        [ 1.5363,  0.0067, -1.3474],\n",
      "        [ 0.4699,  1.3878, -2.1345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2400, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8149,  0.3363, -1.5637],\n",
      "        [ 1.7136,  0.0255, -1.5502],\n",
      "        [ 1.5262,  0.2287, -1.3274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1035, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6746, -0.2467, -1.2855],\n",
      "        [-1.0945,  2.4794, -1.1185],\n",
      "        [-0.5920,  2.2562, -1.6262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0935, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1491,  2.5477, -1.0469],\n",
      "        [-1.2816,  2.4598, -0.7883],\n",
      "        [ 1.7714, -0.0775, -1.8524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1659, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3172,  1.8189, -2.3310],\n",
      "        [ 2.3945, -0.0145, -1.6171],\n",
      "        [ 1.9406,  0.2020, -1.8435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1324, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1687, -0.0804, -1.5598],\n",
      "        [ 2.0326,  0.4408, -2.2118],\n",
      "        [-0.3571,  2.3270, -1.9962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1709, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4997,  2.5654, -1.0295],\n",
      "        [-0.7900,  2.5633, -1.7147],\n",
      "        [ 1.4746,  0.7768, -2.1586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1331, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3257, -0.3766, -1.5270],\n",
      "        [-1.6715,  2.5346, -0.6912],\n",
      "        [ 1.6511,  0.3740, -2.2641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0649, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1570, -0.5428, -1.3362],\n",
      "        [-1.4656,  2.6284, -1.0474],\n",
      "        [ 2.5936, -0.5865, -1.2839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6011, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3616,  0.4558, -2.0520],\n",
      "        [-1.4340,  2.3997, -1.2737],\n",
      "        [ 1.3290,  0.8299, -2.4281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0005, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2637, -0.8012, -1.3678],\n",
      "        [ 2.2860, -0.4274, -1.4132],\n",
      "        [ 2.3865, -0.7610, -1.3210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0743, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2908, -0.6429, -1.4284],\n",
      "        [ 2.5523, -0.5793, -1.6464],\n",
      "        [ 2.0925, -0.8614, -1.0457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0599, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5031, -0.8800, -1.5012],\n",
      "        [-1.7393,  2.5448, -0.7237],\n",
      "        [ 2.4464, -0.2921, -1.6416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4843, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2722, -0.5646, -1.4746],\n",
      "        [ 1.6068,  0.6660, -2.0574],\n",
      "        [ 2.2197, -0.4926, -1.5675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9279,  2.9362, -0.7686],\n",
      "        [ 2.2227, -0.1975, -1.9027],\n",
      "        [-1.7609,  2.7123, -0.8205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1383, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7276,  2.6645, -0.7971],\n",
      "        [ 1.3173,  0.2503, -2.1133],\n",
      "        [-2.1270,  2.7471, -0.3264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6909, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8357,  2.8557, -0.4971],\n",
      "        [-1.6981,  2.8452, -0.7795],\n",
      "        [ 1.2761,  0.6546, -2.2160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.7662, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5430,  2.3187, -0.9410],\n",
      "        [-1.4336,  2.5923, -0.9991],\n",
      "        [-1.4398,  2.6965, -1.2823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0040, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7742,  1.9192, -1.5622],\n",
      "        [-0.7886,  1.9440, -1.8255],\n",
      "        [-0.7626,  1.8490, -1.5655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8739, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2020,  1.5018, -1.8212],\n",
      "        [-0.9030,  1.5797, -1.6663],\n",
      "        [-0.9196,  1.8212, -1.8822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3018, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0991,  1.0230, -2.2054],\n",
      "        [ 0.2408,  1.1259, -2.0401],\n",
      "        [ 1.6287, -0.1534, -2.5804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5164, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1084,  0.0239, -2.4302],\n",
      "        [ 0.0429,  0.7366, -1.6564],\n",
      "        [ 0.2274,  0.6201, -1.8437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4072, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2335,  0.5853, -2.0171],\n",
      "        [ 2.2665, -0.2356, -2.5378],\n",
      "        [ 0.4948,  0.8529, -2.1012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4505, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4562,  0.8399, -2.1480],\n",
      "        [ 0.4987,  0.4331, -1.7747],\n",
      "        [ 2.1395, -0.3675, -2.2692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1279, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9389, -0.2800, -2.4417],\n",
      "        [ 1.8583, -0.3323, -2.5568],\n",
      "        [ 1.5833, -0.3613, -2.2497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4774, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5603,  0.6116, -1.8091],\n",
      "        [ 0.9776,  0.4880, -2.0173],\n",
      "        [ 1.8120, -0.0214, -2.4905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7061, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5468,  0.5106, -1.9969],\n",
      "        [ 0.4538,  0.6338, -1.7341],\n",
      "        [ 0.5884,  0.1543, -2.3605]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8202, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5186,  0.3167, -2.1945],\n",
      "        [ 1.2459,  0.0320, -2.2572],\n",
      "        [ 1.9567, -0.1289, -2.3753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1071, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3597, -0.2740, -2.8091],\n",
      "        [ 1.9388, -0.3737, -2.3945],\n",
      "        [ 1.8412, -0.1263, -2.6927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2571, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6129,  0.8018, -2.0467],\n",
      "        [ 2.5408, -0.5502, -2.4456],\n",
      "        [ 2.2381, -0.3217, -2.1379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7489, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6688,  0.5596, -2.1313],\n",
      "        [ 0.6752,  0.5400, -2.2550],\n",
      "        [ 0.7266,  0.8131, -2.2590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3088, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5298,  0.2500, -2.2502],\n",
      "        [ 2.2335, -0.2828, -2.3993],\n",
      "        [ 0.9289,  0.6315, -2.3580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2352, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6201, -0.4722, -2.2116],\n",
      "        [ 0.4830,  0.7377, -2.2131],\n",
      "        [ 2.5121, -0.5812, -2.4851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6667, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2363,  0.6825, -2.4870],\n",
      "        [ 2.6665, -0.4905, -2.1950],\n",
      "        [ 0.9044,  0.7910, -2.3839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2495, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3352, -0.3838, -2.0919],\n",
      "        [ 2.4705, -0.4495, -2.2908],\n",
      "        [ 0.8382,  1.0429, -2.4283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2794, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5082, -0.2619, -2.2665],\n",
      "        [ 0.6120,  1.4604, -2.1739],\n",
      "        [ 2.6304, -0.7199, -1.9334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3409, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3330,  1.1214, -2.6143],\n",
      "        [ 1.2280,  0.4787, -2.2355],\n",
      "        [ 2.2845, -0.0944, -2.6029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2438, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1712,  0.1048, -2.6339],\n",
      "        [ 0.5761,  1.6225, -2.1243],\n",
      "        [ 0.4030,  1.5933, -1.9911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4943, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0069,  0.7036, -2.4214],\n",
      "        [ 0.2304,  1.0362, -2.5777],\n",
      "        [ 0.2114,  1.6918, -2.2784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2619, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0563,  1.6872, -1.9321],\n",
      "        [ 1.4076,  0.7199, -2.4832],\n",
      "        [-0.0494,  1.6833, -2.1444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4793, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0430,  1.9675, -2.2349],\n",
      "        [-0.0043,  2.0483, -2.2247],\n",
      "        [ 0.3145,  1.0611, -2.3978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3786, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4262,  0.5923, -2.7518],\n",
      "        [ 1.3481,  0.7851, -2.5338],\n",
      "        [ 1.6401,  0.5481, -2.5410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8112, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1157,  2.1489, -2.0363],\n",
      "        [-0.2074,  1.9736, -1.9398],\n",
      "        [-0.1228,  1.9071, -2.1003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6718, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3150,  1.9826, -2.0446],\n",
      "        [-0.4510,  2.1263, -2.1514],\n",
      "        [-0.1996,  1.4167, -2.2967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1063, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4667,  1.8908, -2.0270],\n",
      "        [-0.4124,  1.7534, -2.2395],\n",
      "        [-0.5900,  2.0603, -1.9146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3579, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6606,  0.8064, -2.8426],\n",
      "        [-0.4304,  1.6797, -1.8327],\n",
      "        [-0.3689,  1.6173, -2.0714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8524, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3496,  1.7889, -2.3869],\n",
      "        [-0.4130,  1.7819, -1.8905],\n",
      "        [-0.3551,  1.9887, -1.9569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5852, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3590,  1.5253, -2.1145],\n",
      "        [ 0.1634,  1.3181, -2.4338],\n",
      "        [-0.2928,  1.6973, -2.2272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3629, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7289,  0.6355, -3.0550],\n",
      "        [ 0.2275,  1.0865, -2.5446],\n",
      "        [ 0.1691,  1.3529, -2.3597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7731, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5740,  0.4959, -3.1007],\n",
      "        [-0.3956,  1.1305, -2.4686],\n",
      "        [ 0.0254,  1.2633, -2.3957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5808, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0207,  1.5138, -2.8837],\n",
      "        [ 0.0032,  1.2352, -2.7055],\n",
      "        [ 0.0160,  0.9209, -2.5602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5663, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8959,  0.5251, -3.0334],\n",
      "        [-0.1223,  0.5462, -2.6760],\n",
      "        [ 0.0270,  0.9541, -2.3792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2416, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5305,  0.1180, -2.9850],\n",
      "        [ 1.9180,  0.1203, -3.2880],\n",
      "        [-0.0036,  0.9648, -2.7169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2830, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3532,  0.6709, -2.8572],\n",
      "        [ 2.1748,  0.1913, -2.8324],\n",
      "        [ 1.9783,  0.1233, -3.1307]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3622, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5341,  0.4927, -2.6950],\n",
      "        [ 2.2084, -0.0908, -3.3454],\n",
      "        [ 1.3722,  0.0911, -3.1601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5412, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0696,  0.0103, -3.1011],\n",
      "        [ 0.5387,  0.5169, -2.9876],\n",
      "        [ 0.4466,  0.3138, -2.9291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7412, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9857,  0.2596, -2.9241],\n",
      "        [ 1.5410,  0.0497, -3.1557],\n",
      "        [ 2.2391,  0.0741, -2.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2958, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1451,  0.3565, -2.9086],\n",
      "        [ 2.1436,  0.1121, -3.0328],\n",
      "        [ 2.0044,  0.1163, -3.2630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6323, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5521,  0.2688, -3.0768],\n",
      "        [ 2.2168,  0.1680, -3.1799],\n",
      "        [ 0.8164,  0.4503, -2.6045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3521, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9399,  0.2647, -3.0467],\n",
      "        [ 0.3950,  0.3735, -2.8175],\n",
      "        [ 1.9220,  0.0807, -2.8106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3541, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5336,  0.4734, -2.9975],\n",
      "        [ 0.1078,  1.1065, -3.1111],\n",
      "        [ 0.3231,  0.9770, -2.8706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2270, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1203,  1.3984, -2.7512],\n",
      "        [ 0.2048,  1.2083, -2.6389],\n",
      "        [ 2.2204, -0.1367, -3.0515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3821, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0696,  0.1096, -2.8239],\n",
      "        [ 1.8979,  0.2280, -2.8466],\n",
      "        [ 0.9495,  0.7149, -2.9686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2938, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6581,  0.5843, -2.9483],\n",
      "        [ 0.4206,  1.3957, -3.1874],\n",
      "        [ 0.1881,  1.4808, -2.8616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1644, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1295,  0.1991, -3.1426],\n",
      "        [ 0.2548,  1.7885, -2.6520],\n",
      "        [ 1.9087,  0.0136, -2.7513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2127, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9804e+00, -1.5065e-03, -3.0220e+00],\n",
      "        [ 3.7324e-01,  1.8527e+00, -2.9442e+00],\n",
      "        [ 3.0390e-01,  1.4337e+00, -2.7724e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1747, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1259,  0.3090, -3.2361],\n",
      "        [ 0.0333,  2.0226, -2.4615],\n",
      "        [ 0.1798,  1.5661, -3.0194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2200, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8872,  0.0040, -3.2575],\n",
      "        [ 0.5458,  1.5843, -3.2726],\n",
      "        [ 1.5986,  0.0740, -3.0749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2073, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1559,  0.0291, -3.0619],\n",
      "        [ 1.9506,  0.1351, -3.3362],\n",
      "        [ 1.3037,  0.4089, -3.3543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1255, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0200, -0.1934, -3.0664],\n",
      "        [ 1.7774, -0.1483, -3.3765],\n",
      "        [ 2.1422,  0.1025, -3.2952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1125, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0768,  2.1739, -2.2810],\n",
      "        [-0.0336,  2.0879, -2.5151],\n",
      "        [-0.1906,  2.1614, -2.0279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7225, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9604,  0.3363, -3.1465],\n",
      "        [ 0.2418,  1.9079, -3.1814],\n",
      "        [ 0.1353,  2.1879, -1.7403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1405, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1881,  1.9918, -1.8976],\n",
      "        [ 2.1257,  0.2732, -3.1215],\n",
      "        [ 2.2883,  0.4032, -2.9210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9921, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1787,  2.0948, -1.9679],\n",
      "        [ 1.0358,  0.9977, -3.1809],\n",
      "        [ 2.1840,  0.1286, -2.9410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1143, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8782, -0.2802, -2.8134],\n",
      "        [ 0.0511,  2.2264, -2.3641],\n",
      "        [-0.2266,  2.1416, -1.7155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1241, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0926,  2.2464, -2.2290],\n",
      "        [ 0.2623,  2.1402, -2.7859],\n",
      "        [ 2.1979,  0.0948, -2.7452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3463, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4249, -0.2961, -2.7250],\n",
      "        [ 2.2867, -0.1915, -3.0983],\n",
      "        [ 1.5325,  1.1882, -3.2560]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0845, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3111, -0.0214, -2.4582],\n",
      "        [-0.0596,  2.5549, -2.3764],\n",
      "        [ 2.5813, -0.0285, -2.6724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8709, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5728, -0.1118, -2.6808],\n",
      "        [ 2.4327, -0.0790, -3.2186],\n",
      "        [ 2.4753,  0.1090, -2.8343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0908, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0419,  2.5373, -2.3360],\n",
      "        [ 2.1327, -0.1248, -3.2692],\n",
      "        [ 2.3733, -0.1593, -2.6239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7961, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2968,  0.6821, -3.4281],\n",
      "        [-0.3413,  2.4548, -2.5258],\n",
      "        [ 2.1396,  0.4245, -3.2398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0916,  0.3187, -2.7349],\n",
      "        [ 2.3009, -0.0079, -2.7215],\n",
      "        [ 2.3249, -0.1964, -2.9703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0881, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1082,  0.1032, -2.3824],\n",
      "        [ 2.4150,  0.0156, -3.1334],\n",
      "        [-0.6657,  2.8086, -2.0771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0647, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6547,  2.6375, -2.2645],\n",
      "        [-0.6812,  2.5657, -1.8681],\n",
      "        [ 2.2791, -0.0224, -2.8027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0981, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3105,  2.3179, -3.1643],\n",
      "        [-0.5876,  2.5775, -2.2017],\n",
      "        [ 2.3667,  0.2206, -2.8667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8932, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7576,  0.4410, -3.3630],\n",
      "        [ 2.2535,  0.1157, -2.5220],\n",
      "        [ 2.1036,  0.4519, -2.9694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0979, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4921, -0.0093, -2.9273],\n",
      "        [-0.6319,  2.4285, -2.4319],\n",
      "        [ 2.0712,  0.2708, -3.1404]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1073, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4079,  2.3348, -3.0186],\n",
      "        [-0.0885,  2.4585, -2.6066],\n",
      "        [ 2.3503,  0.0586, -2.9816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9469, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6182,  2.9269, -2.3642],\n",
      "        [-0.0329,  2.5788, -2.7021],\n",
      "        [ 2.0259, -0.0917, -2.9056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6094, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3782,  1.8679, -3.2196],\n",
      "        [-0.6992,  2.4739, -2.2861],\n",
      "        [-0.0144,  2.5190, -2.8241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3502, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5089,  2.6165, -2.3271],\n",
      "        [ 2.2184,  0.4520, -3.1951],\n",
      "        [ 0.7706,  1.0299, -3.2335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4446, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1455,  0.0793, -3.0353],\n",
      "        [-0.3015,  2.2974, -2.8460],\n",
      "        [ 0.8689,  1.6024, -2.9859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6029, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5426, -0.0400, -3.1181],\n",
      "        [ 1.6649,  0.8503, -3.4191],\n",
      "        [ 1.1969,  1.5251, -3.3024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7875, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2749,  0.2024, -3.2869],\n",
      "        [ 2.6315,  0.0862, -3.1272],\n",
      "        [-0.0984,  2.3363, -2.7795]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5336, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9496,  0.5470, -3.0261],\n",
      "        [ 2.4140,  0.0118, -3.2813],\n",
      "        [ 2.5800, -0.2412, -2.9027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0849, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1919, -0.0041, -2.7298],\n",
      "        [-0.3363,  2.2268, -3.0029],\n",
      "        [-0.1715,  2.6250, -2.7486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1144, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1705,  0.3202, -3.1038],\n",
      "        [ 2.1929, -0.1978, -2.9636],\n",
      "        [ 2.2536, -0.0551, -2.9212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0864, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1658,  0.7298, -3.1246],\n",
      "        [ 1.9154,  0.6529, -3.6520],\n",
      "        [ 2.3364, -0.0658, -2.9799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3263, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4107,  2.0165, -3.1615],\n",
      "        [ 1.0227,  1.4476, -3.2382],\n",
      "        [ 1.6420,  0.4830, -2.7313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7493, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8915,  0.5610, -2.9651],\n",
      "        [-0.4337,  2.5547, -2.9146],\n",
      "        [ 2.0195,  0.2236, -3.1760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7632, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2347,  2.6122, -2.9376],\n",
      "        [ 1.1430,  1.3889, -3.2069],\n",
      "        [ 2.0980,  0.6710, -3.1724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3494, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0126,  0.5724, -2.9698],\n",
      "        [ 0.6232,  2.1323, -3.0283],\n",
      "        [ 1.4991,  1.3458, -2.9638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4681, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6953,  0.5615, -2.7645],\n",
      "        [ 1.6569,  1.0978, -2.7380],\n",
      "        [-0.1280,  2.2131, -2.8610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5230, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4873,  2.0734, -2.9241],\n",
      "        [ 1.4747,  1.1401, -3.1027],\n",
      "        [ 1.5686,  1.3264, -2.4798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5208, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3157,  1.3416, -2.7090],\n",
      "        [ 1.6875,  1.3036, -2.4566],\n",
      "        [ 1.7321,  0.7212, -2.7655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4447, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4108,  1.1557, -3.0299],\n",
      "        [ 1.3052,  1.1168, -2.6897],\n",
      "        [ 0.2741,  2.2276, -2.4892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5168, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7435,  0.7251, -2.7177],\n",
      "        [ 1.5919,  1.1424, -2.6491],\n",
      "        [ 1.2404,  1.2991, -2.8146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5407, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2273,  2.0823, -3.1652],\n",
      "        [ 1.7626,  1.1523, -2.8990],\n",
      "        [ 1.5973,  0.9318, -2.9081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2525, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3431,  2.6122, -2.7738],\n",
      "        [ 1.8174,  0.8303, -2.9418],\n",
      "        [ 1.7765,  0.8160, -2.7258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3799, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3779,  1.3084, -2.8940],\n",
      "        [ 1.6351,  0.6874, -2.7480],\n",
      "        [-0.1328,  2.5866, -2.9277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3896, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9246,  0.8575, -2.9075],\n",
      "        [ 1.8427,  0.7266, -2.5971],\n",
      "        [ 1.3404,  1.0585, -2.4487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5251, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9776,  0.6560, -2.5666],\n",
      "        [ 0.4863,  2.4106, -2.8687],\n",
      "        [ 1.6967,  0.8801, -2.8437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1346, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2437,  2.7750, -2.5773],\n",
      "        [ 1.8409,  0.8095, -2.5492],\n",
      "        [-0.6431,  2.7358, -2.7329]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0679, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2509,  2.7228, -2.8707],\n",
      "        [-0.0457,  2.5405, -3.1984],\n",
      "        [-0.6031,  2.6196, -2.7267]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1244, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6078,  2.6503, -2.4677],\n",
      "        [ 1.7660,  0.6399, -2.2277],\n",
      "        [-0.7080,  2.8122, -2.3152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2011, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9727,  0.6249, -2.2589],\n",
      "        [ 1.8194,  0.3978, -2.3601],\n",
      "        [ 2.1523,  0.1417, -2.7002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1118, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8747,  0.7119, -2.3411],\n",
      "        [ 2.1208,  0.1650, -2.0901],\n",
      "        [ 1.9932,  0.4588, -2.0318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1674, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3000,  0.3411, -2.3136],\n",
      "        [ 1.8321,  0.2801, -2.6390],\n",
      "        [ 2.1729,  0.3784, -2.7240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2781, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1194,  0.8048, -2.5294],\n",
      "        [ 2.0766,  0.3236, -2.3615],\n",
      "        [ 1.7570,  1.0701, -2.2933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0894, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5503,  2.6477, -2.5630],\n",
      "        [-0.7392,  3.1798, -2.2217],\n",
      "        [ 1.9732,  0.3976, -2.3489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1476, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1098,  0.2640, -2.2492],\n",
      "        [ 1.8920,  0.1511, -2.1842],\n",
      "        [ 2.1712, -0.1234, -2.0403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5013, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7769,  3.0215, -2.3638],\n",
      "        [ 2.3736, -0.0532, -2.2334],\n",
      "        [ 1.8372,  0.7540, -2.5844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2971, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4208,  2.4973, -3.1760],\n",
      "        [-0.5857,  3.0472, -2.3122],\n",
      "        [ 2.3003,  0.0129, -2.2351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0398, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7699,  2.8770, -2.4197],\n",
      "        [-0.3552,  2.6249, -2.2732],\n",
      "        [-0.6139,  2.9731, -2.3434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4836, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7130,  1.8470, -2.5748],\n",
      "        [-0.1050,  2.7303, -2.5634],\n",
      "        [ 2.2103,  0.1682, -2.2842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1343, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8469,  0.5239, -2.5255],\n",
      "        [-0.3095,  2.6774, -2.2740],\n",
      "        [ 2.2603, -0.0911, -2.2466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1781, -0.1084, -1.8849],\n",
      "        [ 1.9359,  0.6064, -2.1097],\n",
      "        [ 2.2513,  0.1181, -2.3762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4487, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2101, -0.0684, -2.2554],\n",
      "        [ 2.1803, -0.1952, -1.7936],\n",
      "        [ 0.7282,  1.4361, -1.8850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2192, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5614,  2.1547, -2.0928],\n",
      "        [ 2.1644,  0.0079, -1.8656],\n",
      "        [ 0.8752,  1.8581, -1.8630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2999, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2410,  1.1372, -1.8349],\n",
      "        [ 2.1910, -0.1800, -1.7960],\n",
      "        [ 2.0768, -0.0662, -1.9329]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8218, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3075, -0.2417, -1.7707],\n",
      "        [ 2.2040,  0.0440, -2.1711],\n",
      "        [ 2.2312, -0.2816, -1.8097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2157, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3279, -0.2632, -1.9626],\n",
      "        [ 0.9463,  1.5581, -1.9874],\n",
      "        [ 1.9967, -0.4784, -1.4353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0805, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0576,  1.1377, -2.0631],\n",
      "        [ 2.3798, -0.0174, -1.6038],\n",
      "        [ 2.4598, -0.4655, -1.8190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0051, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1207, -0.0463, -1.5793],\n",
      "        [ 2.2455, -0.4617, -1.7389],\n",
      "        [ 2.1372, -0.4079, -1.6700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1463, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3058, -0.2023, -1.7826],\n",
      "        [ 1.8328,  0.1140, -1.6768],\n",
      "        [ 2.0759,  0.1520, -1.7408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8314, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0706, -0.3114, -1.7343],\n",
      "        [ 2.0126,  0.3122, -2.0201],\n",
      "        [ 1.9540, -0.1030, -1.5730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6457, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4873,  0.8758, -2.0208],\n",
      "        [ 2.0273, -0.3431, -1.9012],\n",
      "        [ 1.1260,  1.0247, -2.1063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0537, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5173,  0.9942, -1.9774],\n",
      "        [ 1.3444,  0.8197, -1.8863],\n",
      "        [ 1.5953,  0.8685, -1.9904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6485, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3679,  1.5609, -1.8312],\n",
      "        [ 1.5564,  1.1188, -2.0099],\n",
      "        [ 1.3211,  1.3448, -1.8296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5482, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5660,  0.9388, -1.6824],\n",
      "        [ 1.8325,  0.4978, -1.7006],\n",
      "        [ 0.7915,  1.8910, -1.7068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3542, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9033,  0.0748, -1.8851],\n",
      "        [ 1.2392,  1.2199, -2.0119],\n",
      "        [ 2.0651,  0.2891, -1.9201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5527, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8483,  1.7464, -1.8912],\n",
      "        [ 1.0180,  1.1785, -2.0803],\n",
      "        [ 0.9861,  1.4640, -2.0557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2600, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5311,  1.1068, -2.1316],\n",
      "        [ 0.3423,  2.1274, -1.9414],\n",
      "        [ 0.0439,  2.4991, -2.0807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2685, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5103,  1.9647, -1.7977],\n",
      "        [ 1.6832,  0.8331, -1.9738],\n",
      "        [ 0.3369,  1.9307, -1.8706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3982, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6331,  0.5351, -2.1468],\n",
      "        [ 1.2890,  1.2007, -2.0253],\n",
      "        [ 0.0392,  2.1096, -2.0447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5180, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9696,  0.4879, -1.9877],\n",
      "        [ 1.0298,  1.4344, -2.0030],\n",
      "        [ 1.6803,  0.9199, -2.1069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3810, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1137,  2.7864, -2.2187],\n",
      "        [ 1.1333,  1.6233, -2.2558],\n",
      "        [-0.0997,  2.3906, -2.1568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6422, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9305,  1.5211, -2.3752],\n",
      "        [-0.0648,  2.6264, -2.3126],\n",
      "        [ 0.8221,  1.9198, -2.0013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5545, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0202,  2.8682, -2.6370],\n",
      "        [ 0.5029,  1.7871, -2.0394],\n",
      "        [-0.2542,  2.6324, -2.4674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6227, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3197,  1.1542, -2.1814],\n",
      "        [ 0.9838,  1.6284, -2.4455],\n",
      "        [ 0.5774,  2.3671, -2.5855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6495, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6913,  0.7763, -2.7370],\n",
      "        [-0.5990,  2.7632, -2.2797],\n",
      "        [ 0.6324,  1.9452, -2.2630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5056, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4374,  3.0194, -2.5988],\n",
      "        [ 0.9520,  1.8741, -2.1658],\n",
      "        [ 0.5495,  2.0457, -2.2824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4575, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2250,  2.6411, -2.2425],\n",
      "        [ 1.7297,  0.8287, -2.6689],\n",
      "        [-0.0865,  2.7890, -2.5394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1684, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1757,  0.2516, -2.7972],\n",
      "        [ 1.0343,  2.0217, -2.4673],\n",
      "        [-0.3743,  2.9729, -2.6268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1959, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4311,  2.9244, -2.6355],\n",
      "        [ 1.6715,  1.2823, -3.3195],\n",
      "        [-0.4307,  3.2213, -2.6341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0854, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3697,  2.9675, -2.6434],\n",
      "        [-0.3254,  2.8403, -2.6841],\n",
      "        [ 0.4848,  2.2069, -2.4284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1119, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2570,  0.1938, -2.5872],\n",
      "        [ 1.9318,  0.1956, -2.3522],\n",
      "        [-0.3895,  3.0279, -2.7902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1360, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0810,  0.2495, -2.5855],\n",
      "        [ 2.1803,  0.6387, -3.1867],\n",
      "        [-0.3564,  2.6511, -2.5063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1048, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4875,  3.0736, -2.8947],\n",
      "        [-0.2635,  2.9759, -2.6321],\n",
      "        [ 1.9191,  0.5978, -3.0302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9728, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9951,  0.3056, -2.7109],\n",
      "        [ 1.3446,  1.6282, -2.6536],\n",
      "        [ 2.1466,  0.6070, -3.0159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0722, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2206,  0.0721, -2.7525],\n",
      "        [-0.6155,  2.9399, -2.6262],\n",
      "        [ 2.7847,  0.0840, -2.8574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4320, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8112,  1.0452, -2.9470],\n",
      "        [-0.0901,  2.9365, -2.9158],\n",
      "        [ 2.4564,  0.0649, -2.7656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1442, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4092, -0.0851, -2.7301],\n",
      "        [ 0.7832,  2.0335, -2.9992],\n",
      "        [ 2.5317,  0.1366, -2.9377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1510,  0.8616, -3.1684],\n",
      "        [ 2.0049,  0.6866, -3.0789],\n",
      "        [ 1.5729,  1.5291, -3.2137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0908, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0075,  0.3511, -2.7171],\n",
      "        [-0.5954,  2.8866, -2.8839],\n",
      "        [-0.1617,  2.7203, -3.1586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2251,  0.4774, -3.1325],\n",
      "        [-0.4794,  2.7044, -2.8625],\n",
      "        [ 2.1364,  0.9574, -3.0045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0499, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3829,  2.7028, -2.9528],\n",
      "        [-0.2123,  2.7448, -2.9627],\n",
      "        [-0.1046,  2.9655, -2.9698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6266, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3761,  0.2598, -3.1669],\n",
      "        [ 2.0840,  0.5598, -2.8806],\n",
      "        [-0.3863,  2.9979, -3.1122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0701, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2070e-01,  2.7247e+00, -3.2726e+00],\n",
      "        [ 2.5346e+00, -3.1944e-03, -3.1768e+00],\n",
      "        [-1.2388e-01,  2.8719e+00, -3.5145e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0373,  2.6274, -3.2371],\n",
      "        [ 2.2022,  0.5546, -2.9879],\n",
      "        [ 2.6400,  0.1441, -2.9898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1326, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9410,  0.6797, -3.3374],\n",
      "        [ 2.3261,  0.1688, -2.8971],\n",
      "        [-0.6339,  2.9433, -2.9661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2615, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2033,  1.3978, -2.8419],\n",
      "        [ 1.8950,  0.8206, -3.0333],\n",
      "        [ 2.0260,  0.4122, -2.7515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6911, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8040,  2.3238, -3.1363],\n",
      "        [-0.1868,  3.1069, -2.8033],\n",
      "        [ 1.6417,  0.6098, -2.8259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4361, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7815,  1.3696, -3.0658],\n",
      "        [ 1.9414,  0.7142, -3.1944],\n",
      "        [ 2.5749,  0.5136, -3.4446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0988, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1078,  0.7002, -3.2647],\n",
      "        [-0.7016,  2.8141, -2.9892],\n",
      "        [-0.2611,  2.9706, -2.8884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4333, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1748,  1.9803, -3.4594],\n",
      "        [ 1.4083,  1.2674, -2.8812],\n",
      "        [ 1.8594,  0.7661, -2.9652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2352, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8254,  2.1557, -3.2851],\n",
      "        [ 2.1175,  0.7605, -2.8062],\n",
      "        [ 2.3286,  0.9674, -2.8112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5455, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0093,  1.8222, -3.0390],\n",
      "        [ 1.9304,  0.6397, -3.0137],\n",
      "        [ 0.7305,  2.2382, -3.4750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8005, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0991,  0.8561, -3.4441],\n",
      "        [ 0.4350,  2.3251, -3.0556],\n",
      "        [ 2.2996,  0.1089, -3.0483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1651, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2998,  0.5274, -3.1191],\n",
      "        [ 2.1256,  0.7518, -3.1232],\n",
      "        [ 0.3282,  2.5615, -3.1643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5789, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8830,  1.1464, -2.6412],\n",
      "        [ 1.3240,  1.8255, -3.1089],\n",
      "        [ 0.5890,  2.6562, -2.6286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1581, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0815,  0.1421, -2.9835],\n",
      "        [-0.0360,  2.7458, -3.0960],\n",
      "        [ 0.7917,  1.9874, -2.6733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3374, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2714,  1.6225, -2.8767],\n",
      "        [ 1.6488,  0.6328, -3.0806],\n",
      "        [ 2.2542,  0.4589, -3.1594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3508, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9936, -0.0086, -2.5766],\n",
      "        [ 2.4841,  0.7140, -2.8813],\n",
      "        [ 1.2775,  1.1725, -2.8512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2569, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0584,  0.4698, -2.7656],\n",
      "        [-0.0791,  3.1323, -2.6809],\n",
      "        [ 1.1706,  1.5368, -2.6210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2508, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2155,  0.5383, -2.7920],\n",
      "        [ 1.3258,  1.7257, -2.8466],\n",
      "        [ 0.1446,  3.0455, -2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8384, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1885,  0.4252, -3.0931],\n",
      "        [ 1.2350,  1.8468, -2.6540],\n",
      "        [ 2.2256,  0.3608, -2.9650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2282, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7656,  1.1904, -2.7999],\n",
      "        [ 2.1432,  0.3902, -3.0936],\n",
      "        [-0.0168,  2.7014, -2.9093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1959, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1349,  0.0807, -2.8020],\n",
      "        [ 1.8539,  0.7502, -2.7198],\n",
      "        [ 0.4565,  2.2022, -2.8203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3920, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5010,  1.0941, -2.6032],\n",
      "        [-0.2773,  2.9843, -2.7385],\n",
      "        [ 1.9115,  0.4010, -2.6224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2783, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6566,  2.9159, -2.8521],\n",
      "        [ 2.2984,  0.3689, -2.6702],\n",
      "        [ 1.5930,  1.3633, -2.9010]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4755, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7375,  3.3226, -2.5895],\n",
      "        [ 1.8222,  0.9929, -2.9919],\n",
      "        [ 2.1236,  0.6161, -2.3034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7298, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1577,  0.5970, -2.9328],\n",
      "        [ 1.8878,  1.1903, -2.9733],\n",
      "        [-0.4449,  3.3672, -2.8367]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3796, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5170,  1.1209, -2.7888],\n",
      "        [ 0.1088,  2.8628, -2.8995],\n",
      "        [ 2.1832,  0.3487, -2.5951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5720, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6359,  2.2477, -2.9184],\n",
      "        [ 1.9558,  0.9365, -3.4121],\n",
      "        [ 1.9047,  1.0506, -2.4569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2030, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2438,  2.9401, -2.5645],\n",
      "        [ 2.0929,  0.6426, -2.5534],\n",
      "        [ 1.9473,  1.0403, -2.6932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8279, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7353,  2.2560, -2.8343],\n",
      "        [ 2.2936,  0.4694, -2.9197],\n",
      "        [ 1.2353,  1.4447, -2.3601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4132, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3723,  1.2776, -2.9350],\n",
      "        [ 1.1758,  1.8712, -2.9646],\n",
      "        [ 2.0492,  0.3577, -2.7849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7936, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5177,  2.5981, -3.0037],\n",
      "        [-0.0181,  3.1004, -2.6802],\n",
      "        [-0.9057,  3.4159, -3.0015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8368, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5758,  2.4945, -2.2964],\n",
      "        [ 1.7966,  0.8941, -2.6292],\n",
      "        [ 0.1716,  2.5019, -2.5721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1291, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1903,  3.2704, -2.4553],\n",
      "        [-0.3404,  2.9545, -2.8120],\n",
      "        [ 1.7660,  0.8074, -2.6660]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2859, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4600,  1.0749, -2.8624],\n",
      "        [ 0.6937,  2.1257, -2.4677],\n",
      "        [ 2.5786,  0.3741, -2.8558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4335, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4230,  1.3432, -2.8345],\n",
      "        [ 1.4963,  1.1467, -2.7298],\n",
      "        [-0.8777,  3.3447, -2.6070]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4300, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1918,  2.6502, -2.5064],\n",
      "        [ 0.0046,  2.8909, -2.6103],\n",
      "        [ 1.1178,  1.9065, -2.7638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3351, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2623,  0.3387, -2.8992],\n",
      "        [ 0.7428,  2.1007, -2.3362],\n",
      "        [ 1.3932,  1.2284, -2.4440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2097, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7033,  0.2513, -2.7662],\n",
      "        [ 0.8602,  1.5694, -2.4231],\n",
      "        [ 2.2692,  0.2319, -2.4846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6106, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2102,  1.5636, -2.6552],\n",
      "        [ 1.2080,  1.7522, -2.6391],\n",
      "        [ 1.4512,  1.2174, -2.6744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0852, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5615, -0.0521, -2.8752],\n",
      "        [ 2.7391, -0.2799, -2.3879],\n",
      "        [ 2.3754,  0.3349, -2.7377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2042, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2412,  1.8384, -2.6141],\n",
      "        [ 0.2036,  2.4746, -2.5525],\n",
      "        [ 2.6766, -0.1622, -2.4537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1231, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6679,  2.0895, -1.9722],\n",
      "        [ 2.5587, -0.1527, -2.4403],\n",
      "        [ 2.6599, -0.0538, -2.7010]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3360, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8710,  1.4553, -2.2126],\n",
      "        [ 2.2314, -0.2403, -2.4664],\n",
      "        [ 1.5790,  1.0103, -2.5215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1524, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6941, -0.5256, -2.6324],\n",
      "        [ 0.7655,  1.5982, -2.0617],\n",
      "        [ 2.8254, -0.7128, -2.3108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3232, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2544, -0.6850, -2.3675],\n",
      "        [ 0.8498,  1.9150, -2.3864],\n",
      "        [ 1.2415,  1.4594, -2.3292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2098, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0551,  1.9462, -2.3960],\n",
      "        [ 2.5146, -0.6517, -2.0585],\n",
      "        [ 2.6332, -0.2454, -2.7421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1895, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0331,  1.7853, -2.1349],\n",
      "        [ 0.0369,  2.5493, -2.7600],\n",
      "        [ 2.6584,  0.1972, -2.6233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5451, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6198,  1.2267, -2.6581],\n",
      "        [ 0.9443,  2.0178, -2.2430],\n",
      "        [ 0.8880,  1.6007, -2.2221]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1652, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0330,  1.7956, -2.6411],\n",
      "        [ 2.6937, -0.3062, -2.6127],\n",
      "        [ 2.7411, -0.3106, -2.4187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6730, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0150,  0.4804, -3.1008],\n",
      "        [ 0.7009,  2.0566, -2.5730],\n",
      "        [ 2.6410, -0.5118, -2.5246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3670, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6854,  1.9749, -2.0366],\n",
      "        [ 1.8406,  0.5837, -2.8772],\n",
      "        [ 1.3284,  1.5787, -2.3905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7492, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6504,  0.5081, -2.0404],\n",
      "        [ 0.5587,  2.0319, -2.4989],\n",
      "        [ 0.7757,  2.0122, -2.4912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1280, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1374,  2.5027, -1.9667],\n",
      "        [ 0.1949,  2.4436, -2.1534],\n",
      "        [ 2.1660,  0.4629, -2.5963]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4020, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1672,  2.4712, -2.2473],\n",
      "        [ 0.0303,  2.4390, -1.8657],\n",
      "        [ 1.0280,  1.5621, -2.7117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0029, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8723,  1.7744, -2.6366],\n",
      "        [ 0.0693,  2.9924, -2.4529],\n",
      "        [ 0.7175,  2.2079, -2.4741]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6433, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7725,  1.9380, -2.3775],\n",
      "        [ 0.7304,  1.8408, -2.2810],\n",
      "        [ 2.0228,  0.3872, -2.6099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6464, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6123,  2.0956, -2.5025],\n",
      "        [ 0.5224,  2.2925, -2.6881],\n",
      "        [ 2.5260, -0.0392, -2.4935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1584, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4326,  0.8230, -3.1192],\n",
      "        [ 1.8879,  0.5117, -2.3157],\n",
      "        [-0.2354,  2.8120, -2.1905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1122, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4328,  0.2323, -2.7019],\n",
      "        [ 0.3474,  2.0616, -2.5652],\n",
      "        [-0.0984,  2.9225, -2.3665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1525, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9932,  0.6925, -2.6482],\n",
      "        [-0.2192,  3.0975, -2.5618],\n",
      "        [ 0.2783,  2.0573, -2.0483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1873, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2031,  1.7023, -2.5998],\n",
      "        [ 1.0140,  1.8300, -2.7797],\n",
      "        [ 0.0546,  2.6875, -2.8501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1048, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7958,  0.0068, -2.5493],\n",
      "        [ 2.4577, -0.0384, -2.7185],\n",
      "        [ 0.2327,  2.0334, -2.1465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7530, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3174,  0.4903, -2.9627],\n",
      "        [ 0.0602,  2.6151, -2.6275],\n",
      "        [ 0.6104,  2.1695, -2.4925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1168, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5002,  0.3541, -2.9288],\n",
      "        [ 2.3882,  0.0913, -2.7385],\n",
      "        [ 2.4149,  0.4554, -3.3197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4980, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5880,  2.2746, -2.4324],\n",
      "        [ 0.8569,  1.7923, -2.8497],\n",
      "        [-0.3449,  2.9246, -2.1883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0790, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5348,  0.1436, -3.0412],\n",
      "        [ 2.5316, -0.0033, -3.3103],\n",
      "        [ 0.0719,  2.7967, -2.7868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5159, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6473,  2.0655, -2.4504],\n",
      "        [ 0.3161,  2.6468, -2.7249],\n",
      "        [ 2.0484,  1.1759, -3.3053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0908, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5684,  0.1008, -2.9030],\n",
      "        [-0.0200,  2.8862, -2.6531],\n",
      "        [ 2.2901,  0.2954, -3.5480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1385, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2640,  0.8655, -3.3082],\n",
      "        [ 2.5434, -0.0550, -2.8445],\n",
      "        [ 2.2751,  0.1427, -3.2197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1000, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5841,  0.3065, -3.0936],\n",
      "        [ 2.0054,  0.2108, -3.0278],\n",
      "        [-0.1165,  3.1442, -2.9187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2831, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7795e+00,  1.6168e+00, -3.4389e+00],\n",
      "        [ 2.2343e+00,  2.5303e-01, -2.6300e+00],\n",
      "        [ 2.3383e+00, -3.1159e-03, -3.1876e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1148, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3667,  2.6048, -2.7164],\n",
      "        [ 0.0629,  3.0014, -2.7767],\n",
      "        [ 0.6090,  2.2388, -2.8596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5682, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7649,  2.0968, -2.6857],\n",
      "        [ 0.0906,  2.8581, -2.7370],\n",
      "        [ 2.7041,  0.0036, -3.2584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0780, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4994,  0.3240, -3.5636],\n",
      "        [ 0.0430,  2.7346, -2.7923],\n",
      "        [ 2.7083, -0.2199, -3.0476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6535, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0829,  2.5780, -2.7255],\n",
      "        [ 2.2136,  0.6889, -3.0425],\n",
      "        [ 2.0843,  0.2336, -3.1105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1713, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9809,  0.8885, -3.1899],\n",
      "        [ 2.6146,  0.1958, -3.2471],\n",
      "        [ 0.3633,  2.3785, -2.4021]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5255, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9620, -0.3136, -3.1688],\n",
      "        [ 1.7918,  1.6139, -3.3536],\n",
      "        [ 1.4840,  1.3906, -2.8338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0323, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8311, -0.2290, -3.4015],\n",
      "        [ 1.1959,  1.8379, -3.0529],\n",
      "        [ 0.3959,  2.2189, -2.6377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0648, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3397,  0.3257, -3.2465],\n",
      "        [-0.2737,  3.0936, -2.7051],\n",
      "        [-0.4556,  3.1628, -2.7444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3296, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7511, -0.4342, -3.2615],\n",
      "        [ 1.9413,  1.5248, -3.7753],\n",
      "        [ 1.0615,  1.6835, -2.7389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0540, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3120,  3.2322, -2.8488],\n",
      "        [ 0.2189,  2.9600, -2.5351],\n",
      "        [ 2.9831, -0.2313, -3.4825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0387, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0404,  3.1463, -2.6334],\n",
      "        [-0.0159,  3.0712, -3.0085],\n",
      "        [-0.5499,  3.3238, -2.8629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1260, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8963, -0.4925, -2.8795],\n",
      "        [ 0.7552,  1.9515, -3.0108],\n",
      "        [ 0.1522,  2.7890, -2.8959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0170, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4025,  0.4930, -3.1878],\n",
      "        [ 0.4934,  2.7158, -2.8888],\n",
      "        [ 1.4452,  1.8070, -3.2529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4275, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8282, -0.2225, -3.1187],\n",
      "        [ 1.4720,  2.0462, -2.9050],\n",
      "        [ 0.6860,  2.1821, -2.7017]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1121, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9112, -0.4246, -3.3831],\n",
      "        [ 2.3580,  0.6266, -3.6206],\n",
      "        [ 0.4614,  2.4365, -2.8082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6098, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3643,  3.2598, -2.9713],\n",
      "        [ 1.3314,  1.8384, -3.2846],\n",
      "        [ 1.0914,  2.1007, -2.7129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9089, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9659,  1.2509, -3.5639],\n",
      "        [ 0.4225,  2.5807, -2.7561],\n",
      "        [ 0.0052,  2.9533, -2.8527]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0963, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7616,  0.0999, -3.4841],\n",
      "        [-0.6436,  3.1530, -2.8126],\n",
      "        [ 0.7765,  2.3478, -2.5764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8624, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8519,  1.6350, -3.3015],\n",
      "        [ 0.0454,  3.0978, -3.0271],\n",
      "        [ 2.2046,  0.6771, -3.0369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0312, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1393, -0.6870, -3.0501],\n",
      "        [-0.3990,  3.1757, -2.7947],\n",
      "        [ 2.8119, -0.4595, -3.0519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2290, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8782,  1.2413, -3.1171],\n",
      "        [ 0.4737,  2.2274, -3.2528],\n",
      "        [ 2.6600,  0.3325, -3.5607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7509, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2667,  2.6158, -2.8624],\n",
      "        [ 0.6070,  2.3028, -2.5705],\n",
      "        [ 2.1614,  1.0484, -3.4130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1503, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6827,  0.1311, -3.5699],\n",
      "        [ 1.2103,  2.1763, -3.0057],\n",
      "        [ 2.8250, -0.2508, -3.2035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4699, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0962,  1.6609, -3.0232],\n",
      "        [ 2.9368, -0.2649, -3.1702],\n",
      "        [ 0.9350,  1.8376, -2.8608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6887, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3996,  1.8998, -2.8963],\n",
      "        [ 2.1417,  0.9204, -3.1421],\n",
      "        [ 2.6859,  0.4459, -3.2908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0900, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6224,  2.3549, -2.9181],\n",
      "        [ 2.5908, -0.0361, -3.4066],\n",
      "        [ 3.0444, -0.4767, -3.3109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2001, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0652,  1.8768, -2.7967],\n",
      "        [ 2.5008,  0.2756, -3.5351],\n",
      "        [ 2.3071,  0.2209, -2.9878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0590, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8701, -0.3028, -3.3489],\n",
      "        [-0.0889,  3.1018, -2.8322],\n",
      "        [ 0.3949,  2.7809, -2.8975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1441, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7574,  2.1611, -3.0910],\n",
      "        [ 0.2928,  2.6755, -3.3950],\n",
      "        [ 0.5277,  2.6373, -2.9195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1370, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3118,  2.9936, -2.9854],\n",
      "        [ 0.8131,  2.4194, -2.8798],\n",
      "        [ 2.3996,  0.6027, -3.5999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0747, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6523,  0.2027, -3.3796],\n",
      "        [ 2.8791,  0.0669, -3.7244],\n",
      "        [ 2.7069,  0.1840, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2072, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0834,  3.2102, -2.9051],\n",
      "        [ 0.0855,  2.8657, -3.1055],\n",
      "        [ 1.5440,  1.1481, -3.1147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1484, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6445,  3.2841, -2.9032],\n",
      "        [ 3.0846, -0.5743, -3.5429],\n",
      "        [ 1.8724,  1.1417, -3.3028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2195, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8043,  0.3262, -3.2558],\n",
      "        [ 1.7238,  1.0549, -3.2764],\n",
      "        [ 0.7291,  2.5235, -2.8885]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0926, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8318,  2.4365, -2.7850],\n",
      "        [ 3.0356, -0.5789, -2.5735],\n",
      "        [ 2.7499, -0.0664, -3.3933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0496, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4851,  0.2037, -3.4957],\n",
      "        [ 3.2126, -0.6316, -3.3568],\n",
      "        [-0.2946,  3.3707, -3.1296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8747, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3812, -0.0547, -3.0650],\n",
      "        [ 3.1311, -0.8301, -2.8559],\n",
      "        [ 0.4262,  2.9577, -2.9732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2304, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3700, -1.1081, -3.0808],\n",
      "        [ 2.9569, -0.1332, -3.6124],\n",
      "        [ 1.4267,  1.5690, -2.6257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3811, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8073,  1.1329, -3.4874],\n",
      "        [ 3.0268, -0.6224, -2.9487],\n",
      "        [-0.8431,  3.0233, -2.2338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5669,  3.4776, -2.9348],\n",
      "        [-0.7958,  3.2871, -3.0868],\n",
      "        [-0.9264,  2.9087, -2.6881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0615, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9862, -0.8173, -2.9848],\n",
      "        [ 0.4543,  2.7058, -3.2802],\n",
      "        [ 2.7647, -0.0959, -3.4196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9777, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2093, -0.7244, -2.9849],\n",
      "        [ 0.1559,  2.4164, -2.5963],\n",
      "        [ 1.1388,  1.4737, -3.0597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0305, grad_fn=<NllLossBackward0>), logits=tensor([[-8.4305e-01,  3.1616e+00, -2.7382e+00],\n",
      "        [-7.7189e-01,  3.3315e+00, -2.6785e+00],\n",
      "        [ 2.9608e+00,  3.4320e-03, -3.6180e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4448, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0113e+00,  1.6026e+00, -2.8873e+00],\n",
      "        [ 3.0127e+00, -1.6817e-03, -3.4468e+00],\n",
      "        [ 2.1726e+00,  8.8117e-01, -3.3502e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1284, -0.6691, -2.8707],\n",
      "        [ 2.9563, -0.8447, -2.9646],\n",
      "        [ 2.9396, -0.4256, -3.3508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3102, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7857,  1.1137, -3.1650],\n",
      "        [ 1.8050,  1.3291, -3.0382],\n",
      "        [-0.7846,  3.0055, -2.8463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2544, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1301,  2.1518, -2.4716],\n",
      "        [ 2.6614,  0.2506, -3.5643],\n",
      "        [ 1.3593,  1.7007, -2.9856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0636, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8447,  3.1704, -2.6503],\n",
      "        [ 2.3726,  0.5759, -3.4498],\n",
      "        [-1.0467,  3.3870, -2.7181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1700, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0998,  2.9713, -2.7389],\n",
      "        [-0.5146,  3.2929, -2.7949],\n",
      "        [ 1.6301,  0.9812, -2.8909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4710, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7991,  0.2077, -3.7190],\n",
      "        [ 0.4696,  2.5403, -2.6309],\n",
      "        [ 1.0200,  1.8742, -2.7613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0778, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6143,  0.1269, -2.9877],\n",
      "        [-0.8581,  3.3672, -2.5128],\n",
      "        [ 0.2587,  2.2780, -2.4033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2919, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9972, -0.2213, -3.0832],\n",
      "        [ 2.4591,  0.3496, -3.2914],\n",
      "        [ 1.6140,  1.5783, -2.8369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0707, -0.5979, -2.9036],\n",
      "        [ 3.2029, -1.3306, -2.5485],\n",
      "        [-0.9050,  3.1232, -2.5713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0331, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2029,  2.9523, -2.5857],\n",
      "        [-0.8688,  3.4893, -2.6780],\n",
      "        [ 3.2562, -0.7940, -3.1959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0927, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3402,  0.9967, -3.6732],\n",
      "        [ 2.7883, -1.0791, -2.5618],\n",
      "        [-0.8482,  3.2550, -2.6996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0359, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0138,  3.4132, -2.5646],\n",
      "        [ 3.1898, -1.2647, -2.6648],\n",
      "        [ 0.1104,  2.6548, -3.0097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9366, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0295, -1.2576, -2.4748],\n",
      "        [-0.9342,  3.1483, -2.6498],\n",
      "        [ 2.7242,  0.0185, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0371, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1248, -1.2567, -2.4254],\n",
      "        [-0.9818,  3.4571, -2.4557],\n",
      "        [ 0.0690,  2.6180, -2.5369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0687, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7836,  3.2315, -2.7276],\n",
      "        [ 2.8072, -0.2099, -2.9257],\n",
      "        [ 2.4399,  0.4808, -3.2568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0310, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9020, -0.4147, -3.0844],\n",
      "        [-0.1543,  3.1458, -2.7703],\n",
      "        [ 3.2087, -1.2243, -2.2511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4790, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1632,  3.4618, -2.4611],\n",
      "        [-1.2277,  2.8982, -2.4691],\n",
      "        [ 2.1789,  1.0612, -3.2297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1935, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5778,  3.1002, -2.6752],\n",
      "        [-0.6936,  3.3365, -2.6178],\n",
      "        [ 1.5501,  1.1894, -3.6209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0712, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9778,  3.4068, -2.3835],\n",
      "        [ 3.0694,  0.1536, -3.5709],\n",
      "        [ 2.1438,  0.2583, -3.5773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0228, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6914,  3.4371, -2.6640],\n",
      "        [-0.6689,  2.7559, -2.6165],\n",
      "        [-0.9611,  3.4903, -2.7089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0164, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1697,  3.2510, -2.2805],\n",
      "        [-0.8671,  3.0836, -2.4888],\n",
      "        [-1.4268,  3.4083, -2.5303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5877, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6469,  1.1874, -2.7938],\n",
      "        [-0.8340,  3.4038, -2.7754],\n",
      "        [-1.1823,  3.4600, -2.5340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0971, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3301,  3.6438, -2.5382],\n",
      "        [ 1.7919,  0.5894, -3.0503],\n",
      "        [-1.0000,  3.4637, -2.5985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5748, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7700,  1.8593, -2.6773],\n",
      "        [ 1.9561,  0.8834, -3.2897],\n",
      "        [ 1.1540,  1.8838, -3.0322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0091, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0987,  2.6829, -2.6610],\n",
      "        [-0.1079,  3.2829, -2.8512],\n",
      "        [ 0.9034,  1.8622, -2.8814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4096, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0126,  3.1030, -2.6685],\n",
      "        [ 0.7567,  1.4880, -2.3369],\n",
      "        [ 2.9002, -0.2906, -3.1939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1295, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1009,  2.7372, -2.7363],\n",
      "        [-0.7741,  3.1113, -2.5650],\n",
      "        [ 1.9420,  0.8396, -3.0510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4101, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1697,  1.7802, -2.5929],\n",
      "        [ 2.4208,  0.4990, -3.3195],\n",
      "        [ 2.9937, -0.2957, -3.2697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1081, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7915, -0.0451, -3.5045],\n",
      "        [ 2.3073,  0.5981, -2.9788],\n",
      "        [ 0.2217,  2.5982, -2.3887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5864, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3653,  1.3269, -2.7830],\n",
      "        [ 1.6873,  1.0536, -2.9075],\n",
      "        [ 3.3387, -1.3569, -2.9000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0936, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9066, -0.4372, -3.2520],\n",
      "        [ 3.0990, -0.5049, -3.2864],\n",
      "        [ 0.5828,  2.0547, -2.4339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2050, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9623,  0.7457, -2.7220],\n",
      "        [ 2.4217,  0.4354, -3.0597],\n",
      "        [ 3.1700, -1.3043, -3.0167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4496, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0435,  1.7004, -2.4347],\n",
      "        [ 1.2450,  1.3468, -2.6896],\n",
      "        [ 0.8361,  2.0523, -2.4613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5606, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2295,  2.5819, -3.0784],\n",
      "        [ 1.3849,  1.1550, -2.4145],\n",
      "        [ 1.3579,  1.2425, -2.8073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5596, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5191,  0.2607, -3.3801],\n",
      "        [ 2.4769,  0.3603, -3.1772],\n",
      "        [ 2.1029,  0.9134, -2.9390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1377, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4648, -1.4414, -2.9519],\n",
      "        [ 0.7071,  1.5296, -2.0389],\n",
      "        [ 3.0409, -0.9545, -2.9745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3424, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4316,  1.2935, -2.4914],\n",
      "        [ 0.8082,  1.6884, -2.1042],\n",
      "        [ 3.2368, -0.4040, -3.1061]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3065, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7642,  1.9452, -2.4909],\n",
      "        [ 0.0599,  2.8874, -2.8641],\n",
      "        [ 1.3710,  1.1097, -2.5343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4038, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2282,  0.9622, -3.2284],\n",
      "        [ 0.8454,  1.3521, -2.0321],\n",
      "        [ 1.7653,  1.2255, -2.5914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2987, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0430,  0.5216, -3.0538],\n",
      "        [ 2.2339,  0.7656, -3.3819],\n",
      "        [ 0.8963,  1.4278, -1.9643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1581, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8397, -0.5214, -3.2704],\n",
      "        [ 0.6538,  2.4316, -2.4728],\n",
      "        [ 0.5868,  1.7855, -2.3694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1044, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8603, -0.3451, -3.4042],\n",
      "        [ 3.1412, -0.7080, -3.0431],\n",
      "        [ 2.0090,  0.7261, -3.3135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3426, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5155,  1.2082, -2.9787],\n",
      "        [-0.3402,  3.1122, -2.8469],\n",
      "        [ 1.0925,  1.7113, -3.0430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0805, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0184,  3.1893, -2.8787],\n",
      "        [-0.3600,  3.4446, -2.8753],\n",
      "        [ 0.4584,  2.1595, -2.4672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3164, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6012,  1.3026, -3.1124],\n",
      "        [ 3.0037, -0.5982, -3.2737],\n",
      "        [ 0.1143,  2.9109, -3.0011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3647, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1184,  2.9121, -2.8567],\n",
      "        [ 3.0854, -1.6554, -2.1524],\n",
      "        [ 1.0372,  1.6123, -2.7109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3198, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1489,  2.8228, -2.5720],\n",
      "        [ 0.7175,  1.6922, -2.6633],\n",
      "        [ 1.5818,  1.3212, -3.2720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5358, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5641, -1.4842, -3.1221],\n",
      "        [ 0.9325,  2.2508, -3.3590],\n",
      "        [ 2.9375, -0.2610, -3.4809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1055, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4110,  3.1857, -3.0007],\n",
      "        [ 2.7572, -0.3717, -3.3292],\n",
      "        [ 2.0412,  0.7299, -3.2805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2199, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1419,  2.7714, -2.6924],\n",
      "        [ 3.2546, -1.6415, -2.8407],\n",
      "        [ 1.3056,  1.5681, -2.9657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1700, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9516, -1.9572, -2.4247],\n",
      "        [ 1.6182,  1.1143, -3.2073],\n",
      "        [-0.5305,  3.4485, -2.7956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5164,  3.3114, -2.8891],\n",
      "        [ 3.2530, -1.1154, -3.2376],\n",
      "        [-0.7943,  3.4031, -3.0053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3008, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4405,  0.5963, -3.7488],\n",
      "        [-0.0329,  2.9831, -3.0366],\n",
      "        [-0.2411,  3.4365, -3.2809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0255, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2049, -0.7948, -3.4561],\n",
      "        [ 3.1043, -1.4938, -2.8365],\n",
      "        [ 0.0132,  3.1447, -3.2284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0056, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0245,  3.1739, -3.2203],\n",
      "        [ 3.1196, -1.8982, -2.3826],\n",
      "        [ 2.7761, -0.1312, -3.3759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0290, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9358,  0.2527, -4.0860],\n",
      "        [ 3.4630, -1.7229, -2.3457],\n",
      "        [ 3.3826, -1.2027, -3.3156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0207, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4317, -1.7180, -2.5332],\n",
      "        [-0.0185,  3.1458, -3.0007],\n",
      "        [ 3.4588, -1.2288, -3.2609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1520, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6302,  1.0154, -3.3715],\n",
      "        [ 3.3903, -1.5833, -2.8688],\n",
      "        [ 3.2427, -1.6964, -2.4074]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2617,  3.1978, -3.0327],\n",
      "        [ 3.2593, -0.9982, -3.7363],\n",
      "        [ 3.5856, -1.4760, -3.1262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1904, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7348,  1.9618, -2.7172],\n",
      "        [ 0.8603,  2.2393, -3.1638],\n",
      "        [ 0.2319,  2.7665, -3.1993]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7620, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0228,  3.0738, -2.9943],\n",
      "        [ 2.4388,  0.3356, -3.7280],\n",
      "        [ 3.1760, -0.7984, -3.8728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3383, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3949, -1.2315, -3.3616],\n",
      "        [ 1.1577,  1.6818, -3.0459],\n",
      "        [ 3.3867, -1.5367, -2.8285]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0420, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1539e+00, -5.0131e-01, -3.6183e+00],\n",
      "        [ 1.2482e-03,  2.9132e+00, -3.3080e+00],\n",
      "        [ 5.6302e-02,  3.1806e+00, -3.3244e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0747, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4531,  2.9165, -3.6401],\n",
      "        [-0.0222,  2.8963, -3.2299],\n",
      "        [ 2.5897,  0.1636, -3.6506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0419, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2067,  2.7033, -3.3466],\n",
      "        [ 3.2461, -0.4151, -4.0562],\n",
      "        [ 3.2201, -0.8362, -3.5049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0832, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6511,  2.6958, -3.4063],\n",
      "        [ 2.8796,  0.2198, -3.7703],\n",
      "        [ 0.0741,  2.9410, -3.3011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1699, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9404,  1.0558, -3.2031],\n",
      "        [ 2.6124,  0.3050, -3.9067],\n",
      "        [ 0.2201,  2.9660, -3.4616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1757, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1071,  3.4234, -3.4863],\n",
      "        [ 1.2549,  1.8467, -2.9215],\n",
      "        [ 0.0169,  2.9925, -3.3333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0766,  0.7687, -3.5811],\n",
      "        [ 1.8296,  1.3590, -3.3764],\n",
      "        [ 2.4996,  0.4521, -3.7475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1470, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0585,  3.2395, -3.2981],\n",
      "        [ 2.0198,  0.9986, -3.3022],\n",
      "        [ 2.6037,  0.2519, -4.0189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1045, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4121,  3.3797, -3.2440],\n",
      "        [ 3.0290,  0.0394, -3.9127],\n",
      "        [ 0.7622,  2.0935, -2.8820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1067, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7049,  2.3520, -3.2575],\n",
      "        [ 2.7598,  0.1841, -3.8344],\n",
      "        [-0.0331,  2.6911, -3.1332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1792, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7860,  1.2838, -3.2914],\n",
      "        [-0.1326,  3.2440, -3.2106],\n",
      "        [-0.2609,  3.4323, -3.6795]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1554, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5193,  0.4548, -4.3160],\n",
      "        [-0.1487,  2.9406, -3.1886],\n",
      "        [ 1.9823,  0.9137, -3.2995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2226, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9875,  1.0851, -3.4063],\n",
      "        [ 1.8172,  0.7464, -3.2644],\n",
      "        [ 3.1024, -0.6352, -3.6659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0282, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4218, -1.0578, -3.2977],\n",
      "        [-0.6442,  3.5398, -3.2567],\n",
      "        [ 0.0592,  2.9626, -3.0172]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.1002, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0584,  3.0285, -3.2765],\n",
      "        [-0.0521,  3.1524, -3.2359],\n",
      "        [-0.2514,  3.2078, -3.1020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6247, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3439,  3.4521, -3.3899],\n",
      "        [ 0.7181,  2.2847, -3.4043],\n",
      "        [ 0.3560,  2.7224, -3.2672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0549, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5415, -1.2487, -3.4885],\n",
      "        [ 0.4977,  2.4611, -3.1580],\n",
      "        [-0.5825,  3.3535, -3.1114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4674, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0256,  3.0673, -3.3982],\n",
      "        [ 1.8567,  0.9314, -3.5192],\n",
      "        [ 2.6283,  0.2921, -3.7165]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5520, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4643, -0.8393, -3.6729],\n",
      "        [ 2.3629,  0.6456, -3.5826],\n",
      "        [ 1.9354,  0.7278, -3.0508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3692, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5357, -1.8013, -2.7905],\n",
      "        [ 0.8591,  1.5348, -3.3214],\n",
      "        [ 3.4723, -1.5314, -2.6114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3128, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0606,  1.7574, -3.0731],\n",
      "        [ 0.9759,  1.3877, -2.8481],\n",
      "        [ 3.4409, -1.0737, -3.3564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0254, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2622,  3.0263, -3.5656],\n",
      "        [ 3.6203, -1.4684, -3.1517],\n",
      "        [ 3.5130, -1.8161, -2.9350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0883, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3946, -1.4973, -2.7784],\n",
      "        [ 0.9590,  2.2466, -3.2616],\n",
      "        [ 3.4744, -1.4176, -3.2847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7341, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4640,  2.4355, -3.6860],\n",
      "        [ 0.5324,  2.1574, -3.4242],\n",
      "        [ 2.4292,  0.7086, -3.6707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3936, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6856,  1.5242, -3.3138],\n",
      "        [ 0.9642,  1.9278, -3.4564],\n",
      "        [ 0.7732,  2.1171, -3.3796]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4323, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1492,  1.6388, -2.9336],\n",
      "        [ 1.4411,  1.2372, -3.3781],\n",
      "        [ 3.5365, -1.4206, -3.4914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0762, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6341,  2.9041, -3.6168],\n",
      "        [ 3.3821, -1.3792, -2.7651],\n",
      "        [ 0.4468,  2.5408, -3.5370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7110, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8463,  2.6750, -3.3752],\n",
      "        [ 1.7437,  1.3809, -3.2967],\n",
      "        [ 0.9805,  2.1600, -3.5239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1262, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3787,  3.1684, -3.4803],\n",
      "        [ 1.9380,  0.7204, -3.1156],\n",
      "        [ 0.1736,  3.0992, -3.5670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0773, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0946,  2.8408, -3.1889],\n",
      "        [ 0.4502,  2.4055, -3.1878],\n",
      "        [ 3.0887, -0.3734, -3.7753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0901, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4025,  0.6031, -3.4302],\n",
      "        [ 3.4415, -0.8093, -3.2645],\n",
      "        [ 0.4979,  2.7780, -3.3476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6712, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9354,  1.7955, -3.0568],\n",
      "        [ 1.7672,  1.4936, -3.4647],\n",
      "        [ 0.7413,  2.1363, -2.9302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2706, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7978,  1.1556, -3.1542],\n",
      "        [ 0.8665,  1.8657, -3.1699],\n",
      "        [ 0.1236,  2.8265, -3.6493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0811, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3421,  2.8785, -3.5129],\n",
      "        [ 0.0625,  2.9881, -3.6344],\n",
      "        [ 0.4214,  2.5762, -3.4050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0905, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6734,  2.2706, -3.3558],\n",
      "        [ 3.2368, -1.0010, -3.4727],\n",
      "        [ 2.8097,  0.1449, -3.8158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1247, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8693,  1.0257, -3.1028],\n",
      "        [ 0.5957,  2.8301, -3.4764],\n",
      "        [ 2.8888,  0.0380, -3.9141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2721, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5292,  0.6502, -3.6399],\n",
      "        [ 2.8169, -0.1144, -3.3370],\n",
      "        [ 1.4137,  1.2448, -3.0609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8599, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2008,  3.0019, -3.3908],\n",
      "        [ 2.0671,  1.0038, -3.4940],\n",
      "        [ 1.1124,  1.9126, -3.1639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4983, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9131,  2.0567, -2.9968],\n",
      "        [ 3.4738, -0.7873, -3.7800],\n",
      "        [ 0.0989,  3.0015, -3.2856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3151, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2291,  3.1534, -3.4199],\n",
      "        [ 1.7455,  1.1866, -3.1899],\n",
      "        [ 1.2356,  1.8521, -3.4292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2921, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9406,  1.6697, -3.2325],\n",
      "        [ 1.1764,  1.8228, -3.1366],\n",
      "        [ 2.9641,  0.0087, -3.8591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4472, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1649,  0.7359, -3.7317],\n",
      "        [ 2.0268,  1.2835, -3.3042],\n",
      "        [ 1.4718,  1.4033, -3.3131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3381, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1862,  3.1214, -3.5785],\n",
      "        [ 1.5200,  1.8347, -3.4708],\n",
      "        [ 2.7317,  0.4166, -3.7534]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0416, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9286,  2.0025, -3.2090],\n",
      "        [ 2.1882,  0.8853, -3.8123],\n",
      "        [ 2.1398,  0.6578, -3.5334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4764, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7219, -0.2766, -3.4492],\n",
      "        [ 1.1526,  1.9371, -3.4031],\n",
      "        [ 0.9859,  2.4224, -3.8585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1446, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2848,  0.6608, -3.4867],\n",
      "        [-0.2315,  3.2142, -3.5024],\n",
      "        [ 0.9844,  2.4051, -3.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4086, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6945,  0.3086, -3.6169],\n",
      "        [ 1.2954,  1.7610, -3.3943],\n",
      "        [ 2.2948,  0.6514, -3.2738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4048, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2330,  3.4575, -3.8779],\n",
      "        [ 1.7215,  1.3462, -3.6314],\n",
      "        [ 1.2419,  2.3452, -3.8169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0928, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4984,  2.8036, -4.0412],\n",
      "        [ 0.2674,  2.8544, -3.8671],\n",
      "        [ 2.5774,  0.3896, -3.3872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2889, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8467, -0.2617, -3.6119],\n",
      "        [ 1.3863,  1.6705, -3.2273],\n",
      "        [ 1.9967,  0.7539, -3.6731]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2688, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2244,  2.0450, -3.9671],\n",
      "        [-0.0429,  2.7105, -3.9341],\n",
      "        [ 2.0934,  1.3063, -3.7938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1668, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9797,  2.4656, -3.8805],\n",
      "        [-0.1143,  2.8195, -3.3861],\n",
      "        [ 2.0643,  0.7520, -3.5904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8265, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1874,  3.1126, -3.9516],\n",
      "        [ 1.9427,  0.9499, -3.4148],\n",
      "        [ 1.7985,  1.0852, -3.6884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3741, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0530,  0.7207, -3.8406],\n",
      "        [ 1.6727,  1.3704, -3.9126],\n",
      "        [-0.3049,  3.2803, -3.9685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1004, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6551, -1.1520, -3.5363],\n",
      "        [ 1.9819,  0.8177, -3.6897],\n",
      "        [ 3.1906, -0.8938, -3.4684]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0465, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4041,  2.8249, -4.0424],\n",
      "        [ 3.0997, -0.4932, -4.1016],\n",
      "        [-0.8060,  2.9153, -3.5170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0739, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6240,  2.9263, -3.4261],\n",
      "        [ 2.3273,  0.6208, -3.9630],\n",
      "        [-0.4864,  3.2914, -3.7816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0249, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4259, -0.3953, -4.0047],\n",
      "        [-0.8701,  2.7840, -3.1283],\n",
      "        [ 3.3345, -0.4052, -3.8118]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3266, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7512e+00,  1.4078e+00, -3.8924e+00],\n",
      "        [ 1.3057e-03,  3.3231e+00, -4.0124e+00],\n",
      "        [ 2.6381e+00,  1.8290e-01, -3.8978e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4672, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6875,  3.2165, -3.3654],\n",
      "        [ 1.0996,  2.4931, -4.0554],\n",
      "        [ 0.3625,  3.0606, -4.0580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1106, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3660, -1.2164, -3.2685],\n",
      "        [ 2.4063,  0.3608, -3.8715],\n",
      "        [ 2.2608,  0.7263, -3.8089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5710, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5081,  3.0706, -3.4871],\n",
      "        [ 2.2934,  0.9373, -3.8352],\n",
      "        [ 2.5858,  0.2902, -4.1496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8744, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5134,  3.0507, -3.5176],\n",
      "        [ 1.4545,  1.4895, -3.5068],\n",
      "        [ 2.3574,  0.6041, -3.8828]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2831, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2202, -0.5207, -3.7938],\n",
      "        [ 3.4980, -1.7133, -2.8597],\n",
      "        [ 2.7538,  0.2083, -3.7987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3729, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5574,  1.0710, -3.6988],\n",
      "        [ 0.3157,  2.7286, -3.9044],\n",
      "        [ 0.1306,  2.8837, -4.0017]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3673, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6117,  1.6071, -3.4863],\n",
      "        [ 2.3794,  1.0203, -3.5964],\n",
      "        [ 2.2704,  0.5896, -3.8200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1212, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4405,  3.0554, -3.9344],\n",
      "        [ 0.8589,  2.2927, -3.6257],\n",
      "        [ 2.6379,  0.0834, -3.9740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2181, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5014,  2.8454, -3.8051],\n",
      "        [ 1.9544,  1.0042, -3.6192],\n",
      "        [ 0.9495,  2.3007, -3.8125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3297, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4216,  0.8459, -3.9979],\n",
      "        [ 1.3300,  1.4667, -3.6159],\n",
      "        [ 2.4061,  0.7061, -3.8372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6473, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4725,  1.9627, -3.5873],\n",
      "        [ 1.8090,  1.1440, -3.4739],\n",
      "        [ 1.8489,  1.2439, -3.6793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3010, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0267,  2.2024, -3.6504],\n",
      "        [ 1.9258,  1.0563, -3.5748],\n",
      "        [ 2.0046,  0.8604, -3.7287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3661,  1.5744, -3.2587],\n",
      "        [ 1.2263,  2.1157, -3.2264],\n",
      "        [ 1.9110,  0.8432, -4.0103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4600, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4379,  1.1916, -3.3523],\n",
      "        [ 1.5544,  0.9047, -3.6099],\n",
      "        [ 1.2173,  2.0174, -3.5072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1645, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0212,  3.1139, -3.8730],\n",
      "        [ 0.2008,  2.9602, -3.9014],\n",
      "        [ 1.8062,  1.0435, -3.6360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4302, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6707,  1.2175, -3.7102],\n",
      "        [ 2.4667,  0.4270, -3.7903],\n",
      "        [ 2.2321,  0.8147, -4.0284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3037, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7974,  0.9980, -3.5156],\n",
      "        [-0.0997,  2.9844, -3.8707],\n",
      "        [ 1.6526,  1.1877, -3.6376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3304, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2249,  3.0986, -3.8891],\n",
      "        [ 2.8162,  0.2209, -4.0800],\n",
      "        [ 1.3047,  1.6457, -3.3424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2366, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3129,  2.6286, -3.8182],\n",
      "        [ 1.9724,  1.1748, -3.6008],\n",
      "        [ 2.1463,  0.8317, -3.9732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4957, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3060,  0.6979, -3.6611],\n",
      "        [ 2.3632,  0.6469, -3.7056],\n",
      "        [ 1.2112,  1.9537, -3.3840]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2954, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0951,  1.6912, -3.4848],\n",
      "        [ 1.0819,  2.0018, -3.6073],\n",
      "        [ 2.4314,  0.2200, -3.8372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2945, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3582,  2.6993, -3.4915],\n",
      "        [ 1.4390,  1.3819, -3.0546],\n",
      "        [ 0.1250,  2.8985, -3.5949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3275, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8281,  1.5912, -3.4659],\n",
      "        [ 1.7997,  0.8006, -3.4191],\n",
      "        [ 0.4270,  2.9162, -4.0094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2354,  0.6259, -3.6628],\n",
      "        [ 2.2890,  0.7264, -3.5970],\n",
      "        [ 3.1378, -0.1743, -3.8730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1674, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5017,  0.4093, -3.5428],\n",
      "        [ 2.5054,  0.4231, -3.7014],\n",
      "        [ 0.7709,  1.9767, -3.7227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3160, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5226,  0.2240, -3.8421],\n",
      "        [ 3.0205, -0.1866, -3.5271],\n",
      "        [ 1.4948,  1.2792, -3.7127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0639, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7120,  3.3992, -3.6074],\n",
      "        [ 0.6273,  2.5587, -3.8845],\n",
      "        [-0.1516,  3.1348, -3.7373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2993, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1845,  1.8141, -3.3511],\n",
      "        [ 0.4957,  2.4964, -3.6947],\n",
      "        [ 1.1496,  2.0666, -3.7531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0273, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1938, -0.3689, -3.7228],\n",
      "        [ 3.2635, -0.2758, -3.5846],\n",
      "        [-0.3182,  3.4731, -3.5203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3602, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4941,  1.2013, -3.1817],\n",
      "        [ 2.8973, -0.0598, -3.5327],\n",
      "        [ 2.1527,  0.4682, -3.6425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2184, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1784, -0.2987, -3.7292],\n",
      "        [ 1.9162,  1.0776, -3.6167],\n",
      "        [ 2.0212,  0.8036, -3.7347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2198, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3639, -0.6055, -3.5908],\n",
      "        [ 1.6929,  1.3516, -3.5929],\n",
      "        [ 0.4101,  2.6771, -3.8705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0372, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2671,  2.8629, -3.9421],\n",
      "        [ 3.2234, -0.5109, -3.8271],\n",
      "        [-0.9358,  3.4216, -3.1742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1137, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9440,  2.8934, -3.1259],\n",
      "        [-1.0423,  3.1101, -3.1605],\n",
      "        [ 0.9583,  2.0192, -3.6073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0227, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1517, -0.0616, -4.0301],\n",
      "        [-1.0638,  3.2385, -3.2555],\n",
      "        [-0.9155,  3.5212, -3.2803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1192,  3.1797, -3.7481],\n",
      "        [-0.7561,  3.2386, -3.6686],\n",
      "        [ 3.6638, -0.7702, -3.6386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0740, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0913, -0.3407, -3.7906],\n",
      "        [ 3.6263, -1.2120, -3.4088],\n",
      "        [ 2.6342,  1.0074, -4.0624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9095, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2984,  1.1300, -3.8075],\n",
      "        [-0.2413,  3.1344, -4.1805],\n",
      "        [ 2.5588,  0.2316, -3.8871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2041, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4168, -0.7541, -3.8391],\n",
      "        [ 3.0013, -0.5306, -3.7465],\n",
      "        [ 3.2078, -0.1604, -4.2764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1616,  3.1743, -2.8045],\n",
      "        [-1.2111,  3.3034, -3.0357],\n",
      "        [ 3.3240, -0.7101, -3.7051]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1167, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0697,  2.9792, -3.0931],\n",
      "        [ 3.4258, -0.9482, -3.9376],\n",
      "        [ 0.9099,  1.9031, -3.8292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8743, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4280,  0.8338, -3.9549],\n",
      "        [-1.2046,  3.1798, -2.9180],\n",
      "        [ 1.4473,  1.6952, -3.5676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8430, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8291,  2.2596, -3.7719],\n",
      "        [ 3.4616, -0.9446, -3.7202],\n",
      "        [ 1.4074,  1.7305, -4.1602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1150, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1519,  0.7391, -3.6265],\n",
      "        [ 2.4077,  0.2449, -3.9110],\n",
      "        [-1.1936,  3.2010, -3.0886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0422, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8530, -0.4657, -3.6922],\n",
      "        [ 0.0890,  2.8773, -3.9380],\n",
      "        [ 3.1271, -0.4279, -3.9026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2573, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1533,  0.6664, -3.6635],\n",
      "        [ 1.7675,  1.2218, -3.9733],\n",
      "        [ 0.2665,  2.7452, -3.6592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7224, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0799,  0.7800, -3.6177],\n",
      "        [-1.3772,  3.2084, -2.6804],\n",
      "        [ 1.1828,  1.3635, -3.5902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0596, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2978,  0.0484, -4.0982],\n",
      "        [ 2.1945,  0.1492, -3.5280],\n",
      "        [-0.9337,  3.3113, -3.3173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2879, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0442,  3.4129, -3.0961],\n",
      "        [ 3.2997, -0.6375, -4.0199],\n",
      "        [ 1.4965,  1.2441, -3.6101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4575, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8108,  2.1473, -3.7013],\n",
      "        [ 1.4422,  1.4971, -3.7817],\n",
      "        [ 1.6748,  0.9996, -4.2489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0265, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0052, -0.0108, -3.6778],\n",
      "        [-0.8667,  3.4288, -3.2541],\n",
      "        [-0.7087,  3.5028, -3.4923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0587, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2973,  3.1303, -3.7873],\n",
      "        [ 2.1727,  0.1721, -3.5541],\n",
      "        [-0.5911,  3.7622, -3.3679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6382, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7341,  2.3639, -3.6946],\n",
      "        [-0.0536,  2.5955, -3.7850],\n",
      "        [-0.0827,  3.3144, -3.5692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8242, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2194,  2.9204, -3.8464],\n",
      "        [-1.3572,  3.6348, -2.9957],\n",
      "        [ 0.2492,  2.5504, -3.7228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1400, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1308,  3.2563, -3.2651],\n",
      "        [-0.9678,  3.5234, -3.6137],\n",
      "        [ 1.6017,  0.8600, -3.3686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0347,  3.4381, -3.2354],\n",
      "        [-0.6070,  3.5582, -3.4986],\n",
      "        [-0.7860,  3.4749, -3.2306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1008, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8487,  1.1667, -3.4507],\n",
      "        [ 2.3834,  0.5910, -3.4679],\n",
      "        [ 2.2119,  0.9847, -3.8114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0608, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0901,  3.4444, -3.7137],\n",
      "        [-0.9074,  3.3656, -3.1831],\n",
      "        [ 2.2651,  0.3338, -3.6779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0931, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3954,  0.2382, -3.6400],\n",
      "        [ 2.9795, -0.3275, -3.9716],\n",
      "        [ 0.6780,  2.6568, -3.7658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2505, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8241,  0.7595, -3.5724],\n",
      "        [ 2.1936,  0.2560, -3.8252],\n",
      "        [ 2.0022,  0.9990, -3.5970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0505, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4644,  2.6290, -3.6432],\n",
      "        [-0.7319,  3.5580, -3.5958],\n",
      "        [-0.3836,  3.2611, -3.7615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3972, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6891,  1.5039, -3.6499],\n",
      "        [ 1.8027,  1.0325, -3.5880],\n",
      "        [ 2.2703,  0.7559, -3.8894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5405,  0.6097, -3.6282],\n",
      "        [ 2.7975,  0.2901, -4.1524],\n",
      "        [ 0.7278,  2.3116, -3.8899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6242, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3725,  2.8146, -3.7948],\n",
      "        [ 2.0810,  0.6979, -3.3625],\n",
      "        [ 2.2812,  0.6269, -3.4862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3040, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9530,  1.1162, -3.6757],\n",
      "        [ 1.3118,  1.6696, -3.4246],\n",
      "        [-0.5396,  3.6506, -3.2677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0856, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8032, -0.3367, -3.9536],\n",
      "        [ 0.5428,  2.7532, -3.7938],\n",
      "        [ 2.4789,  0.2898, -3.7117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2022, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6017,  1.2519, -3.8438],\n",
      "        [ 3.0104, -0.2868, -3.2497],\n",
      "        [ 3.2000, -0.2334, -3.9245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1267, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5594,  0.3856, -3.7915],\n",
      "        [ 2.0644,  1.2742, -4.0010],\n",
      "        [ 2.9351,  0.0979, -3.9852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5845, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3121,  1.7746, -3.6437],\n",
      "        [ 1.8272,  0.9249, -3.8107],\n",
      "        [-0.6037,  3.5281, -3.2546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6277, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6925,  2.5115, -4.0760],\n",
      "        [ 1.9916,  0.5357, -3.5648],\n",
      "        [ 0.2266,  2.9746, -3.8734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2860,  0.3200, -3.6600],\n",
      "        [-0.6155,  3.5884, -3.5951],\n",
      "        [-0.0637,  3.2425, -3.6051]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0485, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8145,  0.0697, -3.7475],\n",
      "        [ 2.8859,  0.1365, -3.9104],\n",
      "        [-0.6014,  3.4171, -3.4602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8761, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9201,  1.9643, -3.4534],\n",
      "        [ 1.2256,  1.3114, -3.5921],\n",
      "        [ 1.9947,  0.5382, -3.8939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1150, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4580,  0.6708, -3.7438],\n",
      "        [ 2.1532,  0.4936, -3.8971],\n",
      "        [-0.6978,  3.7531, -3.4604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2690, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2301,  3.4588, -3.2924],\n",
      "        [ 1.2984,  1.3844, -3.4593],\n",
      "        [ 0.0363,  2.9267, -3.6865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1289, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3212,  3.0457, -3.6424],\n",
      "        [ 0.2700,  2.8266, -3.8644],\n",
      "        [ 1.9543,  0.7983, -3.8994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1268, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2051,  3.5229, -2.9464],\n",
      "        [-0.9864,  3.5023, -3.3101],\n",
      "        [-0.0851,  3.2363, -3.8124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4063, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2004,  0.3585, -3.6101],\n",
      "        [ 2.0992,  0.9712, -3.4547],\n",
      "        [ 1.4026,  1.5752, -3.8044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1591, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1344,  3.0962, -3.6642],\n",
      "        [ 2.0306,  0.7433, -3.5779],\n",
      "        [ 2.3451,  0.7033, -3.6705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1501, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4502,  3.4938, -3.6726],\n",
      "        [ 1.6795,  1.0173, -3.5376],\n",
      "        [-1.0210,  3.6296, -3.0298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3362, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5466,  0.9133, -3.3666],\n",
      "        [ 0.2161,  2.9806, -4.0815],\n",
      "        [ 1.1443,  1.5478, -3.3685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1162, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2547,  0.4610, -4.0381],\n",
      "        [ 2.6092,  0.1015, -3.4804],\n",
      "        [ 2.4599,  0.3159, -3.6331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2763, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4093,  3.1303, -3.4293],\n",
      "        [ 2.0318,  0.4575, -3.5138],\n",
      "        [ 2.7724,  0.1013, -4.0235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4304, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6313,  0.9631, -3.1915],\n",
      "        [ 0.6684,  2.2728, -3.5636],\n",
      "        [-0.5317,  3.5140, -3.3876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4704, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6976,  0.7739, -3.2537],\n",
      "        [ 2.7270,  0.0890, -3.6884],\n",
      "        [ 2.5513,  0.0069, -3.6219]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0764, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4977,  0.3388, -3.4360],\n",
      "        [ 2.6905, -0.1349, -3.2337],\n",
      "        [ 2.7360, -0.1277, -3.4098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3062, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2593,  1.4610, -3.1292],\n",
      "        [ 2.4194,  0.0376, -3.3740],\n",
      "        [-0.4050,  3.4330, -2.9736]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8146, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4003,  2.9419, -3.8032],\n",
      "        [ 2.0362,  0.6268, -3.0531],\n",
      "        [ 2.5028,  0.4866, -3.4495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1358, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7573,  2.5998, -3.5129],\n",
      "        [ 0.6295,  2.5656, -3.9059],\n",
      "        [ 2.2789,  0.2191, -3.6609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5385, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7784,  0.1163, -3.4694],\n",
      "        [ 1.4127,  1.6336, -3.8502],\n",
      "        [ 1.6710,  1.2069, -3.6169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9541,  0.6156, -3.3804],\n",
      "        [ 2.6830, -0.2364, -3.2130],\n",
      "        [ 2.7845, -0.1823, -3.6023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2139, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7426,  1.0267, -3.3288],\n",
      "        [-0.0299,  2.8358, -3.3176],\n",
      "        [ 2.2279,  0.6003, -3.4669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1997, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4550,  2.4387, -3.8924],\n",
      "        [ 0.9525,  1.8729, -3.6249],\n",
      "        [ 0.5354,  2.5267, -3.3323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7865, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8281,  0.6918, -3.4191],\n",
      "        [ 3.2323, -0.0697, -3.6534],\n",
      "        [ 1.0723,  1.4475, -3.2653]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3303, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5546,  1.2041, -3.5971],\n",
      "        [ 2.8599, -0.0192, -3.4436],\n",
      "        [ 2.7274, -0.3350, -3.5782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1957, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5894,  0.9077, -3.1279],\n",
      "        [ 0.3977,  2.4336, -3.2770],\n",
      "        [ 2.9700, -0.1172, -3.4824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3645, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0590,  3.0495, -3.2162],\n",
      "        [ 1.8699,  1.3112, -3.5368],\n",
      "        [ 2.9753, -0.4293, -3.6406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1017, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5093,  2.0999, -3.5314],\n",
      "        [ 0.0160,  3.0731, -3.2987],\n",
      "        [ 2.5996, -0.0614, -3.8312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7353, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2349,  1.4564, -3.4254],\n",
      "        [ 1.4567,  1.0847, -3.2129],\n",
      "        [ 1.2306,  1.2541, -3.1083]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1696, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5656,  0.0782, -3.7283],\n",
      "        [ 1.1143,  1.8914, -3.5384],\n",
      "        [ 2.6806, -0.4113, -3.7953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1028, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7453,  2.2049, -3.8390],\n",
      "        [ 2.7068, -0.1426, -3.6939],\n",
      "        [ 3.0429, -0.1912, -3.8368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9619, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1468,  3.2926, -3.3455],\n",
      "        [ 2.3424,  0.5940, -4.1983],\n",
      "        [ 0.4170,  3.0253, -3.6964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1948, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2487,  1.7729, -3.2700],\n",
      "        [-0.1968,  3.0864, -2.9771],\n",
      "        [ 0.1575,  2.7152, -3.6480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3693, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7249,  0.9560, -3.6714],\n",
      "        [ 1.5429,  1.1136, -3.4617],\n",
      "        [ 0.5307,  1.9712, -2.9290]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1821, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0057,  0.8706, -3.8345],\n",
      "        [ 2.5264,  0.3997, -3.5449],\n",
      "        [ 0.5451,  2.3755, -3.6665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1630, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9651,  1.1421, -3.6107],\n",
      "        [ 0.3269,  2.7619, -3.7655],\n",
      "        [-0.2558,  3.0728, -3.1335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0614, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3109,  3.0353, -3.7040],\n",
      "        [-0.2440,  3.0354, -2.9820],\n",
      "        [ 0.2388,  2.7433, -3.5027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5165, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0244,  2.1404, -3.5821],\n",
      "        [ 0.2853,  2.8870, -3.5505],\n",
      "        [ 2.7645,  0.1907, -3.7859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9519, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3800,  0.1485, -3.6461],\n",
      "        [ 2.1728,  0.6955, -3.4985],\n",
      "        [ 0.8713,  1.8878, -3.6770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8584, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2531,  3.2342, -3.4247],\n",
      "        [ 0.4608,  2.7510, -3.5398],\n",
      "        [ 0.6951,  2.4905, -3.6451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5410, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9321,  2.0058, -3.2783],\n",
      "        [ 3.0408, -0.3545, -3.7413],\n",
      "        [ 0.7351,  2.1671, -3.2987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0601, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4218,  2.8286, -3.3908],\n",
      "        [ 2.7086,  0.1812, -3.7407],\n",
      "        [ 3.3660, -0.9929, -3.4522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7552, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2389,  3.2791, -3.4157],\n",
      "        [ 1.5959,  0.9984, -3.2473],\n",
      "        [ 0.6099,  2.2163, -3.5771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0765, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5800,  0.7061, -3.9898],\n",
      "        [-0.1496,  2.9347, -3.2423],\n",
      "        [-0.1184,  3.1622, -3.1665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1443, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2574,  3.2431, -3.1625],\n",
      "        [ 0.6721,  2.2635, -3.3680],\n",
      "        [ 0.7718,  2.1704, -3.5347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1331, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1342,  0.7397, -3.7639],\n",
      "        [ 2.9889,  0.0096, -4.0647],\n",
      "        [ 2.3153,  0.2864, -3.8751]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0470, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7197,  0.2844, -3.9413],\n",
      "        [ 0.0241,  3.0997, -3.2452],\n",
      "        [ 3.4820, -1.3913, -2.9985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0333, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6889,  0.0312, -3.9568],\n",
      "        [ 3.3554, -0.6041, -3.5201],\n",
      "        [ 3.2965, -1.4106, -2.9414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3538, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9296,  2.0619, -3.4842],\n",
      "        [ 1.7093,  1.8250, -3.8712],\n",
      "        [ 2.9658, -0.8038, -3.3171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0411, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7899,  0.2250, -4.1155],\n",
      "        [ 1.2381,  1.9432, -3.3850],\n",
      "        [ 0.4016,  2.9225, -3.5842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1167, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7909,  2.4447, -3.7920],\n",
      "        [ 0.4991,  2.6285, -3.5240],\n",
      "        [ 3.0225,  0.2124, -4.2892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1865, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3285, -0.4407, -3.7044],\n",
      "        [ 1.7866,  1.0726, -3.3644],\n",
      "        [ 0.5841,  2.5489, -3.4883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0267, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5429, -0.8192, -3.4804],\n",
      "        [ 3.2458, -0.1539, -3.6134],\n",
      "        [ 2.9975, -0.4433, -3.6417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5818, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9688,  0.7893, -3.7462],\n",
      "        [ 2.2796,  0.7449, -3.8830],\n",
      "        [ 0.4392,  2.7325, -3.3519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1540, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9400,  1.6104, -3.5057],\n",
      "        [ 3.5867, -1.3031, -3.3465],\n",
      "        [ 3.0605, -0.2597, -3.9268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2643, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9802,  0.7617, -3.4480],\n",
      "        [ 3.3012, -1.0238, -3.3765],\n",
      "        [ 1.4083,  1.8062, -3.7184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3917, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6867,  1.2726, -3.8643],\n",
      "        [ 3.5481, -1.3095, -3.2638],\n",
      "        [ 0.9149,  2.2260, -3.1387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0684, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6650,  2.8627, -3.7802],\n",
      "        [ 2.6126,  0.0231, -4.0713],\n",
      "        [ 3.2872, -0.4044, -4.0420]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3541, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9330,  2.2913, -3.8025],\n",
      "        [ 2.6350,  0.2307, -3.6072],\n",
      "        [ 1.6277,  1.5353, -3.5246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3696, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3166, -0.5640, -3.6092],\n",
      "        [ 1.3377,  1.8751, -3.2962],\n",
      "        [ 0.3501,  2.7739, -3.7096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3637, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9920, -0.2069, -3.9395],\n",
      "        [ 3.3721, -0.4483, -3.9905],\n",
      "        [ 2.1761,  0.6945, -3.4873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0500, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9231,  0.0731, -3.8764],\n",
      "        [ 0.0813,  3.0021, -3.7410],\n",
      "        [-0.1828,  3.0694, -3.6275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3780, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5772,  1.0278, -3.4849],\n",
      "        [-0.1844,  3.1696, -3.0938],\n",
      "        [ 2.6620,  0.2708, -4.0344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2094, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4421,  3.1998, -3.3212],\n",
      "        [ 3.4810, -1.1340, -3.4599],\n",
      "        [ 1.4048,  1.6280, -3.8120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0350, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5847, -0.9234, -3.4918],\n",
      "        [ 3.2977, -0.4571, -4.1730],\n",
      "        [ 2.7697,  0.1228, -4.0923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0673, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2114,  3.0839, -3.6311],\n",
      "        [ 2.3411,  0.4756, -4.2670],\n",
      "        [-0.7129,  3.3151, -3.0327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1106, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6577,  1.1206, -3.5919],\n",
      "        [ 2.1028,  0.8031, -3.7312],\n",
      "        [ 0.2888,  2.8368, -3.5345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8427, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6380,  1.3095, -3.8903],\n",
      "        [-0.0426,  3.0847, -3.2824],\n",
      "        [ 2.4140,  0.6323, -3.9797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1205, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8020,  2.0159, -3.5644],\n",
      "        [ 2.6517,  0.0771, -3.8451],\n",
      "        [-0.3537,  3.4252, -3.4890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0714, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4034,  3.4431, -3.5550],\n",
      "        [ 2.5245,  0.5872, -4.0693],\n",
      "        [ 0.2044,  3.0747, -3.5568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2741, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7592e-05,  3.1188e+00, -3.3533e+00],\n",
      "        [ 1.5170e+00,  1.0551e+00, -3.6874e+00],\n",
      "        [ 8.3196e-01,  1.9548e+00, -3.2887e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7519, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7671,  3.6141, -3.3665],\n",
      "        [ 1.4881,  1.5858, -3.9671],\n",
      "        [ 1.8378,  0.5987, -3.5514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2911, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4802,  1.3319, -3.2621],\n",
      "        [ 2.1850,  0.7844, -3.9397],\n",
      "        [-0.3989,  3.3329, -3.3253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0893, grad_fn=<NllLossBackward0>), logits=tensor([[-6.6125e-04,  3.1653e+00, -3.4998e+00],\n",
      "        [ 2.1114e+00,  5.9171e-01, -3.7445e+00],\n",
      "        [-2.2906e-01,  3.4788e+00, -3.6340e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3202, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4439e-01,  2.5809e+00, -3.3192e+00],\n",
      "        [ 1.2973e+00,  1.5586e+00, -3.6466e+00],\n",
      "        [ 2.9253e-03,  3.2530e+00, -3.8555e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1934, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2856,  3.5742, -3.5090],\n",
      "        [ 0.1812,  3.1943, -3.6704],\n",
      "        [ 1.2737,  1.6882, -3.8023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3530, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4651,  0.3201, -4.0870],\n",
      "        [ 1.6354,  1.2445, -3.8295],\n",
      "        [ 3.0944, -0.2000, -4.2457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0364, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3430,  3.4799, -3.6320],\n",
      "        [-0.2161,  3.1068, -3.4023],\n",
      "        [ 0.1742,  3.1632, -3.8740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7806, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0840,  2.9088, -3.5573],\n",
      "        [-0.4381,  3.3766, -3.6815],\n",
      "        [ 2.6497,  0.5009, -4.2142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9661, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3157,  0.6101, -4.3794],\n",
      "        [ 0.7751,  1.9571, -3.7764],\n",
      "        [ 0.9575,  1.9055, -3.6206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3131, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2229,  2.9500, -3.4671],\n",
      "        [ 1.3232,  1.0342, -3.3088],\n",
      "        [-0.3717,  3.5254, -3.6039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2828, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3374,  0.8376, -3.8853],\n",
      "        [-0.3554,  3.4353, -3.6322],\n",
      "        [ 1.2397,  1.3967, -3.3756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4351, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7096,  0.5251, -4.3654],\n",
      "        [ 1.0780,  1.8886, -3.5616],\n",
      "        [-0.5819,  3.5570, -3.5616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8899, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7944,  0.8318, -3.2812],\n",
      "        [ 1.3583,  1.3968, -3.2926],\n",
      "        [ 2.2267,  0.7756, -4.2288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0792, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1460,  2.7674, -3.5706],\n",
      "        [ 0.4821,  2.2747, -3.4671],\n",
      "        [-0.2476,  3.4230, -3.3382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3532, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2000,  3.0493, -3.8955],\n",
      "        [-0.0961,  3.4246, -3.3457],\n",
      "        [ 1.4688,  0.9814, -3.1635]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1822, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6517,  0.3558, -4.1587],\n",
      "        [ 2.3600,  0.7465, -4.2266],\n",
      "        [ 2.0596,  0.8699, -4.4277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0841, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2872,  3.1756, -3.3333],\n",
      "        [ 0.2161,  2.8816, -3.7839],\n",
      "        [ 0.7316,  2.5523, -3.8908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1016, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2489,  3.4363, -3.4455],\n",
      "        [ 2.6060,  0.7543, -4.3897],\n",
      "        [ 0.5548,  2.5260, -3.5918]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4319, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4131,  1.0691, -3.3608],\n",
      "        [ 0.8282,  2.0728, -3.7932],\n",
      "        [ 1.2061,  1.6480, -3.6080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6196, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2496,  1.3307, -3.3434],\n",
      "        [ 2.0895,  0.8162, -3.9497],\n",
      "        [ 1.4710,  1.0133, -3.6170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2348, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2667,  0.5929, -4.0912],\n",
      "        [ 1.9969,  1.0360, -3.4473],\n",
      "        [ 2.2576,  0.7641, -4.1124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0264, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2161,  3.5906, -3.7021],\n",
      "        [-0.1918,  3.5035, -3.5534],\n",
      "        [-0.3235,  3.1638, -3.6097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0424, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8581,  3.8118, -3.5757],\n",
      "        [-1.1155,  3.6380, -2.8967],\n",
      "        [ 2.6623,  0.4762, -4.4584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1394, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6552,  2.1654, -3.3801],\n",
      "        [ 2.1706,  0.7012, -4.1989],\n",
      "        [-1.2612,  3.8352, -3.2037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6155, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6207,  1.4872, -3.7559],\n",
      "        [ 2.5529,  0.6085, -4.4234],\n",
      "        [ 1.0520,  1.7129, -3.5079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4466,  0.5683, -4.0148],\n",
      "        [ 2.5366,  0.5384, -4.1388],\n",
      "        [ 2.2111,  0.7003, -4.3211]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2533, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1867,  0.8726, -3.6120],\n",
      "        [-1.1998,  3.1097, -3.0530],\n",
      "        [ 1.7796,  1.3531, -4.0682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0713, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4900,  2.3445, -3.2102],\n",
      "        [-0.0622,  3.2164, -3.9549],\n",
      "        [-0.1596,  3.4506, -3.6616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0868, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2375,  0.7570, -3.9013],\n",
      "        [ 0.0055,  3.1833, -3.8305],\n",
      "        [-0.8045,  3.6749, -3.5052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0342, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9471,  3.7741, -3.4418],\n",
      "        [ 3.0039,  0.2538, -4.5428],\n",
      "        [ 0.0530,  3.5497, -3.8729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1579, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6764,  1.0590, -3.8333],\n",
      "        [ 2.5786,  0.3291, -4.1388],\n",
      "        [ 2.8696,  0.2568, -4.3378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0184, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2589, -0.6041, -4.4178],\n",
      "        [-0.2144,  3.4726, -3.9932],\n",
      "        [-1.1353,  3.7240, -3.3332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0722,  2.9192, -3.7804],\n",
      "        [-0.8137,  3.7705, -2.8869],\n",
      "        [-0.6850,  3.3858, -3.8580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7502, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2948,  3.7873, -3.1729],\n",
      "        [ 2.6151,  0.5594, -3.9069],\n",
      "        [ 2.4354,  0.4427, -4.0340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0888, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4359,  0.1745, -4.0561],\n",
      "        [ 2.7112,  0.6565, -4.3682],\n",
      "        [-0.1294,  2.9863, -3.5789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1063, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3718,  1.0808, -3.8983],\n",
      "        [ 0.2482,  2.9552, -3.6491],\n",
      "        [-1.1595,  3.7616, -2.8870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0163, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4040,  3.6230, -3.5374],\n",
      "        [-1.0908,  3.0158, -2.7772],\n",
      "        [-0.8714,  3.6938, -3.2764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1412, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2961,  0.9121, -4.3823],\n",
      "        [-0.7152,  3.6479, -3.5874],\n",
      "        [ 0.6426,  2.2465, -3.5889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1752, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3409,  3.4978, -2.8782],\n",
      "        [ 1.7704,  1.3059, -3.8185],\n",
      "        [-0.4278,  3.2490, -3.6275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1421, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5004,  0.8237, -4.3169],\n",
      "        [ 2.2395,  0.9653, -4.3211],\n",
      "        [-1.6897,  3.6990, -2.6583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1991,  3.8427, -2.9806],\n",
      "        [-1.1170,  3.8177, -3.4575],\n",
      "        [-1.3079,  3.5970, -2.7049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1400, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0247,  1.1019, -4.0964],\n",
      "        [ 2.7641,  0.2220, -4.4722],\n",
      "        [-1.1275,  3.8882, -3.3219]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0537, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3940,  0.5441, -3.9668],\n",
      "        [-1.5284,  3.7630, -2.8071],\n",
      "        [-1.5305,  3.6973, -2.5616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1549, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4844,  3.5829, -2.9523],\n",
      "        [ 2.1978,  1.6323, -4.0883],\n",
      "        [-1.6414,  3.7271, -2.9337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2373, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2007,  3.8045, -2.8574],\n",
      "        [ 2.3945,  0.8507, -4.3907],\n",
      "        [ 1.8943,  1.4817, -4.1177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0450, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1648,  3.8901, -3.2789],\n",
      "        [ 2.9014,  0.2979, -4.2210],\n",
      "        [ 2.7105, -0.1614, -4.4317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0459, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5774,  0.3037, -4.3746],\n",
      "        [-1.4808,  3.4666, -2.9297],\n",
      "        [ 3.2282, -0.2689, -4.5127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8419, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5169,  2.9421, -4.1160],\n",
      "        [-0.9129,  3.9589, -3.2830],\n",
      "        [-1.4668,  3.8103, -2.6932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2975,  3.5071, -3.7879],\n",
      "        [ 2.6313,  0.3599, -3.8657],\n",
      "        [-0.4794,  3.7145, -3.4524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0770, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6595,  0.0711, -4.0882],\n",
      "        [ 2.8874,  0.1224, -4.1301],\n",
      "        [ 2.6044,  0.2928, -4.2984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0567, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4035,  3.9161, -2.9897],\n",
      "        [ 2.3466,  0.5701, -4.0821],\n",
      "        [-1.5478,  3.7380, -2.7044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0661, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6959,  2.4772, -3.9224],\n",
      "        [ 3.4052, -0.6683, -3.9396],\n",
      "        [ 3.2755, -0.4809, -4.1405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0463, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0030,  4.0075, -3.5629],\n",
      "        [ 3.4937, -0.9585, -3.6540],\n",
      "        [ 2.4892,  0.4112, -4.0085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2144,  3.8214, -2.5600],\n",
      "        [ 3.6421, -1.0508, -3.5490],\n",
      "        [-0.4754,  3.7391, -3.6952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7496, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2570, -0.2005, -4.0688],\n",
      "        [-1.6250,  3.7508, -3.1233],\n",
      "        [ 2.6663,  0.5722, -3.9699]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0329, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6891, -1.0895, -4.1252],\n",
      "        [ 0.5402,  3.0363, -4.3903],\n",
      "        [ 3.3642, -1.3322, -3.5613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0613,  4.1956, -3.1542],\n",
      "        [ 3.7959, -1.4372, -3.5999],\n",
      "        [-0.3796,  3.7566, -3.5938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0192, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8417, -0.1424, -4.2438],\n",
      "        [ 3.3737, -1.0059, -3.5747],\n",
      "        [ 3.4475, -1.2669, -3.6002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0092, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4291,  0.7915, -3.9877],\n",
      "        [ 1.8642,  1.0531, -3.6244],\n",
      "        [ 3.0516, -0.4890, -4.0158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3527, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6949,  1.1242, -3.8602],\n",
      "        [ 3.4917, -0.5933, -4.1045],\n",
      "        [ 3.4511, -0.4932, -3.9968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4913, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1292,  1.0012, -4.0552],\n",
      "        [-0.0635,  3.1776, -3.6033],\n",
      "        [ 3.1962, -0.5209, -4.3340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9609, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4432,  3.7462, -2.8445],\n",
      "        [ 3.0222, -0.2421, -3.7571],\n",
      "        [ 2.9539,  0.1779, -4.0717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8679,  3.8109, -3.1225],\n",
      "        [ 3.5145, -0.4557, -4.1324],\n",
      "        [-1.3207,  4.1703, -2.9432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8063,  0.2989, -4.2948],\n",
      "        [ 3.3577, -0.2284, -4.0529],\n",
      "        [-0.7469,  3.5420, -3.2261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0911, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2226,  3.1692, -3.8973],\n",
      "        [ 0.6859,  2.1294, -3.2517],\n",
      "        [-1.3815,  4.0332, -2.6589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0429, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6338,  0.4775, -4.2550],\n",
      "        [-0.7089,  3.8983, -3.3587],\n",
      "        [-1.1339,  3.8921, -3.0652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0607, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0525, -0.0843, -4.2527],\n",
      "        [-1.0215,  3.9792, -3.2172],\n",
      "        [ 2.6820,  0.7127, -4.2558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1369, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1847,  3.9708, -2.8012],\n",
      "        [ 1.0311,  1.9871, -3.3675],\n",
      "        [ 2.8637,  0.3013, -4.0740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0115, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9909,  3.8986, -3.2056],\n",
      "        [ 3.2395, -0.8787, -3.3776],\n",
      "        [-0.9448,  3.9426, -2.8495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9753, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3405,  1.8903, -3.5308],\n",
      "        [ 0.3490,  2.0959, -2.8949],\n",
      "        [-1.6487,  4.1169, -2.6298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2093,  3.9077, -2.9213],\n",
      "        [ 2.4397,  0.4176, -4.1733],\n",
      "        [-1.5479,  4.0071, -2.4747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2470, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8210,  1.1826, -3.7203],\n",
      "        [-1.4387,  3.9840, -2.6727],\n",
      "        [ 1.8822,  0.8536, -3.6519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2663,  3.9011, -2.9023],\n",
      "        [-1.4335,  3.8758, -2.7238],\n",
      "        [-1.6032,  3.8061, -2.2988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0925, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7149,  0.1063, -4.3089],\n",
      "        [-0.9196,  3.8137, -3.1365],\n",
      "        [ 2.2449,  0.7039, -3.8237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6124, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4770,  2.3679, -3.2813],\n",
      "        [ 1.9705,  0.8227, -4.0546],\n",
      "        [ 0.6964,  1.8253, -2.8619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2364, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8137,  1.2773, -3.8445],\n",
      "        [ 2.7618, -1.0238, -2.6548],\n",
      "        [ 2.1990,  0.7865, -3.9340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4895, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5371,  1.2880, -3.4391],\n",
      "        [ 1.8881,  1.1185, -3.6007],\n",
      "        [ 2.1918,  0.9531, -4.1291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0173, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9824,  3.9452, -3.1337],\n",
      "        [ 2.8108, -0.5415, -3.6510],\n",
      "        [-1.1336,  3.9399, -2.5015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1278, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4292,  4.0829, -3.0741],\n",
      "        [ 2.2381,  0.5899, -4.2431],\n",
      "        [ 0.6558,  2.1866, -2.8266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1639, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6087,  0.4294, -4.0802],\n",
      "        [ 2.3842,  0.3529, -4.1833],\n",
      "        [ 2.2738,  1.0476, -3.9103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1536, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2451,  0.8555, -3.6404],\n",
      "        [-0.9889,  3.8540, -2.9332],\n",
      "        [ 2.0713,  0.6984, -4.3144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0740, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3461,  0.7390, -3.9587],\n",
      "        [ 3.1497, -0.5481, -4.0462],\n",
      "        [ 3.0239, -2.1489, -1.9339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0282, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5413, -2.3390, -1.0043],\n",
      "        [-1.6394,  4.0338, -2.1373],\n",
      "        [ 3.1353,  0.0098, -4.6375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0480, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7283,  0.6974, -4.6148],\n",
      "        [ 3.1895, -1.6283, -2.5716],\n",
      "        [ 3.4840, -1.4615, -2.8543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0517, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1242, -0.3574, -4.3451],\n",
      "        [ 2.6096,  0.4805, -4.1021],\n",
      "        [ 3.3809, -1.2577, -3.4941]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1929, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9668,  3.8436, -3.3956],\n",
      "        [ 1.6343,  1.2444, -3.6345],\n",
      "        [ 3.0234,  0.0380, -4.7087]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4814, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1954,  3.7333, -2.8079],\n",
      "        [ 2.7964,  0.5720, -4.3756],\n",
      "        [ 2.5231,  0.5461, -4.1171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6772, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0624,  2.8188, -3.2714],\n",
      "        [ 0.5578,  2.7047, -3.4303],\n",
      "        [ 2.3004,  0.6085, -4.0767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4751, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5581,  0.4799, -4.4011],\n",
      "        [-1.3314,  3.8108, -2.6907],\n",
      "        [ 2.1155,  1.1371, -3.9044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1571, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8209,  1.8035, -3.4407],\n",
      "        [ 2.4217,  0.5549, -4.5935],\n",
      "        [-1.5442,  4.0241, -2.6693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0682, -0.8307, -4.3139],\n",
      "        [-1.3745,  3.8974, -3.0042],\n",
      "        [-1.4501,  3.9150, -2.3162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3875, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1838, -0.2788, -4.3944],\n",
      "        [-1.3995,  3.7421, -2.5161],\n",
      "        [ 1.1500,  1.8756, -3.5406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0453, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1381, -1.3775, -3.5493],\n",
      "        [ 2.6080, -2.1336, -2.1104],\n",
      "        [ 2.5460,  0.3479, -4.1563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4842, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3616, -1.8780, -2.4730],\n",
      "        [ 2.2210,  0.2213, -3.9592],\n",
      "        [ 2.0088,  1.0074, -4.3518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5880, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7533,  2.3095, -3.5985],\n",
      "        [-1.2145,  3.6900, -3.0933],\n",
      "        [-1.6249,  3.8957, -2.3977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4087, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0477, -1.8953, -2.6255],\n",
      "        [ 0.9129,  1.7557, -3.6163],\n",
      "        [ 2.9881, -2.0018, -2.3579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4848, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2809,  2.7214, -3.5301],\n",
      "        [ 1.4023,  1.3777, -3.9469],\n",
      "        [ 0.9821,  1.8848, -3.9090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1995, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4093,  3.6787, -2.7418],\n",
      "        [-1.4310,  3.8216, -2.8091],\n",
      "        [ 1.4679,  1.2315, -3.9854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1282, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3457,  0.7091, -4.5110],\n",
      "        [ 2.2520,  0.7338, -4.6298],\n",
      "        [-1.1492,  3.9318, -3.2856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8389, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5963,  1.1815, -3.8964],\n",
      "        [-1.0784,  3.9219, -3.2708],\n",
      "        [ 2.2153,  0.8613, -4.3894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4694, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4522,  0.6793, -4.4978],\n",
      "        [ 2.5750,  0.3974, -4.6610],\n",
      "        [ 3.3436, -0.7823, -4.3845]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0538, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2471, -1.2371, -3.8398],\n",
      "        [ 2.9802, -1.0268, -4.3114],\n",
      "        [ 2.5637,  0.5909, -4.9002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4964, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4557,  1.3461, -3.4832],\n",
      "        [ 0.9546,  2.0171, -3.6190],\n",
      "        [ 1.3132,  1.9198, -4.1572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2048, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3209,  0.5872, -4.0028],\n",
      "        [ 0.9852,  1.8740, -3.7755],\n",
      "        [ 0.4810,  2.7123, -3.7216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7952, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6332,  0.4593, -4.5025],\n",
      "        [ 2.5349,  0.3893, -4.7954],\n",
      "        [-0.3479,  3.5692, -3.8009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6403, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2611,  1.2416, -4.4680],\n",
      "        [ 1.9795,  1.0060, -3.7511],\n",
      "        [ 1.0734,  2.0724, -3.7404]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3564, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0605,  0.6614, -4.0019],\n",
      "        [ 0.4919,  2.7522, -3.8341],\n",
      "        [ 1.5006,  1.4035, -3.5588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1737, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0826,  1.1050, -4.1947],\n",
      "        [ 0.5704,  2.6292, -3.9332],\n",
      "        [ 2.7039,  0.2016, -4.8807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5107, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4040,  0.8790, -4.7703],\n",
      "        [ 1.2069,  1.8307, -3.6506],\n",
      "        [ 0.8276,  1.9753, -3.4483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4893, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4157,  1.3440, -3.5525],\n",
      "        [ 1.5561,  1.4241, -3.9577],\n",
      "        [ 2.4917,  0.2636, -4.6183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3213, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3219,  1.3358, -3.3329],\n",
      "        [ 2.4164,  0.6248, -4.8727],\n",
      "        [ 2.6676,  0.5882, -4.7782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2232, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2598,  3.5148, -3.3087],\n",
      "        [ 2.0254,  0.9995, -4.6278],\n",
      "        [ 1.9714,  1.0585, -4.3821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4221, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3371,  2.4978, -3.3575],\n",
      "        [ 1.3249,  1.3547, -3.7857],\n",
      "        [ 1.0871,  1.6025, -3.1890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5263, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4050,  1.7196, -3.5349],\n",
      "        [ 0.6213,  2.0610, -3.1541],\n",
      "        [ 1.5508,  1.3365, -3.2526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5454, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7931,  1.2705, -4.0982],\n",
      "        [ 1.2506,  1.7135, -3.5722],\n",
      "        [ 2.5244,  0.7350, -5.0276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1431, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3555,  0.8624, -5.0107],\n",
      "        [ 2.3363,  0.8311, -4.8978],\n",
      "        [-0.2129,  3.5027, -3.4319]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2310, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9209,  0.8259, -4.1365],\n",
      "        [ 2.0174,  0.9348, -4.5176],\n",
      "        [ 2.5563,  0.3983, -4.8024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3210, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3566,  1.2626, -3.2846],\n",
      "        [ 0.0420,  2.7534, -3.2682],\n",
      "        [ 2.0590,  0.7685, -4.4930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1208, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3121,  0.7176, -4.4407],\n",
      "        [-0.1688,  3.0066, -3.5689],\n",
      "        [ 0.5137,  2.4669, -3.8257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2265, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9457,  1.0761, -4.4264],\n",
      "        [-0.8734,  3.8534, -3.2050],\n",
      "        [ 1.0144,  2.0023, -3.8639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5958,  3.4414, -3.2251],\n",
      "        [-0.4894,  3.5496, -3.0620],\n",
      "        [ 2.3443,  0.4708, -4.6496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1929, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4860,  0.4873, -5.0065],\n",
      "        [ 1.7438,  1.1488, -3.6539],\n",
      "        [-1.1094,  3.7666, -2.7738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2060, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4553,  3.5012, -3.1217],\n",
      "        [ 2.0549,  0.8634, -4.2096],\n",
      "        [ 0.8953,  1.8468, -3.2203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1663, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7955,  1.1188, -3.8746],\n",
      "        [-1.3246,  3.8927, -2.4672],\n",
      "        [ 2.7063,  0.1970, -4.9494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2756, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5795,  3.5682, -3.3246],\n",
      "        [ 1.2571,  1.4612, -3.5435],\n",
      "        [-1.4670,  3.9412, -2.4864]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1847, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5024,  2.2865, -3.0950],\n",
      "        [-1.1567,  3.8002, -3.1095],\n",
      "        [ 1.6582,  0.9002, -3.9115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3355, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1157,  1.6798, -3.0869],\n",
      "        [-0.5178,  3.4218, -2.6441],\n",
      "        [ 1.5724,  1.2071, -4.0546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0300, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6402,  3.7426, -2.1163],\n",
      "        [ 2.6518,  0.1191, -4.6024],\n",
      "        [-1.4232,  4.0159, -2.6594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2928, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9310,  1.1263, -2.6620],\n",
      "        [ 2.7125, -0.0199, -4.6660],\n",
      "        [-1.3013,  4.0029, -2.2719]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3534, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9543,  1.4678, -4.4415],\n",
      "        [ 3.0256,  0.0073, -4.7301],\n",
      "        [ 3.0139, -0.0710, -4.6601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0280, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2941,  3.9064, -2.4261],\n",
      "        [ 2.8390,  0.2227, -4.8518],\n",
      "        [-1.7907,  3.9268, -2.0773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0298, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1654, -0.4244, -4.6143],\n",
      "        [ 2.5341, -0.3535, -4.3167],\n",
      "        [-1.6156,  3.8074, -2.3896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.1816, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0330,  0.0766, -4.5459],\n",
      "        [ 1.1524,  1.8158, -4.2033],\n",
      "        [-1.5161,  3.8911, -2.5121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0336, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6696,  3.9212, -3.5551],\n",
      "        [-1.3625,  3.7900, -2.5048],\n",
      "        [ 2.7247,  0.2606, -4.3328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0940, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4995,  0.2448, -4.3285],\n",
      "        [ 2.5717,  0.4734, -4.3713],\n",
      "        [ 2.8978,  0.1846, -4.5204]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2378, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0552,  3.6142, -2.8187],\n",
      "        [ 2.4751,  0.2099, -4.5053],\n",
      "        [ 3.1807, -0.3941, -4.5733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0272, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8066, -0.5394, -4.6734],\n",
      "        [-1.1794,  3.4324, -3.2257],\n",
      "        [ 3.1426, -0.1966, -4.6761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0407, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7993, -0.4087, -4.2870],\n",
      "        [ 2.9173, -0.1638, -4.5104],\n",
      "        [ 2.9389, -0.3875, -4.1947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0289, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2856, -0.2901, -4.5984],\n",
      "        [ 2.9304, -0.1511, -4.4318],\n",
      "        [-1.0348,  3.3989, -3.0888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9362,  3.7809, -3.4561],\n",
      "        [-0.7658,  3.8443, -3.5726],\n",
      "        [-0.7437,  3.3772, -3.2815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0199, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6217,  3.7332, -3.6612],\n",
      "        [-0.6902,  3.3915, -3.4469],\n",
      "        [ 3.0753, -0.4867, -4.4362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0298, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1185,  3.1062, -3.4268],\n",
      "        [ 2.7884, -0.5369, -4.6466],\n",
      "        [-0.7964,  3.6261, -3.1355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0391, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9013,  2.6871, -2.8687],\n",
      "        [-0.7483,  2.8843, -3.2900],\n",
      "        [ 2.5535, -0.2760, -4.4951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9468, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0310, -0.3476, -4.6010],\n",
      "        [ 2.7915,  0.0703, -4.6304],\n",
      "        [-0.2610,  3.6360, -3.5179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5342, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2526,  0.9604, -4.5757],\n",
      "        [ 2.8824, -0.2870, -4.2838],\n",
      "        [-0.0380,  3.6462, -4.3055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0369, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0580, -0.2948, -4.8391],\n",
      "        [ 3.0555, -0.2621, -4.8698],\n",
      "        [ 2.8458, -0.3665, -4.5818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0511, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6034, -0.2049, -4.7666],\n",
      "        [ 2.9211,  0.0961, -4.8307],\n",
      "        [ 2.9596, -0.3546, -4.5304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0727, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5197,  2.9757, -3.2765],\n",
      "        [ 2.4758,  0.5336, -4.6231],\n",
      "        [ 2.6948, -0.2529, -4.8295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0361, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9787,  3.4579, -3.4335],\n",
      "        [ 2.9677,  0.3707, -4.7509],\n",
      "        [-0.3375,  3.4513, -3.5683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0482, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9342,  3.1690, -3.3669],\n",
      "        [ 3.0767,  0.3167, -4.8690],\n",
      "        [ 2.5641, -0.1444, -4.6581]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5258, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8467,  3.1646, -3.1234],\n",
      "        [ 2.2344,  1.0010, -4.3614],\n",
      "        [ 2.7986,  0.1278, -4.8716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6634,  2.8757, -3.1674],\n",
      "        [ 2.9445, -0.3619, -4.8175],\n",
      "        [-0.6847,  2.8689, -3.3843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0285, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9253, -0.3517, -4.9003],\n",
      "        [-0.4882,  3.2499, -3.4711],\n",
      "        [-0.7630,  3.0635, -3.2482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2117, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8775,  0.2647, -4.9430],\n",
      "        [ 1.2464,  1.6817, -4.0208],\n",
      "        [ 2.7015, -0.0424, -4.7860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0947, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5248,  3.4956, -3.4562],\n",
      "        [-0.5035,  3.4025, -3.2389],\n",
      "        [ 2.3058,  1.0181, -4.8927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0475, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7197,  3.4788, -3.2444],\n",
      "        [ 2.6947, -0.2239, -4.2820],\n",
      "        [ 2.8008,  0.2135, -4.9153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0277, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0827,  0.0604, -4.7971],\n",
      "        [-0.8075,  3.2596, -3.3324],\n",
      "        [-0.7249,  3.4274, -3.2396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0568, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9660, -0.1195, -4.6442],\n",
      "        [-0.2916,  3.2208, -3.8971],\n",
      "        [ 2.6654,  0.3550, -4.6611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0282, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9223, -0.0813, -4.6601],\n",
      "        [-0.6948,  3.2791, -3.4709],\n",
      "        [-0.8694,  3.3403, -3.4336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0371, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2475,  3.3210, -3.5879],\n",
      "        [ 3.1195, -0.2873, -4.2327],\n",
      "        [ 3.1445,  0.1503, -4.6573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0524, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9624,  0.4950, -4.5470],\n",
      "        [-0.5058,  3.6604, -3.5393],\n",
      "        [ 2.9247,  0.1171, -4.6081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0477, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7161,  3.3547, -3.4987],\n",
      "        [ 2.3040,  0.1389, -4.3097],\n",
      "        [-0.8247,  3.4329, -3.2451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0730, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2604,  0.6245, -4.6723],\n",
      "        [-0.4528,  3.5468, -3.2493],\n",
      "        [ 3.2941, -0.5846, -4.6267]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2394, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4769,  1.4819, -3.8764],\n",
      "        [-0.8536,  3.8636, -3.5282],\n",
      "        [-0.9371,  3.2868, -3.3632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4838, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7528,  3.6153, -3.7530],\n",
      "        [ 3.1046,  0.0606, -4.5480],\n",
      "        [ 3.3218, -0.4510, -4.4123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0298, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8253,  3.4458, -3.3088],\n",
      "        [ 3.0536,  0.2283, -4.7622],\n",
      "        [-0.7492,  3.4133, -3.6384]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0246, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1830, -0.3777, -4.5271],\n",
      "        [ 3.0184, -0.5784, -4.3309],\n",
      "        [-0.5162,  3.5717, -3.2873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6908, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2719,  0.4206, -4.0720],\n",
      "        [ 0.2253,  2.8562, -3.7139],\n",
      "        [ 2.6883,  0.5803, -4.4453]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1289, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0552, -0.0606, -4.6923],\n",
      "        [-0.5571,  3.1359, -3.3782],\n",
      "        [ 1.0730,  2.0682, -3.6548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1985, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2363,  3.3242, -3.6802],\n",
      "        [ 2.8750,  0.3054, -4.2941],\n",
      "        [ 1.8957,  1.4384, -4.0804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6504, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4634,  3.3586, -3.3556],\n",
      "        [ 0.8422,  2.5699, -3.9318],\n",
      "        [ 3.1678, -0.1522, -4.5712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1212, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1672,  2.1209, -3.8296],\n",
      "        [ 3.4696, -0.4022, -4.2750],\n",
      "        [ 3.3560, -0.9192, -3.7163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7065, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3284,  0.6252, -4.5621],\n",
      "        [-0.1366,  3.0147, -3.5655],\n",
      "        [ 0.7528,  2.4984, -3.8022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0259, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2070, -0.0074, -4.0494],\n",
      "        [ 3.3500, -0.9486, -4.2041],\n",
      "        [ 3.0795, -0.6837, -4.1458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1041, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1557,  2.8206, -3.7041],\n",
      "        [ 3.1678, -1.3113, -3.8858],\n",
      "        [ 2.1707,  0.8193, -4.1677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3134, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4362,  1.8094, -3.7645],\n",
      "        [ 3.4975, -0.9141, -4.1535],\n",
      "        [ 3.2261, -0.3497, -4.1277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6958, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0916, -0.8406, -3.7700],\n",
      "        [ 2.6456,  0.7856, -4.4289],\n",
      "        [-0.1074,  2.6781, -3.7168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4402, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0143,  2.9186, -3.5196],\n",
      "        [ 3.3798, -0.9201, -4.2605],\n",
      "        [ 2.0908,  1.1758, -4.1340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5373, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3904, -0.8116, -4.3200],\n",
      "        [ 2.5527,  1.1991, -4.4389],\n",
      "        [ 3.3505, -1.1011, -3.7256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4095, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4440, -1.0818, -3.8779],\n",
      "        [ 2.6764,  0.1779, -4.6172],\n",
      "        [ 1.8976,  1.1492, -4.0837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2503, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2947,  2.1338, -4.0232],\n",
      "        [ 3.3692, -1.3971, -3.6512],\n",
      "        [ 1.8817,  1.1079, -4.1471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0043, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7666,  2.9801, -4.0699],\n",
      "        [ 0.2463,  2.6788, -3.9006],\n",
      "        [ 2.9303,  0.1693, -4.3823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1872, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5576,  3.0353, -3.7769],\n",
      "        [ 3.2051,  0.0741, -4.3014],\n",
      "        [ 1.9136,  1.3078, -4.3950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4765, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6908,  1.4233, -3.9427],\n",
      "        [ 1.5737,  1.3322, -3.9095],\n",
      "        [ 3.3675,  0.0448, -4.9212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1609, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3548,  3.0487, -4.2273],\n",
      "        [ 1.1777,  1.8949, -3.9656],\n",
      "        [ 3.3302, -0.7634, -3.9126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4883, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8350,  1.1037, -4.0980],\n",
      "        [ 1.1997,  1.5720, -3.5466],\n",
      "        [ 2.3823,  0.6870, -3.9611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1711, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8471,  0.0064, -4.3696],\n",
      "        [ 2.0540,  1.3540, -4.4653],\n",
      "        [ 0.1561,  3.1050, -4.1792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0931, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3611,  2.9830, -4.1761],\n",
      "        [ 2.7610,  0.1842, -4.4506],\n",
      "        [ 2.5475,  0.6008, -4.2638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3627, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3127,  1.5035, -4.0866],\n",
      "        [ 2.1063,  0.6473, -4.3619],\n",
      "        [ 2.6173,  0.1570, -4.3526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1777, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6419,  0.6525, -4.7682],\n",
      "        [ 1.2700,  2.1731, -3.7945],\n",
      "        [ 2.7943,  0.0374, -4.5049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4435, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3389,  2.9460, -4.3973],\n",
      "        [ 0.0884,  2.5656, -3.5772],\n",
      "        [ 1.9872,  1.1822, -4.1042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3570, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1725,  1.2083, -4.3022],\n",
      "        [ 3.0078,  0.3343, -4.3802],\n",
      "        [ 1.4163,  1.4488, -3.9057]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2152, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9222,  1.9957, -3.4921],\n",
      "        [ 0.5729,  2.9145, -3.9427],\n",
      "        [ 0.8513,  2.0917, -4.1446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0587, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8118,  0.3921, -4.6171],\n",
      "        [ 2.8346, -0.0584, -4.7064],\n",
      "        [ 2.8647, -0.4663, -4.1857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1831, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1087,  1.9912, -3.7012],\n",
      "        [ 3.1815, -0.4478, -4.3797],\n",
      "        [ 2.3056,  0.6382, -4.4243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1230, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7351,  0.7574, -4.5502],\n",
      "        [ 0.9667,  2.4594, -4.1329],\n",
      "        [ 0.0737,  3.4260, -4.1002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2966, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0627,  3.2252, -4.0696],\n",
      "        [ 2.4840,  0.4643, -4.3887],\n",
      "        [ 1.7001,  1.6369, -4.0337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6513, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4973,  1.4124, -4.2872],\n",
      "        [ 1.5197,  1.7014, -4.1183],\n",
      "        [ 1.5914,  1.5866, -3.9903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4947, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0841,  2.9226, -4.0840],\n",
      "        [-0.1842,  3.2387, -3.7251],\n",
      "        [ 2.2107,  1.1035, -4.8483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2542, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8116,  1.2318, -4.0900],\n",
      "        [ 2.2896,  1.0721, -4.5459],\n",
      "        [ 2.9389,  0.0783, -4.9080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2029, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2096, -0.7573, -4.3290],\n",
      "        [ 1.1984,  2.0622, -3.9767],\n",
      "        [ 1.0798,  2.4099, -3.9523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4994, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4844,  1.9691, -4.3895],\n",
      "        [ 0.7449,  2.3391, -3.7549],\n",
      "        [ 2.1036,  1.2154, -4.3577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1567, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9720,  2.5856, -3.9153],\n",
      "        [-0.4564,  3.3917, -3.6099],\n",
      "        [ 0.9026,  2.1036, -3.7980]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0420, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2591, -1.4833, -3.6875],\n",
      "        [-0.5121,  2.8837, -3.3612],\n",
      "        [ 2.8365,  0.3651, -4.7584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3927, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5941, -1.3095, -3.7087],\n",
      "        [ 1.1399,  1.8951, -3.9907],\n",
      "        [-0.4580,  3.1521, -3.6175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2397, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3023,  1.7332, -4.1355],\n",
      "        [-0.5935,  3.0469, -3.5330],\n",
      "        [ 0.7147,  2.2927, -3.9062]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0533,  0.0308, -4.6523],\n",
      "        [ 3.0556,  0.2338, -4.7380],\n",
      "        [ 3.4142, -0.8358, -4.3177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4887, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2949,  1.1885, -4.5714],\n",
      "        [ 0.2487,  3.2758, -4.3309],\n",
      "        [-0.1762,  3.5061, -4.0490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0183, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5176,  3.6480, -3.5116],\n",
      "        [-0.7535,  2.9144, -3.7844],\n",
      "        [-0.9810,  3.4768, -3.6903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3527, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2619, -0.5476, -4.6190],\n",
      "        [ 2.6632,  0.4872, -4.5736],\n",
      "        [ 0.8975,  2.9799, -4.0214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1494, -0.2460, -4.4176],\n",
      "        [ 3.1960, -1.8332, -3.1557],\n",
      "        [ 3.2187, -1.2776, -3.4265]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0199, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4649,  3.5626, -3.7204],\n",
      "        [-0.1902,  3.7496, -4.0695],\n",
      "        [-0.5239,  3.3351, -3.7743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1201, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4055, -0.8984, -4.2005],\n",
      "        [ 1.0701,  2.0121, -4.2603],\n",
      "        [-0.4567,  3.7210, -3.6121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0182, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3304, -0.7024, -4.2518],\n",
      "        [-0.1748,  3.6461, -3.8423],\n",
      "        [-0.6265,  3.6539, -4.0589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9903, -0.9496, -4.0860],\n",
      "        [ 3.1854, -0.8022, -3.9966],\n",
      "        [-0.6860,  3.3943, -3.9074]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1385, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4301,  0.9265, -4.7543],\n",
      "        [ 0.9223,  2.7000, -4.3467],\n",
      "        [ 2.7885, -0.0497, -4.9264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0229, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8120,  3.1954, -3.6575],\n",
      "        [-0.1799,  3.3275, -3.7797],\n",
      "        [ 2.9433, -1.0355, -4.2521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0240, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9119,  3.4130, -3.6354],\n",
      "        [ 2.8809, -0.4088, -4.7080],\n",
      "        [-0.1520,  3.7240, -4.0856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3758, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9865, -0.0231, -4.6222],\n",
      "        [ 2.8913,  0.1729, -4.8234],\n",
      "        [ 1.5006,  2.0630, -4.3179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3374, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5137,  2.0011, -4.3849],\n",
      "        [ 2.9492, -0.4926, -4.5551],\n",
      "        [-0.9097,  3.5261, -3.4104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1559, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2880, -0.1564, -4.5841],\n",
      "        [ 0.9414,  2.3143, -4.3033],\n",
      "        [ 2.4569,  0.9949, -5.0543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2299, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8545,  3.8427, -3.8515],\n",
      "        [ 1.6065,  1.5499, -4.3870],\n",
      "        [-0.4152,  3.9059, -3.8727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9961, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4016,  0.7867, -4.7069],\n",
      "        [ 2.8405,  0.1150, -4.6072],\n",
      "        [ 3.2358, -0.8787, -4.3247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0607, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7900,  0.1493, -4.9271],\n",
      "        [ 2.7730,  0.5138, -4.8085],\n",
      "        [ 3.5509, -0.8161, -4.5127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1055, -0.6689, -4.5952],\n",
      "        [ 3.2457, -0.3469, -4.4847],\n",
      "        [ 3.0853, -0.6781, -4.4261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0236, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2538,  3.4370, -4.2894],\n",
      "        [-0.3368,  3.7105, -4.3086],\n",
      "        [ 3.1435, -0.4311, -4.3601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5874, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5050,  0.9379, -4.9779],\n",
      "        [ 1.9546,  0.8059, -4.6493],\n",
      "        [ 2.5637,  0.7189, -4.8670]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5891, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3681,  4.0035, -4.3135],\n",
      "        [ 1.1019,  2.6534, -4.5220],\n",
      "        [-0.6701,  3.9623, -3.9174]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3223, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0866,  0.9155, -4.8092],\n",
      "        [ 1.5627,  1.5062, -4.0993],\n",
      "        [ 3.0451, -0.4959, -4.6409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6190, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2921,  0.8035, -4.8372],\n",
      "        [ 2.8618,  0.2400, -4.9649],\n",
      "        [ 0.4489,  2.7772, -4.6124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0431, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4597,  3.7648, -3.8468],\n",
      "        [ 2.6702,  0.4130, -4.8988],\n",
      "        [-0.5526,  3.7302, -3.7567]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8677, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7967,  0.4341, -5.0650],\n",
      "        [ 0.6889,  2.7266, -4.2472],\n",
      "        [ 3.3646, -0.2442, -4.7998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9428, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3205,  1.7073, -4.3827],\n",
      "        [ 2.6517,  0.4656, -5.0870],\n",
      "        [-0.5573,  3.6197, -3.7347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3762, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7768,  1.7876, -4.3762],\n",
      "        [ 2.1599,  1.5365, -4.5872],\n",
      "        [-0.8875,  3.7967, -3.8473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1307, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5001,  1.1789, -4.7520],\n",
      "        [ 2.6337,  0.4080, -5.0656],\n",
      "        [ 3.0207,  0.0863, -4.8823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1682, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2848,  0.7774, -5.0120],\n",
      "        [ 2.3964,  0.7872, -4.9539],\n",
      "        [ 2.4153,  0.3617, -5.0079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3341, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3472,  3.9288, -4.1865],\n",
      "        [ 1.8420,  1.5795, -4.2802],\n",
      "        [ 0.6464,  2.4504, -3.9371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0950, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6659,  0.3205, -4.9304],\n",
      "        [ 0.0408,  3.9069, -4.5591],\n",
      "        [ 2.3442,  0.6682, -4.9554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2922, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4594,  0.5908, -5.1940],\n",
      "        [ 1.8816,  1.8554, -4.5082],\n",
      "        [-0.0744,  3.6023, -4.1824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0774, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0603,  3.3393, -3.9637],\n",
      "        [ 2.7295,  0.9621, -5.1075],\n",
      "        [ 0.0504,  3.3494, -4.7949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1664, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2736,  3.0450, -3.8005],\n",
      "        [ 2.2347,  1.1380, -5.0558],\n",
      "        [ 2.3235,  0.4882, -4.6178]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0960, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3379,  0.4186, -5.0881],\n",
      "        [ 2.8919,  0.1880, -4.9593],\n",
      "        [ 2.6858,  0.2635, -4.9380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4091, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5742,  1.8435, -4.5953],\n",
      "        [ 1.9904,  1.1035, -4.6060],\n",
      "        [ 0.3036,  3.4220, -4.5654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5381,  0.1140, -5.0837],\n",
      "        [ 1.3122,  1.5515, -4.1732],\n",
      "        [ 2.8647,  0.0156, -4.8472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4118, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4971,  0.6838, -4.7031],\n",
      "        [ 1.9709,  1.3156, -4.4728],\n",
      "        [-0.8852,  3.8514, -3.7837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0462, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9241,  4.0265, -3.9464],\n",
      "        [ 2.6301,  0.4958, -4.9864],\n",
      "        [-0.1740,  3.7897, -4.7560]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0409, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9106,  0.2278, -5.0055],\n",
      "        [-0.5097,  3.9200, -3.9654],\n",
      "        [ 2.8208, -0.2941, -4.8649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2830, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2800,  3.3962, -4.1273],\n",
      "        [-0.8020,  3.8003, -3.7545],\n",
      "        [ 1.5295,  1.7547, -4.5473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2784, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0249,  1.0452, -5.1112],\n",
      "        [ 1.2616,  2.0682, -4.4049],\n",
      "        [ 2.7324,  0.8772, -4.9402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1285, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9320,  3.7312, -3.7102],\n",
      "        [ 2.2659,  1.4146, -5.0333],\n",
      "        [ 3.4528, -0.4840, -4.3698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3658, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1970,  1.1644, -4.6895],\n",
      "        [ 0.9383,  2.4250, -4.1285],\n",
      "        [ 1.6552,  1.8815, -4.7050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0423, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7151,  3.0371, -4.2976],\n",
      "        [ 3.2986, -0.3370, -4.5685],\n",
      "        [-0.9583,  4.1518, -4.0586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0673, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7214, -0.0169, -4.5129],\n",
      "        [ 0.2912,  3.1204, -4.3707],\n",
      "        [ 0.5001,  2.9866, -4.0826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5563, -0.8780, -4.0659],\n",
      "        [-1.1159,  3.9471, -3.4752],\n",
      "        [ 3.3413, -0.9004, -3.3756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0746,  3.9128, -3.5829],\n",
      "        [ 3.1512, -0.8694, -4.0721],\n",
      "        [ 3.4634, -0.8761, -3.9756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2357, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4567,  1.1852, -4.5452],\n",
      "        [ 1.7087,  1.1439, -4.3006],\n",
      "        [-0.9965,  3.9290, -3.8522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4604,  1.4343, -4.5855],\n",
      "        [ 3.0763, -0.1275, -4.6287],\n",
      "        [-1.3204,  3.6531, -3.0272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0814, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3367,  0.8236, -4.7324],\n",
      "        [-0.9947,  3.8627, -3.8498],\n",
      "        [ 3.1547, -0.1538, -4.5093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0344, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5043,  2.8980, -4.0505],\n",
      "        [-0.7321,  4.0637, -3.5363],\n",
      "        [-0.8543,  4.2858, -3.8370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2477, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6301,  1.6321, -4.3664],\n",
      "        [-1.2537,  3.8013, -3.2482],\n",
      "        [ 2.9255, -0.2186, -4.8828]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3738, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1241, -1.7593, -2.4541],\n",
      "        [-0.9563,  4.2727, -3.7533],\n",
      "        [ 1.6003,  2.3014, -4.6825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1745, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0410,  3.5531, -3.2820],\n",
      "        [ 3.4074, -1.0349, -4.1232],\n",
      "        [ 1.6412,  1.2023, -3.8888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8541, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3747, -1.1285, -3.6001],\n",
      "        [-0.9129,  4.0209, -3.6825],\n",
      "        [ 0.3735,  2.8328, -3.6392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1467, -0.3606, -4.4546],\n",
      "        [-0.8289,  4.1840, -3.6256],\n",
      "        [ 3.3580, -2.0904, -2.4331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3711, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3769, -0.4782, -4.3162],\n",
      "        [ 3.4319, -1.3055, -3.5791],\n",
      "        [ 1.9562,  1.2887, -4.7257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1002, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9929,  2.3373, -4.7437],\n",
      "        [-0.7949,  3.9400, -3.7799],\n",
      "        [ 0.2093,  3.0298, -3.6046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2063, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2098, -2.2465, -1.7587],\n",
      "        [ 1.5638,  1.7644, -4.4912],\n",
      "        [-0.8393,  3.9290, -4.2100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0578,  3.9774, -3.5144],\n",
      "        [ 3.5075, -2.1331, -2.4492],\n",
      "        [-1.1143,  3.9804, -3.4709]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3093, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2479,  3.6110, -3.0065],\n",
      "        [ 1.8135,  1.4135, -4.4043],\n",
      "        [-1.5048,  4.0101, -3.5275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0375, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1803,  4.1513, -3.4942],\n",
      "        [ 0.3354,  2.5836, -4.1524],\n",
      "        [-1.3987,  3.9065, -3.3671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1800, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2105, -2.0200, -1.9808],\n",
      "        [-1.1817,  3.6382, -3.4448],\n",
      "        [ 1.9018,  1.5168, -4.3253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4394, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1776,  2.1617, -4.1958],\n",
      "        [-1.0286,  4.0083, -3.8647],\n",
      "        [ 3.3897, -1.9826, -2.1900]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5866, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5028,  0.9509, -4.7319],\n",
      "        [-1.1711,  3.7076, -3.5879],\n",
      "        [-1.2547,  3.8116, -3.6746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0522, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3076, -0.6131, -4.3448],\n",
      "        [ 0.6251,  2.6216, -4.0828],\n",
      "        [ 3.6786, -1.2024, -4.1223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0891, -2.2617, -1.7774],\n",
      "        [ 3.4574, -1.5258, -3.3622],\n",
      "        [-0.1773,  3.4852, -4.0326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3041, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2779,  1.5768, -4.1991],\n",
      "        [ 3.4017, -2.0319, -2.9006],\n",
      "        [ 3.0453,  0.0812, -4.7822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7384, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2314, -0.8360, -4.1811],\n",
      "        [-0.8893,  4.0558, -3.6501],\n",
      "        [ 2.3411,  1.0551, -4.3592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1146, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1176, -2.1731, -1.9357],\n",
      "        [ 3.1697, -0.1151, -4.6011],\n",
      "        [-1.1466,  3.5413, -3.0929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2015, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7063,  1.4401, -4.6076],\n",
      "        [-0.0251,  3.6239, -4.5460],\n",
      "        [ 3.3398, -1.5188, -3.6608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0838, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2411,  3.4240, -3.0677],\n",
      "        [ 2.7622,  0.6799, -4.5555],\n",
      "        [ 2.7771,  0.7367, -5.0322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0730, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7187,  0.5728, -4.3234],\n",
      "        [ 2.6053,  0.2850, -4.6406],\n",
      "        [-0.9854,  3.3998, -3.5868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0178, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9261,  3.6098, -3.4333],\n",
      "        [-0.9518,  3.4342, -3.7408],\n",
      "        [ 3.0179, -0.5492, -4.0370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7242, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1315,  1.4577, -4.6893],\n",
      "        [ 3.0483,  0.0281, -4.4935],\n",
      "        [ 0.9167,  2.4280, -4.1294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1438, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4092, -0.5224, -4.6407],\n",
      "        [ 2.1079,  1.0031, -4.4218],\n",
      "        [ 0.6883,  2.7163, -4.1906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3585, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4160,  1.2205, -4.3559],\n",
      "        [ 1.5397,  1.5880, -4.4449],\n",
      "        [ 2.8702,  0.5189, -4.4540]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2357, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1890,  2.0438, -4.1802],\n",
      "        [-0.8028,  3.5220, -4.0388],\n",
      "        [ 1.9880,  1.0720, -4.4253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1328, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7405,  0.3998, -4.6083],\n",
      "        [ 2.0594,  0.9349, -4.9086],\n",
      "        [-0.4347,  3.3285, -3.4634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7525, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7239,  3.2686, -3.6607],\n",
      "        [ 2.5498,  0.4475, -4.3261],\n",
      "        [ 3.3714, -0.5605, -4.4262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2406, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8452,  0.2423, -4.1715],\n",
      "        [ 1.7319,  1.5721, -4.7879],\n",
      "        [ 3.2199, -0.2107, -4.6598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0445, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6938,  0.5034, -4.7891],\n",
      "        [ 3.6245, -1.2020, -3.9038],\n",
      "        [ 3.3906, -0.6136, -4.5427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2749, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3757,  2.7481, -3.4872],\n",
      "        [ 1.3852,  1.9262, -4.4848],\n",
      "        [ 1.2694,  2.2479, -4.3816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0855, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1887,  0.7850, -4.6342],\n",
      "        [ 3.1247, -0.5972, -4.1644],\n",
      "        [ 3.3906, -1.1270, -4.0390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1559, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0754,  0.8970, -4.4195],\n",
      "        [ 2.3452,  0.7115, -4.6876],\n",
      "        [-0.4353,  3.5381, -4.1223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6998, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5708, -0.8069, -4.1987],\n",
      "        [ 2.6038,  0.7512, -4.8805],\n",
      "        [ 2.7955,  0.3965, -4.4339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8148, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6399,  0.7616, -4.5447],\n",
      "        [ 1.2950,  1.9882, -4.7115],\n",
      "        [ 3.5284, -0.5662, -4.3577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2832, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2652,  1.4301, -4.5365],\n",
      "        [ 3.8079, -1.3845, -3.6916],\n",
      "        [ 1.4697,  1.9502, -4.6956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3630, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5756,  2.1064, -4.7328],\n",
      "        [ 1.6488,  1.8200, -4.2737],\n",
      "        [ 3.6154, -0.7241, -4.0398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0419, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6619,  3.0366, -3.5379],\n",
      "        [ 3.0538,  0.1439, -4.2872],\n",
      "        [ 3.2409,  0.1809, -4.6960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2434, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8590,  1.4362, -4.5172],\n",
      "        [ 2.3471,  0.9214, -4.5984],\n",
      "        [ 3.4994, -1.3012, -3.6723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0754, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4738, -0.7396, -4.2677],\n",
      "        [ 2.4818,  0.8775, -4.5512],\n",
      "        [-0.7536,  2.9108, -3.4091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3460, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5668,  1.6342, -4.3992],\n",
      "        [-0.5234,  3.3038, -4.0443],\n",
      "        [ 2.1300,  1.0258, -4.4740]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3093,  1.0432, -4.6989],\n",
      "        [-0.5909,  3.1046, -3.5263],\n",
      "        [ 2.4857,  0.9610, -4.6542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2509, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4903, -0.8371, -3.7024],\n",
      "        [-0.1552,  3.5207, -4.3004],\n",
      "        [ 0.0809,  3.3534, -4.7030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3210, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2243,  2.0694, -4.8122],\n",
      "        [-0.3759,  3.1770, -4.1981],\n",
      "        [-0.4195,  3.3482, -4.3285]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4536, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2485,  2.2575, -4.7367],\n",
      "        [ 3.5219, -0.9410, -4.1191],\n",
      "        [-0.5323,  3.0462, -3.8251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4549, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9737,  1.5111, -4.8509],\n",
      "        [ 3.2764, -0.0514, -4.4543],\n",
      "        [-0.1380,  2.8524, -4.4379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1268, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3716,  2.5107, -3.4222],\n",
      "        [-0.1780,  2.8103, -3.9445],\n",
      "        [ 1.2603,  2.4202, -4.8961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3235, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1093,  3.1568, -4.4650],\n",
      "        [ 1.4934,  1.7914, -4.5724],\n",
      "        [ 3.1173,  0.4782, -4.6491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6577, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2915,  2.9916, -4.6153],\n",
      "        [ 1.4509,  1.7556, -4.6726],\n",
      "        [ 1.9693,  1.3524, -4.2871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1161, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8936,  2.6077, -4.6265],\n",
      "        [-0.1820,  2.1807, -3.7069],\n",
      "        [-0.2490,  2.1475, -3.7424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1675, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2794,  2.2776, -3.6904],\n",
      "        [ 1.1730,  2.4992, -5.0582],\n",
      "        [ 2.1582,  0.5855, -4.3953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9925, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6748,  2.2706, -5.1900],\n",
      "        [-0.0938,  2.2439, -3.7391],\n",
      "        [ 2.3698,  0.6945, -4.5134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6970, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9065,  2.2793, -4.8809],\n",
      "        [ 0.0750,  2.2613, -3.7428],\n",
      "        [ 1.9318,  1.1677, -5.0035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5793, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9660,  2.3199, -4.9936],\n",
      "        [ 2.6215,  0.4803, -4.3835],\n",
      "        [ 3.1340, -0.0248, -4.6824]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1395, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5187,  0.8554, -5.0869],\n",
      "        [ 0.8327,  2.5638, -5.1773],\n",
      "        [ 2.8006,  0.3219, -4.4372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4498, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2145,  1.3670, -4.8426],\n",
      "        [ 3.3157, -0.6242, -4.2484],\n",
      "        [ 2.6417,  0.6168, -4.3577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6874, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7775e+00,  3.6507e-01, -4.2294e+00],\n",
      "        [ 2.5876e+00,  9.1001e-01, -4.4565e+00],\n",
      "        [ 4.4061e-03,  2.0270e+00, -4.1474e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2008, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3873,  1.1329, -4.8786],\n",
      "        [ 2.8088,  0.4283, -4.7217],\n",
      "        [ 2.1151,  0.9062, -4.8217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0692, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7385,  0.3913, -4.4995],\n",
      "        [ 2.8476, -0.0539, -4.6802],\n",
      "        [ 2.8869,  0.1210, -4.6487]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7700, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6827,  1.8728, -5.2803],\n",
      "        [ 0.4160,  1.9807, -4.1837],\n",
      "        [ 2.3537,  1.3381, -4.9034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2857, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4069,  2.0960, -5.4238],\n",
      "        [ 2.1767,  1.1882, -5.0728],\n",
      "        [ 2.5167,  0.5615, -4.6448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1724, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1760,  1.2878, -4.6337],\n",
      "        [ 3.1700,  0.0890, -4.3115],\n",
      "        [ 2.4671,  0.4584, -4.9436]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3465, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7381,  1.8298, -5.2206],\n",
      "        [ 0.4348,  2.0747, -4.2243],\n",
      "        [ 2.3117,  0.8638, -4.8221]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2414, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3673,  1.8891, -4.9927],\n",
      "        [ 0.5730,  2.1080, -4.3856],\n",
      "        [ 2.9360,  0.1645, -4.5555]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4404, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6370,  0.7371, -4.8147],\n",
      "        [ 1.9013,  1.6879, -5.3091],\n",
      "        [ 1.1793,  1.9677, -4.9767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2998, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6605,  1.7792, -5.4435],\n",
      "        [ 2.8743,  0.3244, -4.7371],\n",
      "        [ 0.3476,  1.9345, -4.2726]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1056, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6835,  2.4810, -4.5609],\n",
      "        [ 0.2689,  2.3742, -4.4371],\n",
      "        [ 2.8245, -0.2395, -3.9714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8837, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2839,  1.1475, -4.6778],\n",
      "        [ 2.2130,  1.0922, -4.9698],\n",
      "        [ 1.6743,  2.1393, -5.2762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2334, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9408,  1.4601, -4.9016],\n",
      "        [ 0.0580,  2.5219, -4.1844],\n",
      "        [ 0.4981,  2.4364, -4.5617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1224, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3614, -0.3848, -4.1877],\n",
      "        [ 2.1262,  1.0725, -4.6715],\n",
      "        [ 3.0021, -0.1263, -4.7122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2420, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2786,  1.0343, -5.0607],\n",
      "        [ 2.1883,  0.9408, -4.9849],\n",
      "        [ 2.3566,  0.9474, -4.6134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0505, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0226, -0.2131, -4.6676],\n",
      "        [-0.1841,  2.7448, -3.7875],\n",
      "        [-0.3203,  2.5053, -3.9246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2419, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3661,  0.9260, -4.7165],\n",
      "        [ 1.7673,  2.2187, -5.3390],\n",
      "        [ 3.5335, -0.4179, -4.1794]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1408, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0775,  2.5772, -4.2818],\n",
      "        [ 2.9712,  0.3072, -4.7915],\n",
      "        [ 3.0670, -0.1689, -4.8112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0191, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9674,  0.0229, -4.6832],\n",
      "        [-0.8952,  2.5590, -3.4546],\n",
      "        [ 3.1595, -0.4459, -4.0685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0611, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2678,  3.0219, -3.8544],\n",
      "        [ 0.3290,  2.4882, -4.7496],\n",
      "        [-0.4990,  2.8672, -3.4163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2653, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7416,  1.7567, -5.1419],\n",
      "        [-0.1217,  2.8829, -4.1560],\n",
      "        [-0.8304,  2.4061, -2.5507]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1670, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2339,  2.9426, -4.0880],\n",
      "        [ 1.3709,  2.1525, -5.4205],\n",
      "        [ 2.6174,  0.1478, -4.5033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1411, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5290,  0.4887, -4.8950],\n",
      "        [ 0.9005,  2.5599, -5.0742],\n",
      "        [ 2.5629,  0.5514, -4.9764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1941, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5421,  0.9583, -4.7279],\n",
      "        [ 2.2429,  1.1669, -4.7341],\n",
      "        [ 2.6954,  0.4498, -4.8215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9553, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1843,  2.5660, -4.8404],\n",
      "        [ 1.0577,  2.4486, -5.0034],\n",
      "        [ 0.9388,  2.6109, -5.2945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7897, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1719,  2.8524, -2.5677],\n",
      "        [ 0.4199,  2.5479, -4.4228],\n",
      "        [ 2.8181,  0.6188, -5.0388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2239, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7424,  0.2942, -4.5547],\n",
      "        [-0.4922,  2.8298, -3.3495],\n",
      "        [-0.4473,  3.0722, -3.0894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3339, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4191,  2.7186, -3.1056],\n",
      "        [ 2.5774,  0.6050, -4.9822],\n",
      "        [ 1.9735,  1.7249, -4.8069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0911, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2055,  2.5147, -3.7446],\n",
      "        [ 2.6723,  0.5366, -4.7236],\n",
      "        [ 2.5729,  0.2645, -4.6859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8490, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6514,  0.4719, -4.9431],\n",
      "        [ 2.2907,  0.8549, -4.5596],\n",
      "        [-0.2371,  2.8484, -4.0601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2235, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7222,  0.2717, -4.7099],\n",
      "        [-0.0613,  2.5280, -3.7514],\n",
      "        [ 1.6920,  2.0929, -5.3036]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4615, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9641,  1.6890, -4.9886],\n",
      "        [ 1.5872,  1.6972, -5.2547],\n",
      "        [-0.2621,  2.4283, -3.2871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5199, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0430,  2.1849, -3.7503],\n",
      "        [ 0.0753,  2.1206, -3.2271],\n",
      "        [ 2.1251,  1.1160, -4.9936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7400, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7161,  0.3591, -4.6622],\n",
      "        [ 2.3764,  1.6747, -5.3403],\n",
      "        [ 1.9321,  1.3536, -5.0808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1416, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3159,  0.6497, -4.9655],\n",
      "        [ 2.4714,  0.7084, -4.7857],\n",
      "        [ 0.1077,  2.4602, -4.0514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5369,  0.3630, -4.8037],\n",
      "        [ 0.2981,  2.3288, -3.2231],\n",
      "        [ 1.9702,  1.2927, -5.2271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1707, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1860,  2.4379, -3.5974],\n",
      "        [ 2.4260,  1.2719, -4.7774],\n",
      "        [ 2.4200,  0.4818, -4.7512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1983, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3582,  0.9244, -4.6337],\n",
      "        [ 2.4156,  1.2343, -5.3710],\n",
      "        [ 0.5747,  2.7081, -4.7276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7061, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3538,  0.9992, -4.3564],\n",
      "        [ 0.0356,  2.4799, -3.6940],\n",
      "        [ 1.5684,  2.1403, -5.2521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2576, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6380,  2.7151, -5.2064],\n",
      "        [ 0.1696,  2.5598, -3.5480],\n",
      "        [ 1.8224,  1.5455, -5.0937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0946, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6897,  0.8187, -4.7204],\n",
      "        [ 0.3014,  2.9675, -4.2098],\n",
      "        [ 0.1821,  2.7911, -3.9364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1757, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1265,  2.7152, -4.1004],\n",
      "        [ 2.3841,  0.6899, -4.9550],\n",
      "        [ 2.2468,  1.1321, -4.8975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4155, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4565,  0.7684, -4.5513],\n",
      "        [ 2.1592,  1.6876, -5.1794],\n",
      "        [ 1.9942,  1.7768, -5.4283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4531, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1554,  1.4986, -4.9025],\n",
      "        [ 0.3616,  3.0728, -4.5325],\n",
      "        [ 2.4431,  1.0352, -5.0314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4975, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8460,  1.5729, -5.0168],\n",
      "        [ 1.6110,  1.0863, -4.8745],\n",
      "        [ 0.8452,  2.4307, -4.7174]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1443, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5200,  0.2863, -4.9673],\n",
      "        [ 2.2350,  1.1556, -4.8812],\n",
      "        [-0.4368,  2.8669, -3.7728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8863, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1839,  1.1449, -4.7826],\n",
      "        [ 1.5416,  2.0771, -5.1088],\n",
      "        [ 2.0652,  1.0858, -4.7919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3430, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9016,  2.6167, -4.9709],\n",
      "        [ 1.6327,  1.5582, -4.7767],\n",
      "        [ 2.7414,  0.7736, -4.9366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0528, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7251,  0.8894, -4.7199],\n",
      "        [ 0.7985,  2.7103, -4.9544],\n",
      "        [ 1.8793,  1.4035, -4.9686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0753, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2079,  3.0246, -4.0046],\n",
      "        [ 2.4738,  0.6749, -4.6920],\n",
      "        [-0.2960,  3.1282, -4.2311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1726, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1409,  3.0202, -4.0486],\n",
      "        [ 3.0210,  0.4910, -4.8437],\n",
      "        [ 1.2618,  1.9782, -4.9129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1056, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0857,  3.0483, -4.6258],\n",
      "        [ 2.3229,  0.7051, -4.9017],\n",
      "        [ 2.7828,  0.4382, -4.6080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0505, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0113,  2.7581, -4.7309],\n",
      "        [-0.0309,  3.1009, -3.9677],\n",
      "        [-0.2517,  2.8411, -4.2510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7617, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9845,  1.1736, -4.9825],\n",
      "        [ 2.0619,  1.5086, -4.4983],\n",
      "        [ 2.0597,  1.6687, -4.8999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1175, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7436,  0.3903, -4.4736],\n",
      "        [ 2.3679,  0.8666, -4.4878],\n",
      "        [ 0.0552,  2.8719, -3.9405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1367, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6739,  0.8381, -4.9416],\n",
      "        [ 2.2725,  0.8432, -4.4740],\n",
      "        [-0.1077,  2.9655, -4.3209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0946, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3718,  2.7538, -4.9700],\n",
      "        [ 0.0191,  2.8456, -4.5639],\n",
      "        [ 0.5966,  2.5193, -5.3422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1187, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5018,  0.2083, -4.6528],\n",
      "        [ 2.9223,  0.1149, -4.6487],\n",
      "        [ 2.4361,  0.9253, -4.7252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8767,  0.4973, -4.7071],\n",
      "        [ 2.1367,  1.6229, -5.0382],\n",
      "        [ 1.1408,  2.8037, -5.0955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1940, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5198,  2.0445, -5.2737],\n",
      "        [-0.3460,  3.1713, -4.2327],\n",
      "        [ 2.7287,  0.3233, -4.6691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4605, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3139,  3.0628, -5.0591],\n",
      "        [ 2.0554,  1.1158, -4.7890],\n",
      "        [-0.0645,  2.9416, -4.1961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0930, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0396,  2.9311, -5.1422],\n",
      "        [ 0.2984,  2.9541, -4.9797],\n",
      "        [ 2.7383,  0.1047, -4.5606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2035, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6276, -0.2298, -4.5739],\n",
      "        [ 1.2438,  2.0552, -4.9691],\n",
      "        [ 1.0153,  2.6059, -5.4523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0674, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2135,  2.8891, -4.9075],\n",
      "        [-0.4181,  3.3998, -4.0403],\n",
      "        [ 2.8977,  0.7712, -4.9145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0908, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5968,  3.2893, -3.9350],\n",
      "        [ 0.5122,  2.9730, -5.1549],\n",
      "        [ 2.2836,  0.5873, -4.8334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0451, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3393,  3.4101, -3.9082],\n",
      "        [ 2.8161,  0.3445, -4.6833],\n",
      "        [-0.4037,  3.1138, -4.6000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1828, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7983,  0.4801, -4.7507],\n",
      "        [-0.6930,  3.4500, -3.8317],\n",
      "        [ 2.0350,  1.4342, -5.0954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7000,  3.2482, -3.5527],\n",
      "        [-0.8671,  3.6140, -3.7757],\n",
      "        [-0.6592,  3.7879, -4.2491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1451, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8407,  1.3713, -4.9130],\n",
      "        [ 2.6405,  0.3953, -4.6871],\n",
      "        [ 0.3683,  3.1559, -4.8495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4395, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8159,  2.5676, -5.4359],\n",
      "        [ 1.5998,  2.3519, -5.0747],\n",
      "        [-0.4669,  3.4706, -4.4536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2259, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7216,  1.5573, -4.7083],\n",
      "        [-0.3395,  3.1222, -4.4573],\n",
      "        [-0.1156,  3.3531, -4.4847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4867, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9726,  0.1853, -4.6122],\n",
      "        [ 1.0969,  2.0973, -5.2882],\n",
      "        [ 2.9271,  0.5109, -5.0054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1991, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1631, -0.5649, -4.1569],\n",
      "        [ 1.6330,  1.3429, -4.6924],\n",
      "        [-0.8947,  3.4883, -3.6447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1521, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7445,  3.5338, -3.8889],\n",
      "        [ 1.9898,  1.3572, -4.3680],\n",
      "        [-0.7918,  3.4822, -3.6503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3043, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6270,  3.3562, -4.1412],\n",
      "        [-0.7230,  3.4341, -3.8423],\n",
      "        [ 1.9453,  1.6060, -4.7988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0160, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6743,  3.4378, -3.9806],\n",
      "        [ 3.5941, -0.7891, -4.3460],\n",
      "        [-0.3915,  3.6136, -4.1622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1714, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7848,  2.7604, -5.1169],\n",
      "        [ 2.4974,  0.0776, -4.3344],\n",
      "        [ 1.3130,  2.3740, -4.9198]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5127,  3.3044, -4.3356],\n",
      "        [-0.7077,  3.2752, -3.6112],\n",
      "        [-1.0004,  3.5489, -3.6882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0223, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6346,  3.4334, -3.5226],\n",
      "        [ 3.0235, -0.1219, -4.7256],\n",
      "        [ 3.7090, -1.3998, -3.9428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2566, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8820,  1.7881, -5.0565],\n",
      "        [-0.7624,  3.4006, -3.7890],\n",
      "        [-0.7824,  3.6817, -3.8644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4660, -0.9908, -3.7243],\n",
      "        [ 3.8123, -1.1139, -3.7452],\n",
      "        [-0.6056,  3.6933, -3.6622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8141, -1.2387, -3.7291],\n",
      "        [ 3.6380, -0.8244, -3.9187],\n",
      "        [ 3.3228, -1.3292, -2.8825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0386, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8340,  3.3266, -3.4917],\n",
      "        [ 2.5071,  0.0860, -4.2011],\n",
      "        [ 3.5263, -0.8522, -4.0455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2373, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3504, -0.9626, -4.1459],\n",
      "        [-0.9046,  3.4256, -3.4070],\n",
      "        [ 1.7003,  1.7204, -4.8306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6620, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4770,  0.6568, -4.2695],\n",
      "        [ 3.6155, -1.5232, -3.7108],\n",
      "        [ 3.7119, -1.1631, -3.9221]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1138, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0329, -0.4653, -4.3881],\n",
      "        [-0.4583,  3.3091, -4.3359],\n",
      "        [ 0.9914,  2.0929, -4.8023]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0744,  3.6063, -3.7425],\n",
      "        [ 3.4023, -0.7654, -4.3270],\n",
      "        [-0.8058,  3.4833, -3.7546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8070, -1.5026, -3.6622],\n",
      "        [ 3.6273, -0.6144, -4.0471],\n",
      "        [-0.5091,  3.4662, -4.1809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4196, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0838,  3.0035, -4.8817],\n",
      "        [ 1.9068,  1.3604, -4.4379],\n",
      "        [ 2.0185,  0.8925, -4.5914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6941, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6172e+00, -1.2062e+00, -3.8042e+00],\n",
      "        [ 8.1262e-01,  2.6895e+00, -5.0820e+00],\n",
      "        [ 2.9041e+00,  3.6768e-03, -4.7424e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0547, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8527,  2.7258, -5.0255],\n",
      "        [-0.7846,  3.7006, -3.7084],\n",
      "        [-0.9943,  3.7829, -3.5657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0570, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7213,  3.7796, -3.8761],\n",
      "        [ 0.6899,  2.5353, -4.8726],\n",
      "        [ 3.1361, -1.3483, -3.7500]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4972, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6489,  3.5812, -3.6370],\n",
      "        [ 0.8285,  2.9356, -5.2345],\n",
      "        [ 0.7628,  2.7287, -4.9943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0666, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3813, -0.7882, -3.9668],\n",
      "        [ 3.7434, -1.1139, -3.7815],\n",
      "        [ 0.9142,  2.5676, -5.0379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5423, -1.4587, -3.8442],\n",
      "        [ 3.5191, -1.3366, -3.9175],\n",
      "        [ 3.3668, -1.1254, -4.0680]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5776,  3.0982, -3.7474],\n",
      "        [ 3.3317, -1.2968, -3.6660],\n",
      "        [-0.5194,  3.2571, -3.9033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0221, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3226, -0.6095, -4.1295],\n",
      "        [-0.1897,  3.0264, -4.4556],\n",
      "        [ 3.6409, -1.5288, -3.3715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4038, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6216,  3.1767, -4.0511],\n",
      "        [ 3.6197, -1.4900, -3.3957],\n",
      "        [ 1.9358,  1.1221, -4.5991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0679, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7931,  0.3632, -4.3703],\n",
      "        [-0.4275,  3.1759, -4.1392],\n",
      "        [ 2.6022,  0.2415, -4.4268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1561, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6768, -1.0313, -3.9673],\n",
      "        [ 1.2471,  1.9154, -5.2114],\n",
      "        [ 3.0294, -0.0723, -4.3643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0709, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2723,  2.9758, -4.8670],\n",
      "        [ 2.6526,  0.4955, -4.2986],\n",
      "        [-0.2909,  3.0042, -3.9814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0301, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5554,  2.9856, -4.3083],\n",
      "        [-0.3918,  3.2242, -4.1896],\n",
      "        [-0.3887,  3.0029, -4.0663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0398, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2763, -0.8106, -4.4931],\n",
      "        [ 2.5424,  0.1375, -4.1216],\n",
      "        [ 3.3591, -0.9048, -3.9535]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3887, -1.0220, -4.1178],\n",
      "        [ 3.6489, -0.9032, -4.3521],\n",
      "        [-0.3229,  3.0572, -4.5111]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6700, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9949,  2.0303, -4.4858],\n",
      "        [ 0.8009,  2.2265, -4.6259],\n",
      "        [ 2.7819,  0.0413, -4.2786]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4621, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0707,  1.0386, -4.6066],\n",
      "        [-0.2925,  2.9226, -4.0534],\n",
      "        [ 3.5383, -1.3117, -3.9271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0478, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3839,  2.7759, -5.1223],\n",
      "        [ 3.5541, -0.7300, -4.1759],\n",
      "        [-0.2534,  2.9254, -4.3074]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1309, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4959, -1.5373, -3.7918],\n",
      "        [ 0.9993,  2.0623, -4.6879],\n",
      "        [ 2.8400,  0.4452, -4.1636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3146, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3175, -0.9028, -4.1342],\n",
      "        [ 1.4871,  1.9011, -4.5330],\n",
      "        [ 3.7125, -1.5314, -3.1717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3781, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5732, -1.2846, -3.8087],\n",
      "        [ 1.2725,  1.9921, -4.7735],\n",
      "        [ 3.2671, -1.6368, -3.2957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0794, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1325,  3.0047, -4.5496],\n",
      "        [ 2.5438,  0.2544, -3.9641],\n",
      "        [ 2.8182,  0.3845, -4.3565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3094, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5464e+00,  1.2841e+00, -4.1171e+00],\n",
      "        [-2.9898e-01,  3.2164e+00, -4.2580e+00],\n",
      "        [-3.8291e-03,  2.7320e+00, -4.3915e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0619, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5260,  3.0943, -4.0556],\n",
      "        [ 3.1857, -0.0281, -4.1225],\n",
      "        [ 0.5416,  2.6213, -4.6410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4849, -1.9458, -3.2171],\n",
      "        [ 3.5099, -1.7334, -3.1676],\n",
      "        [ 3.4470, -0.8106, -4.2313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1184, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5514, -1.5994, -3.8143],\n",
      "        [ 1.2037,  2.1729, -4.6795],\n",
      "        [-0.5130,  3.1427, -4.0376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2491,  3.0358, -3.8320],\n",
      "        [ 2.8111,  0.3557, -4.2124],\n",
      "        [ 1.3316,  2.0782, -4.9437]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0292, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1640,  2.9485, -4.5891],\n",
      "        [ 3.4843, -1.7328, -3.4666],\n",
      "        [-0.3922,  2.8953, -4.5542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1230, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1259,  2.9280, -4.8278],\n",
      "        [ 1.0947,  2.1396, -4.3844],\n",
      "        [ 3.4219, -1.6642, -3.3463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6056, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9020,  1.0657, -4.5175],\n",
      "        [ 3.4166, -0.9365, -4.3347],\n",
      "        [ 1.1184,  2.2907, -4.9850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5885, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5582, -1.7536, -3.2096],\n",
      "        [ 1.2633,  1.9188, -4.6715],\n",
      "        [ 1.7491,  1.7677, -4.4995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7022, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5904, -1.4425, -3.6697],\n",
      "        [ 0.2913,  3.0684, -4.7102],\n",
      "        [ 3.7327, -1.6103, -3.2536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0395, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3388,  3.2988, -4.3814],\n",
      "        [ 3.4717, -0.7532, -4.1695],\n",
      "        [ 0.3468,  2.8772, -4.9837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6362, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4146, -1.2089, -4.1060],\n",
      "        [ 2.3575,  1.1672, -4.6547],\n",
      "        [ 3.6200, -1.1790, -4.3212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6245, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2880,  0.6338, -4.5100],\n",
      "        [-0.2611,  3.1237, -3.9777],\n",
      "        [ 3.5791, -1.1447, -4.3449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0189, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3997,  3.2415, -3.7398],\n",
      "        [ 3.3180, -0.8590, -4.3204],\n",
      "        [ 3.2514, -1.0390, -4.1576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8889, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4271, -0.5011, -4.5639],\n",
      "        [ 1.8009,  1.4717, -4.7707],\n",
      "        [ 1.3830,  2.1887, -4.7231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0543, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3363,  2.7336, -4.9113],\n",
      "        [-0.2500,  3.0191, -4.7872],\n",
      "        [-0.1912,  3.0843, -4.2859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3984, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3822,  3.1558, -4.1342],\n",
      "        [ 1.3273,  2.0501, -4.5981],\n",
      "        [-0.0120,  3.0394, -4.8836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0545, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1823,  3.0386, -5.0454],\n",
      "        [ 2.8390,  0.2547, -5.1452],\n",
      "        [-0.2389,  3.1471, -3.8195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1079, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8618,  0.4537, -5.0946],\n",
      "        [ 2.3920,  0.2976, -4.8927],\n",
      "        [ 2.5666,  0.5064, -4.7396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2808, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0362,  3.3068, -4.2103],\n",
      "        [ 1.6114,  1.4934, -4.5621],\n",
      "        [-0.0846,  2.8593, -4.3185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4034, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3295,  1.9097, -4.8222],\n",
      "        [-0.4157,  3.3634, -3.7780],\n",
      "        [ 2.5755,  0.8285, -4.9411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1075, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5971,  0.4564, -5.1616],\n",
      "        [ 2.7779,  0.5676, -4.8575],\n",
      "        [ 0.4185,  2.6091, -4.7240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0748, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2945,  3.0837, -4.2050],\n",
      "        [-0.5003,  3.2500, -3.8649],\n",
      "        [ 2.4319,  0.7175, -5.1070]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1191, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1124,  1.4361, -5.2135],\n",
      "        [ 2.4697,  0.3461, -5.3141],\n",
      "        [-0.3416,  3.0952, -3.9796]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0701, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3134,  3.0817, -4.0264],\n",
      "        [ 2.3115,  0.4129, -5.4374],\n",
      "        [-0.2898,  3.0259, -3.8764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2505, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8568,  2.3633, -4.3960],\n",
      "        [ 2.6500,  0.2936, -5.2177],\n",
      "        [ 1.2892,  1.8314, -4.4987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1982, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2222,  1.3289, -4.8310],\n",
      "        [ 2.4723,  1.0177, -5.0547],\n",
      "        [-0.1248,  3.0685, -4.5708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5343, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9806,  1.3407, -4.9212],\n",
      "        [ 2.3638,  0.8842, -5.0276],\n",
      "        [ 1.2688,  1.7651, -4.5430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5443, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7228,  2.1182, -4.8153],\n",
      "        [ 1.8705,  1.6027, -4.7441],\n",
      "        [ 0.9968,  2.1272, -4.5909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0860, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4287,  3.1897, -4.0920],\n",
      "        [-0.2613,  3.2097, -4.0431],\n",
      "        [ 2.4243,  0.9117, -5.0569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1229, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3861,  3.1469, -3.9020],\n",
      "        [ 2.4648,  0.8653, -4.9587],\n",
      "        [ 2.5295,  0.7382, -5.3689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6115, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3322,  1.9793, -4.6921],\n",
      "        [ 1.4792,  1.5679, -4.5469],\n",
      "        [-0.4654,  3.2080, -3.9749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2017, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0000,  1.2916, -4.8289],\n",
      "        [ 2.2529,  0.6322, -5.1300],\n",
      "        [-0.6415,  3.1489, -3.9695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3392, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4358,  3.1170, -4.0826],\n",
      "        [ 2.1194,  1.5571, -5.2486],\n",
      "        [ 1.9046,  1.5616, -4.6116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3451, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5893,  0.6543, -5.2712],\n",
      "        [ 1.3679,  1.4642, -4.3789],\n",
      "        [ 1.0157,  2.2683, -4.4903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1607, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3352,  0.7584, -5.4584],\n",
      "        [ 2.1849,  0.9812, -4.6876],\n",
      "        [-0.3938,  3.0987, -3.8973]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4165, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7792,  1.5456, -4.5666],\n",
      "        [ 2.7412,  0.5003, -5.5173],\n",
      "        [ 2.0870,  1.1471, -4.7988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2701, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5446,  1.5216, -4.1082],\n",
      "        [ 0.4132,  2.9359, -4.7382],\n",
      "        [-0.4535,  3.2004, -4.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0529, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6900,  0.2424, -5.2657],\n",
      "        [-0.4660,  3.4394, -4.2657],\n",
      "        [ 0.2162,  3.0982, -4.2879]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0516, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7936,  0.5708, -5.3344],\n",
      "        [-0.5866,  3.2847, -3.8958],\n",
      "        [-0.5022,  3.0138, -3.8346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5801,  3.4800, -3.6483],\n",
      "        [-0.6213,  3.4076, -3.9686],\n",
      "        [-0.2870,  3.1806, -4.1546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2834, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9473,  0.9888, -4.8985],\n",
      "        [ 1.8761,  1.4521, -4.3857],\n",
      "        [-0.5486,  3.3805, -3.7883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1539, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3426,  2.0315, -4.6349],\n",
      "        [-0.5322,  3.3141, -3.8655],\n",
      "        [-0.1823,  3.2550, -4.4595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2432, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5120,  3.1938, -3.4834],\n",
      "        [ 1.8097,  1.5624, -4.2097],\n",
      "        [ 2.4228,  0.4078, -5.4624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7802, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8186,  3.1065, -3.9874],\n",
      "        [ 1.6504,  1.7569, -4.9975],\n",
      "        [ 1.1315,  2.4697, -4.7076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0613, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6417,  0.1811, -5.2760],\n",
      "        [ 0.4089,  2.9856, -4.9094],\n",
      "        [-0.2105,  3.3796, -3.5780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0556, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3816,  3.1891, -4.3802],\n",
      "        [ 0.4523,  3.0488, -4.9524],\n",
      "        [ 2.8582,  0.1765, -5.3765]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0691, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7195,  0.5001, -4.9451],\n",
      "        [-0.6976,  3.2697, -3.5131],\n",
      "        [ 2.7478,  0.3051, -4.9198]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0897, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7875,  0.2113, -5.1905],\n",
      "        [ 2.6294,  0.2844, -5.2752],\n",
      "        [ 2.5694,  0.3497, -4.9985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0700, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8218,  3.3963, -3.5635],\n",
      "        [ 2.7021,  0.7555, -4.9165],\n",
      "        [ 2.8929,  0.1159, -5.0767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3166, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6924, -0.2538, -5.1561],\n",
      "        [ 3.0060,  0.3245, -4.8684],\n",
      "        [ 1.6125,  1.3556, -4.2102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2888, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9424,  0.0121, -5.2521],\n",
      "        [ 2.6357,  0.5655, -4.7960],\n",
      "        [ 1.6318,  1.6309, -4.5184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0513, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5096,  0.6120, -4.3381],\n",
      "        [ 3.0395,  0.3930, -4.9818],\n",
      "        [ 2.0830,  0.9662, -5.1223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0522, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7583,  3.2403, -3.7927],\n",
      "        [ 3.1368, -0.2263, -5.3689],\n",
      "        [ 0.6661,  2.8930, -4.1945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6420, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5078,  1.4637, -4.3774],\n",
      "        [-0.5136,  3.2537, -4.2732],\n",
      "        [ 2.0203,  1.2024, -3.7213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1886, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5356,  1.1659, -4.1702],\n",
      "        [-0.3700,  3.3440, -4.3291],\n",
      "        [-0.8614,  3.4662, -3.6200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0408, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7624,  3.4161, -3.8058],\n",
      "        [ 2.8007,  0.4100, -4.8183],\n",
      "        [-0.5942,  3.4335, -3.8390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3054, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0913, -0.0165, -4.9145],\n",
      "        [ 0.7686,  2.2771, -4.9593],\n",
      "        [ 1.6629,  1.7085, -4.3995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0595, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6394,  3.4111, -4.0667],\n",
      "        [ 2.8182,  0.3143, -4.8538],\n",
      "        [ 2.7734,  0.3024, -4.8249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4236, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1848,  1.1706, -4.6324],\n",
      "        [-0.6163,  3.4199, -3.8969],\n",
      "        [ 1.3848,  1.8310, -4.2275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3594, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1768,  1.0132, -3.9620],\n",
      "        [ 1.7827,  1.6678, -4.1737],\n",
      "        [ 0.6415,  2.3607, -4.2405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1813, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0832,  1.8444, -3.9685],\n",
      "        [ 2.3546,  0.4956, -4.4454],\n",
      "        [-0.9706,  3.4624, -3.5000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1859, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0778,  3.4176, -3.5858],\n",
      "        [ 1.5676,  1.0951, -3.9900],\n",
      "        [ 0.4709,  3.2814, -4.8231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3707, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7563,  0.6793, -4.2978],\n",
      "        [ 1.2115,  1.7200, -3.8392],\n",
      "        [-1.0931,  3.4223, -3.8219]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1319, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4253,  0.4900, -4.1873],\n",
      "        [ 2.6428, -0.0148, -4.8969],\n",
      "        [ 2.4125,  0.8511, -4.4252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1602, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0098,  0.5707, -4.0164],\n",
      "        [ 1.8692,  0.6194, -3.9230],\n",
      "        [-1.2066,  3.3596, -3.4708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0692, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4686,  3.6004, -4.4557],\n",
      "        [ 2.3163,  0.6778, -4.2606],\n",
      "        [-0.9669,  3.5590, -3.4811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5207, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0051,  1.4600, -3.5842],\n",
      "        [-1.0895,  3.4553, -3.5141],\n",
      "        [ 0.9451,  1.5671, -3.6387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0267, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0101,  0.0922, -5.2491],\n",
      "        [-0.5810,  3.6337, -4.1157],\n",
      "        [-0.8566,  3.6059, -3.7319]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5329, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2570,  0.1865, -4.5680],\n",
      "        [ 2.4097,  0.6483, -4.4694],\n",
      "        [ 2.1468,  1.0401, -4.4506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0272, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7810,  3.3677, -3.4954],\n",
      "        [-1.3097,  3.5877, -3.5725],\n",
      "        [ 2.9080,  0.0573, -4.7028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0286, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0706,  3.5945, -3.6341],\n",
      "        [ 3.2337, -0.0100, -4.7888],\n",
      "        [ 3.1423, -0.1409, -4.9523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1312, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7277,  3.5043, -3.3506],\n",
      "        [ 1.6373,  0.8098, -3.8498],\n",
      "        [-0.9892,  3.4562, -3.5704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9550, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9784,  1.7464, -3.6101],\n",
      "        [ 1.0396,  2.5152, -4.4336],\n",
      "        [ 3.2758, -0.2118, -4.8899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0341, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1224, -0.1653, -4.7355],\n",
      "        [-1.0787,  3.6071, -3.5867],\n",
      "        [ 3.0618,  0.1858, -4.7678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2594, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5304,  1.5738, -4.2134],\n",
      "        [ 3.2172, -0.1379, -4.7572],\n",
      "        [ 3.3822, -0.2265, -5.0175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0553, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5687, -0.2782, -4.6844],\n",
      "        [ 2.9424,  0.3226, -4.5978],\n",
      "        [ 2.6759,  0.0939, -4.3060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0333, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2853, -0.3544, -5.0313],\n",
      "        [-0.5401,  3.5660, -4.3555],\n",
      "        [ 2.8337, -0.0111, -4.4261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2408, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9014,  3.3606, -4.0140],\n",
      "        [ 1.5272,  1.5339, -4.5554],\n",
      "        [-0.8264,  3.2743, -4.1625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3683, grad_fn=<NllLossBackward0>), logits=tensor([[-2.3332e-03,  3.2669e+00, -4.6138e+00],\n",
      "        [-7.9641e-01,  3.7815e+00, -3.8130e+00],\n",
      "        [ 1.8296e+00,  1.2034e+00, -4.0411e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0435, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5393, -0.1564, -5.1167],\n",
      "        [-0.6287,  3.4569, -3.6722],\n",
      "        [ 2.6335,  0.2418, -4.2943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0210, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3367, -0.3187, -5.1408],\n",
      "        [ 3.4564, -0.5737, -5.0865],\n",
      "        [ 3.3072, -0.6319, -4.9614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0224, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4847, -0.4419, -5.0993],\n",
      "        [ 3.3438, -0.3739, -5.0508],\n",
      "        [ 2.4194,  0.4551, -4.3636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5961, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2702,  0.7303, -4.1635],\n",
      "        [-1.0155,  3.4123, -3.3054],\n",
      "        [ 2.9296, -0.2849, -4.9099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7718, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0964,  0.6467, -3.9741],\n",
      "        [ 1.4247,  1.0751, -3.4905],\n",
      "        [ 2.8577,  0.1575, -4.7253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2288, -0.4181, -5.0924],\n",
      "        [ 2.7191, -0.0632, -4.8857],\n",
      "        [ 3.4073,  0.0695, -4.6929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7713, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9763,  3.6811, -3.7173],\n",
      "        [ 2.6034,  0.4178, -4.4423],\n",
      "        [-0.8099,  3.7303, -4.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2899, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4459,  1.3881, -3.7450],\n",
      "        [-0.9157,  3.5817, -3.4571],\n",
      "        [ 2.4639,  0.5000, -4.2741]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1114, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1702,  0.7657, -4.0874],\n",
      "        [ 2.9160,  0.1453, -4.9160],\n",
      "        [ 3.0924,  0.1520, -4.5248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4050, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5096,  0.9964, -3.7181],\n",
      "        [ 2.2175,  0.6698, -3.7204],\n",
      "        [-0.1695,  3.2020, -4.5632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0399, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6977,  0.3758, -4.5286],\n",
      "        [-0.9601,  3.5481, -3.3776],\n",
      "        [-0.7792,  3.5821, -3.4991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1127, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9516,  2.3320, -4.3045],\n",
      "        [-0.8324,  3.7712, -3.6598],\n",
      "        [ 2.5171,  0.2783, -4.3953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9889,  3.4662, -3.6424],\n",
      "        [-0.9594,  3.6406, -3.7370],\n",
      "        [-0.9808,  3.5970, -3.8637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4262, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9010,  3.6531, -3.9694],\n",
      "        [ 1.5434,  1.1170, -3.6617],\n",
      "        [ 1.4511,  1.5790, -3.6607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1433, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6977,  0.9993, -3.6063],\n",
      "        [-1.1592,  3.4089, -3.4722],\n",
      "        [-0.8732,  3.6455, -3.7661]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2009, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1040,  0.5542, -4.1962],\n",
      "        [ 1.7585,  1.0277, -3.2563],\n",
      "        [-0.8757,  3.7020, -3.4722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8577, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7700,  0.8791, -3.8525],\n",
      "        [-1.1209,  3.6188, -3.8819],\n",
      "        [ 0.5834,  2.6850, -4.7617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0869, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1218,  0.6875, -4.2215],\n",
      "        [-0.0135,  3.2852, -4.4190],\n",
      "        [-1.1187,  3.7019, -3.3382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9677, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8265,  3.9162, -3.9394],\n",
      "        [ 0.2119,  3.0400, -4.2484],\n",
      "        [-1.1184,  3.7921, -3.6372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1846, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9348,  0.9820, -3.5474],\n",
      "        [ 2.6615,  0.4689, -4.4512],\n",
      "        [ 2.6719,  0.5881, -4.6203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1415, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9020,  1.9583, -4.2823],\n",
      "        [ 2.6393,  0.4584, -4.3580],\n",
      "        [-0.5238,  3.5852, -3.9678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1114, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0359,  3.3621, -4.7631],\n",
      "        [ 2.5155,  0.8831, -4.5132],\n",
      "        [ 2.2918,  0.2158, -4.4187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4876, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5007,  0.9755, -3.5724],\n",
      "        [-0.0559,  3.3283, -4.7353],\n",
      "        [ 0.9206,  1.5382, -3.5589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1113, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7980,  2.6101, -4.4998],\n",
      "        [ 2.6533,  0.2458, -4.4877],\n",
      "        [ 2.7487,  0.4359, -4.7114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1277, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2654,  3.3827, -4.7412],\n",
      "        [ 2.0428,  0.7742, -3.9676],\n",
      "        [ 2.8304,  0.4612, -4.9475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4811, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9935,  1.8081, -3.4880],\n",
      "        [ 1.0909,  1.6767, -4.0742],\n",
      "        [ 3.1629,  0.0242, -5.0786]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6445, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2369,  3.5459, -4.4368],\n",
      "        [ 1.0887,  1.7934, -3.6338],\n",
      "        [ 2.0919,  0.8381, -4.4406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4496, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9706,  0.1537, -4.8442],\n",
      "        [ 2.7656,  0.6494, -4.5310],\n",
      "        [ 1.7720,  0.9677, -4.0546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1519, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1013,  0.1314, -4.8643],\n",
      "        [ 2.4599,  0.2049, -4.7518],\n",
      "        [ 1.9259,  0.8864, -3.8979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8761, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1271,  1.9774, -4.1919],\n",
      "        [ 1.6046,  1.1907, -4.1767],\n",
      "        [ 1.3871,  0.9349, -3.3845]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2326, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7205,  0.7095, -4.1568],\n",
      "        [ 3.0013,  0.0560, -4.7645],\n",
      "        [ 1.0748,  2.0096, -3.6815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3763, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5204,  1.0872, -3.4652],\n",
      "        [ 0.8192,  2.6956, -4.4679],\n",
      "        [ 3.2448,  0.2402, -5.2986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5487, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5008,  0.2521, -4.4936],\n",
      "        [ 0.7350,  3.1874, -4.9045],\n",
      "        [ 0.9094,  2.1044, -3.5572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2823, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0885,  3.1618, -4.9638],\n",
      "        [ 2.4090,  0.5430, -4.3796],\n",
      "        [ 1.4608,  1.5403, -3.8185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2403, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0337,  1.6029, -3.5054],\n",
      "        [ 0.3411,  2.9673, -4.8705],\n",
      "        [ 2.0708,  0.5419, -3.9213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2803, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1221,  1.6811, -3.7439],\n",
      "        [ 1.2303,  2.4247, -4.4225],\n",
      "        [ 2.6540,  0.5916, -4.3331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3043, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0082,  0.5770, -3.8730],\n",
      "        [ 2.2629,  0.4136, -4.0903],\n",
      "        [ 1.0536,  2.2379, -4.2134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6353, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3226,  3.5777, -4.7501],\n",
      "        [ 1.9124,  0.8866, -3.8968],\n",
      "        [ 1.4743,  1.1559, -3.3389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2410, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8687,  0.3060, -5.2008],\n",
      "        [ 0.7974,  1.6276, -2.9269],\n",
      "        [ 0.8622,  2.0038, -3.8316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4558, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0048,  1.8523, -3.6116],\n",
      "        [ 0.5628,  3.1234, -4.5731],\n",
      "        [ 2.7598,  0.3361, -4.6806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1724, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9039,  2.4646, -4.6149],\n",
      "        [ 0.5906,  2.5690, -4.5908],\n",
      "        [ 2.2420,  0.7002, -3.8351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2723, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2449,  0.6836, -4.4473],\n",
      "        [-0.6901,  3.5348, -4.1430],\n",
      "        [ 1.2499,  1.0603, -3.0345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1129, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6927,  2.6706, -4.4094],\n",
      "        [ 2.0767,  0.5173, -4.0658],\n",
      "        [-0.5016,  3.6727, -4.1825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5856, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8482,  2.3609, -4.1522],\n",
      "        [-0.6509,  3.5595, -3.7422],\n",
      "        [-0.1311,  3.4252, -4.5930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5621, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1835,  1.7977, -3.5780],\n",
      "        [ 1.1792,  1.0051, -2.5539],\n",
      "        [-0.7138,  3.5959, -3.9533]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6055, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1276,  1.2135, -2.9475],\n",
      "        [ 1.5702,  1.2213, -3.7975],\n",
      "        [ 1.5136,  1.1544, -3.1896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5225, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7850,  1.8567, -3.5599],\n",
      "        [ 0.7174,  2.3168, -4.0373],\n",
      "        [-0.7311,  3.6933, -3.8143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0752, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7342,  2.2432, -3.6992],\n",
      "        [-0.4888,  3.9622, -4.1212],\n",
      "        [-0.5998,  3.8760, -4.0492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9519, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1192,  1.7956, -3.6959],\n",
      "        [ 1.5285,  1.0799, -3.5248],\n",
      "        [ 1.0656,  2.3061, -4.3406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1467, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4674,  0.2946, -4.0251],\n",
      "        [-0.6186,  3.5506, -4.0991],\n",
      "        [ 1.0768,  2.0753, -4.0743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4363, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5406,  0.2161, -4.7092],\n",
      "        [ 1.5724,  0.7217, -3.5720],\n",
      "        [ 1.1692,  1.4630, -3.3906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7746, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5681,  2.1379, -4.4171],\n",
      "        [ 2.4343,  0.4614, -4.4492],\n",
      "        [ 1.6406,  1.0139, -3.3138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2349, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6450,  0.2189, -4.3753],\n",
      "        [ 1.1683,  1.5994, -3.8406],\n",
      "        [ 2.3202,  0.2143, -4.5449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8117,  0.1300, -4.4947],\n",
      "        [-0.9307,  3.7404, -3.8950],\n",
      "        [ 3.0501,  0.2005, -5.1763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4160, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1082,  0.0090, -5.0601],\n",
      "        [ 1.8407,  1.1356, -4.1837],\n",
      "        [ 2.5679,  0.2612, -4.7728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0623, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0002, -0.0895, -5.1843],\n",
      "        [ 2.6883,  0.3457, -4.4711],\n",
      "        [ 2.9257, -0.0593, -4.6687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0363, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2671,  0.0581, -4.8869],\n",
      "        [ 2.9637,  0.0482, -5.3376],\n",
      "        [-0.6053,  3.5548, -3.8055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0911, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4685,  0.5379, -4.3129],\n",
      "        [-0.8191,  3.5282, -3.7399],\n",
      "        [ 0.7630,  2.7976, -4.7177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0930, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6221,  0.3488, -4.7344],\n",
      "        [ 3.1845, -0.0537, -4.9975],\n",
      "        [ 2.1458,  0.2503, -4.1695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4199, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0661,  1.8028, -4.0019],\n",
      "        [ 1.7552,  1.2913, -4.4905],\n",
      "        [ 1.2065,  1.9898, -4.4619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8451, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4214,  1.1715, -4.0774],\n",
      "        [ 1.1527,  1.7593, -4.7688],\n",
      "        [ 1.2375,  1.3057, -3.3691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0437, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0767,  0.0226, -4.8052],\n",
      "        [-0.5142,  3.6365, -4.4520],\n",
      "        [-0.8120,  3.4649, -3.6852]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1989, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1199,  1.6182, -4.2395],\n",
      "        [ 3.1017, -0.3196, -4.7004],\n",
      "        [ 2.5576,  0.1592, -4.5227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7609, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4409,  0.6907, -4.6592],\n",
      "        [ 2.8716, -0.2082, -4.9349],\n",
      "        [ 1.9167,  0.9573, -3.9619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1780, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0731,  0.0150, -4.8023],\n",
      "        [ 1.0285,  2.5792, -5.1055],\n",
      "        [ 1.7985,  0.7210, -4.0957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2918, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2406,  1.3622, -3.3697],\n",
      "        [-0.8771,  3.7199, -4.0704],\n",
      "        [ 1.0536,  2.4281, -4.8467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0443, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9333, -0.0692, -4.7023],\n",
      "        [-0.1652,  3.8081, -4.7213],\n",
      "        [ 2.9794,  0.2752, -4.6205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3626, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3388,  3.1364, -4.9410],\n",
      "        [ 1.2690,  1.3029, -3.6378],\n",
      "        [ 1.7888,  0.9053, -3.8218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1537, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7317e+00,  8.8885e-01, -4.0924e+00],\n",
      "        [ 2.9430e+00,  3.8973e-03, -5.0842e+00],\n",
      "        [ 3.0836e+00,  8.9220e-02, -4.7117e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0159, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2510, -0.3281, -5.0043],\n",
      "        [-0.6700,  3.7861, -4.1455],\n",
      "        [-1.1802,  3.7480, -3.4353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1782, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0734, -0.1863, -5.0885],\n",
      "        [ 1.2397,  1.8878, -4.5024],\n",
      "        [ 2.8146,  0.2556, -4.7977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2884, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4802,  0.5946, -4.7206],\n",
      "        [ 2.0385,  0.8317, -3.5455],\n",
      "        [ 1.5986,  1.0516, -4.1505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4504, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9781,  1.7154, -3.9432],\n",
      "        [ 2.8740,  0.1286, -5.1131],\n",
      "        [ 1.3922,  1.7600, -4.2816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8326,  3.6081, -4.0823],\n",
      "        [-0.7390,  3.7915, -4.0866],\n",
      "        [-0.8792,  3.4683, -3.6621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0699, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1264,  3.6922, -3.8078],\n",
      "        [-1.2128,  3.6995, -4.0250],\n",
      "        [ 0.9612,  2.5105, -4.5914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1132, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9894, -0.1064, -4.8482],\n",
      "        [ 2.7953,  0.0234, -4.7770],\n",
      "        [ 1.9429,  0.5901, -3.3410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5343, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5834,  1.9332, -4.6084]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 1/3, Loss_per_epoch: 409.0516459820792\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1238, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2541,  3.6562, -4.7442],\n",
      "        [ 3.0707,  0.2276, -4.6802],\n",
      "        [ 1.9181,  0.8355, -3.7149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0307, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2011,  3.4389, -3.1159],\n",
      "        [ 3.1861, -0.1119, -5.2601],\n",
      "        [ 3.1703,  0.0757, -4.6001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1402, -0.1342, -4.9002],\n",
      "        [-1.2840,  3.6653, -3.4550],\n",
      "        [-1.1893,  3.7536, -3.4269]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0163, -0.2866, -5.2324],\n",
      "        [ 3.0919, -0.1739, -4.8743],\n",
      "        [ 1.0391,  2.3149, -4.2999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0949, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2079,  0.1149, -4.4624],\n",
      "        [-1.3263,  3.9289, -3.2408],\n",
      "        [ 2.2871,  0.9458, -3.9322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1099, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8354,  0.8330, -3.6288],\n",
      "        [-1.1434,  3.7747, -3.2300],\n",
      "        [-1.3899,  3.8576, -3.5137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3008, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2075,  3.5925, -4.5951],\n",
      "        [ 3.3505, -0.4162, -4.9426],\n",
      "        [ 2.9854,  0.1413, -4.7754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0549, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0975,  3.7296, -3.6589],\n",
      "        [ 2.4881,  0.6361, -4.4924],\n",
      "        [-1.0636,  3.6568, -3.4716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0099, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0348,  3.8280, -3.5438],\n",
      "        [-0.8583,  3.7217, -3.5981],\n",
      "        [-0.7956,  3.7722, -4.3111]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0185, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4139, -0.5853, -5.2896],\n",
      "        [-0.9302,  3.6968, -3.5431],\n",
      "        [ 3.2414, -0.3757, -5.0818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1280, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4580, -0.4676, -5.0258],\n",
      "        [ 1.7148,  0.7664, -3.5401],\n",
      "        [ 3.2370, -0.1670, -4.9300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2796,  3.6947, -2.9966],\n",
      "        [-1.2334,  3.9727, -3.2844],\n",
      "        [-1.1959,  3.9102, -3.6620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1112, -0.6231, -5.0106],\n",
      "        [-1.1493,  3.7123, -3.6487],\n",
      "        [-0.9134,  3.8714, -3.8032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2188, -0.4951, -5.1248],\n",
      "        [-0.7209,  3.5988, -4.2373],\n",
      "        [-1.1356,  3.7449, -3.5796]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0188, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0649,  3.9818, -3.8244],\n",
      "        [ 3.4283, -0.2906, -5.0902],\n",
      "        [ 3.1746, -0.4943, -4.7131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3353, -0.2062, -5.1883],\n",
      "        [ 3.1299, -0.6999, -5.0370],\n",
      "        [ 3.5766, -0.2514, -5.0959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0362, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2694, -0.5864, -4.8769],\n",
      "        [ 3.3873, -0.5510, -5.0384],\n",
      "        [ 0.3308,  2.9956, -4.6620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0283, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6066, -0.6283, -5.0467],\n",
      "        [ 3.2320, -0.1281, -4.8853],\n",
      "        [ 0.1215,  3.4370, -5.0132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3934, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3326,  1.3415, -3.4255],\n",
      "        [ 2.3384,  0.8577, -4.8027],\n",
      "        [ 0.8032,  1.9378, -3.7159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6987, -0.6518, -4.9238],\n",
      "        [-1.1127,  3.9150, -3.4610],\n",
      "        [ 3.5366, -0.5619, -4.9263]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3762, -0.7959, -4.8204],\n",
      "        [-0.8696,  3.6188, -3.9549],\n",
      "        [ 3.6982, -0.6777, -5.0897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3756, -0.8801, -4.9160],\n",
      "        [-0.3955,  3.6502, -4.5573],\n",
      "        [-0.7494,  3.6506, -4.4850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5234, -0.7319, -4.8701],\n",
      "        [-1.0865,  3.9375, -3.4996],\n",
      "        [ 3.8516, -0.7778, -4.6491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3610, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1030, -0.3586, -4.6719],\n",
      "        [ 3.6754, -0.1906, -4.8671],\n",
      "        [ 0.5909,  2.3217, -3.9353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3521,  3.6758, -4.1639],\n",
      "        [-1.1613,  3.9372, -3.6541],\n",
      "        [-0.9533,  3.8666, -3.6784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1027, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4126, -0.8702, -5.0172],\n",
      "        [ 1.0787,  2.2306, -3.7432],\n",
      "        [ 3.5153, -0.5307, -4.9634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2385, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9338,  0.1288, -4.7271],\n",
      "        [ 3.6082, -0.7089, -4.9721],\n",
      "        [ 1.6126,  1.7185, -4.3278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0482, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0221,  3.8028, -3.4848],\n",
      "        [ 2.4741,  0.3519, -4.3618],\n",
      "        [ 3.4109, -0.4219, -4.4669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0258, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3490, -0.3290, -4.5997],\n",
      "        [ 3.4573, -0.5575, -4.6838],\n",
      "        [ 3.1676, -0.2022, -5.0975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0447, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6828,  0.2376, -4.8237],\n",
      "        [-0.8288,  3.9820, -3.8816],\n",
      "        [-0.0570,  3.1040, -4.7385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0097, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1767,  3.9117, -3.6971],\n",
      "        [ 3.6005, -0.5945, -4.9375],\n",
      "        [-1.1018,  3.8675, -3.6720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0149, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7248, -0.7824, -4.8732],\n",
      "        [ 3.6788, -0.3220, -4.9186],\n",
      "        [-0.4448,  3.7520, -4.4162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7486, -0.7194, -4.6331],\n",
      "        [ 0.2568,  3.4683, -4.3767],\n",
      "        [-1.0097,  3.6641, -3.4821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0233, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8659, -0.0482, -4.6909],\n",
      "        [-1.0812,  3.8298, -3.4886],\n",
      "        [-0.9466,  3.8616, -3.5862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0313, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2576,  3.8340, -3.5751],\n",
      "        [-1.0484,  3.9361, -3.8469],\n",
      "        [ 0.4706,  2.9621, -4.7420]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0474, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7361,  0.2550, -4.5785],\n",
      "        [ 2.9074, -0.0284, -4.1298],\n",
      "        [-1.0258,  3.7996, -3.3606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0130, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4564, -0.1918, -4.6727],\n",
      "        [-1.2743,  3.7218, -3.4289],\n",
      "        [-1.3411,  3.9613, -3.4262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3768, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0655, -0.3495, -4.9578],\n",
      "        [ 1.5563,  1.1498, -3.4883],\n",
      "        [ 2.1853,  0.4795, -4.1231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3861, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6319,  1.0205, -3.4255],\n",
      "        [ 1.2667,  1.3046, -3.6042],\n",
      "        [-1.2994,  4.1389, -3.6516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7409, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2809,  4.0742, -3.0077],\n",
      "        [ 1.8289,  0.9579, -4.4958],\n",
      "        [ 0.6574,  2.3541, -4.1552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0210, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2260, -0.4324, -4.5697],\n",
      "        [ 3.7215, -0.7273, -4.9520],\n",
      "        [ 3.4438, -0.2310, -5.0505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0531, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5587, -0.6082, -5.0432],\n",
      "        [-0.1749,  3.3850, -4.3567],\n",
      "        [ 2.6778,  0.5702, -4.6601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8481, -0.3533, -4.8420],\n",
      "        [-1.3549,  3.7998, -3.7554],\n",
      "        [ 3.3140, -0.4647, -5.0096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3968, -0.4856, -4.8019],\n",
      "        [-1.1436,  3.9975, -3.5104],\n",
      "        [-1.3248,  3.9490, -3.7995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0223, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2166, -0.3148, -4.6694],\n",
      "        [ 3.5691, -0.3734, -4.8367],\n",
      "        [ 3.4311, -0.5792, -4.8674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1533,  3.3476, -4.8168],\n",
      "        [ 3.6547, -0.7442, -4.8446],\n",
      "        [ 3.5449, -0.5485, -4.4280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6047,  3.9132, -4.1606],\n",
      "        [-1.0630,  3.9223, -3.5047],\n",
      "        [ 3.6265, -0.7147, -4.9558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4995, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6056, -0.3201, -4.9217],\n",
      "        [-1.2917,  4.2073, -3.5978],\n",
      "        [ 1.1050,  2.3178, -4.2506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7554, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5960, -0.5145, -4.5646],\n",
      "        [ 3.2801, -0.3390, -4.8633],\n",
      "        [ 2.4485,  0.3415, -4.1073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4012, -0.2837, -4.9308],\n",
      "        [ 3.6751, -0.3559, -4.7293],\n",
      "        [ 3.1383, -0.5606, -5.0202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0144, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2884, -0.4783, -4.7648],\n",
      "        [-1.0312,  4.1633, -3.5857],\n",
      "        [-0.4874,  3.7968, -4.5517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2146, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1976,  1.3806, -3.3658],\n",
      "        [-0.9257,  3.7215, -4.3054],\n",
      "        [ 3.4341, -0.3172, -4.8577]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6643, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2441,  2.0821, -3.7851],\n",
      "        [ 3.4313, -0.6043, -5.0854],\n",
      "        [ 1.0748,  1.3320, -3.3439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0957, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9845,  2.2798, -4.1746],\n",
      "        [ 3.3960, -0.4697, -5.0824],\n",
      "        [ 3.2539, -0.5216, -4.6719]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2595, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2052, -0.2155, -4.5830],\n",
      "        [-1.1389,  3.9020, -3.3122],\n",
      "        [ 0.9070,  1.8979, -3.7843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8872, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4152,  3.7931, -3.4650],\n",
      "        [ 2.6836,  0.1369, -4.4552],\n",
      "        [ 3.0843, -0.3282, -5.2664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0731, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9148, -0.2166, -5.0831],\n",
      "        [ 2.2519,  0.5508, -3.7974],\n",
      "        [-1.1728,  3.9394, -3.4623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0778, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6045,  0.3988, -4.3434],\n",
      "        [ 3.1412,  0.3617, -4.4895],\n",
      "        [ 2.8631,  0.1934, -4.7017]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0368, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8606,  0.0674, -5.0636],\n",
      "        [ 2.9691, -0.0990, -5.1663],\n",
      "        [-1.3702,  4.0349, -3.6648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2794, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3670,  1.3159, -3.2746],\n",
      "        [ 2.9838, -0.2796, -4.9645],\n",
      "        [ 2.8189,  0.2813, -4.8484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0305, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0850,  3.9534, -3.9975],\n",
      "        [-1.2399,  4.0425, -3.9725],\n",
      "        [ 2.7433,  0.2455, -5.2409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0664, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6844,  0.5821, -5.1128],\n",
      "        [-0.3372,  3.5103, -4.2512],\n",
      "        [ 2.8741,  0.1196, -5.0734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2357, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7582,  1.0678, -3.6865],\n",
      "        [ 0.4796,  2.4968, -4.1573],\n",
      "        [ 0.5806,  2.2632, -4.1918]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0953, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8358,  3.8291, -4.1838],\n",
      "        [ 2.6141,  0.5432, -4.8367],\n",
      "        [ 2.4233,  0.6471, -4.8700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1938, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9591,  1.4140, -3.0652],\n",
      "        [-0.8932,  4.1192, -3.9672],\n",
      "        [ 2.6726,  0.1312, -5.0606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4185, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9211,  2.1593, -3.9836],\n",
      "        [-1.2123,  4.0141, -3.7771],\n",
      "        [ 1.4822,  0.9602, -3.2038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5137,  4.0868, -3.4168],\n",
      "        [-1.4093,  4.1936, -3.4872],\n",
      "        [-1.0930,  4.0732, -3.9390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1180, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6983,  0.5773, -5.3562],\n",
      "        [-0.7535,  4.0760, -3.9575],\n",
      "        [ 1.9720,  0.6172, -3.7387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0569, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5083,  0.4825, -4.8979],\n",
      "        [ 0.1666,  3.3584, -4.3825],\n",
      "        [-1.1986,  4.0464, -3.8190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1457, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8786,  1.0104, -3.8829],\n",
      "        [ 0.2842,  2.8052, -4.4789],\n",
      "        [-1.2735,  3.8804, -3.2928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1791, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2570,  4.1462, -3.6049],\n",
      "        [ 1.4486,  1.0726, -3.5761],\n",
      "        [-1.0805,  4.1411, -4.1122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1016, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0892,  4.3262, -3.7229],\n",
      "        [ 1.9869,  0.4796, -4.0300],\n",
      "        [ 2.6821,  0.4035, -4.7142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0418, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5351,  3.8675, -3.6661],\n",
      "        [-1.2409,  4.1828, -3.6509],\n",
      "        [ 2.5733,  0.4705, -5.1163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2601, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2921,  2.8278, -4.5701],\n",
      "        [ 2.9584,  0.2159, -5.4032],\n",
      "        [ 1.3464,  1.2274, -3.2450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3997, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1508,  0.9404, -4.7821],\n",
      "        [ 2.6962,  0.3627, -4.6126],\n",
      "        [ 1.0841,  1.3545, -3.0497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3285, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0869,  4.4744, -3.9110],\n",
      "        [ 0.4220,  2.0330, -3.3776],\n",
      "        [ 1.3025,  1.1153, -3.5103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1601,  3.9925, -3.5206],\n",
      "        [-1.0611,  4.3441, -3.7466],\n",
      "        [ 3.1540,  0.3415, -5.0666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2089, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5243,  1.0947, -3.4430],\n",
      "        [ 2.8593,  0.0718, -5.5615],\n",
      "        [ 3.0211,  0.2568, -5.3097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3659, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6154,  4.1237, -4.2863],\n",
      "        [ 1.5497,  1.2261, -3.9583],\n",
      "        [ 2.1569,  0.7404, -4.2214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0496, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3497,  4.0833, -3.7637],\n",
      "        [ 3.3612,  0.0168, -5.5995],\n",
      "        [ 2.4405,  0.2766, -5.0446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1764, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0953,  3.2376, -4.2937],\n",
      "        [ 2.3401,  0.1257, -4.8820],\n",
      "        [ 2.8932,  0.0413, -5.1915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1120, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3084,  4.0005, -3.3335],\n",
      "        [ 2.0524,  0.9577, -4.2255],\n",
      "        [ 3.1442, -0.0526, -5.3296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2193, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1281,  4.1430, -3.7458],\n",
      "        [ 1.3780,  1.2721, -3.7599],\n",
      "        [-0.8493,  4.0418, -4.3580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0321,  4.3263, -3.7630],\n",
      "        [-1.2950,  3.8327, -3.3396],\n",
      "        [-0.9209,  4.3014, -3.8980]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0567, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0892,  3.2846, -4.1414],\n",
      "        [-0.0206,  3.4918, -4.3746],\n",
      "        [ 2.5574,  0.3614, -4.9286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0212, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0528, -0.0387, -5.2471],\n",
      "        [-0.4981,  3.7668, -4.5065],\n",
      "        [-1.5063,  3.9348, -3.6385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0845, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3163,  4.0077, -3.5778],\n",
      "        [ 2.8626,  0.2275, -5.2408],\n",
      "        [ 2.3312,  0.6941, -4.5206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9878, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1738,  1.7748, -3.6430],\n",
      "        [ 2.4445,  0.6881, -4.3939],\n",
      "        [-1.0738,  4.1341, -3.7340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0358, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9587,  0.0518, -5.3497],\n",
      "        [ 2.8260, -0.1743, -5.5086],\n",
      "        [-1.4492,  3.9027, -3.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0287, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1836,  4.1535, -3.5793],\n",
      "        [ 0.1598,  3.2739, -4.7750],\n",
      "        [ 3.4304,  0.1489, -5.4937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1101, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7089,  2.5550, -4.5980],\n",
      "        [ 0.7325,  2.7249, -4.7262],\n",
      "        [ 3.0982,  0.2164, -5.2834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9217e+00,  1.6201e-03, -5.0373e+00],\n",
      "        [ 2.8253e+00,  2.3019e-01, -5.0601e+00],\n",
      "        [ 2.8554e+00, -3.1226e-04, -5.1829e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0646, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6830,  2.4852, -4.4516],\n",
      "        [-1.5046,  3.9998, -3.4041],\n",
      "        [ 3.2465, -0.0773, -5.3602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1127, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3659, -0.2757, -5.3844],\n",
      "        [ 2.6539,  0.5444, -4.5883],\n",
      "        [ 0.7307,  2.2603, -4.5958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4677, -0.3688, -5.3153],\n",
      "        [-0.4272,  3.7406, -4.5665],\n",
      "        [ 3.2695, -0.6304, -5.1982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0203, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3749, -0.1727, -5.4451],\n",
      "        [ 3.4255, -0.5995, -5.1289],\n",
      "        [ 3.6119, -0.6217, -5.2540]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0217, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3305,  3.4914, -3.4359],\n",
      "        [-0.1491,  3.1089, -4.7027],\n",
      "        [-0.2178,  3.7972, -4.8865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0233, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2583, -0.2771, -5.2650],\n",
      "        [ 3.5796, -0.4975, -5.2177],\n",
      "        [ 3.4534, -0.2750, -5.1397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0132, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5474,  3.9768, -4.0524],\n",
      "        [-1.1978,  4.1400, -3.5026],\n",
      "        [ 3.3292, -0.4256, -5.5351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1103, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3217,  4.1633, -3.5831],\n",
      "        [ 3.4728, -0.7595, -5.3588],\n",
      "        [ 1.0550,  2.0654, -4.3539]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5495,  4.0380, -3.3092],\n",
      "        [-1.2015,  3.8357, -3.4236],\n",
      "        [ 0.0152,  3.3037, -4.1757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0150, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3358, -0.4336, -5.3920],\n",
      "        [-1.5406,  4.0385, -3.0734],\n",
      "        [ 3.4631, -0.5794, -5.3888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6378, -0.3219, -5.3036],\n",
      "        [-1.5434,  3.7890, -3.1014],\n",
      "        [-1.2225,  3.8695, -4.0997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3861,  4.0409, -4.4438],\n",
      "        [ 3.3078, -0.4541, -5.3081],\n",
      "        [ 3.1598, -0.3743, -5.3862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2885, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5312,  1.0338, -3.9195],\n",
      "        [ 0.1147,  3.4511, -4.8423],\n",
      "        [ 3.2093, -0.9255, -4.9236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3329,  4.1222, -3.4024],\n",
      "        [ 3.6221, -0.5988, -5.3723],\n",
      "        [ 3.6152, -0.6293, -5.2723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0175, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4601, -0.4966, -5.2587],\n",
      "        [ 3.1529, -0.3848, -5.3575],\n",
      "        [-1.3081,  4.1937, -3.3913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7173, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5285,  0.5660, -4.8853],\n",
      "        [ 0.3061,  3.4838, -4.5820],\n",
      "        [ 3.5583, -0.5644, -5.2335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1751, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7441,  2.6315, -4.4188],\n",
      "        [ 3.5269, -0.7063, -4.8948],\n",
      "        [ 0.9147,  1.7373, -3.2706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8912, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2875e+00, -4.2194e-01, -5.1790e+00],\n",
      "        [ 2.7653e+00,  3.5437e-01, -4.7068e+00],\n",
      "        [ 3.1027e+00, -5.0978e-03, -4.9948e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1643, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4198, -0.7218, -5.3446],\n",
      "        [-1.6147,  4.0832, -3.1068],\n",
      "        [ 0.9087,  1.4251, -3.4217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0708, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4703,  4.2097, -3.3553],\n",
      "        [ 3.4393, -0.9014, -4.9983],\n",
      "        [ 0.6072,  2.1497, -4.1712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2174, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4182,  2.3943, -3.8833],\n",
      "        [ 1.1168,  1.5604, -4.0019],\n",
      "        [ 3.4444, -0.3412, -5.1652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0827, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1582,  3.9490, -3.9251],\n",
      "        [-1.5328,  4.0257, -3.2177],\n",
      "        [ 0.7606,  2.0860, -4.2712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0200,  4.3012, -3.7667],\n",
      "        [ 3.2674, -0.3698, -5.4956],\n",
      "        [-1.0859,  4.0692, -3.8651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0460, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0490,  0.0297, -5.1128],\n",
      "        [ 2.8219,  0.3967, -4.8473],\n",
      "        [-1.1099,  4.2972, -3.9099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1939, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9902,  1.5191, -3.5599],\n",
      "        [ 2.5514,  0.0848, -4.5720],\n",
      "        [ 3.1557, -0.2697, -5.0814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1025, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7491,  2.5253, -4.0042],\n",
      "        [ 0.4300,  2.5387, -4.3241],\n",
      "        [ 2.9749, -0.3828, -5.3771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7513, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8333,  2.0305, -3.8643],\n",
      "        [ 0.9289,  1.0820, -3.0133],\n",
      "        [-0.6232,  4.0964, -4.3916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1055, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1448,  0.6601, -4.2351],\n",
      "        [ 2.9630, -0.0292, -5.0028],\n",
      "        [ 0.2447,  3.0125, -4.0202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1270, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0526,  1.1984, -3.8859],\n",
      "        [ 3.4608, -0.4748, -5.1123],\n",
      "        [-1.2981,  4.0643, -3.8669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2691, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3015, -0.1641, -5.5950],\n",
      "        [ 3.0937, -0.3282, -5.1863],\n",
      "        [ 1.5385,  1.4433, -4.0065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9379,  4.0072, -4.1377],\n",
      "        [-1.3412,  4.0213, -3.6258],\n",
      "        [-0.2476,  3.9289, -4.7112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0390, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4548, -0.3238, -5.5216],\n",
      "        [ 2.9386,  0.1667, -4.8867],\n",
      "        [ 3.3570, -0.0390, -5.3363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6636, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4653, -0.3403, -5.2000],\n",
      "        [ 0.2111,  3.1493, -4.3780],\n",
      "        [ 0.4975,  2.2514, -3.3914]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0243, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0851, -0.0668, -5.0876],\n",
      "        [-1.2399,  4.3344, -3.5231],\n",
      "        [ 3.2152, -0.4038, -5.2755]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2668, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0752, -0.1715, -5.1270],\n",
      "        [ 1.5554,  1.0067, -3.8835],\n",
      "        [ 0.9176,  1.9620, -4.1370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8355, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8135,  1.0268, -4.2712],\n",
      "        [-0.9753,  4.1126, -4.1091],\n",
      "        [ 3.1034, -0.2304, -5.2271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0231, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0552, -0.0839, -5.1968],\n",
      "        [-0.6760,  3.7577, -4.2246],\n",
      "        [-0.3390,  3.9149, -4.2148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0288, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0970,  0.1106, -4.9491],\n",
      "        [-0.6943,  4.2990, -4.3184],\n",
      "        [ 3.1300, -0.3779, -4.8674]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1469, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8788,  0.9359, -4.1156],\n",
      "        [ 2.8327,  0.1255, -4.6287],\n",
      "        [ 0.2677,  3.3537, -4.7818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3617,  3.9739, -4.2987],\n",
      "        [-0.7660,  3.9186, -4.1129],\n",
      "        [ 3.5216, -0.2423, -5.2952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7268, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5949,  3.5378, -4.0367],\n",
      "        [ 2.4765,  0.4661, -4.6675],\n",
      "        [ 3.2357, -0.3663, -5.0827]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0453, -0.0909, -5.0577],\n",
      "        [ 3.3871, -0.1802, -5.2158],\n",
      "        [ 3.3102, -0.3041, -5.0949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0555, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1022, -0.2373, -5.4836],\n",
      "        [-0.2674,  3.5957, -4.6691],\n",
      "        [ 2.5527,  0.3967, -4.5212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1077, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1617,  3.7554, -4.4999],\n",
      "        [-0.3153,  3.8774, -4.3905],\n",
      "        [ 0.8161,  1.9263, -3.5199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0990, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6488,  2.9942, -4.5558],\n",
      "        [ 2.4336,  0.7574, -4.0613],\n",
      "        [-0.0859,  3.3450, -4.6709]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2088, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0101,  3.5170, -4.3491],\n",
      "        [ 1.0714,  1.3396, -2.8338],\n",
      "        [ 3.3663, -0.5292, -5.3478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1772, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8763, -0.0787, -4.9542],\n",
      "        [ 1.6218,  0.9599, -3.5147],\n",
      "        [ 2.7143,  0.5125, -4.6210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1318, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7027,  2.3201, -4.0408],\n",
      "        [ 2.9038, -0.0097, -4.9844],\n",
      "        [ 0.5872,  2.3501, -4.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0479, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5593,  0.2596, -4.8780],\n",
      "        [-0.1685,  3.4087, -4.8557],\n",
      "        [-0.2865,  3.6402, -4.2300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2759, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1363,  1.2335, -2.8422],\n",
      "        [ 3.4443, -0.5348, -5.5069],\n",
      "        [ 2.9705,  0.1299, -4.8344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0615, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1695,  3.4423, -4.4436],\n",
      "        [ 2.4389,  0.5091, -4.5998],\n",
      "        [-0.2678,  3.5908, -4.1239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0589, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6036,  2.7604, -3.5321],\n",
      "        [ 0.2202,  3.2731, -4.4583],\n",
      "        [-0.0745,  3.8842, -4.4946]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0495, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4993e+00, -3.9512e-01, -5.5847e+00],\n",
      "        [ 2.5123e-03,  3.3177e+00, -4.7164e+00],\n",
      "        [ 2.8077e+00,  4.6660e-01, -5.2726e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6315, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7457,  2.3444, -3.9299],\n",
      "        [ 0.2328,  2.6563, -3.6801],\n",
      "        [-0.1500,  3.6033, -4.3446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2485, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4717,  1.3823, -3.8129],\n",
      "        [ 3.2437, -0.0894, -5.1971],\n",
      "        [ 0.2607,  3.0880, -4.3103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1143, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8108,  0.0939, -5.0070],\n",
      "        [ 0.0928,  3.2445, -4.5682],\n",
      "        [ 0.6722,  2.0101, -3.4460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1151, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8038,  1.9278, -4.2438],\n",
      "        [-0.1325,  3.2815, -4.3189],\n",
      "        [ 3.0923, -0.4289, -4.7553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1881, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2446, -0.5195, -4.9805],\n",
      "        [ 3.1979, -0.0475, -4.9255],\n",
      "        [ 0.9820,  1.4263, -2.9954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0266, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4281, -0.7508, -5.1418],\n",
      "        [-0.2070,  3.5795, -4.4913],\n",
      "        [ 3.3142,  0.1527, -5.3684]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0398, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0710,  0.3326, -5.2018],\n",
      "        [ 0.1817,  3.4865, -4.5380],\n",
      "        [-0.2733,  3.6282, -4.6099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0241, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2462, -0.5769, -5.2436],\n",
      "        [-0.1863,  3.8862, -4.3491],\n",
      "        [ 3.0955, -0.2942, -5.0915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1803, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4926,  1.0389, -3.6880],\n",
      "        [ 3.3648, -0.2524, -5.1708],\n",
      "        [-0.3205,  3.6542, -4.3875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0131, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5514,  3.5506, -4.3871],\n",
      "        [-0.4281,  3.7982, -4.2053],\n",
      "        [-0.8100,  4.0594, -4.4481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3309, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2682,  4.0109, -4.6395],\n",
      "        [ 3.2635, -0.6431, -5.2106],\n",
      "        [ 1.3531,  0.8886, -3.1910]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0583, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3101, -0.6567, -4.9651],\n",
      "        [ 0.1318,  2.7231, -4.1819],\n",
      "        [ 1.3168,  0.8508, -3.6248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9607, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4223,  3.7327, -4.5864],\n",
      "        [ 1.7629,  0.8085, -3.8332],\n",
      "        [ 0.4790,  2.9341, -4.5689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0316, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4073,  3.8525, -4.4807],\n",
      "        [-0.3278,  3.7164, -4.4420],\n",
      "        [ 2.8708,  0.1292, -5.1815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0537, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2006, -0.0716, -5.1745],\n",
      "        [ 3.6672, -0.6193, -5.1561],\n",
      "        [ 0.4539,  2.6146, -4.4943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4922, -0.5736, -5.1487],\n",
      "        [ 3.2367, -0.0861, -5.2573],\n",
      "        [-0.2174,  3.5614, -4.5145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1047, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0268,  2.3605, -4.4620],\n",
      "        [ 2.8824,  0.1751, -5.0697],\n",
      "        [-0.2492,  3.9884, -4.3423]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0329,  3.7480, -4.7702],\n",
      "        [ 3.6053, -0.6624, -5.1660],\n",
      "        [ 0.9734,  1.7718, -3.4676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3020, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0078,  0.5948, -4.5624],\n",
      "        [ 3.2397, -0.4546, -5.1253],\n",
      "        [ 1.2452,  1.3159, -3.5691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2841, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3197, -0.4507, -5.1536],\n",
      "        [ 2.7487,  0.4685, -4.7623],\n",
      "        [ 1.3872,  1.3169, -3.9238]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1066,  3.4740, -4.8413],\n",
      "        [-0.5107,  3.5531, -4.5148],\n",
      "        [ 3.2160, -0.9301, -4.8381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0397, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2225,  3.0975, -5.1169],\n",
      "        [ 2.9846, -0.0351, -5.1952],\n",
      "        [-0.3921,  3.7505, -4.6449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1952, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6491,  0.8678, -3.8123],\n",
      "        [ 0.9587,  2.5069, -4.9443],\n",
      "        [ 3.4833, -0.9035, -5.0283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1908, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2472, -0.8091, -4.7471],\n",
      "        [ 1.0307,  2.2141, -4.4134],\n",
      "        [ 1.1151,  2.2220, -4.3883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2140, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2684,  1.5589, -4.3481],\n",
      "        [-0.5745,  3.6808, -4.6546],\n",
      "        [ 0.4449,  3.1096, -5.0527]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4517, -0.7220, -5.2941],\n",
      "        [-0.4115,  3.8315, -4.3322],\n",
      "        [-0.3716,  3.6814, -4.5998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0255, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5750, -0.6465, -5.0252],\n",
      "        [ 2.9286, -0.0327, -4.9946],\n",
      "        [-0.7090,  3.8244, -4.4538]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1565, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6753,  1.1019, -3.7648],\n",
      "        [-0.7869,  3.9903, -4.0701],\n",
      "        [ 3.3295, -1.2149, -4.6375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0697, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8394,  2.4539, -4.5058],\n",
      "        [ 3.5420, -0.6762, -5.2227],\n",
      "        [-0.5206,  3.9078, -4.4167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3206,  3.3181, -4.5290],\n",
      "        [ 3.7087, -0.8291, -4.8814],\n",
      "        [ 3.6402, -0.6703, -5.1246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5878, -0.8171, -4.9994],\n",
      "        [-0.6816,  3.8670, -4.2804],\n",
      "        [-0.1873,  3.4088, -4.7763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0141, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9739,  4.2667, -4.2248],\n",
      "        [ 3.4323, -0.3186, -5.6073],\n",
      "        [-0.4398,  3.8765, -4.5817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3692, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9568,  2.2888, -4.6768],\n",
      "        [-0.9617,  4.1001, -4.3379],\n",
      "        [ 1.3668,  1.6844, -4.2656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0997, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9780,  0.1633, -5.3704],\n",
      "        [-0.4881,  3.7654, -4.2243],\n",
      "        [ 2.1654,  0.7902, -4.7585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1660,  3.5661, -4.7371],\n",
      "        [-0.0299,  3.8300, -5.0028],\n",
      "        [ 3.0296,  0.0556, -5.1856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6778, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6831,  1.1055, -4.4614],\n",
      "        [ 0.8831,  2.4730, -4.6777],\n",
      "        [ 1.1587,  2.2749, -4.7511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0620, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7968, -0.8880, -4.7372],\n",
      "        [ 2.6951,  0.2848, -4.9952],\n",
      "        [ 2.8751,  0.5120, -5.1224]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0631, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1763,  4.1077, -3.6171],\n",
      "        [-1.0425,  4.1073, -4.1779],\n",
      "        [ 0.7044,  2.3463, -4.6352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2926, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5248, -0.9082, -4.9268],\n",
      "        [ 1.8021,  1.5622, -4.7510],\n",
      "        [ 0.2085,  3.3010, -5.2106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0656,  3.8934, -3.9108],\n",
      "        [-1.1524,  3.9089, -3.7315],\n",
      "        [ 3.4872, -0.7717, -5.2934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5457, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9200,  2.2871, -4.6777],\n",
      "        [ 0.9566,  2.0716, -4.2029],\n",
      "        [-0.8371,  3.8810, -4.2171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0354, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4245,  2.8147, -4.9212],\n",
      "        [ 3.5133, -0.8434, -4.6304],\n",
      "        [-1.3152,  4.0468, -4.0983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0382, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8506,  0.4955, -5.1294],\n",
      "        [ 3.7770, -0.9528, -4.9739],\n",
      "        [ 3.2817, -0.9552, -4.4870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3515, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9264,  1.3291, -4.8485],\n",
      "        [ 3.5482, -1.1862, -4.6198],\n",
      "        [ 3.6162, -1.1133, -4.8611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4236,  3.9251, -3.5285],\n",
      "        [ 3.6350, -1.1203, -4.7219],\n",
      "        [ 3.8887, -0.8922, -4.8427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4321, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6876, -1.3274, -5.1330],\n",
      "        [ 3.6174, -0.8551, -5.1453],\n",
      "        [ 3.6639, -0.6000, -5.3682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6731, -0.8450, -4.8499],\n",
      "        [-1.1177,  4.1507, -3.9467],\n",
      "        [ 3.7357, -1.2712, -5.2658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3537, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6112,  1.0072, -4.3268],\n",
      "        [ 3.3654, -0.9282, -4.7595],\n",
      "        [-1.1060,  4.1247, -4.1196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5661, -1.1073, -4.7519],\n",
      "        [-0.0540,  3.7250, -4.8297],\n",
      "        [ 3.5355, -0.9194, -5.1725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0509, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0608,  3.8247, -3.7886],\n",
      "        [ 0.8368,  2.7941, -4.7239],\n",
      "        [ 3.2235, -1.2251, -4.7502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0070, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5902, -1.0038, -5.0612],\n",
      "        [-0.7784,  4.2270, -4.3228],\n",
      "        [-1.4126,  4.2017, -3.9431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2414, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1168,  3.7819, -3.5984],\n",
      "        [ 3.3602, -0.9247, -4.5819],\n",
      "        [ 1.4306,  1.4424, -3.6473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5380, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4130,  0.7469, -5.2061],\n",
      "        [ 3.0571, -0.4798, -5.6742],\n",
      "        [ 0.9455,  2.0765, -4.6564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0115, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5825, -0.8202, -5.2521],\n",
      "        [-1.0995,  3.8191, -3.9712],\n",
      "        [ 3.4394, -0.7910, -5.1315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0118, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3142, -0.5591, -5.1019],\n",
      "        [-1.1630,  3.9470, -4.0827],\n",
      "        [-0.8343,  3.9787, -4.5209]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2571, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2767,  4.1052, -3.8597],\n",
      "        [ 1.3838,  1.5117, -4.3080],\n",
      "        [-1.2428,  3.9991, -3.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0096, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8466,  3.9653, -4.5021],\n",
      "        [ 3.7317, -0.7262, -5.2564],\n",
      "        [-0.7941,  3.9470, -4.4530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0164, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2423,  3.9883, -3.8447],\n",
      "        [ 2.9288, -0.3801, -5.3304],\n",
      "        [-0.9864,  3.9914, -4.0996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5520,  3.9796, -3.5846],\n",
      "        [-1.5134,  3.9903, -3.4082],\n",
      "        [-1.2571,  3.9619, -3.5292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6030, -0.7874, -4.8721],\n",
      "        [-1.5641,  4.1592, -3.5613],\n",
      "        [-1.0909,  4.0188, -4.0162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6870, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3966,  0.4955, -4.6760],\n",
      "        [-0.7861,  3.7984, -4.2313],\n",
      "        [-1.0106,  3.6809, -4.3123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0306, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4494, -0.4330, -5.3042],\n",
      "        [ 3.1042, -0.6191, -4.8635],\n",
      "        [ 0.1066,  3.1422, -5.0337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0461, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3000, -0.9832, -4.7942],\n",
      "        [ 3.4677, -0.8380, -5.2001],\n",
      "        [ 0.6576,  2.8043, -5.3117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0150, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6479, -1.0431, -5.0157],\n",
      "        [ 3.2794, -0.6413, -4.9719],\n",
      "        [ 3.4823, -0.6613, -5.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0170, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9701, -0.3152, -5.2384],\n",
      "        [-1.0574,  4.2267, -4.2631],\n",
      "        [-1.1118,  3.6711, -4.0532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9507,  3.6697, -4.3689],\n",
      "        [-1.2836,  3.8581, -3.7560],\n",
      "        [-0.4193,  3.6663, -4.7663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1909, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5830,  1.0657, -4.2909],\n",
      "        [ 3.6559, -0.2412, -5.5683],\n",
      "        [ 0.6087,  3.0568, -4.7732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4206, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1533,  4.1915, -4.0229],\n",
      "        [ 1.8779,  0.9758, -3.9900],\n",
      "        [-0.8054,  3.6242, -4.7425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0311, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0929,  0.0613, -5.1724],\n",
      "        [ 3.4627, -0.5917, -5.0362],\n",
      "        [ 3.4661, -0.0792, -5.8048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5692, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6714,  0.4215, -5.2165],\n",
      "        [ 2.1784,  0.8163, -4.6381],\n",
      "        [ 3.5205, -0.6228, -5.2682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1436, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1889,  0.3316, -5.5099],\n",
      "        [ 3.4279, -0.8990, -4.9039],\n",
      "        [ 1.1985,  2.0326, -4.9096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0538, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5348,  0.6429, -5.5420],\n",
      "        [-1.1483,  4.1414, -3.5304],\n",
      "        [ 3.3790, -0.8114, -5.2401]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4736,  3.8089, -3.7109],\n",
      "        [ 3.4232, -0.6036, -5.1726],\n",
      "        [ 2.7618,  0.2955, -5.3068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1070, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6134, -0.6619, -5.2263],\n",
      "        [ 1.0511,  2.0984, -4.7787],\n",
      "        [-1.2960,  3.9970, -3.7318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1978, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5974,  1.3606, -3.9562],\n",
      "        [-1.2887,  4.2086, -3.3347],\n",
      "        [-1.2846,  4.1428, -3.6554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1897,  3.9766, -3.9863],\n",
      "        [ 1.6363,  1.1917, -4.6230],\n",
      "        [-1.1234,  3.9381, -3.8006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0041, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5047,  4.0486, -3.5594],\n",
      "        [-1.6129,  3.9576, -3.6738],\n",
      "        [-1.5494,  4.1459, -3.6217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8163,  4.0863, -4.2676],\n",
      "        [ 3.4962, -0.5417, -5.3075],\n",
      "        [-1.7429,  3.9503, -3.3302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3310, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2814,  0.9468, -4.7879],\n",
      "        [ 2.8586,  0.2225, -5.2561],\n",
      "        [ 1.4037,  1.3912, -4.0706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0161,  0.1943, -5.4697],\n",
      "        [-1.5116,  4.1214, -3.5204],\n",
      "        [-1.4084,  3.8823, -3.8114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3804, -0.6456, -5.3781],\n",
      "        [ 3.3929, -0.3401, -5.1602],\n",
      "        [ 3.4020, -0.4586, -5.3819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1175, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5040,  4.0659, -3.1155],\n",
      "        [ 3.1646,  0.0531, -5.3164],\n",
      "        [ 1.9666,  0.9271, -4.3414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6155, -0.7157, -4.9811],\n",
      "        [-1.2381,  3.9970, -3.6151],\n",
      "        [ 3.3956, -0.0586, -5.6566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3191,  4.1021, -3.4311],\n",
      "        [-1.4916,  4.0740, -3.2817],\n",
      "        [-1.1843,  4.1804, -3.7411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0355, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8243,  0.1346, -5.2470],\n",
      "        [ 3.3147, -0.5473, -5.2446],\n",
      "        [ 3.4074, -0.5321, -4.9741]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0817, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4832,  4.1442, -3.4987],\n",
      "        [ 2.2876,  0.9366, -4.8855],\n",
      "        [ 3.5219, -1.0946, -5.0130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0107, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7536,  3.9828, -3.2684],\n",
      "        [ 3.3997, -0.4605, -5.4235],\n",
      "        [-1.0550,  3.9127, -3.9613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0050, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7092,  4.1789, -3.2448],\n",
      "        [-1.5098,  4.1359, -3.2455],\n",
      "        [ 4.0063, -0.8788, -5.3277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6685, -1.1932, -4.8807],\n",
      "        [ 3.6873, -1.1939, -5.1620],\n",
      "        [-1.4644,  4.0637, -3.5380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4021, -0.4968, -5.4163],\n",
      "        [ 3.7814, -0.2135, -5.6074],\n",
      "        [-0.5286,  3.7427, -4.3669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5629, -0.4610, -5.7417],\n",
      "        [-0.4838,  3.9925, -4.4530],\n",
      "        [ 3.9115, -1.0286, -4.9683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9404, -1.2851, -5.0716],\n",
      "        [ 3.7994, -0.8200, -5.3425],\n",
      "        [-0.4131,  3.6215, -4.5330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9519, -1.0412, -4.8356],\n",
      "        [ 3.8219, -0.8692, -5.1362],\n",
      "        [-1.4915,  4.2470, -3.5820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7240,  4.2750, -3.0163],\n",
      "        [-1.2170,  4.0474, -3.4272],\n",
      "        [ 2.6947,  0.1575, -5.2389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0996, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9860, -1.2550, -4.8925],\n",
      "        [ 2.0162,  0.8984, -4.4671],\n",
      "        [ 3.8551, -0.8177, -5.0850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0064, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1218,  3.7977, -3.3273],\n",
      "        [-1.4612,  4.1171, -3.1804],\n",
      "        [ 3.9289, -1.0832, -5.0395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0270, -1.2241, -5.0794],\n",
      "        [ 3.8655, -1.0023, -4.9941],\n",
      "        [ 3.8622, -0.9434, -5.3012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0512, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9475, -1.1889, -5.2043],\n",
      "        [ 0.2953,  3.3940, -4.8998],\n",
      "        [-1.4700,  4.0496, -3.3185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0560, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1242,  4.0466, -4.2944],\n",
      "        [-1.2492,  3.8469, -3.8630],\n",
      "        [ 2.4309,  0.6455, -5.1421]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8736, -1.3006, -4.3418],\n",
      "        [-1.3548,  4.1116, -3.5320],\n",
      "        [ 4.0107, -1.1627, -4.8969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8504,  4.0606, -4.1358],\n",
      "        [ 3.4888, -0.2286, -5.3427],\n",
      "        [ 3.7176, -0.6811, -5.2091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5306, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9832, -0.8851, -4.7227],\n",
      "        [ 1.0403,  2.7307, -4.8323],\n",
      "        [ 1.8730,  0.7407, -3.8027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2001, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6740, -0.9247, -5.1129],\n",
      "        [ 3.2582, -0.2983, -5.2789],\n",
      "        [ 3.9007, -1.3587, -4.8823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0129, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3230,  3.4817, -4.3637],\n",
      "        [-0.7679,  3.8966, -4.6966],\n",
      "        [-1.3370,  3.7517, -3.8094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0637, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0542,  3.5577, -5.1932],\n",
      "        [ 0.7660,  2.5460, -4.7988],\n",
      "        [ 4.2065, -1.1133, -4.7717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6230, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3074,  0.6246, -4.3400],\n",
      "        [-1.1988,  4.2847, -3.9204],\n",
      "        [-0.8216,  3.7871, -4.5196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0137, -1.0675, -5.2369],\n",
      "        [ 3.5951, -0.9975, -5.1599],\n",
      "        [ 3.7583, -1.1317, -5.0098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5650, -1.0299, -4.8538],\n",
      "        [ 4.0963, -1.1909, -4.9494],\n",
      "        [ 3.7269, -1.1361, -5.3507]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0346, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3332, -0.3151, -4.8507],\n",
      "        [ 3.7877, -1.1743, -5.0725],\n",
      "        [ 0.4640,  3.0792, -5.4443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9906, -1.3217, -4.7972],\n",
      "        [-0.7572,  3.5932, -4.5568],\n",
      "        [ 3.7612, -0.9742, -5.1240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0274, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2474,  2.8631, -4.9631],\n",
      "        [ 3.9688, -1.1664, -4.9174],\n",
      "        [ 4.0928, -1.1449, -5.1858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0467, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1565,  3.3709, -4.7477],\n",
      "        [ 2.7533,  0.4221, -5.0157],\n",
      "        [ 3.8076, -1.1135, -5.0129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0951, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1996,  0.3848, -4.5034],\n",
      "        [ 3.2341, -0.4421, -5.2562],\n",
      "        [ 0.6712,  2.8437, -4.8920]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0106, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6748,  3.8320, -4.4531],\n",
      "        [ 3.7658, -0.5212, -5.1889],\n",
      "        [ 3.7401, -1.2588, -5.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0706, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4868,  0.9036, -4.8664],\n",
      "        [ 3.4968, -0.6318, -5.1593],\n",
      "        [-0.8652,  3.9421, -4.5897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2198,  3.6966, -3.9886],\n",
      "        [ 3.7336, -0.9586, -5.0911],\n",
      "        [ 3.8982, -0.9164, -4.9255]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1519, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1974,  2.3722, -4.6029],\n",
      "        [ 0.9462,  2.6311, -5.1031],\n",
      "        [ 3.5604, -0.6116, -5.3658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3639, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9467, -1.1151, -5.1632],\n",
      "        [ 1.7014,  1.0427, -4.2796],\n",
      "        [ 3.8404, -1.0102, -5.4626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0179, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4965, -0.2394, -5.3330],\n",
      "        [-1.2508,  4.0024, -4.1301],\n",
      "        [ 3.2046, -0.5004, -5.1104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7892, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6234,  0.3697, -5.2420],\n",
      "        [-1.1797,  3.9450, -3.7486],\n",
      "        [-1.1160,  3.8619, -4.2011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0811, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2267,  0.9224, -4.4931],\n",
      "        [ 3.4662, -0.3212, -5.4247],\n",
      "        [ 1.0280,  2.4958, -5.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0419, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1352,  3.7847, -3.7845],\n",
      "        [ 3.3194, -0.0447, -5.1766],\n",
      "        [ 2.6063,  0.1611, -4.8321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1763, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0707,  0.8045, -4.5052],\n",
      "        [ 3.8024, -1.0355, -4.9969],\n",
      "        [ 0.9878,  2.1586, -4.2159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0963, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1840,  0.6024, -4.3274],\n",
      "        [ 0.5618,  3.0260, -5.1850],\n",
      "        [ 3.3808, -0.5986, -5.1296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4479, -0.2844, -5.1292],\n",
      "        [-0.9770,  4.0036, -4.4024],\n",
      "        [ 3.5186, -0.3058, -5.2173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3475, -0.2220, -5.3120],\n",
      "        [-1.2976,  3.9489, -4.0997],\n",
      "        [-1.1611,  3.8862, -3.9705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3432, -0.1683, -5.1498],\n",
      "        [-1.1764,  3.9631, -4.1766],\n",
      "        [-1.0994,  4.2353, -4.3380]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0993, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4165,  0.4837, -5.0551],\n",
      "        [ 0.6929,  2.4747, -4.6648],\n",
      "        [-1.3087,  3.8960, -3.9430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2327, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3557,  0.6682, -4.7521],\n",
      "        [-0.3210,  3.8818, -4.6822],\n",
      "        [ 1.6991,  1.2947, -4.3152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0084, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1275,  3.9965, -3.9058],\n",
      "        [ 3.6201, -1.2067, -4.8435],\n",
      "        [-0.6249,  3.9321, -4.8886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0894, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4821,  2.8592, -4.9867],\n",
      "        [ 2.4736,  0.6978, -4.5639],\n",
      "        [-0.3972,  3.4351, -4.2650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0601, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6413, -0.9781, -5.2714],\n",
      "        [-1.1970,  4.2896, -3.8125],\n",
      "        [ 2.3958,  0.6803, -5.1314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3462, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4164,  2.6589, -5.1610],\n",
      "        [-1.2069,  4.1041, -3.9014],\n",
      "        [ 1.6232,  1.1950, -4.0186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1312, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5941,  3.8892, -4.6588],\n",
      "        [ 1.7180,  0.9310, -3.8562],\n",
      "        [-1.4821,  3.9679, -4.4187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0450, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8338,  2.9229, -5.0444],\n",
      "        [ 3.7295, -1.0575, -5.2112],\n",
      "        [ 3.7888, -0.8627, -5.0838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0594, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3480,  4.2498, -3.8670],\n",
      "        [-0.1498,  3.6025, -5.1410],\n",
      "        [ 0.6455,  2.4640, -5.0463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2837, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4976,  4.1077, -3.5423],\n",
      "        [ 3.1014,  0.4130, -4.7585],\n",
      "        [ 1.2903,  1.1277, -3.9160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0348, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2404, -0.3513, -5.1942],\n",
      "        [-1.2744,  4.0772, -4.1735],\n",
      "        [ 0.4761,  3.0737, -4.7913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1904, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6195, -0.5771, -5.3187],\n",
      "        [ 1.6619,  1.3085, -4.3871],\n",
      "        [-0.2753,  3.4972, -5.1098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5017,  4.3338, -3.8665],\n",
      "        [-1.5065,  4.1372, -3.8103],\n",
      "        [-1.0530,  4.1631, -4.2163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0248, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7918,  0.0429, -5.2839],\n",
      "        [-1.6409,  4.1021, -3.5318],\n",
      "        [ 3.7346, -1.0618, -5.1917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0138, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5973, -0.7131, -5.2492],\n",
      "        [ 3.7496, -0.9010, -5.2774],\n",
      "        [ 3.4888, -0.5139, -5.3505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3521, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9266,  1.0147, -4.5227],\n",
      "        [ 3.7197, -0.8648, -5.1607],\n",
      "        [ 1.5723,  1.5463, -4.6477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3507, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7955, -0.9223, -5.2895],\n",
      "        [ 3.9601, -0.8247, -5.1324],\n",
      "        [ 1.2878,  1.8821, -4.6038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1357, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1058,  4.2600, -4.3047],\n",
      "        [-1.0487,  4.0458, -3.7752],\n",
      "        [ 1.2476,  1.9736, -4.3697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0307, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2867, -0.2602, -5.6839],\n",
      "        [ 0.4137,  3.2224, -4.9877],\n",
      "        [-1.1744,  4.2359, -3.9694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0194, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5597,  4.1138, -3.7642],\n",
      "        [ 3.7899, -0.6668, -5.4091],\n",
      "        [ 0.1204,  3.2584, -5.1365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0146, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9789, -1.0824, -5.2648],\n",
      "        [ 3.8233, -1.0788, -4.8732],\n",
      "        [ 3.4130, -0.0954, -5.2383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1131, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8392, -0.9812, -4.8048],\n",
      "        [ 1.1720,  2.1565, -4.5891],\n",
      "        [-0.4666,  3.8991, -4.8922]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9932,  4.3162, -3.2049],\n",
      "        [-1.3647,  4.2147, -4.1926],\n",
      "        [ 3.7149, -1.3773, -4.7723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0068, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0157, -1.0975, -4.8099],\n",
      "        [ 3.9219, -1.0805, -5.0776],\n",
      "        [ 3.8046, -1.1128, -5.0030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1302, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0498, -1.0024, -5.0688],\n",
      "        [ 1.9309,  1.1500, -4.7483],\n",
      "        [ 3.8233, -1.3118, -4.6614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2240, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8799, -1.0923, -5.0868],\n",
      "        [ 1.2200,  1.2995, -4.3935],\n",
      "        [ 3.7051, -1.0166, -4.5784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0320, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6465,  4.3604, -3.5656],\n",
      "        [-0.7148,  4.0248, -4.6419],\n",
      "        [ 0.5098,  2.9450, -4.9359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0467,  4.2102, -4.1828],\n",
      "        [ 4.0135, -1.1260, -4.9556],\n",
      "        [-1.4029,  3.9436, -3.5365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2121, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6086,  2.8985, -5.0089],\n",
      "        [ 2.2076,  0.9996, -4.8937],\n",
      "        [ 2.3051,  1.1629, -5.2232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4544,  3.7580, -5.0801],\n",
      "        [-1.5409,  4.2164, -3.2194],\n",
      "        [ 3.8633, -0.7682, -5.1355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0978,  3.9380, -4.3466],\n",
      "        [ 3.8704, -1.3613, -4.5221],\n",
      "        [-1.9866,  4.2024, -3.4043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0061, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7855,  4.2472, -3.2188],\n",
      "        [ 3.8984, -1.3332, -4.8983],\n",
      "        [ 3.6247, -1.0135, -5.0942]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3049, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6852,  1.3185, -4.5269],\n",
      "        [ 3.7329, -0.4543, -5.3330],\n",
      "        [ 4.1106, -1.2203, -4.8558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7314, -1.0941, -5.4562],\n",
      "        [-1.9371,  4.4224, -3.3045],\n",
      "        [ 3.7643, -0.7391, -5.3139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6287,  4.3625, -3.5748],\n",
      "        [ 3.9904, -1.2715, -5.2405],\n",
      "        [-1.2448,  4.0142, -4.3668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8215, -1.1769, -4.6426],\n",
      "        [-1.7404,  4.3116, -3.3199],\n",
      "        [-1.7854,  4.2435, -3.4364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0259, -1.5479, -4.5357],\n",
      "        [ 3.9575, -1.4527, -4.8345],\n",
      "        [ 3.5933, -1.3390, -4.3009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9236,  4.5584, -3.1238],\n",
      "        [ 3.6778, -1.4612, -4.3814],\n",
      "        [-1.7717,  4.2977, -3.5877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4834, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6424,  4.3178, -3.8087],\n",
      "        [-1.9579,  4.4185, -3.2313],\n",
      "        [-0.7389,  3.6943, -5.0455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9202,  4.3170, -3.5010],\n",
      "        [ 3.8172, -0.8411, -5.6053],\n",
      "        [ 3.6780, -1.4815, -4.0580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5386, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7103,  4.2113, -3.3639],\n",
      "        [-1.5993,  4.4378, -3.6009],\n",
      "        [ 1.0388,  2.4251, -5.1911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1379, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0093,  1.3011, -4.5322],\n",
      "        [ 3.7248, -1.0813, -5.2019],\n",
      "        [-1.3830,  4.1896, -4.6248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3056, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0179, -1.6205, -4.5425],\n",
      "        [-1.2787,  4.3621, -4.0256],\n",
      "        [ 1.6217,  2.0149, -5.3962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0531, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9223, -1.3329, -4.4340],\n",
      "        [ 0.9722,  2.8026, -5.6801],\n",
      "        [-1.1575,  4.2060, -4.0370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9826,  4.0693, -4.5165],\n",
      "        [-1.0692,  3.7333, -3.8127],\n",
      "        [ 3.9644, -1.5089, -4.4658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0567, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9228, -1.3373, -4.5542],\n",
      "        [ 2.9352, -0.0702, -5.2609],\n",
      "        [ 0.6754,  2.8216, -5.4415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3275, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8164,  1.3199, -5.4523],\n",
      "        [ 3.9129, -1.4038, -4.6548],\n",
      "        [ 4.0509, -1.2762, -4.5046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5363, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7153, -0.5230, -5.6690],\n",
      "        [ 2.2022,  1.2853, -5.3504],\n",
      "        [ 1.0501,  1.9536, -5.0096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3098, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7311,  1.3201, -5.0375],\n",
      "        [ 4.0950, -1.2913, -4.5653],\n",
      "        [ 4.0704, -1.3713, -4.5572]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1977, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5012,  1.7466, -5.5016],\n",
      "        [ 3.9389, -1.1266, -4.9711],\n",
      "        [ 3.8239, -0.9904, -4.9785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1669, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0876,  3.8052, -3.6809],\n",
      "        [ 1.3834,  2.3538, -4.9790],\n",
      "        [ 0.9238,  2.6055, -5.5896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2697,  3.8040, -3.9030],\n",
      "        [ 2.9264,  0.7587, -5.4738],\n",
      "        [ 3.9657, -1.0757, -5.0159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6967, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6818,  2.6114, -5.3535],\n",
      "        [-1.3330,  4.3204, -4.1083],\n",
      "        [-0.2647,  3.5959, -5.1552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0268, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2078,  3.0508, -5.0410],\n",
      "        [ 3.6374, -0.9491, -5.1122],\n",
      "        [ 3.7124, -0.6117, -5.3492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0119,  3.5082, -5.5045],\n",
      "        [ 0.1357,  3.2281, -5.4719],\n",
      "        [-1.3361,  4.1114, -4.0239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3232, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2271,  1.6666, -4.5221],\n",
      "        [ 3.3133, -0.5501, -5.0307],\n",
      "        [ 3.5374, -1.0368, -5.1783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1951, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8360,  1.4571, -4.5142],\n",
      "        [-0.0550,  3.1691, -5.3793],\n",
      "        [-0.2062,  3.5380, -5.1180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0224, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5317, -1.0889, -4.9404],\n",
      "        [ 3.8195, -0.8361, -4.9968],\n",
      "        [ 3.0415,  0.0153, -5.0667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4512, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2465,  1.9794, -5.1073],\n",
      "        [ 0.8155,  2.4101, -5.2356],\n",
      "        [ 3.1577,  0.0138, -5.2853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7329, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3319,  4.0546, -4.2766],\n",
      "        [ 2.4932,  0.4270, -5.0743],\n",
      "        [ 2.5667,  0.6272, -4.9839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5062, -0.6844, -5.2677],\n",
      "        [ 3.4860, -0.7527, -4.5995],\n",
      "        [ 4.0415, -0.9483, -5.2005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0220, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5286,  3.7483, -5.0784],\n",
      "        [ 0.2988,  3.5153, -5.6355],\n",
      "        [ 3.5775, -0.8164, -4.7666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1734, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9700, -0.2823, -5.0561],\n",
      "        [ 1.7986,  1.3018, -4.6688],\n",
      "        [-1.2239,  4.0102, -4.4785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1660, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9386,  0.8700, -4.9345],\n",
      "        [ 2.3470,  0.7691, -4.8995],\n",
      "        [ 3.5996, -0.6923, -4.9020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0393, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9211, -0.1286, -5.3058],\n",
      "        [ 0.4332,  3.3618, -5.3801],\n",
      "        [-0.5434,  3.4186, -4.9124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3228,  4.0242, -4.3003],\n",
      "        [-0.4350,  3.7962, -5.0662],\n",
      "        [-1.3214,  4.3974, -4.2756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0968, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2484,  1.0515, -4.8348],\n",
      "        [ 3.5077, -0.5878, -5.1902],\n",
      "        [-0.7743,  3.9395, -4.8669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1698, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8758,  0.8901, -4.3448],\n",
      "        [ 2.8636,  0.5262, -5.3008],\n",
      "        [ 2.4327,  0.1579, -5.0220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3420, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6007, -0.8407, -4.9081],\n",
      "        [-1.2731,  4.1802, -4.3128],\n",
      "        [ 1.2521,  1.8067, -4.5164]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7775, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6247,  1.4098, -4.6158],\n",
      "        [ 3.5049, -0.3750, -5.1686],\n",
      "        [ 1.5149,  1.2435, -4.4109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0592, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8361,  2.6262, -5.5147],\n",
      "        [-0.6793,  3.9190, -4.7057],\n",
      "        [-0.6225,  3.7466, -4.5629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9306,  4.1916, -4.3348],\n",
      "        [ 3.5609, -0.4772, -5.2421],\n",
      "        [-1.0884,  4.2807, -4.3072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0245, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9809,  0.2349, -5.2079],\n",
      "        [-0.9699,  4.0916, -4.3611],\n",
      "        [-1.4102,  4.1011, -3.8777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0089, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1396,  4.2726, -4.4001],\n",
      "        [-1.1927,  4.3121, -4.2298],\n",
      "        [ 3.5388, -0.4967, -4.9768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0870, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1172, -0.0818, -5.1760],\n",
      "        [ 2.3938,  0.9132, -4.9417],\n",
      "        [ 3.6571, -0.5203, -5.0614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0115, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2317, -0.5822, -4.9730],\n",
      "        [-1.2264,  4.2099, -4.1278],\n",
      "        [-0.8797,  4.0002, -4.7431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0800, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7657,  0.5729, -5.3055],\n",
      "        [-1.5577,  4.2708, -4.1709],\n",
      "        [ 2.4589,  0.4851, -4.6786]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1265, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1422,  2.0845, -5.0766],\n",
      "        [ 3.2413, -0.2991, -5.2426],\n",
      "        [ 3.3710, -0.4896, -4.8778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4300, -0.1231, -5.0239],\n",
      "        [ 3.0653, -0.3007, -4.8302],\n",
      "        [-1.1454,  3.9550, -4.2784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0886, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7330,  2.6486, -5.2696],\n",
      "        [ 2.7949,  0.5217, -4.8764],\n",
      "        [ 0.1606,  3.6624, -5.5358]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0233, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3625, -0.6007, -4.9118],\n",
      "        [ 3.1663, -0.3355, -4.6035],\n",
      "        [-0.0723,  3.7987, -5.1138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5779, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0396,  1.8960, -4.3187],\n",
      "        [ 2.9608,  0.3708, -5.0381],\n",
      "        [ 2.0322,  1.0440, -4.4288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0762, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9948, -0.2347, -4.8920],\n",
      "        [ 3.2264, -0.2432, -5.0435],\n",
      "        [ 0.8213,  2.5864, -4.9839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.9031, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2973, -0.6055, -4.7440],\n",
      "        [-1.4394,  4.2121, -3.4510],\n",
      "        [ 3.2086, -0.1758, -5.0643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0213, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2036,  3.2989, -4.8620],\n",
      "        [ 3.5303, -0.7923, -5.0509],\n",
      "        [-0.8672,  4.2865, -4.0676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3272, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4265, -0.2213, -5.2362],\n",
      "        [-0.2694,  3.9059, -5.0482],\n",
      "        [ 0.8338,  1.9174, -4.5121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0338, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9298, -0.1254, -4.7704],\n",
      "        [-0.7246,  4.2158, -4.4705],\n",
      "        [ 0.3606,  3.3866, -5.4443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0852,  3.3054, -5.4993],\n",
      "        [-0.6221,  3.9826, -4.2266],\n",
      "        [ 3.0507, -0.3307, -4.7302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1052, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6378,  3.1692, -5.5690],\n",
      "        [ 2.1670,  0.8053, -4.7431],\n",
      "        [-0.5465,  4.0723, -4.2197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3362, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0980, -0.2708, -5.0413],\n",
      "        [ 1.5029,  1.5515, -4.7502],\n",
      "        [ 1.1582,  2.3932, -5.2581]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0482, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6172,  2.7532, -5.1091],\n",
      "        [-0.7541,  3.8817, -3.8301],\n",
      "        [-0.3098,  3.4867, -4.2967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0854, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2144,  3.2869, -3.7012],\n",
      "        [-0.2507,  3.4039, -3.7316],\n",
      "        [ 2.2743,  0.7585, -4.6403]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1172, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2806,  3.2900, -3.4471],\n",
      "        [ 2.1011,  1.0553, -4.8045],\n",
      "        [-0.1280,  3.7476, -5.0777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5817, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7924,  1.9438, -4.1703],\n",
      "        [ 2.6642,  0.3859, -4.9627],\n",
      "        [ 1.0818,  2.1584, -4.7269]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4196, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0810,  3.1519, -5.2603],\n",
      "        [ 2.4653,  0.5562, -5.0157],\n",
      "        [ 1.3702,  2.0260, -4.7274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0491, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0645,  3.6266, -4.5558],\n",
      "        [-0.2947,  3.7818, -4.8977],\n",
      "        [ 2.6913,  0.4907, -4.9148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2415, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8394,  0.2594, -4.6209],\n",
      "        [ 2.3433,  0.7305, -4.5451],\n",
      "        [ 1.1219,  1.6404, -4.6169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1721, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9502,  1.1318, -4.9932],\n",
      "        [ 2.7366,  0.0573, -4.9520],\n",
      "        [ 2.6291,  0.1768, -4.5236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0410, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1694,  3.7391, -4.2916],\n",
      "        [-0.2001,  3.3181, -3.5447],\n",
      "        [ 2.7156,  0.1207, -4.9483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1131, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7835,  2.7690, -5.1128],\n",
      "        [ 2.5980, -0.0551, -4.7095],\n",
      "        [ 2.4556,  0.5668, -4.6360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1508, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6664,  0.2172, -4.9603],\n",
      "        [ 2.9971, -0.5492, -4.7952],\n",
      "        [ 2.0138,  1.1088, -4.9374]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0300, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0831, -0.3028, -4.7639],\n",
      "        [-0.1170,  3.5761, -4.1787],\n",
      "        [-0.2247,  3.2777, -3.1467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0643, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0819, -0.2163, -4.7639],\n",
      "        [-0.1609,  3.3692, -3.4820],\n",
      "        [ 2.5144,  0.5055, -5.0659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0744, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3359,  3.3811, -5.5413],\n",
      "        [ 2.4583,  0.4237, -5.0007],\n",
      "        [-0.3378,  2.6609, -2.6722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0250, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5695,  3.3410, -4.1362],\n",
      "        [-0.1758,  3.2423, -4.0811],\n",
      "        [-0.6028,  3.2872, -3.1961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5307, -0.7210, -4.4383],\n",
      "        [ 3.8134, -0.8697, -4.8505],\n",
      "        [-0.0133,  3.2481, -3.7149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6732, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2918, -0.6854, -4.7427],\n",
      "        [ 3.4394, -0.5153, -4.8076],\n",
      "        [ 0.9246,  2.7573, -5.4198]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3115, -0.5688, -4.6671],\n",
      "        [ 3.4272, -0.7779, -4.6596],\n",
      "        [ 3.5734, -1.0085, -4.6798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0262, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2794,  3.4912, -5.1285],\n",
      "        [-0.3163,  3.8750, -4.2729],\n",
      "        [ 3.0625, -0.1362, -4.5613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4529, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7222, -1.1145, -4.8667],\n",
      "        [ 1.9124,  0.8904, -4.3430],\n",
      "        [-0.5260,  3.4179, -3.8807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2894, -0.8061, -4.3792],\n",
      "        [-0.5239,  3.3171, -4.6582],\n",
      "        [ 3.7792, -1.0256, -4.8012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1588, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5943, -1.2467, -4.7258],\n",
      "        [ 0.4808,  2.9031, -5.7761],\n",
      "        [ 1.3634,  2.1268, -5.0128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6889, -1.1588, -4.3275],\n",
      "        [ 3.8863, -1.0866, -4.1524],\n",
      "        [ 3.6508, -1.1347, -4.5630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4113, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4917, -1.1139, -4.3062],\n",
      "        [ 3.4888, -0.8931, -4.5053],\n",
      "        [ 3.4628, -0.7326, -4.6060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0110, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6604, -1.2051, -4.3118],\n",
      "        [ 3.4943, -1.0618, -4.7480],\n",
      "        [-0.4897,  3.7604, -4.6826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0323, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4159, -0.9679, -4.2682],\n",
      "        [ 2.7464,  0.1977, -4.8611],\n",
      "        [ 3.7383, -1.0704, -4.5175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0835, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3741,  3.1950, -5.1494],\n",
      "        [ 3.4877, -0.7266, -4.8940],\n",
      "        [ 2.3230,  0.6799, -4.7738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5787,  3.2374, -5.2872],\n",
      "        [ 3.3212, -0.4953, -4.7414],\n",
      "        [ 3.4738, -0.8125, -4.3579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3440, -0.4672, -5.0503],\n",
      "        [-0.6305,  3.5941, -4.1190],\n",
      "        [ 3.4623, -0.8586, -4.6387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1110, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5267, -1.0683, -4.7950],\n",
      "        [ 3.1128, -0.8289, -4.8856],\n",
      "        [ 3.1662, -0.0993, -5.0766]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3859, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4527,  3.9824, -4.5933],\n",
      "        [ 1.9814,  1.2602, -4.7183],\n",
      "        [ 3.1643, -0.4308, -4.9001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2516, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0899, -0.4500, -4.5293],\n",
      "        [ 3.2736, -0.4403, -4.9826],\n",
      "        [ 3.3714, -0.3046, -5.0125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3015, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3464, -0.2792, -5.3085],\n",
      "        [-0.3761,  3.3997, -4.8442],\n",
      "        [ 1.5579,  1.8580, -4.7867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0191, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0093, -0.3425, -4.9220],\n",
      "        [-0.6921,  3.8554, -3.7906],\n",
      "        [-0.6902,  3.8073, -4.1913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5548, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6325,  2.8533, -5.3988],\n",
      "        [ 2.0340,  0.7280, -4.6645],\n",
      "        [-0.5706,  3.6769, -4.8321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0830, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7905,  3.9441, -4.1880],\n",
      "        [ 2.6065,  0.1409, -4.5138],\n",
      "        [ 2.1892,  0.4178, -5.0434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6714,  0.2308, -5.0168],\n",
      "        [ 2.6728,  0.1509, -5.0170],\n",
      "        [-0.6139,  3.9200, -4.3519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3884, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6419,  0.6201, -5.1243],\n",
      "        [ 1.4725,  1.8122, -5.0024],\n",
      "        [ 2.4006,  0.6623, -5.2358]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1979, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2131,  1.7323, -4.6046],\n",
      "        [ 2.7029,  0.4389, -5.0890],\n",
      "        [-0.1111,  3.5093, -4.9886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0617, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7700,  0.3414, -5.1488],\n",
      "        [ 2.5898,  0.2220, -4.8787],\n",
      "        [-0.8613,  3.7698, -4.1040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1525, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9809,  2.2079, -4.5772],\n",
      "        [ 2.8355,  0.4419, -5.2484],\n",
      "        [ 2.4495,  0.3064, -4.5461]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2629, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6590,  3.9695, -4.8458],\n",
      "        [ 1.7356,  1.5920, -5.1151],\n",
      "        [-0.5749,  3.9700, -4.8318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1280, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1528,  0.5757, -4.5993],\n",
      "        [-0.8396,  4.0172, -3.8597],\n",
      "        [ 2.4053,  0.8213, -5.2132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7579,  0.2978, -5.0736],\n",
      "        [-0.8392,  3.7967, -3.7637],\n",
      "        [-0.8533,  4.0600, -4.2514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0875, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6378,  2.9519, -5.3310],\n",
      "        [ 2.1556,  0.4031, -4.6911],\n",
      "        [-0.8037,  4.1461, -4.3405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0596,  3.5192, -5.0487],\n",
      "        [ 2.4222,  0.4774, -4.8306],\n",
      "        [-1.1816,  3.9707, -3.7424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8787, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8749,  0.2468, -4.8986],\n",
      "        [ 2.9641,  0.4850, -5.0553],\n",
      "        [-1.0744,  4.0508, -4.0793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0772, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5676,  0.5483, -4.7818],\n",
      "        [ 2.6805,  0.3150, -5.2617],\n",
      "        [-0.3536,  3.7639, -5.3058]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2506, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9315,  1.5082, -4.9601],\n",
      "        [ 2.3709,  0.5139, -5.1821],\n",
      "        [ 0.5868,  2.8197, -5.5300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1411, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3467,  0.3943, -4.7654],\n",
      "        [ 2.6889,  0.3024, -5.2885],\n",
      "        [ 2.2693,  0.7654, -4.5544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2149, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7102,  0.1511, -5.0348],\n",
      "        [ 2.4662,  0.6196, -4.7718],\n",
      "        [ 1.3126,  1.9569, -4.8920]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1065, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6673,  0.1570, -4.9922],\n",
      "        [ 2.7592,  0.4354, -4.9902],\n",
      "        [ 2.4280,  0.5837, -5.1847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3229, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6242,  1.8390, -5.1842],\n",
      "        [ 2.8411,  0.3892, -5.2374],\n",
      "        [ 0.4730,  2.9740, -5.4115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0479, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6947,  4.1099, -4.1004],\n",
      "        [-1.0981,  4.0487, -4.0381],\n",
      "        [ 2.5371,  0.5531, -5.1799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0622, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9890,  4.1612, -3.1057],\n",
      "        [ 2.7791, -0.0254, -4.8733],\n",
      "        [ 2.6157,  0.5596, -5.0598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0572, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8934, -0.0449, -4.8916],\n",
      "        [-1.1227,  3.9597, -4.0022],\n",
      "        [ 2.5839,  0.4579, -5.2784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1106,  4.1730, -3.9832],\n",
      "        [-1.0924,  3.9165, -3.3818],\n",
      "        [ 2.7135,  0.5494, -4.9494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0711, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7436,  0.3900, -5.0951],\n",
      "        [ 2.5008,  0.3855, -5.1881],\n",
      "        [-0.7842,  4.0697, -4.8402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0392, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0999,  4.2196, -3.7408],\n",
      "        [ 2.7405,  0.5013, -5.0257],\n",
      "        [-0.7635,  3.8235, -3.7986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1627, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9102,  0.5444, -5.2458],\n",
      "        [ 3.1288, -0.1877, -5.0636],\n",
      "        [ 2.0147,  1.1824, -4.5619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1277, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1729,  4.1103, -3.7502],\n",
      "        [ 1.9536,  0.9844, -4.9540],\n",
      "        [ 2.8993,  0.0231, -5.0463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9966, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8051, -0.0567, -5.0318],\n",
      "        [ 1.9215,  1.0888, -5.1276],\n",
      "        [ 2.0816,  0.5359, -5.1304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0363, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9452, -0.4039, -5.3426],\n",
      "        [ 2.9602,  0.3117, -4.7304],\n",
      "        [-0.9134,  4.3407, -4.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0867, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0271,  4.1667, -3.3938],\n",
      "        [ 0.7193,  2.6657, -5.3375],\n",
      "        [ 2.5826,  0.5221, -5.2003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0070,  4.3306, -4.2663],\n",
      "        [-1.0898,  4.0456, -3.6734],\n",
      "        [-1.0679,  4.0809, -3.6498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0943, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3026,  4.2093, -3.7736],\n",
      "        [ 2.0517,  0.7600, -4.3868],\n",
      "        [ 3.0241, -0.3346, -5.2181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1036, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6477,  0.3304, -5.0694],\n",
      "        [ 0.6662,  2.7934, -5.4470],\n",
      "        [ 2.6620,  0.4432, -5.1223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0705, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6896,  0.5591, -5.0752],\n",
      "        [ 2.6122,  0.2934, -4.7987],\n",
      "        [-1.1568,  4.3299, -4.0495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3129, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1478,  0.0091, -5.2392],\n",
      "        [ 1.6863,  1.3279, -4.7562],\n",
      "        [-0.9820,  4.0530, -3.9652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1864, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3360,  1.7263, -4.6227],\n",
      "        [ 3.0655, -0.2316, -4.8202],\n",
      "        [-1.2098,  4.2308, -3.7953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3622, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2172,  3.8296, -4.9276],\n",
      "        [-1.2136,  4.0400, -3.7626],\n",
      "        [-0.5021,  3.5914, -4.8308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8025, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7868,  2.8561, -5.4546],\n",
      "        [ 0.1650,  3.3122, -5.2385],\n",
      "        [ 3.2488, -0.2621, -4.7887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1585,  4.2168, -3.8179],\n",
      "        [ 2.7200, -0.1059, -5.2056],\n",
      "        [ 3.4096, -0.3911, -4.9588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5424,  4.1084, -4.8420],\n",
      "        [-1.0366,  4.0764, -4.3258],\n",
      "        [-1.1966,  4.2789, -4.1842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1285, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1581, -0.1842, -5.0341],\n",
      "        [ 0.1007,  3.1998, -5.1448],\n",
      "        [ 1.9823,  0.9524, -4.7843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0900, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0407,  0.7801, -4.5870],\n",
      "        [-0.8217,  4.1197, -4.5025],\n",
      "        [-0.7307,  3.7283, -3.9881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1586, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4332,  2.1056, -5.3382],\n",
      "        [ 3.1712, -0.1275, -4.8450],\n",
      "        [ 3.2710, -0.3579, -5.0372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0904, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2646, -0.5667, -5.0579],\n",
      "        [ 2.6435,  0.6381, -4.9359],\n",
      "        [ 2.4269,  0.3860, -4.8275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4672, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9841, -0.3504, -5.0142],\n",
      "        [ 1.5367,  1.9831, -5.1271],\n",
      "        [ 1.8760,  1.5480, -5.0495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3634, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1522, -0.5452, -5.1972],\n",
      "        [ 1.8231,  1.5542, -5.2493],\n",
      "        [ 1.0741,  2.4351, -5.5972]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0752, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7500, -0.3275, -5.3755],\n",
      "        [ 2.6620, -0.0583, -5.2025],\n",
      "        [ 2.9531, -0.3000, -4.5367]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0218, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6057,  3.7254, -4.6877],\n",
      "        [-0.6723,  3.6787, -4.5169],\n",
      "        [ 2.9676, -0.2714, -4.5846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2591, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4224,  3.7615, -4.6583],\n",
      "        [ 1.3055,  2.4084, -6.0503],\n",
      "        [ 1.7487,  1.2482, -4.3956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0329, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4968,  4.1756, -4.9143],\n",
      "        [ 3.1388, -0.4069, -5.1816],\n",
      "        [ 2.7162, -0.0666, -4.6513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0546, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4440,  2.6032, -5.5719],\n",
      "        [ 3.2433, -0.5542, -4.7178],\n",
      "        [ 2.9615, -0.4858, -4.6559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0562, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8466,  4.1470, -4.3256],\n",
      "        [ 3.1454, -0.7136, -4.3852],\n",
      "        [ 0.9271,  2.8227, -5.4073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1496, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2426,  2.3996, -5.6394],\n",
      "        [ 0.9718,  2.8191, -5.9928],\n",
      "        [ 3.0582, -0.4946, -4.9494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0599, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4895,  3.4100, -5.8170],\n",
      "        [-0.7870,  4.0380, -4.8497],\n",
      "        [ 2.5201,  0.4477, -5.0296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0381, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5807,  4.2883, -4.4872],\n",
      "        [ 2.7162, -0.3829, -5.1264],\n",
      "        [ 0.4212,  3.1739, -5.3968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1692, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6715,  1.1458, -4.3563],\n",
      "        [ 3.2605, -0.0952, -5.1929],\n",
      "        [-0.9576,  3.9948, -4.4669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0439, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8088, -0.2423, -4.9491],\n",
      "        [ 2.8822,  0.2175, -4.8266],\n",
      "        [ 3.3434, -0.7166, -5.3090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5060, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7410,  1.0760, -4.6956],\n",
      "        [-0.9878,  4.0574, -4.4677],\n",
      "        [ 2.1753,  1.5538, -5.2922]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0760, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5381,  3.6726, -4.7375],\n",
      "        [-0.8992,  4.1571, -4.2774],\n",
      "        [ 2.3787,  0.9035, -4.9869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1132, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2938,  3.4763, -5.4347],\n",
      "        [ 1.1440,  2.2849, -4.8384],\n",
      "        [ 3.3375, -0.5334, -5.0497]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1096, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2120,  3.3721, -5.8677],\n",
      "        [ 3.2158, -0.3298, -5.1481],\n",
      "        [ 1.9133,  0.6856, -4.3659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0532, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2237, -0.3839, -5.4180],\n",
      "        [ 0.4769,  3.2724, -5.4033],\n",
      "        [ 0.5138,  3.0964, -5.2470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0169, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0133,  3.2678, -5.5647],\n",
      "        [-0.9602,  3.9511, -4.3556],\n",
      "        [-1.0715,  4.2641, -4.3020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0208, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9513,  4.2961, -4.0171],\n",
      "        [ 3.3242, -0.4358, -5.0117],\n",
      "        [-0.0549,  3.3229, -5.4127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7811, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6574,  0.4719, -4.8904],\n",
      "        [-1.3172,  3.9867, -3.7676],\n",
      "        [ 0.1673,  3.2396, -5.5166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7177, -0.1343, -4.6979],\n",
      "        [-0.9974,  4.0926, -4.1709],\n",
      "        [-1.0173,  4.1704, -3.8063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2667, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3489,  1.5273, -4.3468],\n",
      "        [-1.1057,  4.0454, -3.9514],\n",
      "        [-0.7929,  4.3263, -4.4246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0590, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1811,  4.2994, -3.9505],\n",
      "        [ 3.4259, -0.1661, -5.0231],\n",
      "        [ 2.4019,  0.5407, -4.9431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0394, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9872,  4.2827, -4.4560],\n",
      "        [ 0.5320,  2.8392, -4.8330],\n",
      "        [ 3.5267, -0.5204, -5.0843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8931, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7246,  0.9140, -4.0116],\n",
      "        [ 0.7369,  2.6553, -4.8794],\n",
      "        [ 0.1887,  3.4025, -5.5498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0645, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2662,  4.0502, -3.9509],\n",
      "        [ 0.1617,  3.4440, -5.2834],\n",
      "        [ 1.0320,  2.8466, -4.9777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1602, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3087, -0.2450, -5.3080],\n",
      "        [ 1.1662,  1.7692, -4.5291],\n",
      "        [ 3.1140,  0.1499, -4.9701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0324, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2244, -0.8056, -5.0523],\n",
      "        [ 2.9648, -0.0740, -5.0747],\n",
      "        [ 3.1424, -0.2851, -5.2952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0138, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6199,  3.5580, -4.7592],\n",
      "        [ 3.3692, -0.4656, -4.9295],\n",
      "        [-1.2320,  4.3127, -3.8476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0353, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4827,  3.9533, -4.8763],\n",
      "        [ 3.2161, -0.0441, -5.3564],\n",
      "        [ 2.9236,  0.0641, -5.2192]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0921, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0748, -0.0577, -5.1745],\n",
      "        [ 1.1754,  2.5483, -5.3620],\n",
      "        [-1.1023,  3.9022, -3.7648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0585, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3195,  3.1282, -5.1907],\n",
      "        [ 3.1364, -0.3485, -5.1254],\n",
      "        [ 0.7037,  3.1128, -5.4552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8906,  4.1213, -4.3873],\n",
      "        [-0.8508,  4.0768, -4.3951],\n",
      "        [ 3.2417, -0.2616, -5.1007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0297, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1656,  3.5276, -5.2117],\n",
      "        [-0.0550,  3.4641, -5.1014],\n",
      "        [ 2.9721, -0.3742, -5.0672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0395, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8103, -0.2704, -4.7659],\n",
      "        [ 2.9868, -0.0523, -5.4354],\n",
      "        [ 3.2596, -0.3827, -5.5347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0264, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1084,  4.1347, -4.2016],\n",
      "        [ 0.0455,  3.1823, -5.1810],\n",
      "        [ 2.9014, -0.5709, -4.8045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1200, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0788,  3.4102, -5.0530],\n",
      "        [ 2.9594, -0.1620, -5.0923],\n",
      "        [ 2.5418,  0.8197, -5.4235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1714, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1859,  1.8534, -4.3264],\n",
      "        [ 2.7655,  0.1634, -5.2800],\n",
      "        [ 3.0898, -0.5269, -4.9364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0327, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5874,  3.9598, -4.5456],\n",
      "        [ 3.2244, -0.1802, -5.1387],\n",
      "        [ 2.8132, -0.0744, -5.0454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0724, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5497,  0.3584, -5.4611],\n",
      "        [ 2.6726,  0.4514, -5.0833],\n",
      "        [-0.9252,  4.0036, -4.3951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1750, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1901,  0.1398, -5.3893],\n",
      "        [ 1.1304,  1.7880, -4.9514],\n",
      "        [ 0.1927,  2.9724, -5.4171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0400, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1762, -0.1722, -5.0771],\n",
      "        [ 2.8948, -0.0544, -5.4393],\n",
      "        [ 3.1497, -0.2255, -5.6702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0337, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5684,  3.9851, -4.9248],\n",
      "        [ 3.2400, -0.2011, -4.9264],\n",
      "        [ 2.9141,  0.1061, -5.3858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0515, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2545,  3.4571, -5.5505],\n",
      "        [ 0.1828,  3.4215, -5.1296],\n",
      "        [ 2.8234,  0.2787, -5.3547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2195, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5298,  1.3208, -4.0694],\n",
      "        [ 3.1222, -0.1572, -5.0371],\n",
      "        [ 3.2063, -0.4806, -4.9602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0383, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6771,  0.4594, -5.3205],\n",
      "        [-1.0084,  4.0269, -3.9309],\n",
      "        [-1.2544,  4.2540, -4.0254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0813, -0.0272, -5.1610],\n",
      "        [ 2.5189,  0.3563, -4.8885],\n",
      "        [ 3.3797, -0.1998, -5.1236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1009, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6321,  2.8051, -5.1105],\n",
      "        [ 2.9177, -0.1367, -5.1546],\n",
      "        [ 2.4193,  0.5816, -5.4625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1720, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8410e+00,  1.2870e+00, -5.2677e+00],\n",
      "        [-1.1295e+00,  4.2848e+00, -4.1586e+00],\n",
      "        [ 2.8505e+00,  5.3201e-04, -4.6611e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0380, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9697,  0.1804, -5.1955],\n",
      "        [ 2.8462, -0.1414, -5.0331],\n",
      "        [-1.2863,  4.1680, -4.2856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0685, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5569,  0.2496, -5.2600],\n",
      "        [ 3.1234, -0.2722, -5.0974],\n",
      "        [ 0.6370,  3.1627, -5.6474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0853, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1692, -0.4923, -4.8279],\n",
      "        [ 1.1237,  2.6862, -6.0388],\n",
      "        [ 0.0696,  3.2822, -5.1240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0400, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9378,  3.8512, -4.4295],\n",
      "        [ 0.6746,  2.9853, -5.3632],\n",
      "        [-0.3025,  3.7965, -5.0613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9901, -0.3598, -5.3116],\n",
      "        [ 3.3516, -0.3493, -4.9879],\n",
      "        [ 3.2046, -0.7737, -4.6912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1653, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1129,  3.0052, -4.6041],\n",
      "        [ 1.0608,  1.6631, -5.0417],\n",
      "        [-1.2886,  4.3085, -4.2999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0831, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7600,  2.2240, -4.6177],\n",
      "        [ 3.0947, -0.7217, -5.0599],\n",
      "        [ 3.3924, -0.6171, -5.0355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0243, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6638,  4.0589, -5.0171],\n",
      "        [ 3.3719,  0.0848, -5.3328],\n",
      "        [ 3.1931, -0.4071, -5.4386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0264, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8705,  3.7394, -4.2223],\n",
      "        [-1.1164,  4.3908, -4.4941],\n",
      "        [ 0.3756,  3.0833, -5.1532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2769,  4.3835, -4.1935],\n",
      "        [ 3.2227, -0.4264, -4.7751],\n",
      "        [-1.3498,  4.0185, -3.9322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3742, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9151,  3.7730, -4.5407],\n",
      "        [ 1.9180,  1.4407, -5.0001],\n",
      "        [ 3.0768, -0.0320, -5.3317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4182,  4.3838, -3.5162],\n",
      "        [ 3.1800, -0.5166, -5.0618],\n",
      "        [-0.4585,  3.6702, -5.0936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7914, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5308,  4.3037, -3.8410],\n",
      "        [ 3.5441, -0.4732, -4.8501],\n",
      "        [ 2.6065,  0.3537, -5.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2787, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4307, -0.5025, -4.9781],\n",
      "        [ 1.3979,  1.5654, -4.6242],\n",
      "        [ 3.0543, -0.2879, -5.5480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3377,  3.6858, -3.9516],\n",
      "        [-1.3602,  4.3466, -3.7132],\n",
      "        [-1.4929,  4.3610, -3.3848]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0502, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5103,  4.1397, -4.0413],\n",
      "        [-0.9030,  3.8944, -4.5808],\n",
      "        [ 2.5849,  0.6752, -5.2068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1921, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8011,  0.4117, -5.4195],\n",
      "        [ 1.9043,  1.4297, -4.8789],\n",
      "        [-1.4530,  4.2118, -3.8345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0196, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3155,  4.1293, -3.4970],\n",
      "        [-1.3361,  4.4560, -3.5770],\n",
      "        [ 0.1666,  3.1685, -5.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0027, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7112,  4.5035, -3.4742],\n",
      "        [-1.6616,  4.4727, -3.6535],\n",
      "        [-1.5861,  4.2924, -3.6363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4597, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7550,  4.2414, -3.2961],\n",
      "        [-1.3668,  4.6096, -3.9890],\n",
      "        [ 0.9070,  1.9874, -4.9161]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0339, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9520, -0.0574, -5.1881],\n",
      "        [-1.4349,  4.4883, -3.6799],\n",
      "        [ 0.1874,  3.1556, -5.1832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1055, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1462,  0.9096, -4.8392],\n",
      "        [ 2.8513,  0.0245, -5.5702],\n",
      "        [-1.5856,  4.3739, -3.4112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0276, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1716,  4.2294, -3.7690],\n",
      "        [-0.3487,  3.1010, -4.7696],\n",
      "        [ 3.0287, -0.0239, -5.1530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9960,  0.2121, -4.7923],\n",
      "        [-0.6167,  4.1258, -4.4424],\n",
      "        [-1.6682,  4.1211, -3.7165]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0296, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9998, -0.4112, -5.1919],\n",
      "        [ 2.9329,  0.0265, -5.4054],\n",
      "        [-1.5455,  4.4537, -3.8441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0922, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4318,  2.7989, -5.4582],\n",
      "        [ 2.2128,  0.3445, -5.5211],\n",
      "        [ 3.2616,  0.1268, -5.0837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2434, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5464,  1.2781, -4.9016],\n",
      "        [ 0.5110,  2.9030, -5.4569],\n",
      "        [ 0.3507,  2.9259, -5.4257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0527, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8884e+00, -3.0520e-01, -5.0176e+00],\n",
      "        [ 2.6500e+00, -5.0478e-03, -5.3936e+00],\n",
      "        [ 2.9479e+00, -3.7684e-02, -5.6719e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4749, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6756,  0.7819, -5.2482],\n",
      "        [ 2.1410,  1.1880, -5.6304],\n",
      "        [-1.3060,  4.1087, -3.5516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0549, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3375,  4.2321, -3.6788],\n",
      "        [ 0.7197,  2.7705, -5.0650],\n",
      "        [ 2.9712, -0.2598, -5.1809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2526, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1672,  2.0693, -4.7808],\n",
      "        [ 3.0176, -0.4596, -4.7672],\n",
      "        [ 1.8718,  1.1165, -5.3348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6486,  4.2364, -3.3480],\n",
      "        [-1.5157,  4.0555, -3.9956],\n",
      "        [-0.3040,  3.5504, -5.2001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0214, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3090, -0.5275, -5.2106],\n",
      "        [ 3.3235, -0.3731, -5.1860],\n",
      "        [ 3.4435, -0.5799, -5.1490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6890, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6695,  3.8355, -4.2446],\n",
      "        [ 0.2858,  3.0801, -5.0650],\n",
      "        [ 2.5585,  0.7088, -5.2351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4543, -0.6177, -4.8952],\n",
      "        [ 3.6622, -0.5399, -5.0393],\n",
      "        [ 3.3029, -0.6425, -4.9935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0249, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3466,  4.3905, -3.5014],\n",
      "        [ 0.2365,  3.2851, -5.2002],\n",
      "        [ 3.3511, -0.3542, -4.8821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0597, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5781, -0.5906, -5.3639],\n",
      "        [ 2.6575,  0.2616, -5.1515],\n",
      "        [ 0.3023,  2.8446, -4.9502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2425, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6187,  1.5775, -4.0346],\n",
      "        [-1.0673,  3.9709, -4.0671],\n",
      "        [-1.0822,  4.2868, -4.3336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0379, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5065, -0.4422, -5.1590],\n",
      "        [ 2.7209,  0.0214, -5.0496],\n",
      "        [ 3.3784, -0.1559, -4.9625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1372, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9110,  3.9426, -4.7455],\n",
      "        [ 3.1693, -0.1727, -5.3973],\n",
      "        [ 3.1864, -0.4324, -4.7550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3825,  4.2018, -3.8109],\n",
      "        [ 3.2089, -0.6266, -5.1048],\n",
      "        [-1.1759,  4.0309, -4.3664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6937, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4479,  0.8403, -5.1780],\n",
      "        [ 2.7187, -0.4645, -5.0565],\n",
      "        [ 2.9325,  0.2457, -5.0355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6649, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1205,  1.8141, -4.4736],\n",
      "        [-0.3478,  3.5017, -5.0825],\n",
      "        [ 3.2808, -0.4616, -4.9216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0428, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0113,  0.0304, -4.9797],\n",
      "        [ 2.7473,  0.1858, -5.6058],\n",
      "        [-1.2854,  4.3120, -4.1815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6788, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4411,  2.5062, -4.8956],\n",
      "        [ 0.3262,  3.1104, -5.1319],\n",
      "        [-0.9072,  4.0765, -4.5288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9474, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3168,  3.0879, -5.5012],\n",
      "        [-0.9489,  4.1585, -4.0022],\n",
      "        [-1.3545,  4.2776, -4.1905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0651, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4904,  0.5160, -5.3720],\n",
      "        [-1.3380,  4.1600, -3.3115],\n",
      "        [ 3.0378,  0.2546, -5.1001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0956, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3967,  4.1739, -4.2905],\n",
      "        [ 0.7261,  2.2242, -5.2367],\n",
      "        [ 2.8379,  0.3556, -5.5027]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1810, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3792,  0.4986, -5.3691],\n",
      "        [ 2.1156,  1.1954, -5.3825],\n",
      "        [ 2.6349, -0.0776, -4.4948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1727, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8246,  0.0606, -5.2216],\n",
      "        [ 1.3972,  2.1194, -5.6066],\n",
      "        [ 2.8206,  0.0413, -5.1320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2765, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1272,  1.0256, -4.8905],\n",
      "        [ 1.5707,  1.9712, -4.9926],\n",
      "        [-0.4196,  3.1412, -4.7006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2650, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3817,  2.2955, -5.2818],\n",
      "        [ 1.1256,  2.0682, -4.9353],\n",
      "        [ 2.4086,  0.4126, -5.2744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1190, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7022,  0.3810, -5.1726],\n",
      "        [ 3.1814,  0.2934, -5.2115],\n",
      "        [ 2.2992,  0.8364, -5.2181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1638, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9108,  0.2914, -5.3463],\n",
      "        [ 1.3106,  2.0971, -5.3368],\n",
      "        [ 2.9722, -0.1140, -4.7901]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1214, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2406,  3.6523, -4.5256],\n",
      "        [ 0.5165,  2.7997, -5.2647],\n",
      "        [ 1.1736,  2.4499, -5.5417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5492, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0111,  1.2736, -4.7148],\n",
      "        [ 2.7279,  0.3622, -4.8277],\n",
      "        [ 1.2155,  1.8439, -5.0607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3042, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7337, -0.0390, -4.9226],\n",
      "        [ 2.2380,  1.1482, -5.3654],\n",
      "        [ 1.4047,  1.6896, -5.3911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5193, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2003,  2.8864, -4.7770],\n",
      "        [ 0.7807,  2.2167, -5.2472],\n",
      "        [ 2.0751,  1.0959, -4.8103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5015, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2538,  1.2131, -5.1470],\n",
      "        [ 0.9567,  2.7826, -5.5867],\n",
      "        [-0.5814,  3.9124, -4.5184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1121, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5972,  3.0391, -5.5074],\n",
      "        [ 0.5815,  2.6189, -5.5987],\n",
      "        [ 0.5961,  2.5726, -5.5313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0732, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0068,  0.4281, -4.7603],\n",
      "        [-0.7082,  3.6162, -4.2960],\n",
      "        [-0.5894,  3.4606, -4.7157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0450, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5782e-03,  3.5146e+00, -5.0250e+00],\n",
      "        [ 2.7859e+00,  1.6485e-01, -5.0543e+00],\n",
      "        [ 3.0577e+00, -2.8735e-01, -5.1504e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5033, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3349,  1.5700, -4.4770],\n",
      "        [-0.7426,  3.9278, -4.6422],\n",
      "        [ 1.2858,  1.6901, -4.7043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1071, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4270,  0.7123, -5.2369],\n",
      "        [ 2.7199,  0.0496, -4.6018],\n",
      "        [ 0.3908,  2.7827, -5.3934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1339, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2624,  3.0003, -5.2830],\n",
      "        [-0.4002,  3.3259, -4.8415],\n",
      "        [ 1.8544,  0.8572, -4.9991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0245, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9231,  0.1156, -5.1008],\n",
      "        [-1.0891,  4.2484, -3.9526],\n",
      "        [-0.7891,  3.8809, -4.1792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0396, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9466,  3.7390, -3.8181],\n",
      "        [-0.6318,  3.7129, -4.5285],\n",
      "        [ 2.6577,  0.3565, -5.0116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0732, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9061, -0.0836, -5.5491],\n",
      "        [-0.3978,  4.2166, -4.7536],\n",
      "        [ 2.3941,  0.6440, -5.6923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0082, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1377,  4.0031, -4.3528],\n",
      "        [-0.5446,  3.7654, -4.6115],\n",
      "        [-1.2205,  4.1641, -3.9466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2846,  4.1973, -3.8667],\n",
      "        [-1.2464,  4.2435, -3.3196],\n",
      "        [-0.4878,  3.7681, -4.8996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1154, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1068,  0.6504, -5.2294],\n",
      "        [ 3.0309, -0.2630, -4.5572],\n",
      "        [ 2.4603,  0.1921, -4.6134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0445, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1067,  4.2532, -3.7494],\n",
      "        [ 0.2947,  3.1232, -5.4795],\n",
      "        [ 2.8678,  0.2535, -5.1644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8969, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.8748e-01,  3.0117e+00, -5.0603e+00],\n",
      "        [ 3.1760e+00,  3.2032e-03, -4.9453e+00],\n",
      "        [ 2.9902e+00, -3.5904e-02, -4.8935e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3158, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4610,  1.5734, -4.1800],\n",
      "        [ 0.8657,  2.4494, -4.8893],\n",
      "        [-0.8751,  4.0441, -4.1617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0533, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6341,  2.6219, -5.0067],\n",
      "        [-0.0827,  3.5709, -4.7764],\n",
      "        [-1.3918,  3.9707, -3.6076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4587, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2111,  1.8752, -4.5336],\n",
      "        [-1.2572,  4.1663, -3.5735],\n",
      "        [ 2.2236,  1.1357, -5.0926]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1272, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3771,  3.8951, -4.8344],\n",
      "        [ 2.3749,  0.6141, -4.8987],\n",
      "        [ 2.2107,  0.7440, -4.5637]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0429, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8687,  0.2950, -5.1874],\n",
      "        [ 2.9013, -0.1060, -4.6189],\n",
      "        [-0.8772,  4.2537, -4.0658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1273, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1139,  3.1303, -5.0349],\n",
      "        [-1.1156,  4.2503, -3.9849],\n",
      "        [ 0.9719,  1.9203, -4.3359]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1289, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4059,  3.8481, -4.8443],\n",
      "        [ 3.0280, -0.2858, -4.6674],\n",
      "        [ 0.8972,  1.8199, -4.2537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2860, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0536,  4.0890, -3.3880],\n",
      "        [ 1.4055,  1.1416, -4.2186],\n",
      "        [-0.4015,  3.7488, -4.9552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0186, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2176, -0.3788, -4.7894],\n",
      "        [-1.5783,  4.0190, -2.4483],\n",
      "        [ 3.2664, -0.5016, -4.5327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4640, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8742,  4.1149, -4.2214],\n",
      "        [-0.8377,  3.9363, -4.2589],\n",
      "        [ 2.1561,  1.0718, -4.6098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0897, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2612,  0.7698, -4.9925],\n",
      "        [ 3.0309, -0.3853, -4.6926],\n",
      "        [ 3.1049, -0.3117, -4.7626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0297, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2992, -0.6066, -4.3828],\n",
      "        [ 0.2206,  3.2114, -5.3322],\n",
      "        [ 3.1871, -0.7795, -4.2139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1620, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3380,  3.1031, -5.1793],\n",
      "        [ 1.1776,  2.0273, -5.0054],\n",
      "        [ 0.5226,  3.1789, -5.6771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1529, -0.7127, -4.7630],\n",
      "        [-1.0227,  4.5135, -3.7008],\n",
      "        [-1.3416,  4.0985, -3.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0291, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0438,  3.1220, -5.5957],\n",
      "        [ 2.9322, -0.3918, -4.3461],\n",
      "        [-1.1259,  4.0738, -3.4580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0980, grad_fn=<NllLossBackward0>), logits=tensor([[-2.0901,  3.5424, -0.5556],\n",
      "        [-1.1850,  4.0973, -4.1170],\n",
      "        [ 1.8928,  0.7112, -4.5548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0077, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5068,  3.8241, -2.8930],\n",
      "        [-1.0494,  4.3981, -4.3973],\n",
      "        [ 3.4537, -0.9318, -4.4457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8040,  4.0085, -4.6439],\n",
      "        [-1.6694,  4.0909, -2.9637],\n",
      "        [ 3.2925, -0.8012, -4.3300]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2305,  4.3889, -4.1441],\n",
      "        [ 3.4292, -0.4658, -5.2050],\n",
      "        [-1.1904,  4.3704, -3.1767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7833, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2951, -0.7484, -3.8291],\n",
      "        [ 0.5527,  2.7657, -4.9064],\n",
      "        [ 3.2393, -1.0222, -4.1272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1626, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7439,  3.9412, -4.3757],\n",
      "        [ 1.2020,  1.8374, -4.4585],\n",
      "        [ 3.0107,  0.0784, -5.2085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6303, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2062, -0.4184, -5.0436],\n",
      "        [ 2.6014,  0.2106, -5.4842],\n",
      "        [ 0.7083,  2.2988, -5.0324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0294, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5915, -0.7124, -4.2060],\n",
      "        [-1.1049,  4.0356, -3.9995],\n",
      "        [ 2.8530,  0.1975, -5.0253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0249, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1807,  4.0468, -3.7169],\n",
      "        [ 0.4600,  3.3222, -5.3650],\n",
      "        [ 3.3270, -1.0223, -4.2697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4980, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5598, -1.1266, -4.0434],\n",
      "        [ 0.9570,  2.1770, -5.0807],\n",
      "        [-1.3709,  3.9753, -3.6307]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0101, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5296, -0.8132, -4.4420],\n",
      "        [-0.9294,  4.0214, -3.5155],\n",
      "        [ 3.5207, -1.1997, -3.9202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3925,  4.1137, -3.4201],\n",
      "        [ 3.3035, -0.8724, -4.5465],\n",
      "        [ 3.5267, -1.0753, -3.8937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6470,  4.0590, -4.8023],\n",
      "        [-1.0128,  4.2894, -4.2971],\n",
      "        [ 3.5693, -0.7075, -4.0804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1645, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4715,  2.0123, -4.5976],\n",
      "        [-0.1774,  3.7345, -4.9359],\n",
      "        [ 3.4379, -0.8675, -4.4375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0218, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5566, -0.8377, -4.9821],\n",
      "        [ 2.8824, -0.1811, -5.2520],\n",
      "        [-0.8364,  4.1587, -4.3004]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0287, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1174,  3.4433, -4.7735],\n",
      "        [-1.1716,  3.8642, -3.8290],\n",
      "        [ 2.9440, -0.0173, -5.0044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1707, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9459,  2.2988, -5.0432],\n",
      "        [ 3.4750, -1.0202, -3.8781],\n",
      "        [ 1.1875,  2.3616, -4.9704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0927, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9789,  2.2117, -5.2640],\n",
      "        [ 3.5240, -1.0012, -4.2271],\n",
      "        [-0.8716,  3.7140, -4.1322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1659,  3.3862, -5.0624],\n",
      "        [ 3.2250, -1.1803, -3.5968],\n",
      "        [ 3.5493, -1.2774, -4.1474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2907, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1816,  2.1234, -4.9738],\n",
      "        [ 1.1123,  1.9247, -4.7972],\n",
      "        [ 0.7907,  2.4543, -4.7559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0350, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8754,  0.1187, -4.8951],\n",
      "        [ 2.9732, -0.4245, -4.9866],\n",
      "        [ 3.5219, -1.1402, -4.1363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0112, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4253, -1.1881, -4.2130],\n",
      "        [-0.9312,  4.0489, -4.2079],\n",
      "        [ 3.3471, -0.8056, -4.0992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6710, -1.2856, -4.2648],\n",
      "        [ 3.6506, -1.3190, -3.6895],\n",
      "        [ 3.6842, -1.3786, -3.4430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0336, grad_fn=<NllLossBackward0>), logits=tensor([[-4.7814e-03,  3.7320e+00, -5.0403e+00],\n",
      "        [ 3.7312e+00, -1.3536e+00, -3.7556e+00],\n",
      "        [ 2.3184e-01,  2.8516e+00, -5.5286e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0386, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5503,  2.8651, -5.1358],\n",
      "        [ 3.6104, -0.9707, -4.4561],\n",
      "        [ 3.3745, -1.1965, -4.0278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1248,  3.3103, -5.2650],\n",
      "        [ 3.4084, -0.7293, -4.8203],\n",
      "        [ 3.2718, -1.2728, -4.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0076, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8961, -1.4336, -3.7607],\n",
      "        [-0.9466,  4.1798, -4.1427],\n",
      "        [ 3.3055, -1.2365, -3.7580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1449, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7182, -1.2871, -4.4030],\n",
      "        [ 1.0501,  2.0843, -4.8143],\n",
      "        [ 0.5086,  2.5450, -5.5838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8262,  4.2106, -4.3498],\n",
      "        [ 3.8700, -1.3024, -4.3748],\n",
      "        [-1.0634,  4.2606, -4.1050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6755, -1.4517, -4.2054],\n",
      "        [ 3.2079, -0.7623, -4.8235],\n",
      "        [-1.3744,  4.2236, -3.4313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3370, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6016,  3.7447, -4.6942],\n",
      "        [ 0.6909,  2.1499, -4.3955],\n",
      "        [ 1.5116,  1.3344, -3.9882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0140, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8349, -1.4726, -4.5212],\n",
      "        [-0.2171,  3.4810, -4.9782],\n",
      "        [ 3.6092, -0.8317, -4.4681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0328, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5771, -1.4266, -4.1690],\n",
      "        [ 0.4201,  2.8228, -5.5828],\n",
      "        [-1.4296,  4.1476, -2.9513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0121, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1716,  3.8842, -4.9074],\n",
      "        [-0.4517,  3.7359, -4.8340],\n",
      "        [-1.6665,  4.1329, -3.2159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1632, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7897, -1.6393, -3.4406],\n",
      "        [ 3.6213, -1.4219, -3.8174],\n",
      "        [ 1.0980,  1.5967, -3.7923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4219,  4.4973, -3.5303],\n",
      "        [-1.7390,  4.5026, -3.2939],\n",
      "        [ 3.5482, -0.8513, -4.4573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0446, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4440,  4.4252, -3.6783],\n",
      "        [ 3.3389, -1.7299, -3.6703],\n",
      "        [ 0.7520,  2.7831, -5.1944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4437,  4.1191, -3.6442],\n",
      "        [ 3.6195, -1.3801, -3.9210],\n",
      "        [ 3.7687, -1.3395, -3.8739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1963, -0.8086, -4.5050],\n",
      "        [-1.2044,  4.3857, -4.3698],\n",
      "        [-1.0603,  4.3232, -4.5390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0073, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5515,  4.3759, -3.0968],\n",
      "        [ 3.3060, -0.8584, -5.0920],\n",
      "        [-1.5002,  4.3784, -3.8803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0040, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5707, -1.4854, -3.9532],\n",
      "        [-1.5642,  4.4562, -3.7434],\n",
      "        [-1.6756,  4.5377, -3.5126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9106, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6395,  1.0097, -4.1377],\n",
      "        [ 3.7641, -1.6932, -3.7382],\n",
      "        [ 0.5985,  2.7903, -4.9762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4168, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8530,  4.6124, -3.0675],\n",
      "        [-1.6502,  4.4862, -2.9927],\n",
      "        [ 0.9621,  1.8660, -4.1383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9218,  4.4809, -4.3332],\n",
      "        [ 3.7326, -1.6674, -3.8211],\n",
      "        [ 3.7474, -1.7280, -4.4152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5085, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0685, -1.0058, -3.9642],\n",
      "        [ 1.8637,  0.7007, -3.9310],\n",
      "        [ 2.6850,  0.0610, -4.7266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0933,  3.6716, -5.2160],\n",
      "        [ 3.4938, -1.1339, -4.1894],\n",
      "        [-1.6395,  4.4070, -3.3385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2916, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4036,  4.2146, -4.0742],\n",
      "        [-1.7416,  4.0301, -2.4206],\n",
      "        [ 1.1635,  1.4785, -3.7046]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0079, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4976,  4.1392, -3.6632],\n",
      "        [-1.6991,  4.3929, -3.3069],\n",
      "        [ 3.2378, -0.8488, -4.7717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8613,  4.0648, -4.4771],\n",
      "        [-1.2641,  4.2337, -3.6824],\n",
      "        [-1.1865,  4.2655, -3.9369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9585, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2236,  1.3955, -4.0106],\n",
      "        [ 2.5262,  0.5771, -4.8924],\n",
      "        [ 3.6828, -1.2697, -3.7512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0853, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0467,  3.5458, -4.6148],\n",
      "        [ 3.6875, -0.6827, -4.3917],\n",
      "        [ 0.7669,  2.1973, -4.4076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2644, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6193,  3.3666, -5.1498],\n",
      "        [ 1.6328,  1.4590, -4.2022],\n",
      "        [ 1.6918,  0.8879, -4.0636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5582,  4.1100, -2.9076],\n",
      "        [ 3.7468, -1.5738, -4.0550],\n",
      "        [-1.5993,  4.2223, -2.7214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1297, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1589,  3.2722, -5.2797],\n",
      "        [ 0.9396,  1.8977, -4.5276],\n",
      "        [-0.1543,  3.7859, -4.9383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1490, -1.0270, -4.2957],\n",
      "        [ 2.9149, -0.0289, -4.5755],\n",
      "        [ 1.7343,  0.8824, -4.3413]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3820, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4731,  1.0189, -3.5182],\n",
      "        [ 2.1807,  0.6049, -4.5728],\n",
      "        [ 3.5588, -1.4756, -4.1298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1136, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5023, -1.4562, -4.0256],\n",
      "        [ 2.6573, -0.1083, -3.7221],\n",
      "        [ 0.8803,  2.0528, -4.5574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2403, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3874, -0.4666, -4.4591],\n",
      "        [ 1.1503,  1.1566, -3.7785],\n",
      "        [ 3.7838, -1.4007, -3.9802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9463, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9733,  4.0661, -4.3524],\n",
      "        [ 2.7513, -0.0141, -4.4807],\n",
      "        [-1.0607,  4.2674, -4.0322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0546, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4817,  4.2063, -3.5482],\n",
      "        [ 2.4968,  0.1480, -4.2156],\n",
      "        [ 2.8352,  0.1684, -4.4650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1491, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7970,  2.6316, -5.2293],\n",
      "        [ 0.2230,  3.2526, -5.4680],\n",
      "        [ 0.8171,  2.0745, -4.4313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1927, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7191,  0.3195, -4.4053],\n",
      "        [ 3.6480, -0.8805, -4.7391],\n",
      "        [ 1.4921,  0.9972, -3.7229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0263, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1782,  4.1186, -3.3537],\n",
      "        [-1.1630,  4.0784, -3.9602],\n",
      "        [ 0.3485,  3.0100, -5.1362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1797, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2936,  0.2236, -4.6493],\n",
      "        [ 0.9379,  1.6316, -4.1114],\n",
      "        [-0.5123,  3.9134, -4.5266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3381, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8233,  2.2048, -4.1710],\n",
      "        [ 1.5043,  1.1591, -3.7204],\n",
      "        [ 0.5857,  1.8471, -4.8620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0154, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3361, -0.5253, -4.9623],\n",
      "        [-1.3089,  4.1149, -3.8740],\n",
      "        [ 3.2208, -0.6737, -4.7459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1476, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0127,  3.3494, -4.9453],\n",
      "        [-0.0892,  3.6292, -4.9139],\n",
      "        [ 1.7460,  0.9834, -4.3930]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3352, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8772, -0.3857, -4.7811],\n",
      "        [ 1.8978,  0.8229, -3.8898],\n",
      "        [ 1.2628,  1.2139, -3.9656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1754, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0908,  3.6829, -5.3194],\n",
      "        [ 1.5174,  1.0600, -4.4212],\n",
      "        [-0.3942,  4.0983, -4.8229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2483, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4990,  2.7293, -5.2530],\n",
      "        [ 1.7349,  0.6746, -4.0713],\n",
      "        [ 0.7528,  1.6564, -3.9440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6775, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5895,  2.4232, -4.3889],\n",
      "        [-0.4094,  3.6524, -5.0447],\n",
      "        [ 3.1270, -0.2930, -4.9711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0783, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0852,  0.6355, -4.3975],\n",
      "        [-1.6140,  4.3848, -3.3735],\n",
      "        [ 0.0678,  3.9686, -5.2757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0147, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7014,  4.2981, -2.9595],\n",
      "        [ 3.3334, -0.4071, -4.9275],\n",
      "        [-0.3288,  3.7454, -4.7445]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0726, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9435,  4.2476, -4.3511],\n",
      "        [-1.7784,  4.2904, -2.9977],\n",
      "        [ 0.6628,  2.1324, -3.8776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1438, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8427,  1.9305, -3.6542],\n",
      "        [ 0.3377,  3.3772, -5.2458],\n",
      "        [ 2.6837,  0.3313, -4.8012]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5766, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7688, -0.2922, -4.4400],\n",
      "        [ 0.5701,  2.0409, -3.7980],\n",
      "        [-1.4200,  4.4395, -3.5417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4585, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2183,  1.2942, -3.3531],\n",
      "        [ 2.4600,  0.4741, -4.5899],\n",
      "        [ 1.5310,  1.1120, -3.4664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2734, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4556, -0.0936, -4.6467],\n",
      "        [ 1.1500,  1.2436, -3.4687],\n",
      "        [ 2.5616,  0.2153, -4.8459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1181, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4954,  4.3680, -3.7703],\n",
      "        [-0.9886,  4.3075, -4.3852],\n",
      "        [ 1.6347,  0.7376, -3.5502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6128, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3989,  0.3583, -4.7571],\n",
      "        [ 2.1159,  0.7276, -4.3979],\n",
      "        [ 0.4288,  2.6508, -4.8814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3905, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6142,  4.5471, -3.2601],\n",
      "        [ 3.4289, -1.2485, -4.2729],\n",
      "        [ 0.8189,  1.5958, -3.5533]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6991, -1.0354, -4.2922],\n",
      "        [ 3.0307, -0.1466, -4.4787],\n",
      "        [ 3.3785, -1.3211, -4.3766]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0386, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6145, -0.8673, -4.3258],\n",
      "        [ 2.6213,  0.3478, -4.6348],\n",
      "        [ 3.9304, -1.2839, -4.4708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2911, -1.2362, -3.8843],\n",
      "        [-1.5584,  4.0391, -2.6836],\n",
      "        [-1.4887,  4.2198, -3.4199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0097, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6012, -0.5379, -5.0196],\n",
      "        [ 3.6930, -1.4262, -4.3340],\n",
      "        [ 3.6502, -1.3728, -4.4842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8090,  4.0022, -4.5094],\n",
      "        [-1.1909,  4.0925, -4.6023],\n",
      "        [ 3.0589, -0.8805, -4.3467]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0052, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3050,  4.1001, -3.9175],\n",
      "        [-1.0605,  4.3570, -4.1868],\n",
      "        [ 3.9173, -1.2374, -4.3225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6490,  4.0361, -4.5634],\n",
      "        [ 0.8646,  1.8127, -3.7921],\n",
      "        [ 3.5008, -1.2537, -3.8974]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7757, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5403, -1.1599, -4.1157],\n",
      "        [ 2.1633, -0.0220, -3.6552],\n",
      "        [ 3.0872, -0.7015, -4.3960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0181, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2352, -0.5216, -4.8294],\n",
      "        [ 4.0281, -1.4863, -4.5352],\n",
      "        [ 3.0285, -0.5913, -4.9204]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0252, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1868,  4.3230, -3.8769],\n",
      "        [ 2.5076, -0.1628, -4.7221],\n",
      "        [-1.6324,  4.2465, -2.8708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0048, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7131,  4.4390, -3.2287],\n",
      "        [-0.5748,  4.0693, -4.6318],\n",
      "        [-1.6620,  4.7276, -3.0659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1710, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5263, -1.2880, -4.5837],\n",
      "        [ 1.5446,  1.1029, -3.4258],\n",
      "        [-1.2609,  4.2869, -4.3002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1327, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8473, -1.3085, -4.1913],\n",
      "        [ 0.7734,  2.1882, -4.4628],\n",
      "        [ 0.8353,  2.5023, -4.7573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2452, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5926, -1.3846, -4.3555],\n",
      "        [ 4.0063, -1.5410, -4.4967],\n",
      "        [ 1.1415,  1.0953, -3.0597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0063, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7159, -1.4115, -4.3975],\n",
      "        [-1.2471,  4.2567, -3.6590],\n",
      "        [ 3.6325, -1.1778, -4.4903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0636, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8944,  4.2283, -4.7809],\n",
      "        [-1.6002,  4.3519, -3.3849],\n",
      "        [ 2.1347,  0.5118, -4.0844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7233, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1841,  4.0166, -5.1666],\n",
      "        [ 3.9393, -1.2787, -4.6780],\n",
      "        [ 2.1913,  0.1683, -3.8834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2081, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1773,  0.9593, -2.8954],\n",
      "        [-0.4681,  3.4939, -4.5921],\n",
      "        [ 3.6634, -1.5375, -4.4053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3735, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2461,  0.7640, -3.2577],\n",
      "        [ 0.5407,  2.4095, -4.6858],\n",
      "        [ 3.7304, -1.3859, -3.9908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6791, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4692, -0.6442, -4.9513],\n",
      "        [ 3.7677, -1.1347, -4.4010],\n",
      "        [ 1.4216,  1.0549, -3.0561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0047, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6965, -1.2094, -4.5088],\n",
      "        [-1.7763,  4.2504, -2.5566],\n",
      "        [-1.6544,  4.4056, -2.8929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3420, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9684,  0.5691, -3.5193],\n",
      "        [-1.7303,  4.2160, -2.4872],\n",
      "        [ 1.2168,  1.4109, -3.6928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0033, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4533,  4.6523, -3.4333],\n",
      "        [-1.4000,  4.3614, -2.6981],\n",
      "        [-1.4842,  4.2938, -3.9686]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0642, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4815,  4.4448, -2.7386],\n",
      "        [ 3.4307, -0.8015, -4.3806],\n",
      "        [ 0.6820,  2.3473, -4.3245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1192, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3661,  2.8918, -5.2029],\n",
      "        [-1.7162,  3.8911, -2.3677],\n",
      "        [ 1.7303,  0.5664, -3.7909]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2519, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6594,  4.2795, -2.6446],\n",
      "        [ 2.7831,  0.0250, -4.5871],\n",
      "        [ 1.2513,  1.2362, -3.4850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1503, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4452, -0.4403, -5.1914],\n",
      "        [ 1.6609,  0.9290, -3.9273],\n",
      "        [ 3.2462, -0.0909, -5.2348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0335, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0598, -0.3921, -4.8325],\n",
      "        [ 3.3820, -0.3810, -5.0177],\n",
      "        [-0.0274,  3.0395, -5.2180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0564, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5157,  0.5074, -4.6551],\n",
      "        [ 3.0921, -0.1289, -5.1510],\n",
      "        [-1.7415,  4.3486, -2.7995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0221, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8088,  4.5297, -3.2021],\n",
      "        [ 2.7558, -0.0188, -4.5818],\n",
      "        [-1.8791,  4.3966, -2.4034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1552, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9750,  1.6094, -3.8194],\n",
      "        [ 3.1063, -0.2641, -4.8146],\n",
      "        [-1.5510,  4.4257, -2.9353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6136, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6591,  0.0880, -5.0891],\n",
      "        [ 1.7814,  0.6813, -3.5463],\n",
      "        [ 0.9332,  2.1475, -4.1769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1201, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6722,  0.0587, -5.4384],\n",
      "        [ 1.7788,  0.6629, -3.8056],\n",
      "        [-1.4974,  4.3532, -3.5853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1572, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6382,  1.0847, -3.4193],\n",
      "        [ 3.5666, -1.0044, -4.9380],\n",
      "        [-1.6542,  4.3608, -3.4287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0343, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2297,  4.5395, -4.0949],\n",
      "        [ 2.4646,  0.1734, -4.6512],\n",
      "        [-1.9094,  4.4909, -2.7137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0847, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8646,  0.5802, -4.0826],\n",
      "        [-1.3749,  4.2985, -3.5469],\n",
      "        [-1.6718,  4.2221, -2.6353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0049, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9840,  4.0484, -4.6855],\n",
      "        [-1.4025,  4.2892, -3.1337],\n",
      "        [-1.4078,  4.1988, -3.7722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1625, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8830,  0.5641, -3.6367],\n",
      "        [-1.7909,  4.6101, -2.9453],\n",
      "        [ 0.6303,  1.9200, -4.1022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5802, -0.6127, -5.5335],\n",
      "        [-1.5784,  4.0883, -2.9762],\n",
      "        [-1.3841,  4.5155, -3.7250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4528, -0.6957, -4.8301],\n",
      "        [-0.1053,  3.7312, -5.5553],\n",
      "        [-1.7484,  4.4615, -3.0694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7033, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8233,  1.9445, -3.3928],\n",
      "        [-1.9099,  4.0838, -2.1144],\n",
      "        [ 2.1234,  0.4830, -3.9063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0136, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9224, -0.6852, -5.2255],\n",
      "        [ 3.1861, -0.4069, -5.1362],\n",
      "        [-1.8248,  4.2317, -2.6955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0295, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7341,  0.1102, -4.5372],\n",
      "        [-1.6899,  4.3634, -3.2529],\n",
      "        [ 3.4363, -0.7681, -4.8806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0521, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4368,  2.9891, -5.1796],\n",
      "        [ 2.5854,  0.0732, -4.6880],\n",
      "        [-1.5704,  4.5915, -3.3495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8133, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2085,  0.3212, -4.2207],\n",
      "        [ 2.2098,  0.0468, -4.1520],\n",
      "        [ 3.2260, -0.4957, -5.0222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0607, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1951, -0.7068, -5.1496],\n",
      "        [ 2.4279,  0.3190, -4.2902],\n",
      "        [ 3.1319,  0.0742, -5.3842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4132, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6069,  4.4261, -3.4787],\n",
      "        [ 3.6540, -0.5472, -5.0753],\n",
      "        [ 3.2040, -0.6809, -5.0096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1070, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7631, -1.0396, -4.9978],\n",
      "        [-1.7850,  4.5745, -3.2856],\n",
      "        [ 0.6993,  1.7237, -3.5785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6667,  4.3011, -3.0136],\n",
      "        [-1.4459,  4.3215, -3.3032],\n",
      "        [ 3.6966, -0.6194, -5.3597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0111, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3853,  3.7703, -4.9467],\n",
      "        [ 3.4173, -0.8146, -5.0826],\n",
      "        [-1.5776,  4.4611, -3.2739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0756, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8984,  4.5160, -3.0764],\n",
      "        [ 2.6076,  0.2224, -4.8827],\n",
      "        [ 2.2884,  0.3508, -4.1527]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1306, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5359,  0.7643, -3.3996],\n",
      "        [-1.4371,  4.1652, -3.9022],\n",
      "        [-1.7833,  4.3021, -2.8775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0106, -0.0913, -5.3615],\n",
      "        [ 3.6075, -0.7397, -5.2033],\n",
      "        [-1.6862,  4.4716, -2.8856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0756, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9608,  4.0364, -4.6951],\n",
      "        [ 2.1582,  0.5016, -4.0745],\n",
      "        [ 2.8386, -0.2803, -4.8522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0762, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2126,  2.7916, -5.0839],\n",
      "        [ 2.3371,  0.3941, -4.7798],\n",
      "        [ 3.3013, -0.5791, -5.4949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1157, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8866,  0.4797, -4.3959],\n",
      "        [ 2.6308,  0.1161, -4.7934],\n",
      "        [ 2.7454, -0.2835, -4.0830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0464, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8997, -0.1925, -5.1722],\n",
      "        [ 2.8664,  0.4915, -5.6217],\n",
      "        [-0.9248,  4.3211, -4.6830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5117, grad_fn=<NllLossBackward0>), logits=tensor([[-1.8091,  4.4096, -3.0507],\n",
      "        [-1.6753,  4.3253, -2.6112],\n",
      "        [ 0.5009,  1.7812, -3.6239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.6835, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7660,  4.2486, -4.8941],\n",
      "        [ 3.2337, -0.3774, -5.4682],\n",
      "        [-1.6229,  4.6385, -3.0990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0027, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4351,  4.6219, -3.2751],\n",
      "        [-1.8827,  4.3246, -2.7937],\n",
      "        [-1.4506,  4.5795, -4.0932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0686,  0.3025, -5.0415],\n",
      "        [ 3.3871, -0.5650, -5.2255],\n",
      "        [-1.4638,  4.5099, -3.8727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1378, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6422,  0.6620, -2.9631],\n",
      "        [ 0.2523,  2.6850, -5.0380],\n",
      "        [-1.4370,  4.4509, -3.9237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2287,  4.4007, -4.0346],\n",
      "        [ 3.0165, -0.3605, -5.3710],\n",
      "        [ 3.1126, -0.6217, -5.5050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1229,  4.1627, -4.0450],\n",
      "        [ 3.0584, -0.0571, -5.4219],\n",
      "        [ 3.4054, -0.4925, -5.4511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1732, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7740,  0.6114, -3.5596],\n",
      "        [-1.1477,  4.3369, -4.2588],\n",
      "        [ 2.1058,  0.7912, -4.1090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0066, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1849,  4.1443, -4.5421],\n",
      "        [-0.7245,  4.1761, -4.9323],\n",
      "        [-0.7690,  4.1641, -4.2219]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2214, -0.4821, -5.3829],\n",
      "        [ 3.1583, -0.1162, -5.1032],\n",
      "        [-0.7321,  3.8271, -4.5951]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1993, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0391,  1.5080, -3.4522],\n",
      "        [ 2.8183,  0.0702, -4.7335],\n",
      "        [ 0.2389,  3.3196, -5.4202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6521, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7481,  2.5051, -4.6397],\n",
      "        [-0.6424,  4.2807, -4.6610],\n",
      "        [ 3.2013, -0.2307, -5.4915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8010, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4281,  3.1110, -5.3320],\n",
      "        [ 0.8129,  2.0918, -4.0945],\n",
      "        [ 2.1805,  0.2243, -4.4468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.3586, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3820, -0.4658, -4.7029],\n",
      "        [ 3.3352, -0.8036, -5.3721],\n",
      "        [ 2.8327, -0.0098, -5.0126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1661, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0153,  3.4111, -5.2235],\n",
      "        [ 2.6701,  0.2659, -5.2298],\n",
      "        [ 1.5931,  1.0983, -3.6298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0663, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8715, -0.2325, -5.3365],\n",
      "        [ 0.1713,  3.2923, -5.4974],\n",
      "        [-0.5955,  4.3135, -4.8797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0688, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7951,  0.3204, -5.6684],\n",
      "        [-0.1591,  3.5467, -5.2122],\n",
      "        [ 2.5112,  0.2664, -5.2947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0325, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6904,  3.9256, -4.9419],\n",
      "        [-0.7787,  4.1869, -5.0060],\n",
      "        [ 2.7842,  0.3027, -5.5529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1393, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3163,  3.9087, -5.0018],\n",
      "        [-0.5853,  4.1950, -4.7801],\n",
      "        [ 1.6030,  0.8678, -3.8146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3469, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4563,  3.2654, -5.4765],\n",
      "        [ 1.0501,  1.5399, -4.3151],\n",
      "        [ 1.1220,  1.5576, -3.6038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1408, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0095,  1.0004, -4.9934],\n",
      "        [-0.5261,  3.9157, -4.8422],\n",
      "        [ 2.6722,  0.4087, -5.8763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6116, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4188,  0.8002, -3.9112],\n",
      "        [ 1.0257,  1.4723, -4.0955],\n",
      "        [ 2.0715,  0.9600, -4.7262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3609, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7148,  4.1056, -4.7095],\n",
      "        [ 2.2705,  0.9515, -5.4295],\n",
      "        [ 1.1712,  1.4358, -3.7863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1205, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7522,  0.8379, -5.2455],\n",
      "        [-0.4343,  3.9095, -4.8454],\n",
      "        [-0.3727,  4.1683, -5.1398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1897, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8530,  1.0440, -4.6334],\n",
      "        [-0.7403,  4.1154, -4.6748],\n",
      "        [ 0.9237,  2.4796, -5.3377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1834, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6758,  1.2864, -4.6026],\n",
      "        [-0.0793,  3.6415, -5.1437],\n",
      "        [-0.8444,  4.0207, -4.6026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6352, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1200,  1.3367, -3.6737],\n",
      "        [ 1.8083,  1.0035, -5.0429],\n",
      "        [ 1.4357,  1.4932, -4.0977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1416, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3076,  0.8620, -5.8432],\n",
      "        [ 2.3005,  0.8134, -5.4736],\n",
      "        [-0.7134,  4.0291, -4.6883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3218, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1767,  1.2155, -3.8747],\n",
      "        [ 1.4219,  1.2599, -3.9338],\n",
      "        [ 0.3802,  2.9369, -5.7189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5041, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8054,  1.5427, -4.3377],\n",
      "        [ 1.5313,  1.1846, -4.8419],\n",
      "        [ 2.0523,  0.7304, -5.1782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3434, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8675,  0.8208, -4.9070],\n",
      "        [ 0.9070,  2.3198, -4.5695],\n",
      "        [ 1.2727,  0.8549, -3.7714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3801, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9979,  0.8973, -4.8579],\n",
      "        [ 1.8827,  0.8380, -4.4709],\n",
      "        [ 1.4023,  1.0847, -3.9848]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0604, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4673,  0.6892, -5.7320],\n",
      "        [-0.4874,  4.0415, -4.8945],\n",
      "        [-0.2989,  3.9741, -4.6444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3783, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4972,  1.3629, -4.3381],\n",
      "        [-0.3498,  4.0520, -4.9051],\n",
      "        [ 1.1450,  1.6020, -3.9947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3175, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8411,  3.0255, -5.7239],\n",
      "        [ 1.5513,  0.8764, -3.7315],\n",
      "        [ 1.6195,  0.9951, -4.1655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2605, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1250,  0.6844, -5.4621],\n",
      "        [ 1.9511,  0.5259, -4.8894],\n",
      "        [ 1.8568,  0.9918, -4.9589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2413, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1276,  2.8838, -6.1878],\n",
      "        [ 1.1597,  1.7382, -4.4586],\n",
      "        [ 2.3181,  0.2384, -5.2668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7540, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5957,  0.9249, -3.8875],\n",
      "        [ 0.8749,  2.8938, -5.6351],\n",
      "        [ 1.6391,  1.0227, -3.8895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3572, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9992,  1.7984, -4.4969],\n",
      "        [ 1.4032,  1.0303, -3.2769],\n",
      "        [ 2.2396,  0.5492, -5.7245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1576, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3766,  0.5668, -4.8909],\n",
      "        [ 2.3507,  0.5570, -5.3697],\n",
      "        [ 1.0072,  2.7176, -5.2541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4661, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0275,  1.3753, -3.8189],\n",
      "        [ 1.0625,  1.8557, -4.9191],\n",
      "        [ 2.1231,  0.2062, -4.0047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4846, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9182,  2.3247, -4.9173],\n",
      "        [ 0.7010,  1.6962, -4.1081],\n",
      "        [ 1.1905,  1.5940, -4.0574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1562, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7989,  2.2849, -5.0763],\n",
      "        [ 2.6022,  0.0821, -5.9406],\n",
      "        [ 0.7549,  2.3430, -5.0156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0852, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4755,  3.9390, -4.8019],\n",
      "        [ 1.0063,  2.7857, -5.6227],\n",
      "        [ 2.6607,  0.2603, -4.9034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.7917, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7360,  0.2415, -5.8125],\n",
      "        [-0.8117,  4.3097, -4.3686],\n",
      "        [ 2.2261,  0.5230, -4.7432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1046, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6039,  0.4845, -4.8093],\n",
      "        [ 2.0278,  0.4500, -3.8717],\n",
      "        [-0.5487,  4.0615, -4.7745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1548, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8701,  0.3077, -4.0345],\n",
      "        [-0.5213,  3.8362, -4.7640],\n",
      "        [ 1.6967,  0.4692, -4.3077]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8949, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6042,  0.5802, -5.0006],\n",
      "        [-0.2932,  3.9230, -5.1176],\n",
      "        [ 1.3446,  0.9513, -3.3321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0932, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1239,  0.4987, -3.4645],\n",
      "        [ 2.6677,  0.1999, -5.4112],\n",
      "        [-0.4519,  3.7509, -5.0791]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0393, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6998,  0.2263, -5.1064],\n",
      "        [-0.2379,  3.7010, -5.0593],\n",
      "        [-0.1111,  3.9487, -4.9313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1911, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6271,  0.7375, -3.4761],\n",
      "        [ 2.0051,  0.3449, -4.2278],\n",
      "        [ 2.7257, -0.2669, -5.9737]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1342, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0210,  3.7229, -5.0637],\n",
      "        [ 1.1969,  2.1731, -4.5474],\n",
      "        [ 2.7562, -0.0737, -5.8874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1299, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5896,  0.6559, -3.3640],\n",
      "        [ 0.0508,  3.3963, -5.4161],\n",
      "        [-0.2287,  3.7666, -5.2777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0823, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1624,  0.0563, -5.7266],\n",
      "        [ 2.0389,  0.2892, -3.8146],\n",
      "        [ 3.2105,  0.0172, -5.8246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1398, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5387, -0.1180, -4.8450],\n",
      "        [ 0.8879,  1.8528, -4.3232],\n",
      "        [ 0.0633,  3.6770, -5.0094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1894, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7043,  0.1383, -5.2016],\n",
      "        [ 2.2683,  0.7978, -4.1134],\n",
      "        [ 1.7674,  0.6462, -3.6225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0424, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9869,  0.1270, -5.5026],\n",
      "        [ 0.2581,  3.3605, -5.5423],\n",
      "        [-0.0975,  3.4956, -5.0430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4174, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5851,  1.1638, -3.8994],\n",
      "        [ 2.8075, -0.2935, -5.6589],\n",
      "        [ 1.7781,  0.6363, -3.8862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1015, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0838,  3.6175, -5.3438],\n",
      "        [ 2.2524,  0.7656, -4.1073],\n",
      "        [ 2.7365,  0.1161, -5.4783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0542, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9514, -0.0073, -5.3184],\n",
      "        [-0.0680,  3.4920, -4.9866],\n",
      "        [ 2.6115,  0.1606, -4.5366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2807, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1402, -0.4465, -5.7171],\n",
      "        [ 1.3692,  1.5526, -4.9274],\n",
      "        [-0.1678,  3.5266, -5.0021]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0352, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0575,  3.7914, -5.1037],\n",
      "        [ 0.2419,  3.5415, -5.4413],\n",
      "        [ 2.9485, -0.0660, -5.7671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4935, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0783,  1.0133, -4.4007],\n",
      "        [ 0.2419,  3.9222, -5.5477],\n",
      "        [ 2.2480, -0.0935, -4.2230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0338, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9359e+00, -9.6716e-02, -5.5044e+00],\n",
      "        [-5.2690e-04,  3.4153e+00, -5.1113e+00],\n",
      "        [-5.2517e-02,  3.7764e+00, -5.2746e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0707, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8279, -0.0676, -5.1131],\n",
      "        [ 0.6928,  2.7017, -5.8404],\n",
      "        [-0.0362,  3.3996, -5.1383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0347, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1922,  3.8579, -5.0267],\n",
      "        [ 2.7090,  0.0181, -4.6748],\n",
      "        [-0.3784,  3.5129, -4.7366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0306, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1134,  3.7270, -4.9837],\n",
      "        [ 0.0475,  3.7166, -5.4324],\n",
      "        [ 3.1006,  0.0189, -5.6405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3376, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3694,  3.9658, -4.7994],\n",
      "        [ 1.5386,  1.0447, -3.8153],\n",
      "        [-0.0548,  3.5765, -5.2343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2456, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5271, -0.2579, -5.8119],\n",
      "        [ 1.4214,  1.4003, -3.9678],\n",
      "        [ 3.2017, -0.3218, -5.6194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3703, -0.3375, -5.4094],\n",
      "        [ 3.1062, -0.2332, -5.7645],\n",
      "        [-0.4132,  3.9039, -5.0183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0793, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2168, -0.2672, -5.5484],\n",
      "        [-0.5864,  3.7759, -4.6381],\n",
      "        [ 2.2436,  0.6993, -4.2506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0313, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9055,  0.0112, -5.9900],\n",
      "        [ 3.5154, -0.4638, -5.5924],\n",
      "        [ 3.2979, -0.5528, -5.6904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0416, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5477, -0.2359, -4.3715],\n",
      "        [ 3.1003, -0.2051, -5.7373],\n",
      "        [ 3.2812, -0.2975, -5.5726]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0179, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5919,  3.7384, -4.4009],\n",
      "        [-0.5065,  3.8126, -4.5491],\n",
      "        [ 3.3039, -0.3078, -5.8700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1566, -0.3989, -5.6679],\n",
      "        [ 3.1022, -0.0183, -5.0199],\n",
      "        [ 3.2172, -0.3325, -5.6748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4922, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6760,  3.6340, -4.4202],\n",
      "        [ 1.0972,  2.2628, -5.4363],\n",
      "        [ 3.4569, -0.1949, -5.7343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4684, -0.5060, -5.3581],\n",
      "        [-0.8377,  3.9989, -4.5487],\n",
      "        [ 3.0709, -0.2489, -4.9372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2743, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8525,  3.0387, -5.3859],\n",
      "        [-0.7862,  3.7586, -4.3585],\n",
      "        [ 1.5438,  1.5207, -4.9296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6506, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2768, -0.0068, -4.5511],\n",
      "        [ 3.6613, -0.3544, -5.6562],\n",
      "        [ 2.0484,  0.3885, -4.1114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4648, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2299,  2.2968, -5.2113],\n",
      "        [ 3.3388, -0.4736, -5.2031],\n",
      "        [-0.6427,  4.0327, -4.5141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0508, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8442, -0.1762, -4.6305],\n",
      "        [ 3.1191, -0.0057, -5.4391],\n",
      "        [ 0.2156,  3.2286, -5.2064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0254, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4220, -0.4350, -5.5358],\n",
      "        [ 3.0613, -0.2812, -5.6536],\n",
      "        [ 3.0852, -0.8098, -5.3255]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1990, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7498, -0.3768, -4.8670],\n",
      "        [ 3.3704, -0.5108, -5.6251],\n",
      "        [ 1.6914,  1.3363, -4.0494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0257, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3064, -0.2384, -4.7902],\n",
      "        [ 0.0176,  3.7062, -5.3893],\n",
      "        [-0.0562,  3.6850, -5.1072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4456, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8604,  1.0411, -4.3074],\n",
      "        [ 2.2630,  0.0604, -4.6195],\n",
      "        [ 0.4573,  3.5382, -5.6293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0302, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7333, -0.6888, -5.6529],\n",
      "        [ 3.0074, -0.4568, -5.1729],\n",
      "        [ 2.8564, -0.1822, -4.4575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8430, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3989e+00,  1.1487e-03, -4.6391e+00],\n",
      "        [ 3.2054e+00, -5.2064e-01, -5.6162e+00],\n",
      "        [ 3.3123e+00, -6.2080e-01, -5.4734e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8619, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2279,  0.5775, -4.6514],\n",
      "        [ 3.5252, -0.8158, -5.6591],\n",
      "        [ 1.6077,  1.5070, -4.8821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5846, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9282, -0.0425, -4.6793],\n",
      "        [ 2.1992,  0.7243, -4.7331],\n",
      "        [ 3.2674, -0.5658, -5.2701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3114, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1944, -0.2701, -5.3952],\n",
      "        [ 3.3808, -0.4947, -5.7333],\n",
      "        [ 3.4191, -0.4425, -5.5115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3005, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2179, -0.2830, -5.7349],\n",
      "        [ 3.3337, -0.3583, -5.7966],\n",
      "        [ 1.5318,  1.2478, -4.2257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6148, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2845, -0.6709, -5.4226],\n",
      "        [ 2.1935,  0.6468, -4.7071],\n",
      "        [ 2.7356,  0.3055, -4.9293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5379, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9177, -0.1888, -4.9072],\n",
      "        [ 2.1179,  0.9059, -4.9831],\n",
      "        [ 2.4584,  0.1610, -4.8058]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.1997, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6497,  0.3997, -5.4050],\n",
      "        [ 1.4224,  1.8450, -4.8060],\n",
      "        [ 1.6567,  1.5587, -5.2756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8141, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2074,  0.7896, -5.0922],\n",
      "        [ 2.4096,  0.4696, -4.4776],\n",
      "        [ 0.9688,  2.7931, -5.7412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2307, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1390,  2.1292, -5.6424],\n",
      "        [ 2.7472, -0.2077, -5.1756],\n",
      "        [ 1.3257,  2.2848, -5.8143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0507, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2366, -0.1325, -5.3547],\n",
      "        [ 2.7766,  0.2739, -5.6563],\n",
      "        [ 3.2311,  0.0082, -5.3768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1957, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0128,  1.5018, -5.6230],\n",
      "        [ 2.7044,  0.0997, -5.2466],\n",
      "        [ 0.5879,  3.6650, -5.5276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3279, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0861,  0.6124, -4.9229],\n",
      "        [ 0.7047,  2.8015, -5.9410],\n",
      "        [ 1.4627,  1.5299, -5.3132]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0912, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5920,  0.2083, -5.5770],\n",
      "        [ 2.8969,  0.2295, -5.7224],\n",
      "        [ 2.3297,  0.2470, -5.0416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0782, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2992,  0.4594, -4.9661],\n",
      "        [ 2.8315, -0.1000, -4.3570],\n",
      "        [ 3.0532, -0.3163, -5.7203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1575, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1879,  0.7630, -5.3916],\n",
      "        [ 0.8700,  3.1464, -6.0669],\n",
      "        [ 1.1661,  2.9269, -6.4264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4500, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1978,  2.0801, -5.2666],\n",
      "        [-0.5204,  3.5703, -4.9358],\n",
      "        [ 0.8862,  3.0952, -6.3007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3587, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8315,  1.1097, -5.3789],\n",
      "        [ 2.7754,  0.1352, -5.6249],\n",
      "        [ 1.4195,  1.2447, -4.9938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3622, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4342,  1.6901, -5.4629],\n",
      "        [ 2.3974,  0.6290, -5.7304],\n",
      "        [ 2.6776,  0.4136, -5.4035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2538, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4543,  0.9813, -5.3892],\n",
      "        [ 1.9089,  1.2427, -5.6259],\n",
      "        [ 0.8971,  2.7946, -5.7807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5621, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5508,  2.0017, -5.6761],\n",
      "        [ 1.3836,  2.0924, -5.2028],\n",
      "        [ 0.5146,  2.9580, -6.0625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0726, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0144,  3.6416, -5.4538],\n",
      "        [ 2.7266,  0.0783, -6.0137],\n",
      "        [ 0.7815,  2.8169, -5.8899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0855, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4175,  3.4502, -5.8762],\n",
      "        [ 2.7509,  0.0092, -5.2237],\n",
      "        [ 2.7449,  0.8985, -5.9185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0513, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2311e-03,  3.6891e+00, -5.2774e+00],\n",
      "        [ 3.1856e-01,  3.1414e+00, -5.9824e+00],\n",
      "        [ 2.5736e+00, -3.4098e-02, -5.8969e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0399, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4643,  3.1386, -5.6952],\n",
      "        [ 0.1241,  3.4982, -5.5413],\n",
      "        [-0.2041,  3.7445, -5.1175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3650, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2232,  3.7297, -5.3580],\n",
      "        [-0.3018,  3.8391, -5.1220],\n",
      "        [ 2.0199,  1.3860, -5.3954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0375, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6362,  3.1921, -5.7874],\n",
      "        [-0.1963,  3.7995, -5.1226],\n",
      "        [-0.0919,  3.8520, -5.0631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0692, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6503,  0.4989, -5.5838],\n",
      "        [-0.3016,  3.7443, -4.7981],\n",
      "        [ 0.5719,  3.0628, -5.7610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0691, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3508, -0.4802, -5.7650],\n",
      "        [ 0.0251,  3.6558, -5.2952],\n",
      "        [ 2.3609,  0.6031, -5.6066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4240, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5381,  3.1996, -6.0452],\n",
      "        [ 3.0505, -0.1013, -5.3963],\n",
      "        [ 1.2701,  2.0569, -5.1806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0494, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8556,  0.0541, -5.2154],\n",
      "        [ 3.1416, -0.0109, -5.5089],\n",
      "        [ 0.5446,  3.5793, -5.7457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0386, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9673,  3.3441, -5.6597],\n",
      "        [-0.3004,  3.7903, -5.1291],\n",
      "        [-0.6821,  3.9122, -4.7589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0495, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3910,  3.8791, -4.7697],\n",
      "        [ 0.6770,  3.1332, -5.7417],\n",
      "        [ 0.4510,  3.3854, -5.8651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0391, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2912,  3.9501, -5.1735],\n",
      "        [-0.5418,  4.0902, -4.6465],\n",
      "        [ 2.6170,  0.2882, -6.0179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0148, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5064,  3.9433, -4.8162],\n",
      "        [ 3.3486, -0.3529, -5.5239],\n",
      "        [-0.7072,  4.1340, -5.0020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1185, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7147,  2.7190, -5.6842],\n",
      "        [-0.1751,  3.6563, -5.2967],\n",
      "        [ 2.0757,  0.6034, -5.0614]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0444, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8448, -0.0326, -5.3665],\n",
      "        [ 3.4079, -0.3165, -5.9092],\n",
      "        [ 0.4344,  3.3252, -5.5817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0940, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1211, -0.1078, -5.6324],\n",
      "        [ 0.9696,  2.2937, -5.7259],\n",
      "        [-0.7811,  4.2296, -4.6292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0786, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3993, -0.4164, -5.8195],\n",
      "        [-0.7732,  3.7978, -4.6106],\n",
      "        [ 0.9972,  2.4880, -5.7580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0211, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0276, -0.3842, -5.4145],\n",
      "        [ 3.2281, -0.5733, -5.5081],\n",
      "        [-0.7424,  4.0530, -4.5706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0377, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9861,  4.2765, -4.6144],\n",
      "        [ 0.6038,  3.2110, -5.6787],\n",
      "        [ 3.1968, -0.1012, -5.5596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0397, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1665, -0.1584, -5.6479],\n",
      "        [ 0.0795,  2.9314, -5.3142],\n",
      "        [ 3.3542, -0.2389, -5.5945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0723, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3161, -0.0890, -5.8614],\n",
      "        [ 0.7724,  2.5044, -5.7152],\n",
      "        [ 3.5952, -0.2596, -5.6055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0298, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0925, -0.0498, -5.5616],\n",
      "        [ 3.6845, -0.7115, -5.7142],\n",
      "        [ 3.2080, -0.1379, -5.6306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0261, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5028,  3.2172, -5.9363],\n",
      "        [-0.7541,  4.1756, -4.3617],\n",
      "        [-0.8085,  4.2124, -4.7068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2214, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3001, -0.3166, -5.5038],\n",
      "        [ 0.4198,  3.1303, -5.7056],\n",
      "        [ 1.6993,  1.3863, -5.1186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1711, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9448,  4.0694, -4.5782],\n",
      "        [ 3.2743, -0.6997, -5.5061],\n",
      "        [ 1.2964,  1.7617, -5.4501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3946, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3882,  0.7852, -5.3624],\n",
      "        [-0.7733,  4.5043, -4.7147],\n",
      "        [ 1.7098,  1.1775, -5.4307]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0435, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3082, -0.4086, -5.5684],\n",
      "        [-1.1620,  4.2407, -4.3341],\n",
      "        [ 0.5374,  2.7757, -5.5274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2225, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7068,  1.6000, -5.5403],\n",
      "        [ 3.5234, -0.7044, -5.7470],\n",
      "        [ 3.4714, -1.0225, -4.9194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9411,  4.3495, -4.2679],\n",
      "        [-1.0823,  4.1248, -4.1123],\n",
      "        [-0.8445,  4.3093, -4.3500]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0315, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4602, -0.4602, -5.5410],\n",
      "        [-0.9333,  4.3490, -4.9558],\n",
      "        [ 2.6746,  0.0399, -5.0295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0933, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1528,  4.1716, -4.2456],\n",
      "        [ 2.1679,  0.9601, -5.6122],\n",
      "        [ 3.4689, -0.8760, -5.6119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6123, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4684e+00,  3.2731e-03, -5.7300e+00],\n",
      "        [ 2.0298e-01,  3.1418e+00, -5.5766e+00],\n",
      "        [ 9.2237e-01,  2.4866e+00, -5.7041e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0067, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2246,  4.5735, -4.4547],\n",
      "        [ 3.3660, -1.0237, -4.8106],\n",
      "        [-1.2952,  4.1731, -3.9926]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0159, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9771,  4.5283, -4.4621],\n",
      "        [-0.9907,  4.4004, -4.3672],\n",
      "        [ 0.4268,  3.6554, -5.9639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7001, -1.3527, -5.1020],\n",
      "        [ 3.4594, -0.8949, -5.5856],\n",
      "        [-0.4543,  4.3119, -5.0433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0326, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0151,  3.7134, -5.2974],\n",
      "        [ 0.4939,  3.1612, -5.9372],\n",
      "        [-1.0022,  4.1651, -4.6335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0055, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6927, -1.1036, -5.1101],\n",
      "        [-1.0301,  4.3722, -4.5688],\n",
      "        [-1.2311,  4.4750, -4.2775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2637, -0.8129, -5.0639],\n",
      "        [ 3.7442, -1.0322, -5.1951],\n",
      "        [ 3.6560, -1.0458, -5.3028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2568, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5888, -0.7320, -5.4015],\n",
      "        [ 3.6032, -0.7581, -5.6387],\n",
      "        [ 1.5307,  1.6297, -5.2554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1519, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3005,  3.7802, -6.0475],\n",
      "        [ 2.9413, -0.0488, -5.3346],\n",
      "        [ 1.2903,  2.0760, -5.5789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1729, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1330,  4.5206, -4.0937],\n",
      "        [ 1.3107,  1.7315, -5.2445],\n",
      "        [ 3.8229, -0.8096, -5.4221]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1593,  4.5413, -4.6177],\n",
      "        [ 3.9556, -1.2262, -5.5695],\n",
      "        [ 3.4109, -0.6213, -5.1803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2880, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9270,  4.1912, -4.4693],\n",
      "        [ 1.6690,  1.5549, -5.4172],\n",
      "        [ 1.0323,  2.4367, -6.0659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0749, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2070,  0.7690, -5.0586],\n",
      "        [-1.1371,  4.5357, -3.9225],\n",
      "        [ 3.8314, -1.0914, -5.2235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1227, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  3.3546, -5.8376],\n",
      "        [-1.4253,  4.2576, -4.4437],\n",
      "        [ 2.0516,  1.0738, -5.5825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0225, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6235, -0.1863, -4.9592],\n",
      "        [ 3.9836, -1.3523, -5.1130],\n",
      "        [-1.1478,  4.4822, -4.4415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0249, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3366,  4.4725, -3.9394],\n",
      "        [ 3.1804, -0.5906, -4.8296],\n",
      "        [ 0.2606,  3.2646, -5.6299]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3602, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7571,  1.1047, -5.0985],\n",
      "        [-1.3575,  4.3785, -4.2199],\n",
      "        [ 3.9676, -1.3773, -4.9040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3614, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9850,  2.3693, -5.7490],\n",
      "        [ 1.7473,  1.4542, -5.1326],\n",
      "        [ 3.7036, -0.9770, -5.2417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1302, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2778,  4.5598, -4.1956],\n",
      "        [ 3.7415, -1.1915, -5.0842],\n",
      "        [ 2.0806,  1.3086, -5.6297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0171, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0561,  3.6962, -5.5071],\n",
      "        [-0.3640,  3.6166, -5.1727],\n",
      "        [ 3.9476, -1.0670, -5.1776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4918, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1839,  4.3965, -3.8196],\n",
      "        [ 3.9075, -1.4876, -4.7231],\n",
      "        [ 0.9559,  2.1600, -5.6081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0145, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1877,  4.2945, -3.9067],\n",
      "        [-0.0063,  3.3422, -5.3923],\n",
      "        [-1.0845,  4.3571, -4.6296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3212, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4322, -1.0257, -4.9659],\n",
      "        [ 3.2782, -0.6483, -5.4449],\n",
      "        [ 3.7800, -1.4098, -5.0141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6713, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9495,  2.7516, -5.3224],\n",
      "        [-1.1180,  4.2493, -4.1409],\n",
      "        [ 0.2276,  3.1199, -5.5138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5811, -1.1276, -5.1071],\n",
      "        [-1.3908,  4.4595, -3.9430],\n",
      "        [-1.2438,  4.3106, -4.0543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0314, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6230,  3.8171, -3.7231],\n",
      "        [ 0.6165,  3.0355, -5.4219],\n",
      "        [-1.3213,  4.2695, -4.4344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0468, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9365,  0.1149, -5.8185],\n",
      "        [-1.1684,  4.2494, -4.2071],\n",
      "        [ 2.7083,  0.1880, -5.1482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0215, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0242,  0.0140, -5.5245],\n",
      "        [-0.9644,  4.3518, -4.5901],\n",
      "        [ 3.5235, -0.9773, -5.1982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0221, grad_fn=<NllLossBackward0>), logits=tensor([[-7.9952e-01,  4.2476e+00, -4.6853e+00],\n",
      "        [-1.2509e+00,  4.2191e+00, -4.2103e+00],\n",
      "        [ 2.8810e+00,  4.1068e-03, -4.5307e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1498, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7343,  4.3030, -5.0757],\n",
      "        [ 3.6719, -0.6354, -5.7288],\n",
      "        [ 1.6748,  1.0489, -4.9136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2246, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3395, -0.6851, -5.5601],\n",
      "        [ 1.3486,  1.2622, -4.7269],\n",
      "        [-1.3001,  4.3534, -3.8387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0336, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4393,  2.8358, -5.5567],\n",
      "        [ 3.6426, -1.0716, -5.6449],\n",
      "        [-1.0498,  4.4080, -4.2793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0095, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8826,  4.4674, -4.5465],\n",
      "        [-0.7475,  4.0024, -4.8573],\n",
      "        [ 3.4744, -0.7373, -5.3873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5585, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1284,  3.8254, -5.2914],\n",
      "        [ 2.1950,  0.7578, -4.6899],\n",
      "        [-1.0097,  4.2859, -4.2650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1395, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4559,  0.7569, -5.6967],\n",
      "        [ 1.0379,  2.4102, -5.6188],\n",
      "        [ 3.1708, -0.5504, -5.4968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1301,  4.5363, -4.1275],\n",
      "        [-1.1402,  4.4001, -4.3298],\n",
      "        [-0.9625,  4.3607, -4.2761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0076, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0650,  4.4963, -4.0880],\n",
      "        [ 3.5482, -0.8755, -5.3163],\n",
      "        [-0.9025,  4.1273, -4.6839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0404, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3432,  4.2740, -4.2226],\n",
      "        [-0.9279,  4.0443, -4.6757],\n",
      "        [ 2.6931,  0.5408, -5.1931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0632, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2608,  0.6401, -5.0865],\n",
      "        [-0.9883,  4.4444, -4.1459],\n",
      "        [-1.3872,  4.2381, -3.3800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0454, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4847,  0.3430, -5.0694],\n",
      "        [-0.4334,  3.8947, -4.7148],\n",
      "        [ 3.5099, -0.9666, -5.5764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0281, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2673,  4.4924, -3.6255],\n",
      "        [ 3.5696, -0.8586, -5.5642],\n",
      "        [ 0.4270,  3.0707, -5.6994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1867, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2517,  0.9401, -5.6704],\n",
      "        [ 2.0473,  1.0630, -5.1047],\n",
      "        [-1.3689,  4.4173, -3.9079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0426, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9755, -0.5162, -5.6248],\n",
      "        [ 0.4565,  2.9060, -5.5795],\n",
      "        [-0.5794,  3.6617, -4.7843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5096, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1075,  2.3581, -5.8673],\n",
      "        [ 3.3696, -0.4334, -5.8231],\n",
      "        [-1.0587,  4.5224, -3.7894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0053, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8706, -1.1630, -5.4305],\n",
      "        [ 4.1112, -1.0948, -5.6204],\n",
      "        [-1.4189,  4.2788, -3.8149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0773, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8947, -0.7977, -5.6842],\n",
      "        [-0.3533,  3.7773, -5.0877],\n",
      "        [ 0.8883,  2.3612, -5.6673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5637, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5774, -0.9799, -5.3874],\n",
      "        [-1.3395,  4.2834, -3.8462],\n",
      "        [ 2.1386,  0.6699, -5.0086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0587, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8262,  2.5480, -5.8451],\n",
      "        [ 3.8286, -1.1890, -5.6998],\n",
      "        [-1.3026,  4.0725, -3.9844]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2415,  4.2970, -4.0146],\n",
      "        [ 0.2653,  3.6812, -5.7194],\n",
      "        [-0.9742,  4.6722, -4.4499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0093, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5272, -0.8397, -5.6567],\n",
      "        [-0.6602,  4.0912, -4.8705],\n",
      "        [-1.0147,  4.0478, -5.0144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0107, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1409,  4.6440, -3.9377],\n",
      "        [-0.3615,  3.9820, -5.3899],\n",
      "        [ 3.4309, -0.7197, -5.9887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0092, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7172,  3.9085, -4.7927],\n",
      "        [ 3.5380, -0.9384, -4.9369],\n",
      "        [ 3.9380, -1.1581, -5.5622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0202, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5629, -0.8502, -5.7637],\n",
      "        [ 3.2798, -0.4641, -5.5259],\n",
      "        [ 3.3868, -0.3015, -5.5082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1724, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8602,  0.8758, -5.8061],\n",
      "        [ 3.8103, -1.0045, -5.6253],\n",
      "        [ 1.4117,  2.1831, -5.9872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0137, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0358,  3.7837, -5.4022],\n",
      "        [ 3.4245, -0.8407, -5.6427],\n",
      "        [ 3.9084, -1.3753, -5.0371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0138, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4650, -0.5749, -5.6392],\n",
      "        [ 3.4448, -0.5817, -5.6317],\n",
      "        [-0.8883,  4.2328, -4.7311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2028, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3088,  1.6948, -5.4054],\n",
      "        [ 2.8269, -0.0575, -5.5902],\n",
      "        [ 3.3562,  0.0069, -5.6101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6369, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8411, -1.0809, -5.1799],\n",
      "        [ 2.2191,  0.4871, -5.0541],\n",
      "        [ 3.6403, -1.2389, -5.1874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4625,  3.3287, -5.5578],\n",
      "        [ 3.7408, -0.8322, -5.7506],\n",
      "        [-1.3204,  4.4848, -4.1195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0034, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1781,  4.4444, -4.1051],\n",
      "        [-1.3522,  4.4207, -3.6306],\n",
      "        [-1.7442,  4.2487, -3.5281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7239, -0.8444, -5.1805],\n",
      "        [ 3.3961, -0.7951, -5.4900],\n",
      "        [-1.5572,  4.1544, -3.5326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1834, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2594,  1.5848, -5.2028],\n",
      "        [-1.5313,  4.4702, -3.9280],\n",
      "        [-1.4578,  4.4054, -3.3455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0253, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8500, -1.0298, -5.2338],\n",
      "        [ 3.1005, -0.1156, -6.1419],\n",
      "        [ 3.2448, -0.2986, -5.4168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0113, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6354,  3.9528, -4.9793],\n",
      "        [ 3.5901, -0.6019, -5.9648],\n",
      "        [ 3.8455, -0.9190, -5.3574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0059, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6593, -1.0178, -5.6196],\n",
      "        [ 3.8469, -1.2152, -5.2391],\n",
      "        [-1.6410,  4.8061, -3.6326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2314, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1516,  4.5986, -4.5598],\n",
      "        [ 1.4918,  1.4796, -5.1045],\n",
      "        [-1.7653,  4.2318, -3.3558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0505, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3561,  4.4883, -3.3621],\n",
      "        [ 2.6092,  0.7482, -5.3864],\n",
      "        [-1.3395,  4.4431, -4.2934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0684, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0617, -1.0694, -5.1538],\n",
      "        [ 0.9605,  2.4870, -5.9312],\n",
      "        [-1.5002,  4.6224, -4.0002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5542,  3.0624, -5.8891],\n",
      "        [-1.1777,  4.4909, -4.2762],\n",
      "        [-1.3887,  4.4060, -4.0129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6802, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9205,  2.8160, -6.2013],\n",
      "        [-1.7979,  4.3900, -3.3206],\n",
      "        [-1.7730,  4.3704, -3.2850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0240, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8807,  0.1977, -5.5644],\n",
      "        [-1.2580,  4.4266, -3.9812],\n",
      "        [-1.5894,  4.7087, -3.6626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3489, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5119,  4.5864, -4.0054],\n",
      "        [ 3.0769, -0.0709, -5.4757],\n",
      "        [ 1.9777,  1.4338, -5.6171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4189, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7225,  1.4809, -5.0554],\n",
      "        [ 1.4402,  1.7505, -5.2388],\n",
      "        [ 2.4940,  0.4829, -5.1714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6286,  4.7234, -3.5773],\n",
      "        [-1.0938,  4.3818, -4.3463],\n",
      "        [ 3.2826, -0.6689, -5.6342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0038, grad_fn=<NllLossBackward0>), logits=tensor([[-1.7384,  4.6089, -3.3101],\n",
      "        [-1.6885,  4.6028, -3.4099],\n",
      "        [ 3.7569, -1.2179, -5.3626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6124, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3345, -0.4470, -5.7360],\n",
      "        [ 1.0405,  2.2677, -5.3486],\n",
      "        [ 1.8778,  0.9359, -5.0910]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0103, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6598,  4.3776, -3.1149],\n",
      "        [-1.8544,  4.5380, -3.7239],\n",
      "        [ 3.1980, -0.4376, -5.8311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0081, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6606, -0.7880, -5.6764],\n",
      "        [-1.2963,  4.6042, -4.3174],\n",
      "        [ 3.7578, -0.8863, -5.7601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4587, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6699,  4.3536, -3.4918],\n",
      "        [ 1.1623,  2.2331, -5.7872],\n",
      "        [-0.6055,  4.2797, -4.9502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9053, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2202,  2.0663, -5.6925],\n",
      "        [-1.6543,  4.5888, -3.0879],\n",
      "        [ 1.1960,  2.4562, -5.6492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2791, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5154,  4.6464, -3.6336],\n",
      "        [-0.9741,  4.0406, -4.7588],\n",
      "        [ 1.4520,  1.7057, -5.9051]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1105, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1681,  1.1467, -5.2115],\n",
      "        [ 3.2842, -0.6581, -5.5463],\n",
      "        [-1.1210,  4.4075, -4.5648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2409, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5744,  1.5469, -5.5701],\n",
      "        [ 3.6138, -0.9971, -5.5670],\n",
      "        [ 3.8697, -1.4047, -5.2683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4812,  4.1596, -3.9268],\n",
      "        [ 3.7734, -1.2127, -5.6311],\n",
      "        [ 3.7171, -1.3592, -5.2461]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0739, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9123, -1.5940, -4.8771],\n",
      "        [ 2.1951,  0.7598, -5.5262],\n",
      "        [-1.3662,  4.3373, -3.5248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0060, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0468, -1.4702, -5.1526],\n",
      "        [-0.3612,  4.1955, -5.4646],\n",
      "        [-1.4673,  4.3430, -3.3879]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9442,  4.5188, -4.8324],\n",
      "        [ 0.3691,  3.5142, -5.4205],\n",
      "        [ 3.9783, -1.3753, -4.9560]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2729, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8769,  4.4053, -4.8370],\n",
      "        [ 4.0125, -1.3103, -4.9601],\n",
      "        [ 1.6976,  1.4793, -5.6765]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1786, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.1715, -1.3356, -5.1209],\n",
      "        [ 3.8822, -1.2736, -5.2218],\n",
      "        [ 1.6041,  1.9728, -6.0524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8117, -1.4388, -4.9236],\n",
      "        [-1.3384,  4.5866, -3.7812],\n",
      "        [ 3.4660, -0.1385, -5.7742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3730,  4.4054, -4.2266],\n",
      "        [-0.8054,  4.2760, -4.2733],\n",
      "        [ 4.2526, -1.5768, -5.0447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5613, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3929,  0.9235, -5.7941],\n",
      "        [ 4.0596, -1.7863, -5.2389],\n",
      "        [-1.4601,  4.1102, -3.2898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0442,  3.5604, -5.3163],\n",
      "        [ 0.5818,  3.4690, -5.8121],\n",
      "        [-1.2980,  4.3424, -4.0928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0209, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9866, -1.1077, -5.3688],\n",
      "        [ 4.1538, -1.7331, -5.0616],\n",
      "        [ 2.9726,  0.0690, -5.5213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9618, -1.7097, -5.0843],\n",
      "        [ 3.0424, -0.3606, -5.2513],\n",
      "        [-1.3309,  4.4123, -3.6997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3081, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8205,  1.4317, -5.5932],\n",
      "        [ 3.4574, -0.7985, -5.5822],\n",
      "        [ 4.2251, -1.5062, -4.9633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3971, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.1762, -1.8726, -5.1974],\n",
      "        [ 3.8783, -0.8095, -5.3371],\n",
      "        [ 2.1130,  1.3012, -5.4612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.5386, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.3130, -1.8374, -5.2368],\n",
      "        [-1.5242,  4.6590, -3.5727],\n",
      "        [ 3.5954, -1.0056, -5.4915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0042, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3250,  4.5024, -3.7332],\n",
      "        [ 3.9425, -1.4718, -5.3168],\n",
      "        [-0.9959,  4.3742, -4.2723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0037, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5912,  4.2720, -3.3685],\n",
      "        [-1.4133,  4.5121, -3.6744],\n",
      "        [ 3.9695, -1.4028, -5.2022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.9853, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6556,  4.2812, -3.1406],\n",
      "        [ 3.5350, -0.9307, -5.4565],\n",
      "        [ 3.6640, -1.7851, -4.7858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0145, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9828, -1.0906, -5.7370],\n",
      "        [-1.2506,  4.2547, -3.3633],\n",
      "        [ 2.9721, -0.4426, -5.4803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0617, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4392,  3.9494, -4.9242],\n",
      "        [-1.1609,  4.1111, -3.3614],\n",
      "        [ 2.3731,  0.6635, -4.9009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0119, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7115, -1.2229, -5.3033],\n",
      "        [ 3.5099, -0.4149, -5.4229],\n",
      "        [ 3.7506, -0.9941, -5.5273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1438, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9000,  1.0107, -5.0609],\n",
      "        [ 3.9019, -0.9683, -5.4307],\n",
      "        [ 2.5162,  0.0107, -5.5217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0205, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2291,  2.9976, -1.5775],\n",
      "        [ 3.3792, -1.0375, -5.3389],\n",
      "        [ 3.3189, -0.3756, -5.4323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0128, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7992, -1.1788, -5.4355],\n",
      "        [ 3.3430, -0.6308, -5.6700],\n",
      "        [ 3.6793, -0.6896, -5.6276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0232, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8191,  2.9042, -1.8034],\n",
      "        [-0.9163,  3.1145, -1.7637],\n",
      "        [ 3.6025, -0.8382, -5.5414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5988, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8763,  1.4004, -5.1978],\n",
      "        [ 1.3126,  2.2707, -6.0430],\n",
      "        [-0.9888,  2.9680, -1.5391]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0192, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5556,  4.0857, -4.3176],\n",
      "        [ 3.2084, -0.2183, -5.2451],\n",
      "        [-0.7640,  3.5074, -2.9088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0499, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6871,  3.3843, -2.7730],\n",
      "        [ 2.9825, -0.4988, -5.7180],\n",
      "        [ 0.6866,  2.9385, -6.0121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1247, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6154, -0.8128, -5.6786],\n",
      "        [ 2.9236, -0.0803, -5.6042],\n",
      "        [ 1.2559,  2.2565, -5.3400]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0338, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5430,  2.9099, -2.4089],\n",
      "        [-0.6256,  2.7923, -2.5322],\n",
      "        [-0.5762,  3.1481, -2.2038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0503, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5274,  2.8574, -1.5004],\n",
      "        [ 3.3371, -1.1362, -5.5778],\n",
      "        [ 2.6428,  0.3195, -5.5032]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7553, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6887,  2.7978, -1.9577],\n",
      "        [ 0.7464,  2.8418, -5.6284],\n",
      "        [ 3.3535, -0.7847, -5.8222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0132, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8535, -0.7161, -5.5869],\n",
      "        [ 3.6633, -0.5547, -5.7078],\n",
      "        [ 3.5015, -0.7375, -5.5849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3991, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8056,  1.1317, -5.4778],\n",
      "        [-0.5077,  3.2219, -3.4184],\n",
      "        [ 2.5270,  0.1109, -5.2234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1683, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3570,  3.1115, -3.1600],\n",
      "        [ 1.8676,  1.3150, -5.5534],\n",
      "        [ 3.5103, -0.5345, -5.6412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3244, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7720,  1.7763, -6.1732],\n",
      "        [ 2.2719,  0.9232, -5.7055],\n",
      "        [ 0.2069,  3.1600, -5.4760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0238, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1071,  3.4409, -4.6392],\n",
      "        [-0.3417,  3.2115, -3.3587],\n",
      "        [ 3.7540, -0.5752, -5.7340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0194, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2091,  3.7106, -4.2604],\n",
      "        [-0.2817,  3.5235, -3.9516],\n",
      "        [ 3.5972, -0.5634, -5.8051]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8771, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5433,  3.0728, -5.5828],\n",
      "        [ 0.1750,  3.5272, -5.0028],\n",
      "        [ 2.7681,  0.3323, -6.0402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0465, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7764,  2.8851, -5.6286],\n",
      "        [ 3.5902, -0.9051, -5.7293],\n",
      "        [ 3.3403, -0.9574, -5.8138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0763, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2591,  3.5906, -5.3669],\n",
      "        [ 3.1292, -0.5036, -5.5440],\n",
      "        [ 2.5932,  0.8891, -5.5920]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0918, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4020,  2.7015, -6.0698],\n",
      "        [-0.4639,  3.5126, -4.0623],\n",
      "        [ 3.6633, -0.5323, -5.9122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0110, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7940, -0.7402, -5.4824],\n",
      "        [ 3.6573, -0.9205, -5.6700],\n",
      "        [ 3.3123, -1.1371, -4.7847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0334, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3024,  3.6469, -3.9354],\n",
      "        [ 2.8144,  0.0408, -5.4745],\n",
      "        [-0.4557,  3.4718, -4.6182]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1425, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4338, -0.8939, -5.6692],\n",
      "        [-0.5415,  3.5089, -4.4039],\n",
      "        [ 1.3811,  2.1015, -6.4035]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0859, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3928, -0.8358, -5.3402],\n",
      "        [ 2.2082,  0.8780, -5.0804],\n",
      "        [ 3.7085, -1.1319, -5.4422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0227, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3956, -0.1829, -6.0129],\n",
      "        [ 3.2194, -0.5488, -6.0166],\n",
      "        [-0.3339,  3.7039, -4.8552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3270, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4268, -0.3056, -5.8065],\n",
      "        [ 1.3561,  2.1802, -5.6686],\n",
      "        [ 3.1909, -0.3741, -5.2799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0187, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2398,  3.7705, -5.0524],\n",
      "        [ 3.2178, -0.3523, -5.4642],\n",
      "        [ 3.7002, -0.8990, -5.6732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0918, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0472,  2.3279, -5.5810],\n",
      "        [ 3.4424, -0.8732, -5.5481],\n",
      "        [-0.3316,  3.7843, -4.4062]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3993, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6831,  3.9674, -4.3624],\n",
      "        [ 2.0529,  1.1971, -5.3981],\n",
      "        [ 1.7358,  1.4733, -5.3149]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1596, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1994,  0.7364, -5.5187],\n",
      "        [ 2.1780,  0.9367, -5.3479],\n",
      "        [ 3.4015, -0.7300, -5.7382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0143, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6591, -0.9694, -5.7589],\n",
      "        [-0.0073,  3.6660, -5.1047],\n",
      "        [-0.7599,  4.1213, -4.3850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0188, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6545,  3.6150, -4.2291],\n",
      "        [ 3.2117, -0.4227, -6.1171],\n",
      "        [ 3.4211, -0.7176, -5.5426]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2874, -0.4373, -5.6649],\n",
      "        [ 3.5064, -0.8458, -5.1886],\n",
      "        [-0.6642,  3.9229, -4.0357]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0748, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3344, -0.4750, -4.8297],\n",
      "        [ 2.6237,  0.3141, -5.5570],\n",
      "        [ 2.6800,  0.4999, -5.6770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5420,  4.0209, -4.6484],\n",
      "        [ 3.4701, -0.7742, -5.0720],\n",
      "        [-0.4944,  3.9757, -4.7854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1165, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0257,  2.8669, -6.1770],\n",
      "        [ 0.3964,  3.7478, -5.5795],\n",
      "        [ 0.9809,  2.6817, -6.1370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3867, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8408,  1.4708, -5.4717],\n",
      "        [ 3.3216, -0.8254, -5.5882],\n",
      "        [ 1.0410,  2.3070, -5.3043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0160, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7174,  3.7683, -3.8651],\n",
      "        [ 3.4404, -0.2683, -5.7962],\n",
      "        [-0.4810,  3.9726, -4.0125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0255, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6106,  3.3889, -5.7096],\n",
      "        [-0.5780,  3.9940, -4.5699],\n",
      "        [-0.9635,  4.2446, -3.9730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0172, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9665,  4.2882, -3.8490],\n",
      "        [ 2.9101, -0.3263, -5.5371],\n",
      "        [-0.8198,  4.1300, -4.3332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7246, -0.7206, -5.5117],\n",
      "        [-1.0249,  3.7366, -3.6737],\n",
      "        [-0.6842,  3.8968, -4.4737]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0270, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2751, -0.5101, -5.3112],\n",
      "        [-0.8216,  4.1994, -4.2360],\n",
      "        [ 2.8860, -0.0547, -5.7851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4628,  4.2140, -5.0022],\n",
      "        [ 3.3615, -0.8097, -5.3355],\n",
      "        [ 3.2629, -0.4697, -5.7906]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0166, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1304,  3.9908, -4.1736],\n",
      "        [ 3.0716, -0.2008, -5.4423],\n",
      "        [-0.9056,  4.1811, -4.6750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0096, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9727,  4.1594, -4.1223],\n",
      "        [ 3.5648, -0.5684, -5.4657],\n",
      "        [-0.8541,  4.1925, -4.0762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0683, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3329,  3.8101, -3.7825],\n",
      "        [-0.8988,  4.2501, -4.3588],\n",
      "        [ 0.9641,  2.5155, -5.1778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3651, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5318, -0.7333, -5.7149],\n",
      "        [ 1.1201,  2.3681, -5.5271],\n",
      "        [ 1.6532,  1.9071, -5.4314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3364, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8200,  4.3407, -4.2540],\n",
      "        [ 1.3250,  1.7056, -5.2700],\n",
      "        [ 2.9532,  0.7139, -5.6810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0484, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5663,  0.2622, -5.4454],\n",
      "        [-0.8946,  3.9797, -4.2401],\n",
      "        [ 3.0120, -0.1455, -6.0368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1403, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0116,  4.2739, -4.0043],\n",
      "        [ 1.7494,  1.0681, -4.7100],\n",
      "        [-0.8688,  4.4171, -3.9141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0188, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9938,  4.2722, -4.1210],\n",
      "        [-0.9800,  4.3061, -3.8511],\n",
      "        [ 2.9286, -0.1372, -5.2957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0063, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0260, -1.3154, -5.4629],\n",
      "        [-0.8925,  3.9894, -4.2457],\n",
      "        [-1.0078,  4.1324, -3.8666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0504, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2744, -0.5069, -5.2890],\n",
      "        [ 2.7044,  0.2196, -5.2348],\n",
      "        [ 2.8492, -0.1647, -5.6978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0193, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1997, -0.0390, -5.6566],\n",
      "        [-1.0273,  4.0904, -3.9195],\n",
      "        [ 3.6084, -0.7315, -5.5995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.2044, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4111,  3.9781, -5.9924],\n",
      "        [-0.6207,  3.9875, -5.1789],\n",
      "        [ 3.8340, -0.9857, -5.1853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4759, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5380, -0.8424, -5.3919],\n",
      "        [ 3.2278, -0.3779, -5.8497],\n",
      "        [ 3.8655, -1.0036, -5.2944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0370, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3376,  3.7854, -5.4890],\n",
      "        [ 0.3299,  3.1546, -5.6843],\n",
      "        [ 0.1333,  3.4160, -5.4107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3095, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5127,  0.8284, -5.9665],\n",
      "        [ 3.4924, -0.8223, -5.2807],\n",
      "        [ 1.7445,  1.6450, -5.0410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8237, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6598,  0.3148, -5.9437],\n",
      "        [ 3.6130, -0.2560, -5.4392],\n",
      "        [ 3.5846, -0.7164, -5.5547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1442, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.3088,  0.6189, -5.7748],\n",
      "        [ 3.0499, -0.3972, -5.7453],\n",
      "        [ 1.0733,  2.4188, -6.2084]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0521, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2576, -0.1892, -5.6048],\n",
      "        [ 0.6838,  2.9497, -5.7993],\n",
      "        [ 0.2074,  3.8461, -5.6153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0630, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5209,  3.2317, -5.6571],\n",
      "        [ 0.3692,  2.6350, -5.4913],\n",
      "        [ 3.4092, -0.2545, -5.8697]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6724,  3.8873, -5.0236],\n",
      "        [-0.4595,  3.9679, -5.2661],\n",
      "        [ 3.4639, -0.7371, -5.3579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0220, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3002, -0.5654, -5.2660],\n",
      "        [ 3.3690, -0.5547, -5.5898],\n",
      "        [ 3.4518, -0.2128, -5.8929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1970, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3974,  1.7160, -5.3070],\n",
      "        [ 3.1243, -0.1982, -5.6962],\n",
      "        [-0.7383,  4.0351, -4.9082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3325, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4080,  1.8800, -4.8626],\n",
      "        [-0.3264,  4.0070, -5.2444],\n",
      "        [ 3.0362, -0.5745, -5.1176]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0236, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7457,  4.1936, -4.6029],\n",
      "        [ 3.3262, -0.2068, -5.5375],\n",
      "        [ 0.1042,  3.4571, -5.6723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9170, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5579,  4.0255, -4.8930],\n",
      "        [ 2.7445,  0.3144, -5.6331],\n",
      "        [ 2.0416,  0.6674, -5.1960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1223, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1226,  2.4798, -5.6003],\n",
      "        [ 2.7520, -0.0140, -5.5308],\n",
      "        [ 2.9373,  0.4013, -6.1210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0162, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9097,  4.3404, -4.3031],\n",
      "        [ 3.0825, -0.2242, -4.9953],\n",
      "        [-0.8851,  4.1145, -4.6797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0641, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5240,  0.5939, -6.1152],\n",
      "        [-0.5901,  3.9000, -5.0189],\n",
      "        [ 2.9859, -0.0891, -5.6619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0320, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3172, -0.1000, -5.9573],\n",
      "        [-0.8108,  4.0569, -4.1217],\n",
      "        [ 2.9309,  0.0680, -5.6314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0786, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1383,  2.7316, -6.1462],\n",
      "        [-0.8457,  4.1008, -4.2866],\n",
      "        [ 2.9258, -0.1960, -5.1768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2321, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2481,  2.2304, -5.8750],\n",
      "        [ 0.9848,  2.7836, -5.9278],\n",
      "        [ 2.0681,  0.6879, -5.5381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0810, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0697,  0.1921, -5.9215],\n",
      "        [ 3.0276, -0.0106, -5.7105],\n",
      "        [ 2.4678,  0.5789, -5.6702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2814, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0461,  3.4128, -5.6035],\n",
      "        [ 2.5539,  0.6752, -5.8979],\n",
      "        [ 1.7368,  1.6909, -5.9932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0988, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0811,  2.4427, -5.6519],\n",
      "        [ 2.7163, -0.1191, -5.6824],\n",
      "        [-0.5107,  4.0289, -5.5382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2056, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0120,  1.0308, -5.3266],\n",
      "        [ 3.0051, -0.1006, -5.4972],\n",
      "        [ 1.0749,  2.3169, -5.8056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1767, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3758,  2.1837, -5.5204],\n",
      "        [ 3.1306, -0.0688, -5.6728],\n",
      "        [ 0.7227,  2.7740, -5.7129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0663, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0812,  4.3784, -3.9889],\n",
      "        [ 0.4745,  2.9338, -5.7419],\n",
      "        [ 2.6911,  0.5589, -6.0784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0256, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0698,  3.5231, -5.4928],\n",
      "        [ 2.9626, -0.2485, -5.2705],\n",
      "        [-1.0860,  4.1308, -4.1687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0793, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9217,  4.4129, -4.4713],\n",
      "        [ 2.2078,  0.8305, -5.7422],\n",
      "        [-0.7263,  4.1313, -5.1590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0516, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7628,  0.1002, -5.7300],\n",
      "        [ 0.7477,  3.1903, -6.0913],\n",
      "        [-1.1503,  4.4983, -3.7874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0278, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0648,  4.2348, -4.4177],\n",
      "        [ 0.0795,  3.2732, -5.4560],\n",
      "        [ 3.1220, -0.1395, -5.7653]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1158, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0397,  0.0280, -5.6380],\n",
      "        [ 0.9288,  2.9872, -6.1196],\n",
      "        [ 0.9481,  2.5803, -5.4146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0265, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1334,  4.2840, -3.8279],\n",
      "        [ 0.2973,  3.1694, -5.7268],\n",
      "        [ 3.4542, -0.4827, -5.1856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0436, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2825, -0.4297, -5.7212],\n",
      "        [ 2.7484,  0.1938, -6.2192],\n",
      "        [ 3.1288, -0.3171, -5.8874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4787, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2476,  4.3427, -3.4899],\n",
      "        [ 0.8513,  2.0048, -5.4158],\n",
      "        [-1.1625,  4.4332, -4.1319]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4096, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1997,  2.0716, -5.8443],\n",
      "        [-1.2326,  4.4992, -4.0929],\n",
      "        [-1.3402,  4.3335, -3.5379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1946, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2611,  4.1988, -3.5002],\n",
      "        [-1.4485,  4.5434, -3.6149],\n",
      "        [ 1.6548,  1.4047, -5.5889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1473, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0700,  1.1799, -5.3934],\n",
      "        [ 2.7651,  0.4200, -5.6003],\n",
      "        [-1.0397,  4.2261, -3.3177]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4185, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5521, -0.6715, -5.8794],\n",
      "        [ 3.5946, -0.7320, -5.9738],\n",
      "        [-1.2335,  4.3700, -3.9553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0242, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1115, -0.3052, -5.8734],\n",
      "        [ 2.9776, -0.3649, -5.5132],\n",
      "        [-0.8841,  4.4084, -4.5791]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0230, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6599, -0.3166, -5.7662],\n",
      "        [ 3.2830, -0.4802, -5.9076],\n",
      "        [-0.1112,  3.4821, -5.3337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0210, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1548, -0.1024, -5.4467],\n",
      "        [-1.0690,  4.2341, -4.4517],\n",
      "        [ 3.4489, -0.4593, -5.9536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9639, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8346,  0.0285, -5.8133],\n",
      "        [-1.1774,  4.2993, -4.2466],\n",
      "        [ 3.3505, -0.4485, -5.9225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6968, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2342, -0.1042, -6.0001],\n",
      "        [ 2.5610,  0.6682, -5.8642],\n",
      "        [ 3.3067, -0.5032, -5.3898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0126, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3891,  4.4235, -3.6122],\n",
      "        [ 3.3666, -0.1445, -5.7415],\n",
      "        [-1.2886,  4.1755, -3.1385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6322, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7975,  2.3907, -4.9234],\n",
      "        [ 3.2024, -0.2008, -5.8157],\n",
      "        [ 2.5197,  1.0484, -5.8361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.5589, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2310, -0.1420, -5.9950],\n",
      "        [ 3.1423,  0.2201, -6.1559],\n",
      "        [ 2.0977,  0.7356, -5.9060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9091, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9807,  0.4050, -6.0729],\n",
      "        [ 2.7894,  0.2391, -5.9661],\n",
      "        [-1.3273,  4.6198, -3.9771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0695, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6164,  0.4994, -6.0492],\n",
      "        [ 3.0976,  0.2832, -5.6685],\n",
      "        [ 3.0795, -0.2177, -5.9429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0299, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4628,  2.9529, -5.7347],\n",
      "        [-0.8693,  4.2906, -4.4182],\n",
      "        [-1.1375,  4.4722, -3.8191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1450, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6508,  0.2248, -5.6985],\n",
      "        [ 2.5350,  0.5203, -5.8901],\n",
      "        [ 2.2449,  0.8654, -5.8985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6256, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3766,  4.3188, -3.4491],\n",
      "        [ 2.4896,  0.8489, -6.2428],\n",
      "        [ 3.0991,  0.2249, -5.9232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0965, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8960,  0.6802, -6.3580],\n",
      "        [ 2.4184,  0.3491, -6.0306],\n",
      "        [ 0.3201,  2.9976, -5.5439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0869, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3690,  4.3487, -3.3102],\n",
      "        [ 2.3651,  1.1162, -6.0587],\n",
      "        [-1.3609,  4.2152, -2.9047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1455, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2235,  4.2484, -3.8251],\n",
      "        [ 2.6656,  0.7169, -6.2726],\n",
      "        [ 1.9960,  0.9396, -5.9610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1765, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8296,  0.4078, -6.2270],\n",
      "        [ 2.8359,  0.2928, -6.3404],\n",
      "        [ 2.0018,  1.1927, -5.8702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0869, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6141,  0.3780, -6.1607],\n",
      "        [ 0.8596,  2.6672, -5.4747],\n",
      "        [-0.9449,  4.0502, -4.8780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0501, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4813,  0.5981, -5.9927],\n",
      "        [-1.0643,  4.4194, -3.8776],\n",
      "        [-1.2742,  4.3320, -3.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.6757, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2131,  2.3923, -5.4017],\n",
      "        [ 2.5430,  0.8857, -6.1725],\n",
      "        [ 2.1578,  1.4628, -6.3803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3657, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3898,  4.1597, -3.3782],\n",
      "        [ 1.8985,  1.5148, -5.3950],\n",
      "        [ 1.7362,  1.4777, -5.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4167, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1286,  1.9305, -5.1576],\n",
      "        [-1.4442,  4.2205, -3.4076],\n",
      "        [ 2.8517,  0.2710, -6.4820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0064, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2513,  4.3305, -3.5290],\n",
      "        [-1.2508,  4.3878, -3.3724],\n",
      "        [-0.4150,  4.1038, -4.9525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0605, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2332,  3.7272, -5.3366],\n",
      "        [ 2.6463,  0.8838, -6.0915],\n",
      "        [-1.2813,  4.2838, -3.9309]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0041, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0086,  4.3963, -4.0400],\n",
      "        [-1.1916,  4.3905, -3.8834],\n",
      "        [-1.1024,  4.6029, -4.0435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2413, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1892,  0.9692, -5.8914],\n",
      "        [ 2.1762,  1.1264, -6.2190],\n",
      "        [ 2.3767,  0.6538, -5.5713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0712, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7488,  0.1959, -6.3332],\n",
      "        [ 3.2727,  0.1369, -5.9631],\n",
      "        [ 2.7772,  0.4807, -6.0804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0491, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1915,  4.4816, -4.4196],\n",
      "        [ 2.5534,  0.6507, -5.8625],\n",
      "        [-1.2744,  4.2416, -3.6424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1219, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2279,  0.7718, -5.7781],\n",
      "        [-1.0870,  4.2889, -3.5726],\n",
      "        [ 2.4953,  0.6784, -5.6306]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0527, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6318,  0.3278, -5.6568],\n",
      "        [-0.2554,  3.8865, -5.5289],\n",
      "        [ 3.1524,  0.1090, -5.6476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0772, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8025,  0.4040, -6.0219],\n",
      "        [ 2.7804,  0.3836, -5.8731],\n",
      "        [ 3.1219,  0.2857, -5.8687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0692, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8928,  2.7535, -6.2770],\n",
      "        [ 3.1309,  0.3077, -5.9191],\n",
      "        [-1.0975,  4.2304, -3.7862]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2294, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4472,  1.5508, -4.9199],\n",
      "        [ 3.2470,  0.0112, -6.0988],\n",
      "        [-0.9034,  4.2397, -4.6343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7852, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1138, -0.3738, -5.7556],\n",
      "        [ 2.5437,  0.3645, -5.8068],\n",
      "        [ 3.0613, -0.1697, -6.1866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0394, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1514,  0.0710, -6.0834],\n",
      "        [ 3.1082, -0.1748, -5.6538],\n",
      "        [ 3.1557, -0.1468, -6.0452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0215, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7765, -0.1340, -5.5867],\n",
      "        [-1.0178,  4.2003, -4.1923],\n",
      "        [-0.9430,  4.2877, -4.2185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0804, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1574, -0.2804, -5.5557],\n",
      "        [ 1.0691,  2.5479, -5.6900],\n",
      "        [-1.1907,  4.4198, -4.1651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1191, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0028,  2.2919, -5.2755],\n",
      "        [ 3.2209, -0.1808, -5.6811],\n",
      "        [ 2.8654,  0.3863, -5.7244]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0704, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6088,  0.7948, -5.5692],\n",
      "        [ 2.8852,  0.0417, -6.2150],\n",
      "        [-1.3359,  4.4979, -3.5888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0177, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0693, -0.3947, -5.0822],\n",
      "        [ 3.3505, -0.7446, -5.4425],\n",
      "        [-1.1617,  4.1130, -3.9712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0122, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2199,  4.4267, -4.1829],\n",
      "        [ 3.2555, -0.2559, -5.6833],\n",
      "        [-1.1226,  4.5841, -4.2342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.8438, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2307, -0.4715, -5.4908],\n",
      "        [-1.2039,  4.3556, -3.6593],\n",
      "        [ 0.4681,  2.8852, -5.6626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0768, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4966,  4.0368, -5.1995],\n",
      "        [ 3.2336, -0.1399, -6.2082],\n",
      "        [ 2.3080,  0.7166, -5.2990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0114, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2789,  4.2623, -3.5632],\n",
      "        [-1.2249,  4.4956, -3.7564],\n",
      "        [ 2.9497, -0.6819, -5.5585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0215, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4103, -0.3626, -5.8927],\n",
      "        [ 3.1855, -0.3344, -5.5336],\n",
      "        [ 3.5145, -0.8847, -5.6502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0284, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3102, -0.5251, -5.2645],\n",
      "        [ 2.9737, -0.1782, -5.5012],\n",
      "        [ 3.2600, -0.5731, -5.5308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2219, -0.9252, -4.9387],\n",
      "        [-0.2932,  3.9748, -5.5001],\n",
      "        [ 3.7922, -0.8288, -5.1978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0105, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0775,  4.2697, -3.6746],\n",
      "        [-0.7846,  3.9297, -4.4750],\n",
      "        [ 3.5666, -0.4998, -5.3311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4844, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4546e+00, -3.9019e-01, -5.8299e+00],\n",
      "        [ 2.0218e+00,  9.3338e-01, -5.1989e+00],\n",
      "        [ 2.9178e+00, -4.7955e-03, -5.5087e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0389, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0358, -0.4971, -5.5203],\n",
      "        [ 0.5322,  3.5220, -5.8366],\n",
      "        [ 2.9093, -0.3306, -5.5760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.8234, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3022, -0.5966, -5.5868],\n",
      "        [ 2.3111,  0.1522, -5.2257],\n",
      "        [ 2.9748, -0.1641, -5.4817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1889, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2240, -0.5302, -5.4104],\n",
      "        [ 1.4210,  1.8093, -5.6275],\n",
      "        [ 3.2582, -0.4158, -5.3039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0569, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1684, -0.0117, -5.9567],\n",
      "        [ 0.7118,  2.7568, -5.3394],\n",
      "        [-0.7731,  4.0957, -4.7350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2487, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6845,  1.6394, -5.6294],\n",
      "        [ 3.3656, -0.3791, -5.9661],\n",
      "        [-0.9066,  4.2088, -3.7080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1219, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3211,  4.4883, -3.5799],\n",
      "        [ 2.0193,  1.1024, -4.9128],\n",
      "        [ 3.1755, -0.4879, -5.4055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0494, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0425,  0.0884, -5.3175],\n",
      "        [ 3.3710, -0.6310, -5.2793],\n",
      "        [ 2.7129,  0.2104, -5.9558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0098, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9346, -0.9525, -4.5578],\n",
      "        [-1.4060,  4.0993, -3.5438],\n",
      "        [-1.0469,  4.4893, -4.0015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1354, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7493,  0.9946, -5.0177],\n",
      "        [ 3.0435, -1.1607, -4.6452],\n",
      "        [-1.0203,  4.4006, -3.8676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0165, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2522, -0.3084, -5.7715],\n",
      "        [ 3.2915, -0.7844, -5.2554],\n",
      "        [-1.4569,  4.1169, -3.5119]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0270, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3592, -0.4567, -4.9653],\n",
      "        [ 2.9364, -0.7591, -4.4187],\n",
      "        [ 3.2081, -0.1728, -5.0325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0256, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1030,  4.2026, -4.2577],\n",
      "        [ 2.7229,  0.0561, -5.2047],\n",
      "        [-1.3358,  4.2552, -3.5040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0116, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2829,  3.9541, -5.1282],\n",
      "        [ 3.2636, -0.7764, -5.2789],\n",
      "        [-1.4961,  4.4780, -3.7349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3491, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9861,  0.5157, -4.3717],\n",
      "        [ 0.9858,  2.2459, -5.7181],\n",
      "        [ 1.3259,  1.5483, -4.8553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0720, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0643, -0.3182, -4.9569],\n",
      "        [-1.4663,  4.3464, -3.7535],\n",
      "        [ 2.1481,  0.5125, -4.6314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0402, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4576,  4.4806, -3.8723],\n",
      "        [ 2.4854,  0.2215, -4.8918],\n",
      "        [ 3.3221, -0.6854, -4.8116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0856,  3.3104, -5.1342],\n",
      "        [-0.5873,  4.2660, -4.7926],\n",
      "        [-1.2147,  4.6026, -3.9934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0349, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0730, -0.5321, -5.2246],\n",
      "        [ 2.7528,  0.1842, -4.8690],\n",
      "        [-1.2626,  4.4906, -3.5846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0031, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5462,  4.4629, -3.4455],\n",
      "        [-1.0413,  4.6395, -3.9893],\n",
      "        [-1.5220,  4.4325, -3.7608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0424, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8601,  0.1291, -5.4122],\n",
      "        [ 2.7783,  0.0077, -5.1034],\n",
      "        [-1.5875,  4.4377, -3.6592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1277, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0359,  1.1797, -5.2506],\n",
      "        [ 3.3109, -0.9746, -4.7134],\n",
      "        [ 3.2515, -0.9812, -4.7994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0087, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3432,  4.2710, -3.7964],\n",
      "        [-1.4557,  4.4204, -3.4770],\n",
      "        [ 3.2256, -0.7521, -4.9026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0124, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8602, -0.9730, -5.1302],\n",
      "        [-1.5979,  4.2193, -3.7401],\n",
      "        [ 3.3130, -0.3433, -5.2681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.0207, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8679, -0.0968, -5.2413],\n",
      "        [-1.2496,  4.1833, -3.2220],\n",
      "        [ 0.2139,  3.1696, -5.5030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3020, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5088,  1.4945, -5.1548],\n",
      "        [ 2.0447,  0.5201, -5.0413],\n",
      "        [-0.6562,  4.2831, -5.0105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0030, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5121,  4.6645, -3.7318],\n",
      "        [-1.2872,  4.4837, -3.7470],\n",
      "        [-1.3125,  4.4593, -3.5025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.4973, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5630, -0.6823, -4.7058],\n",
      "        [ 2.6277,  0.1879, -5.1791],\n",
      "        [ 0.9929,  2.0999, -4.5623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0484, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5052,  4.4504, -3.9840],\n",
      "        [ 0.6484,  2.9423, -5.0530],\n",
      "        [ 2.8824, -0.1838, -5.1014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0406, grad_fn=<NllLossBackward0>), logits=tensor([[-1.6360,  4.4670, -3.6566],\n",
      "        [ 3.4519, -1.2554, -4.3688],\n",
      "        [ 2.4521,  0.2922, -4.6427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0419, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1431,  4.5515, -4.0384],\n",
      "        [ 2.2414,  0.1630, -4.8255],\n",
      "        [-1.3131,  4.4600, -3.5022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0201, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5123,  4.4103, -3.3596],\n",
      "        [ 3.6427, -1.0396, -4.6299],\n",
      "        [ 2.8231, -0.2051, -4.7212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.9232, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1615,  4.5762, -3.9505],\n",
      "        [ 2.4129,  0.2371, -4.9856],\n",
      "        [ 1.6514,  1.1697, -4.6083]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3467, -0.7526, -5.0490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 2/3, Loss_per_epoch: 248.66700674942695\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0176, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7384, -0.7391, -4.2540],\n",
      "        [ 3.6298, -0.9959, -4.5313],\n",
      "        [ 3.4021, -1.0942, -4.8337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0841, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9054,  2.1952, -4.7509],\n",
      "        [-1.3698,  4.3072, -3.5708],\n",
      "        [-1.2490,  4.2195, -3.3756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0432, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4946, -0.0930, -4.6170],\n",
      "        [-1.2117,  4.3189, -3.7697],\n",
      "        [ 2.8663, -0.0684, -5.1562]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0178, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2619,  4.4195, -3.5208],\n",
      "        [ 3.1706, -0.7283, -4.6534],\n",
      "        [ 2.9882, -0.5409, -5.1869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0152, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9540,  4.1490, -4.4662],\n",
      "        [ 2.8871, -0.4291, -4.9432],\n",
      "        [-1.2064,  4.6126, -3.7164]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2874,  4.4724, -3.8034],\n",
      "        [-1.3016,  4.2273, -3.8334],\n",
      "        [ 3.0791, -0.4822, -5.3034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4231,  4.4678, -3.2627],\n",
      "        [-0.1949,  3.8511, -5.5105],\n",
      "        [-1.3572,  4.1120, -3.3434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0096, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5049, -1.1935, -4.4716],\n",
      "        [ 3.6831, -1.1040, -4.7974],\n",
      "        [ 3.4816, -1.0701, -4.7018]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.3753, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4371,  0.7445, -4.2042],\n",
      "        [ 3.4835, -1.1976, -4.7492],\n",
      "        [ 3.2351, -0.9348, -4.6412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0982, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2899,  4.6854, -3.7480],\n",
      "        [-1.2081,  4.1846, -3.8226],\n",
      "        [ 1.0717,  2.1768, -4.5112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4052,  4.2768, -3.1491],\n",
      "        [-1.2281,  4.5109, -3.9242],\n",
      "        [ 0.0252,  3.6909, -5.2721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0351, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5230, -1.2209, -4.8069],\n",
      "        [-1.4304,  4.4511, -4.1335],\n",
      "        [ 2.4096,  0.0719, -4.2969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0053, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3215,  4.5064, -3.6352],\n",
      "        [-1.1354,  4.3994, -3.8508],\n",
      "        [ 3.6873, -1.1018, -4.9320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0051, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3030,  4.4614, -3.9641],\n",
      "        [ 3.6802, -1.0177, -5.2217],\n",
      "        [-1.3543,  4.5936, -3.8537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0353, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9001, -0.0084, -5.1861],\n",
      "        [ 2.6946, -0.2874, -4.7979],\n",
      "        [-1.5623,  4.5298, -3.4049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2809, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4207,  1.2236, -4.2493],\n",
      "        [-1.4909,  4.5405, -3.2504],\n",
      "        [ 2.9980, -0.1783, -4.5537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0099, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4659, -0.9100, -4.6790],\n",
      "        [ 3.4728, -1.1389, -4.1438],\n",
      "        [ 3.8755, -1.1922, -4.8889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0030, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5183,  4.3077, -3.6828],\n",
      "        [-1.3951,  4.7440, -3.8717],\n",
      "        [-1.1841,  4.5943, -3.7343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0158, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3866,  4.4843, -3.7608],\n",
      "        [ 2.8641, -0.3116, -4.7379],\n",
      "        [-1.6164,  4.3460, -3.5244]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0202, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5747, -0.3188, -4.6373],\n",
      "        [-1.3422,  4.5771, -3.7944],\n",
      "        [-1.4651,  4.4544, -3.4925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0842, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7481, -1.1409, -4.6898],\n",
      "        [ 3.6783, -1.3906, -4.3877],\n",
      "        [ 1.7274,  0.4029, -4.0091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0133, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5808,  4.5451, -3.3432],\n",
      "        [ 3.1725, -0.1972, -5.4228],\n",
      "        [-1.5565,  4.3768, -2.9665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0152, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8628, -1.1190, -4.6464],\n",
      "        [ 3.4970, -1.3065, -4.2960],\n",
      "        [ 3.0903, -0.4112, -4.9275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0125, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4812,  4.3981, -3.5974],\n",
      "        [ 2.9975, -0.5623, -4.8370],\n",
      "        [ 3.6316, -1.5359, -4.3683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0024, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3053,  4.7198, -4.2517],\n",
      "        [-1.5274,  4.6363, -3.9079],\n",
      "        [-1.5406,  4.5550, -3.8542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0134, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3857, -1.0348, -4.9947],\n",
      "        [-1.3832,  4.4833, -3.7429],\n",
      "        [ 3.2048, -0.4845, -4.8056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(2.0828, grad_fn=<NllLossBackward0>), logits=tensor([[-1.2826,  4.6238, -4.0703],\n",
      "        [ 3.5937, -0.8283, -5.3467],\n",
      "        [-1.4098,  4.8215, -4.4007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1502, grad_fn=<NllLossBackward0>), logits=tensor([[-1.3780,  4.4732, -4.3030],\n",
      "        [-1.5353,  4.5589, -3.8564],\n",
      "        [ 1.5662,  0.9810, -4.0744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0645, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4975,  2.6444, -5.0513],\n",
      "        [-0.5091,  3.8536, -5.3123],\n",
      "        [ 2.4073, -0.2321, -4.4954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0841, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4459,  2.6126, -5.1510],\n",
      "        [ 2.2888,  0.3047, -4.6733],\n",
      "        [ 3.3478, -0.9434, -4.8956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0054, grad_fn=<NllLossBackward0>), logits=tensor([[-1.1728,  4.3961, -4.6024],\n",
      "        [-1.0422,  4.0727, -5.0049],\n",
      "        [-0.9883,  4.1459, -4.6158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1632, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3273,  3.5087, -5.7789],\n",
      "        [ 2.0626,  0.4666, -4.6873],\n",
      "        [ 0.9447,  2.0671, -4.9127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0366, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7831,  4.4456, -5.0236],\n",
      "        [ 2.3852,  0.1246, -5.3151],\n",
      "        [-1.0281,  4.3360, -4.3089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0065, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6414,  4.1161, -5.3767],\n",
      "        [-0.6813,  4.4233, -5.2880],\n",
      "        [-1.0025,  4.3698, -4.7066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6602,  4.2741, -5.2750],\n",
      "        [ 3.8789, -1.7941, -4.5511],\n",
      "        [ 3.7339, -1.3068, -5.0341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5115,  4.6126, -5.0501],\n",
      "        [-0.8787,  4.2910, -5.1106],\n",
      "        [-0.4118,  4.1274, -5.1392]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2535, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3348,  4.0277, -5.1412],\n",
      "        [ 1.5281,  1.6212, -5.1958],\n",
      "        [-0.7201,  4.3280, -5.3777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.2132, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6743,  2.1101, -4.3801],\n",
      "        [ 3.9056, -1.1443, -4.8833],\n",
      "        [ 0.9818,  1.6440, -3.9752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6081,  4.0828, -5.1531],\n",
      "        [ 3.7596, -1.3129, -4.5426],\n",
      "        [ 3.8273, -1.4161, -4.6510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0080, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4051,  4.3121, -5.2614],\n",
      "        [-0.7323,  4.1585, -4.8857],\n",
      "        [ 3.8238, -1.0888, -4.5866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0141, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8788, -1.5375, -4.9545],\n",
      "        [ 3.0536, -0.4576, -4.7224],\n",
      "        [-0.4801,  4.3485, -5.2314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0090, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4516,  4.1525, -5.6540],\n",
      "        [-0.2658,  4.2833, -5.4754],\n",
      "        [-0.7053,  4.3454, -5.2419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0085, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6884,  4.0988, -4.9456],\n",
      "        [-0.4598,  4.2044, -5.2826],\n",
      "        [-0.5934,  4.3067, -5.2622]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6580, -1.0419, -4.9963],\n",
      "        [-0.5969,  4.0043, -5.1298],\n",
      "        [-0.7622,  3.7780, -4.7252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0070, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8300, -1.5071, -4.7473],\n",
      "        [-0.5879,  4.2602, -5.2344],\n",
      "        [-0.5674,  4.2552, -5.1589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0104, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4794,  4.1328, -4.9486],\n",
      "        [-0.3843,  4.0604, -5.5427],\n",
      "        [-0.5307,  4.1459, -5.1336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0086, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3566,  4.2418, -5.2015],\n",
      "        [-0.4846,  4.1784, -5.1791],\n",
      "        [-0.7927,  4.2841, -5.3444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0646, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6494,  4.0592, -5.2771],\n",
      "        [ 0.5064,  2.1638, -4.5247],\n",
      "        [-0.3348,  4.3545, -5.5478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0228, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1411,  3.7808, -5.5757],\n",
      "        [ 3.3672, -0.6710, -5.1461],\n",
      "        [ 0.2487,  3.7103, -6.1001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0064, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6264,  4.1219, -4.7502],\n",
      "        [ 3.9066, -1.5242, -4.7425],\n",
      "        [ 3.6367, -1.5433, -4.7311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.4445, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8018,  4.2099, -5.3589],\n",
      "        [-0.5304,  4.3436, -5.0445],\n",
      "        [ 3.2825, -1.0229, -4.9565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0058, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8097, -1.4554, -4.3525],\n",
      "        [ 3.8639, -1.3689, -4.6088],\n",
      "        [-0.6574,  4.3856, -4.8396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0130, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7760, -1.6297, -5.0782],\n",
      "        [ 3.5697, -1.4328, -4.6873],\n",
      "        [ 3.0346, -0.5551, -5.2524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0070, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5356,  4.0541, -5.0688],\n",
      "        [-0.8237,  4.3106, -4.8921],\n",
      "        [ 4.1190, -1.2336, -4.9763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0069, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8568, -1.6056, -4.9378],\n",
      "        [-0.5011,  4.3437, -5.3360],\n",
      "        [-0.4113,  4.3626, -5.0784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0109, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9304,  4.3399, -4.6795],\n",
      "        [-0.6743,  4.2964, -5.1760],\n",
      "        [ 3.3636, -0.5181, -5.4247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8384,  4.4500, -5.0657],\n",
      "        [-0.9282,  4.2030, -4.8274],\n",
      "        [ 3.8721, -1.2605, -4.9439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0397, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8530,  4.2837, -5.2836],\n",
      "        [ 2.3405,  0.1297, -4.4652],\n",
      "        [-0.4463,  4.3688, -5.1817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7864,  4.3556, -5.2857],\n",
      "        [-0.7812,  4.4445, -5.3965],\n",
      "        [-0.6267,  4.5164, -5.0905]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3416, grad_fn=<NllLossBackward0>), logits=tensor([[-0.8052,  4.0494, -5.2214],\n",
      "        [-0.1090,  3.8618, -5.8239],\n",
      "        [ 3.1326, -0.4593, -5.1080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0385, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1794,  2.8062, -4.8772],\n",
      "        [ 0.3869,  3.7192, -6.0036],\n",
      "        [-0.5018,  4.0952, -5.1412]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0100, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5371,  4.2370, -5.4581],\n",
      "        [-0.4944,  3.9296, -5.2076],\n",
      "        [-0.5682,  4.0822, -5.2388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0595, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7807, -1.1259, -5.3718],\n",
      "        [ 2.1236,  0.3823, -4.4426],\n",
      "        [-0.3798,  4.4160, -5.6623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0348, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0864, -0.6725, -5.5588],\n",
      "        [-0.2623,  4.2827, -5.8015],\n",
      "        [ 2.7288,  0.1068, -4.8271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0123, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5007,  4.1461, -5.2777],\n",
      "        [ 3.6257, -1.1099, -5.5103],\n",
      "        [ 3.4398, -0.5491, -5.8579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0384, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6017,  2.9623, -5.6627],\n",
      "        [ 3.4947, -0.9987, -5.1896],\n",
      "        [ 3.5669, -0.7443, -5.1899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0348, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4537,  3.4727, -6.3929],\n",
      "        [ 3.0070, -0.2659, -5.3981],\n",
      "        [ 3.3806, -0.5591, -5.8100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0542, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2707,  0.3847, -4.8833],\n",
      "        [ 3.5135, -1.0547, -5.2292],\n",
      "        [-0.3973,  4.1933, -5.6473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0808, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0038, -0.1352, -5.5254],\n",
      "        [ 1.0550,  2.7087, -5.5611],\n",
      "        [ 0.1803,  3.8762, -6.0236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0450, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6560,  3.1324, -5.9489],\n",
      "        [ 0.3100,  3.6971, -6.1334],\n",
      "        [-0.0120,  3.8448, -5.7086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0120, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6312, -1.0889, -5.5274],\n",
      "        [ 3.5463, -1.0524, -5.2308],\n",
      "        [ 3.4402, -0.6410, -5.6548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0168, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2281,  3.9783, -5.7852],\n",
      "        [-0.1664,  3.9325, -5.7991],\n",
      "        [ 3.6649, -0.8927, -5.8000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0228, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.6082, -1.0218, -5.2468],\n",
      "        [-0.0089,  3.7719, -5.9447],\n",
      "        [ 0.2874,  3.5959, -6.0261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0806, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8343,  3.4349, -6.3862],\n",
      "        [ 1.0317,  3.2083, -5.9823],\n",
      "        [ 0.5383,  3.2785, -5.9375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0570, grad_fn=<NllLossBackward0>), logits=tensor([[-2.8254e-03,  4.0044e+00, -5.7875e+00],\n",
      "        [ 3.3981e+00, -4.7713e-01, -5.4327e+00],\n",
      "        [ 5.6441e-01,  2.5260e+00, -4.6040e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0276, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1895, -0.1338, -5.3493],\n",
      "        [ 3.3841,  0.0548, -6.0133],\n",
      "        [-0.3345,  4.0986, -5.8001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0518, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6823,  2.6377, -5.4189],\n",
      "        [-0.1769,  4.2069, -5.7209],\n",
      "        [ 3.5594, -1.0212, -5.5440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1421, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0246,  2.3135, -4.9192],\n",
      "        [ 3.8849, -1.1128, -5.2709],\n",
      "        [ 2.2891,  0.6338, -4.9615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0173, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3357,  4.3167, -5.6392],\n",
      "        [ 0.1558,  3.5930, -6.2954],\n",
      "        [-0.6987,  3.8606, -5.3180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0789, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1890,  3.8029, -5.6399],\n",
      "        [ 0.6442,  2.5539, -4.7794],\n",
      "        [ 0.4850,  3.0942, -5.4007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0153, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4919, -0.4683, -5.9157],\n",
      "        [ 3.8307, -1.0486, -5.6410],\n",
      "        [ 3.5182, -0.4327, -5.5049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0197, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4825, -0.8732, -5.3364],\n",
      "        [ 0.2965,  3.5871, -5.9518],\n",
      "        [ 3.6595, -0.9988, -5.4333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0247, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7222, -0.2427, -5.5495],\n",
      "        [-0.3315,  4.3007, -5.7027],\n",
      "        [ 3.5964, -0.6841, -5.6502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0411, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3552,  2.9188, -5.1118],\n",
      "        [ 3.1697, -0.7028, -5.3956],\n",
      "        [ 3.0698, -0.4977, -5.6712]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0274, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1570,  4.0545, -5.8704],\n",
      "        [ 3.6456, -1.0381, -5.4954],\n",
      "        [ 0.2610,  3.0811, -5.1992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0099, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1386,  4.1349, -5.8356],\n",
      "        [ 3.6612, -0.9789, -5.8187],\n",
      "        [ 3.8528, -1.2747, -5.1146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0088, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5358,  4.3192, -5.7502],\n",
      "        [-0.4426,  4.1220, -5.2750],\n",
      "        [ 3.8262, -0.9940, -5.4278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0135, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0143,  3.9104, -5.6087],\n",
      "        [-0.2931,  4.2117, -5.7063],\n",
      "        [-0.5259,  4.0845, -5.3949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0083, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.5155, -0.9049, -5.2525],\n",
      "        [-0.8245,  4.2810, -5.4066],\n",
      "        [-0.6405,  4.3743, -5.6863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0075, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7237,  4.2347, -5.0279],\n",
      "        [-0.4059,  4.1604, -5.9382],\n",
      "        [ 3.8189, -1.5256, -4.9473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0204, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0673,  3.6810, -5.9660],\n",
      "        [-0.4091,  4.2000, -5.2732],\n",
      "        [ 0.1081,  3.6808, -5.6508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1486, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6820,  1.7349, -3.7857],\n",
      "        [ 2.5076,  0.1012, -4.6432],\n",
      "        [ 0.2211,  3.0717, -5.3335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0078, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.2015, -1.0509, -5.1188],\n",
      "        [-0.5389,  3.9991, -5.1852],\n",
      "        [ 3.8400, -1.0703, -4.9924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0056, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.9554, -0.9136, -5.1200],\n",
      "        [ 3.8955, -1.5198, -5.0201],\n",
      "        [ 4.0486, -1.3916, -4.9885]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0083, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9204,  4.3574, -4.4605],\n",
      "        [ 3.4499, -0.7221, -5.4379],\n",
      "        [-0.9610,  4.5178, -5.2002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0094, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7123, -1.5168, -4.4846],\n",
      "        [ 3.6661, -0.6491, -5.5246],\n",
      "        [ 3.5709, -1.1379, -5.4602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0057, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7748, -1.6688, -4.6418],\n",
      "        [ 3.9787, -1.3214, -4.8105],\n",
      "        [ 3.6644, -1.2763, -4.9705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7641, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4185,  0.1270, -4.2940],\n",
      "        [ 2.3883,  0.3428, -4.6431],\n",
      "        [-0.0884,  3.5165, -4.9011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0091, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4396, -0.8610, -5.2832],\n",
      "        [ 4.1643, -1.5129, -5.2001],\n",
      "        [ 3.6208, -0.9737, -5.3003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0156, grad_fn=<NllLossBackward0>), logits=tensor([[-0.6893,  4.5597, -5.0509],\n",
      "        [-0.1168,  3.4136, -5.2291],\n",
      "        [ 3.4441, -0.9536, -5.1355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0045, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9213,  4.5805, -5.1934],\n",
      "        [-0.9874,  4.6444, -4.8584],\n",
      "        [ 3.9499, -1.2430, -5.1867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0272, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.8294,  0.1300, -5.3127],\n",
      "        [ 3.8585, -1.5601, -5.0492],\n",
      "        [-0.3780,  4.0761, -5.5078]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0071, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3375, -0.8370, -5.8001],\n",
      "        [-1.3923,  4.6043, -4.8625],\n",
      "        [-1.2614,  4.4538, -4.6688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0107, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.4203, -1.1242, -5.2053],\n",
      "        [-1.2066,  4.4823, -4.6985],\n",
      "        [-0.2138,  3.8109, -5.7360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0167, grad_fn=<NllLossBackward0>), logits=tensor([[-1.0381,  4.2330, -4.7967],\n",
      "        [ 2.8565, -0.3159, -5.4710],\n",
      "        [-1.1191,  4.5323, -5.1095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.7528, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4229,  2.5260, -4.2857],\n",
      "        [ 3.2946, -0.5376, -5.7649],\n",
      "        [ 3.3090, -0.7274, -5.2771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(1.3822, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.3340, -0.7026, -5.3987],\n",
      "        [-1.0422,  4.5508, -4.8836],\n",
      "        [ 2.5623,  0.1781, -5.5385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.0155, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1308, -0.1016, -5.5986],\n",
      "        [-1.1299,  4.6528, -4.6232],\n",
      "        [-1.0762,  4.3800, -4.9947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n",
      "Outputs: SequenceClassifierOutput(loss=tensor(0.1928, grad_fn=<NllLossBackward0>), logits=tensor([[-0.4081,  3.9809, -5.1751],\n",
      "        [ 1.1531,  1.6715, -4.6516],\n",
      "        [ 0.3171,  2.6007, -4.4687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Working\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[118], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Backpropagation and optimization\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m optim\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutputs:\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_load:\n",
    "        inputs, attention_mask, label = batch\n",
    "        optim.zero_grad()\n",
    "\n",
    "        if label is not None:\n",
    "            print(\"Working\")\n",
    "            batch_output= model(inputs, attention_mask=attention_mask, labels=label)\n",
    "            loss = batch_output.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(\"Outputs:\", batch_output)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss_per_epoch: {total_loss}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')                  ############################## Abdul"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fadd1ea9c9a50e6"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9d6a82f7b677b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:36:17.429013825Z",
     "start_time": "2023-12-01T05:36:15.776026092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model= BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "save_model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95fccc529bc375ac"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Batch: 1/200\n",
      "Evaluating Batch: 4/200\n",
      "Evaluating Batch: 7/200\n",
      "Evaluating Batch: 10/200\n",
      "Evaluating Batch: 13/200\n",
      "Evaluating Batch: 16/200\n",
      "Evaluating Batch: 19/200\n",
      "Evaluating Batch: 22/200\n",
      "Evaluating Batch: 25/200\n",
      "Evaluating Batch: 28/200\n",
      "Evaluating Batch: 31/200\n",
      "Evaluating Batch: 34/200\n",
      "Evaluating Batch: 37/200\n",
      "Evaluating Batch: 40/200\n",
      "Evaluating Batch: 43/200\n",
      "Evaluating Batch: 46/200\n",
      "Evaluating Batch: 49/200\n",
      "Evaluating Batch: 52/200\n",
      "Evaluating Batch: 55/200\n",
      "Evaluating Batch: 58/200\n",
      "Evaluating Batch: 61/200\n",
      "Evaluating Batch: 64/200\n",
      "Evaluating Batch: 67/200\n",
      "Evaluating Batch: 70/200\n",
      "Evaluating Batch: 73/200\n",
      "Evaluating Batch: 76/200\n",
      "Evaluating Batch: 79/200\n",
      "Evaluating Batch: 82/200\n",
      "Evaluating Batch: 85/200\n",
      "Evaluating Batch: 88/200\n",
      "Evaluating Batch: 91/200\n",
      "Evaluating Batch: 94/200\n",
      "Evaluating Batch: 97/200\n",
      "Evaluating Batch: 100/200\n",
      "Evaluating Batch: 103/200\n",
      "Evaluating Batch: 106/200\n",
      "Evaluating Batch: 109/200\n",
      "Evaluating Batch: 112/200\n",
      "Evaluating Batch: 115/200\n",
      "Evaluating Batch: 118/200\n",
      "Evaluating Batch: 121/200\n",
      "Evaluating Batch: 124/200\n",
      "Evaluating Batch: 127/200\n",
      "Evaluating Batch: 130/200\n",
      "Evaluating Batch: 133/200\n",
      "Evaluating Batch: 136/200\n",
      "Evaluating Batch: 139/200\n",
      "Evaluating Batch: 142/200\n",
      "Evaluating Batch: 145/200\n",
      "Evaluating Batch: 148/200\n",
      "Evaluating Batch: 151/200\n",
      "Evaluating Batch: 154/200\n",
      "Evaluating Batch: 157/200\n",
      "Evaluating Batch: 160/200\n",
      "Evaluating Batch: 163/200\n",
      "Evaluating Batch: 166/200\n",
      "Evaluating Batch: 169/200\n",
      "Evaluating Batch: 172/200\n",
      "Evaluating Batch: 175/200\n",
      "Evaluating Batch: 178/200\n",
      "Evaluating Batch: 181/200\n",
      "Evaluating Batch: 184/200\n",
      "Evaluating Batch: 187/200\n",
      "Evaluating Batch: 190/200\n",
      "Evaluating Batch: 193/200\n",
      "Evaluating Batch: 196/200\n",
      "Evaluating Batch: 199/200\n",
      "Evaluating Batch: 202/200\n",
      "Evaluating Batch: 205/200\n",
      "Evaluating Batch: 208/200\n",
      "Evaluating Batch: 211/200\n",
      "Evaluating Batch: 214/200\n",
      "Evaluating Batch: 217/200\n",
      "Evaluating Batch: 220/200\n",
      "Evaluating Batch: 223/200\n",
      "Evaluating Batch: 226/200\n",
      "Evaluating Batch: 229/200\n",
      "Evaluating Batch: 232/200\n",
      "Evaluating Batch: 235/200\n",
      "Evaluating Batch: 238/200\n",
      "Evaluating Batch: 241/200\n",
      "Evaluating Batch: 244/200\n",
      "Evaluating Batch: 247/200\n",
      "Evaluating Batch: 250/200\n",
      "Evaluating Batch: 253/200\n",
      "Evaluating Batch: 256/200\n",
      "Evaluating Batch: 259/200\n",
      "Evaluating Batch: 262/200\n",
      "Evaluating Batch: 265/200\n",
      "Evaluating Batch: 268/200\n",
      "Evaluating Batch: 271/200\n",
      "Evaluating Batch: 274/200\n",
      "Evaluating Batch: 277/200\n",
      "Evaluating Batch: 280/200\n",
      "Evaluating Batch: 283/200\n",
      "Evaluating Batch: 286/200\n",
      "Evaluating Batch: 289/200\n",
      "Evaluating Batch: 292/200\n",
      "Evaluating Batch: 295/200\n",
      "Evaluating Batch: 298/200\n",
      "Evaluating Batch: 301/200\n",
      "Evaluating Batch: 304/200\n",
      "Evaluating Batch: 307/200\n",
      "Evaluating Batch: 310/200\n",
      "Evaluating Batch: 313/200\n",
      "Evaluating Batch: 316/200\n",
      "Evaluating Batch: 319/200\n",
      "Evaluating Batch: 322/200\n",
      "Evaluating Batch: 325/200\n",
      "Evaluating Batch: 328/200\n",
      "Evaluating Batch: 331/200\n",
      "Evaluating Batch: 334/200\n",
      "Evaluating Batch: 337/200\n",
      "Evaluating Batch: 340/200\n",
      "Evaluating Batch: 343/200\n",
      "Evaluating Batch: 346/200\n",
      "Evaluating Batch: 349/200\n",
      "Evaluating Batch: 352/200\n",
      "Evaluating Batch: 355/200\n",
      "Evaluating Batch: 358/200\n",
      "Evaluating Batch: 361/200\n",
      "Evaluating Batch: 364/200\n",
      "Evaluating Batch: 367/200\n",
      "Evaluating Batch: 370/200\n",
      "Evaluating Batch: 373/200\n",
      "Evaluating Batch: 376/200\n",
      "Evaluating Batch: 379/200\n",
      "Evaluating Batch: 382/200\n",
      "Evaluating Batch: 385/200\n",
      "Evaluating Batch: 388/200\n",
      "Evaluating Batch: 391/200\n",
      "Evaluating Batch: 394/200\n",
      "Evaluating Batch: 397/200\n",
      "Evaluating Batch: 400/200\n",
      "Evaluating Batch: 403/200\n",
      "Evaluating Batch: 406/200\n",
      "Evaluating Batch: 409/200\n",
      "Evaluating Batch: 412/200\n",
      "Evaluating Batch: 415/200\n",
      "Evaluating Batch: 418/200\n",
      "Evaluating Batch: 421/200\n",
      "Evaluating Batch: 424/200\n",
      "Evaluating Batch: 427/200\n",
      "Evaluating Batch: 430/200\n",
      "Evaluating Batch: 433/200\n",
      "Evaluating Batch: 436/200\n",
      "Evaluating Batch: 439/200\n",
      "Evaluating Batch: 442/200\n",
      "Evaluating Batch: 445/200\n",
      "Evaluating Batch: 448/200\n",
      "Evaluating Batch: 451/200\n",
      "Evaluating Batch: 454/200\n",
      "Evaluating Batch: 457/200\n",
      "Evaluating Batch: 460/200\n",
      "Evaluating Batch: 463/200\n",
      "Evaluating Batch: 466/200\n",
      "Evaluating Batch: 469/200\n",
      "Evaluating Batch: 472/200\n",
      "Evaluating Batch: 475/200\n",
      "Evaluating Batch: 478/200\n",
      "Evaluating Batch: 481/200\n",
      "Evaluating Batch: 484/200\n",
      "Evaluating Batch: 487/200\n",
      "Evaluating Batch: 490/200\n",
      "Evaluating Batch: 493/200\n",
      "Evaluating Batch: 496/200\n",
      "Evaluating Batch: 499/200\n",
      "Evaluating Batch: 502/200\n",
      "Evaluating Batch: 505/200\n",
      "Evaluating Batch: 508/200\n",
      "Evaluating Batch: 511/200\n",
      "Evaluating Batch: 514/200\n",
      "Evaluating Batch: 517/200\n",
      "Evaluating Batch: 520/200\n",
      "Evaluating Batch: 523/200\n",
      "Evaluating Batch: 526/200\n",
      "Evaluating Batch: 529/200\n",
      "Evaluating Batch: 532/200\n",
      "Evaluating Batch: 535/200\n",
      "Evaluating Batch: 538/200\n",
      "Evaluating Batch: 541/200\n",
      "Evaluating Batch: 544/200\n",
      "Evaluating Batch: 547/200\n",
      "Evaluating Batch: 550/200\n",
      "Evaluating Batch: 553/200\n",
      "Evaluating Batch: 556/200\n",
      "Evaluating Batch: 559/200\n",
      "Evaluating Batch: 562/200\n",
      "Evaluating Batch: 565/200\n",
      "Evaluating Batch: 568/200\n",
      "Evaluating Batch: 571/200\n",
      "Evaluating Batch: 574/200\n",
      "Evaluating Batch: 577/200\n",
      "Evaluating Batch: 580/200\n",
      "Evaluating Batch: 583/200\n",
      "Evaluating Batch: 586/200\n",
      "Evaluating Batch: 589/200\n",
      "Evaluating Batch: 592/200\n",
      "Evaluating Batch: 595/200\n",
      "Evaluating Batch: 598/200\n",
      "Accuracy on the test set: 87.00%\n"
     ]
    }
   ],
   "source": [
    "save_model.eval()  \n",
    "with torch.no_grad(): \n",
    "    total= 0\n",
    "    actual_pred = 0\n",
    "\n",
    "    for batch in test_load:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        print(f\"Working: {total+ 1}/{len(test_load)}\")\n",
    "\n",
    "        test_predictions = save_model(inputs, attention_mask=attention_mask)\n",
    "        _, predicted= torch.max(test_predictions.logits, 1)\n",
    "        actual_pred += (predicted== labels).sum().item()\n",
    "        total+= labels.size(0)\n",
    "\n",
    "    accuracy = actual_pred / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:37:13.390978109Z",
     "start_time": "2023-12-01T05:36:22.712454271Z"
    }
   },
   "id": "f47646cfcec0c03"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "966d1235cb65c49a"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1eb5e5a923dedfa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:51:11.970490432Z",
     "start_time": "2023-12-01T09:51:11.918637039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n"
     ]
    }
   ],
   "source": [
    "chunk= 100\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Read the JSON file in chunks\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        csv= \"chunk.csv\"\n",
    "        x.to_csv(csv, index=False)\n",
    "\n",
    "    df2= pd.read_csv(\"chunk.csv\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Roberta for unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1565d0b43bc1e0d6"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "403ce9ec799a8e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:51:14.790778196Z",
     "start_time": "2023-12-01T09:51:14.773707017Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "text_data = df2['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba29db46d8d0ead"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text_encode = tokenizer(text_data, padding=True, truncation=True, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:51:21.425312222Z",
     "start_time": "2023-12-01T09:51:18.935400300Z"
    }
   },
   "id": "8eb7908ee75a4743"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecff625e6772ccc1"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**text_encode)\n",
    "    embeddings_data = outputs.last_hidden_state.mean(dim=1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:52:28.337054192Z",
     "start_time": "2023-12-01T09:51:50.415625355Z"
    }
   },
   "id": "a213b623a760f9bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting and finding PCA components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ca2f7c6350d09e"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(embeddings_data)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x=range(1, len(pca.explained_variance_ratio_) + 1), y=np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Variance')\n",
    "plt.xlabel('No. of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:52:54.293633171Z",
     "start_time": "2023-12-01T09:52:54.119053791Z"
    }
   },
   "id": "2c50a57aa693d044"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using elbow method to decide the number of components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b0f77f3b2a4bf6"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "cumulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "elbow_index = np.argmax(np.diff(cumulative_var) < 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:00.261570046Z",
     "start_time": "2023-12-01T09:53:00.224459149Z"
    }
   },
   "id": "66a63a969eb68e70"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Number of Components: 14\n"
     ]
    }
   ],
   "source": [
    "components = elbow_index + 1\n",
    "print(f\"Number of Components: {components}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:01.195201679Z",
     "start_time": "2023-12-01T09:53:01.176703949Z"
    }
   },
   "id": "73e9d25ca5c33d76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying PCA with 14 components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88d932ca8052edc3"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=14)  # Adjust the number of components as needed\n",
    "embeddings_pca = pca.fit_transform(embeddings_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:06.578577879Z",
     "start_time": "2023-12-01T09:53:06.525791555Z"
    }
   },
   "id": "5ed9bb820218da83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2533f3082e46c7ad"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:10.097659974Z",
     "start_time": "2023-12-01T09:53:10.046776461Z"
    }
   },
   "id": "97704e7e86756524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustering using Kmeans"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2701cb471eb56eb"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2)  # Positive and Negative\n",
    "clusters = kmeans.fit_predict(embeddings_scaled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:14.793906187Z",
     "start_time": "2023-12-01T09:53:14.767239384Z"
    }
   },
   "id": "4d1465988634079e"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 text  Cluster_pred\n0   If you decide to eat here, just be aware it is...             1\n1   I've taken a lot of spin classes over the year...             0\n2   Family diner. Had the buffet. Eclectic assortm...             1\n3   Wow!  Yummy, different,  delicious.   Our favo...             1\n4   Cute interior and owner (?) gave us tour of up...             1\n..                                                ...           ...\n95  Had to wait until my third trip to NOLA to act...             0\n96  A GREAT EXPERIENCE!!!!!!!!!  I was a completel...             0\n97  Wow! I never thought my sons phone could be re...             0\n98  Service and management terrible... After messi...             1\n99  I have been to a number of dog friendly hotels...             0\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Cluster_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Had to wait until my third trip to NOLA to act...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>A GREAT EXPERIENCE!!!!!!!!!  I was a completel...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>Wow! I never thought my sons phone could be re...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>Service and management terrible... After messi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>I have been to a number of dog friendly hotels...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Cluster_pred'] = clusters\n",
    "# Print the DataFrame with predicted clusters\n",
    "df2[['text', 'Cluster_pred']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:17.283821356Z",
     "start_time": "2023-12-01T09:53:17.261015507Z"
    }
   },
   "id": "fa15a0d26455aae4"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(df2['Cluster_pred'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:20.103084142Z",
     "start_time": "2023-12-01T09:53:20.081219444Z"
    }
   },
   "id": "56256995928f5dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def sentiment(stars):  #Ashken\n",
    "    if 0 <= stars <3:\n",
    "        return 0 #negative\n",
    "    elif 3<= stars <= 5:\n",
    "        return 1 #positive\n",
    "\n",
    "df2['sentiment_label'] = df2['stars'].apply(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:21.856014612Z",
     "start_time": "2023-12-01T09:53:21.838486748Z"
    }
   },
   "id": "dd4542a60146d7d8"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "                 review_id                 user_id             business_id  \\\n0   KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n1   BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n2   saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n3   AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n4   Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n..                     ...                     ...                     ...   \n95  QS7CuOtFLuS3dnwKHRtSYQ  PBnEwGVCBL0N-bET6ZI6kQ  m5-FtgWRd4qA7j0vaOXiXQ   \n96  4PHFo_GRG4FEk1q4X7xQVQ  jbsCBG0A-3wVDjrKar-0Wg  X63jIMRHYBvKKQDuJTRiQQ   \n97  1c6sgLe07HAhipebsQ1wgA  ZRXvbrutBBULaFS6T9NCwA  HnhxO2cpa15AHI1456Pl6Q   \n98  PPgbLBvi34A6m7bKJfTwhw  3TL6HZ1JrKcNTvGDWKlrow  GyC36Pn0Q1-qHnqXys6yFg   \n99  gImS1dtA_TixEouDfp2o4g  xE7AXFF9wVaN6id6OCtH3Q  D5V0Fawd6ODVgqCY8xngsw   \n\n    stars  useful  funny  cool  \\\n0       3       0      0     0   \n1       5       1      0     1   \n2       3       0      0     0   \n3       5       1      0     1   \n4       4       1      0     1   \n..    ...     ...    ...   ...   \n95      5       0      0     0   \n96      5       2      0     1   \n97      5       0      1     0   \n98      1       0      0     0   \n99      4       1      0     2   \n\n                                                 text                 date  \\\n0   If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n1   I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n2   Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n3   Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n4   Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n..                                                ...                  ...   \n95  Had to wait until my third trip to NOLA to act...  2016-11-10 20:56:13   \n96  A GREAT EXPERIENCE!!!!!!!!!  I was a completel...  2014-10-11 13:55:05   \n97  Wow! I never thought my sons phone could be re...  2015-10-17 00:55:35   \n98  Service and management terrible... After messi...  2013-12-07 13:17:13   \n99  I have been to a number of dog friendly hotels...  2017-01-14 21:05:04   \n\n    Cluster_pred  sentiment_label  \n0              1                1  \n1              0                1  \n2              1                1  \n3              1                1  \n4              1                1  \n..           ...              ...  \n95             0                1  \n96             0                1  \n97             0                1  \n98             1                0  \n99             0                1  \n\n[100 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n      <th>Cluster_pred</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>QS7CuOtFLuS3dnwKHRtSYQ</td>\n      <td>PBnEwGVCBL0N-bET6ZI6kQ</td>\n      <td>m5-FtgWRd4qA7j0vaOXiXQ</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Had to wait until my third trip to NOLA to act...</td>\n      <td>2016-11-10 20:56:13</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>4PHFo_GRG4FEk1q4X7xQVQ</td>\n      <td>jbsCBG0A-3wVDjrKar-0Wg</td>\n      <td>X63jIMRHYBvKKQDuJTRiQQ</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A GREAT EXPERIENCE!!!!!!!!!  I was a completel...</td>\n      <td>2014-10-11 13:55:05</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>1c6sgLe07HAhipebsQ1wgA</td>\n      <td>ZRXvbrutBBULaFS6T9NCwA</td>\n      <td>HnhxO2cpa15AHI1456Pl6Q</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Wow! I never thought my sons phone could be re...</td>\n      <td>2015-10-17 00:55:35</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>PPgbLBvi34A6m7bKJfTwhw</td>\n      <td>3TL6HZ1JrKcNTvGDWKlrow</td>\n      <td>GyC36Pn0Q1-qHnqXys6yFg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Service and management terrible... After messi...</td>\n      <td>2013-12-07 13:17:13</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>gImS1dtA_TixEouDfp2o4g</td>\n      <td>xE7AXFF9wVaN6id6OCtH3Q</td>\n      <td>D5V0Fawd6ODVgqCY8xngsw</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>I have been to a number of dog friendly hotels...</td>\n      <td>2017-01-14 21:05:04</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:22.793273619Z",
     "start_time": "2023-12-01T09:53:22.780094391Z"
    }
   },
   "id": "b7503828e8e0835a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with labelled set"
   ],
   "id": "1deecd4647849e6f"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.00%\n"
     ]
    }
   ],
   "source": [
    "true_labels = df2['sentiment_label'].tolist()\n",
    "\n",
    "# encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded1 = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(encoded1, clusters)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:26.565736438Z",
     "start_time": "2023-12-01T09:53:26.546679802Z"
    }
   },
   "id": "7dc06b36467afbc"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABM+UlEQVR4nO3de5xN9f7H8feeYfbcZwzGUGMwUy65RR1ELpFxjegolwyJOINy6TKdLihGOiEqVA5hhIhCketI0ZHcijAuqQxKzJgZtjGzfn/0sH/tBs2eZtvbXq+nx3o8zHet9f1+1j5N59Pn+13fbTEMwxAAAABMw8fdAQAAAOD6IgEEAAAwGRJAAAAAkyEBBAAAMBkSQAAAAJMhAQQAADAZEkAAAACTIQEEAAAwGRJAAAAAkyEBBHBNBw8eVOvWrRUWFiaLxaJly5YVa/9Hjx6VxWLR7Nmzi7XfG1nz5s3VvHlzd4cBwIuRAAI3gEOHDumxxx5TlSpV5O/vr9DQUDVu3Fivv/66zp8/79KxExIStGfPHo0dO1Zz587VHXfc4dLxrqc+ffrIYrEoNDT0ip/jwYMHZbFYZLFY9J///Mfp/o8fP65Ro0Zp586dxRAtABSfEu4OAMC1rVy5Uv/85z9ltVrVu3dv1axZUxcvXtTmzZv15JNP6rvvvtPbb7/tkrHPnz+vLVu26N///rcGDx7skjFiYmJ0/vx5lSxZ0iX9/5USJUooJydHy5cvV7du3RzOpaSkyN/fXxcuXChS38ePH9fo0aNVqVIl1a1bt9D3ffbZZ0UaDwAKiwQQ8GBHjhzRQw89pJiYGK1fv17ly5e3n0tMTFRaWppWrlzpsvF/+eUXSVJ4eLjLxrBYLPL393dZ/3/FarWqcePGev/99wskgPPnz1f79u21ZMmS6xJLTk6OAgMD5efnd13GA2BeTAEDHmzChAnKysrSzJkzHZK/y+Li4vT444/bf7506ZJeeuklxcbGymq1qlKlSnr22Wdls9kc7qtUqZI6dOigzZs36x//+If8/f1VpUoVzZkzx37NqFGjFBMTI0l68sknZbFYVKlSJUm/T51e/vsfjRo1ShaLxaFtzZo1atKkicLDwxUcHKyqVavq2WeftZ+/2hrA9evX6+6771ZQUJDCw8PVqVMn7du374rjpaWlqU+fPgoPD1dYWJj69u2rnJycq3+wf9KjRw99+umnOnv2rL1t27ZtOnjwoHr06FHg+t9++00jR45UrVq1FBwcrNDQULVt21a7du2yX7Nx40bdeeedkqS+ffvap5IvP2fz5s1Vs2ZNbd++XU2bNlVgYKD9c/nzGsCEhAT5+/sXeP74+HiVKlVKx48fL/SzAoBEAgh4tOXLl6tKlSq66667CnX9o48+qhdeeEH16tXTpEmT1KxZMyUnJ+uhhx4qcG1aWpoeeOAB3XvvvXrttddUqlQp9enTR999950kqUuXLpo0aZIkqXv37po7d64mT57sVPzfffedOnToIJvNpjFjxui1117Tfffdpy+++OKa961du1bx8fE6deqURo0apeHDh+vLL79U48aNdfTo0QLXd+vWTefOnVNycrK6deum2bNna/To0YWOs0uXLrJYLPrwww/tbfPnz1e1atVUr169AtcfPnxYy5YtU4cOHTRx4kQ9+eST2rNnj5o1a2ZPxqpXr64xY8ZIkgYMGKC5c+dq7ty5atq0qb2f06dPq23btqpbt64mT56sFi1aXDG+119/XWXLllVCQoLy8vIkSTNmzNBnn32mqVOnqkKFCoV+VgCQJBkAPFJGRoYhyejUqVOhrt+5c6chyXj00Ucd2keOHGlIMtavX29vi4mJMSQZmzZtsredOnXKsFqtxogRI+xtR44cMSQZr776qkOfCQkJRkxMTIEYXnzxReOP/1qZNGmSIcn45Zdfrhr35TFmzZplb6tbt64RGRlpnD592t62a9cuw8fHx+jdu3eB8R555BGHPu+//36jdOnSVx3zj88RFBRkGIZhPPDAA0bLli0NwzCMvLw8Iyoqyhg9evQVP4MLFy4YeXl5BZ7DarUaY8aMsbdt27atwLNd1qxZM0OSMX369Cuea9asmUPb6tWrDUnGyy+/bBw+fNgIDg42Onfu/JfPCABXQgUQ8FCZmZmSpJCQkEJd/8knn0iShg8f7tA+YsQISSqwVrBGjRq6++677T+XLVtWVatW1eHDh4sc859dXjv40UcfKT8/v1D3pKena+fOnerTp48iIiLs7bVr19a9995rf84/GjhwoMPPd999t06fPm3/DAujR48e2rhxo06cOKH169frxIkTV5z+lX5fN+jj8/u/PvPy8nT69Gn79PY333xT6DGtVqv69u1bqGtbt26txx57TGPGjFGXLl3k7++vGTNmFHosAPgjEkDAQ4WGhkqSzp07V6jrf/jhB/n4+CguLs6hPSoqSuHh4frhhx8c2itWrFigj1KlSunMmTNFjLigBx98UI0bN9ajjz6qcuXK6aGHHtKiRYuumQxejrNq1aoFzlWvXl2//vqrsrOzHdr//CylSpWSJKeepV27dgoJCdHChQuVkpKiO++8s8BneVl+fr4mTZqkW265RVarVWXKlFHZsmW1e/duZWRkFHrMm266yakXPv7zn/8oIiJCO3fu1JQpUxQZGVnoewHgj0gAAQ8VGhqqChUq6Ntvv3Xqvj+/hHE1vr6+V2w3DKPIY1xen3ZZQECANm3apLVr1+rhhx/W7t279eCDD+ree+8tcO3f8Xee5TKr1aouXbrovffe09KlS69a/ZOkcePGafjw4WratKnmzZun1atXa82aNbrtttsKXemUfv98nLFjxw6dOnVKkrRnzx6n7gWAPyIBBDxYhw4ddOjQIW3ZsuUvr42JiVF+fr4OHjzo0H7y5EmdPXvW/kZvcShVqpTDG7OX/bnKKEk+Pj5q2bKlJk6cqL1792rs2LFav369NmzYcMW+L8e5f//+Aue+//57lSlTRkFBQX/vAa6iR48e2rFjh86dO3fFF2cuW7x4sVq0aKGZM2fqoYceUuvWrdWqVasCn0lhk/HCyM7OVt++fVWjRg0NGDBAEyZM0LZt24qtfwDmQgIIeLCnnnpKQUFBevTRR3Xy5MkC5w8dOqTXX39d0u9TmJIKvKk7ceJESVL79u2LLa7Y2FhlZGRo9+7d9rb09HQtXbrU4brffvutwL2XN0T+89Y0l5UvX15169bVe++955BQffvtt/rss8/sz+kKLVq00EsvvaQ33nhDUVFRV73O19e3QHXxgw8+0M8//+zQdjlRvVKy7Kynn35ax44d03vvvaeJEyeqUqVKSkhIuOrnCADXwkbQgAeLjY3V/Pnz9eCDD6p69eoO3wTy5Zdf6oMPPlCfPn0kSXXq1FFCQoLefvttnT17Vs2aNdP//vc/vffee+rcufNVtxgpioceekhPP/207r//fg0dOlQ5OTmaNm2abr31VoeXIMaMGaNNmzapffv2iomJ0alTp/TWW2/p5ptvVpMmTa7a/6uvvqq2bduqUaNG6tevn86fP6+pU6cqLCxMo0aNKrbn+DMfHx8999xzf3ldhw4dNGbMGPXt21d33XWX9uzZo5SUFFWpUsXhutjYWIWHh2v69OkKCQlRUFCQGjRooMqVKzsV1/r16/XWW2/pxRdftG9LM2vWLDVv3lzPP/+8JkyY4FR/AMA2MMAN4MCBA0b//v2NSpUqGX5+fkZISIjRuHFjY+rUqcaFCxfs1+Xm5hqjR482KleubJQsWdKIjo42kpKSHK4xjN+3gWnfvn2Bcf68/cjVtoExDMP47LPPjJo1axp+fn5G1apVjXnz5hXYBmbdunVGp06djAoVKhh+fn5GhQoVjO7duxsHDhwoMMaft0pZu3at0bhxYyMgIMAIDQ01OnbsaOzdu9fhmsvj/XmbmVmzZhmSjCNHjlz1MzUMx21gruZq28CMGDHCKF++vBEQEGA0btzY2LJlyxW3b/noo4+MGjVqGCVKlHB4zmbNmhm33XbbFcf8Yz+ZmZlGTEyMUa9ePSM3N9fhumHDhhk+Pj7Gli1brvkMAPBnFsNwYpU0AAAAbnisAQQAADAZEkAAAACTIQEEAAAwGRJAAAAAkyEBBAAAMBkSQAAAAJMhAQQAADAZr/wmkAuX3B0BAFf5Leuiu0MA4CIVwv3cNnbA7YNd1vf5HW+4rO+iogIIAABgMl5ZAQQAAHCKxVw1MRJAAAAAi8XdEVxX5kp3AQAAQAUQAADAbFPA5npaAAAAUAEEAABgDSAAAAC8GhVAAAAA1gACAADAm1EBBAAAMNkaQBJAAAAApoABAADgzagAAgAAmGwKmAogAACAyVABBAAAYA0gAAAAvBkVQAAAANYAAgAAwJtRAQQAADDZGkASQAAAAKaAAQAA4M2oAAIAAJhsCthcTwsAAAAqgAAAAFQAAQAA4NVIAAEAAHwsrjv+hvHjx8tiseiJJ56wtzVv3lwWi8XhGDhwoFP9MgUMAADggbZt26YZM2aodu3aBc71799fY8aMsf8cGBjoVN9UAAEAACw+rjuKICsrSz179tQ777yjUqVKFTgfGBioqKgo+xEaGupU/ySAAAAAFovLDpvNpszMTIfDZrNdM5zExES1b99erVq1uuL5lJQUlSlTRjVr1lRSUpJycnKcelwSQAAAABdKTk5WWFiYw5GcnHzV6xcsWKBvvvnmqtf06NFD8+bN04YNG5SUlKS5c+eqV69eTsXEGkAAAAAXbgOTlJSk4cOHO7RZrdYrXvvjjz/q8ccf15o1a+Tv73/FawYMGGD/e61atVS+fHm1bNlShw4dUmxsbKFiIgEEAABwIavVetWE78+2b9+uU6dOqV69eva2vLw8bdq0SW+88YZsNpt8fX0d7mnQoIEkKS0tjQQQAACg0Cx/b7uW4tKyZUvt2bPHoa1v376qVq2ann766QLJnyTt3LlTklS+fPlCj0MCCAAA4CFCQkJUs2ZNh7agoCCVLl1aNWvW1KFDhzR//ny1a9dOpUuX1u7duzVs2DA1bdr0itvFXA0JIAAAwA3yVXB+fn5au3atJk+erOzsbEVHR6tr16567rnnnOqHBBAAAMCDbdy40f736Ohopaam/u0+SQABAAA8ZA3g9UICCAAAcINMARcXcz0tAAAAqAACAACYbQqYCiAAAIDJUAEEAABgDSAAAAC8GRVAAAAA1gACAADAm1EBBAAAMNkaQBJAAAAAkyWA5npaAAAAUAEEAADgJRAAAAB4NSqAAAAArAEEAACAN6MCCAAAwBpAAAAAeDMqgAAAACZbA0gCCAAAwBQwAAAAvBkVQAAAYHoWKoAAAADwZlQAAQCA6VEBBAAAgFejAggAAGCuAiAVQAAAALOhAggAAEzPbGsASQABAIDpmS0BZAoYAADAZKgAAgAA06MCCAAAAK9GBRAAAJgeFUAAAAB4NSqAAAAA5ioAUgEEAAAwGyqAAADA9FgDCAAAAK9GBRAAAJie2SqAJIAAAMD0zJYAMgUMAABgMlQAAQCA6VEBBAAAgEcYP368LBaLnnjiCXvbhQsXlJiYqNKlSys4OFhdu3bVyZMnneqXBBAAAMDiwqOItm3bphkzZqh27doO7cOGDdPy5cv1wQcfKDU1VcePH1eXLl2c6psEEAAAwMNkZWWpZ8+eeuedd1SqVCl7e0ZGhmbOnKmJEyfqnnvuUf369TVr1ix9+eWX2rp1a6H7JwEEAACmZ7FYXHbYbDZlZmY6HDab7ZrxJCYmqn379mrVqpVD+/bt25Wbm+vQXq1aNVWsWFFbtmwp9POSAAIAALhQcnKywsLCHI7k5OSrXr9gwQJ98803V7zmxIkT8vPzU3h4uEN7uXLldOLEiULHxFvAAADA9Fz5FnBSUpKGDx/u0Ga1Wq947Y8//qjHH39ca9askb+/v8tiIgEEAACm58oE0Gq1XjXh+7Pt27fr1KlTqlevnr0tLy9PmzZt0htvvKHVq1fr4sWLOnv2rEMV8OTJk4qKiip0TCSAAAAAHqJly5bas2ePQ1vfvn1VrVo1Pf3004qOjlbJkiW1bt06de3aVZK0f/9+HTt2TI0aNSr0OB6TAH7++eeaMWOGDh06pMWLF+umm27S3LlzVblyZTVp0sTd4QEAAG/mIftAh4SEqGbNmg5tQUFBKl26tL29X79+Gj58uCIiIhQaGqohQ4aoUaNGatiwYaHH8YiXQJYsWaL4+HgFBARox44d9jdjMjIyNG7cODdHBwAA4DkmTZqkDh06qGvXrmratKmioqL04YcfOtWHxTAMw0XxFdrtt9+uYcOGqXfv3goJCdGuXbtUpUoV7dixQ23btnXqrRZJunDJRYECcLvfsi66OwQALlIh3M9tY5d79AOX9X3y3X+6rO+i8ogK4P79+9W0adMC7WFhYTp79uz1DwgAAMCLeUQCGBUVpbS0tALtmzdvVpUqVdwQEQAAMBNXbgTtiTwiAezfv78ef/xxffXVV7JYLDp+/LhSUlI0cuRIDRo0yN3hAQAAeBWPeAv4mWeeUX5+vlq2bKmcnBw1bdpUVqtVI0eO1JAhQ9wdHgAA8HKeWqlzFY94CeSyixcvKi0tTVlZWapRo4aCg4OL1A8vgQDei5dAAO/lzpdAKjzm3Fu0zjg+o4vL+i4qj5gCnjdvnnJycuTn56caNWroH//4R5GTPwAAAFybRySAw4YNU2RkpHr06KFPPvlEeXl57g4JAACYicWFhwfyiAQwPT1dCxYskMViUbdu3VS+fHklJibqyy+/dHdoAAAAXscjEsASJUqoQ4cOSklJ0alTpzRp0iQdPXpULVq0UGxsrLvDAwAAXs5s28B4xFvAfxQYGKj4+HidOXNGP/zwg/bt2+fukAAAALyKxySAOTk5Wrp0qVJSUrRu3TpFR0ere/fuWrx4sbtDAwAAXs5TK3Wu4hEJ4EMPPaQVK1YoMDBQ3bp10/PPP69GjRq5OywAAACv5BEJoK+vrxYtWqT4+Hj5+vq6OxwAAGAyVADdICUlxd0hAAAAMzNX/ue+BHDKlCkaMGCA/P39NWXKlGteO3To0OsUFQAAgPdz21fBVa5cWV9//bVKly6typUrX/U6i8Wiw4cPO9U3XwUHeC++Cg7wXu78KriKQz52Wd/Hpt7nsr6Lym0VwCNHjlzx7wAAAHAtj9gIesyYMcrJySnQfv78eY0ZM8YNEQEAADMx20bQbpsC/iNfX1+lp6crMjLSof306dOKjIx0+ruBmQIGvBdTwID3cucUcMzQ5S7r+4cpHV3Wd1F5RAXQMIwrZsi7du1SRESEGyLCjWbmO2+rzm1VNSF5rLtDAfA35eXl6b/Tp6p75zaKb3qHenZpqzkzp8sD6hXwYmarALp1G5hSpUrZP5xbb73V4UPKy8tTVlaWBg4c6MYIcSP4ds9uLf5ggW69taq7QwFQDN6f+1999OEiPfPCWFWuEqv9+77TKy8/r6DgEHV9sKe7wwO8glsTwMmTJ8swDD3yyCMaPXq0wsLC7Of8/PxUqVIlvhEE15STna2kp5/Ui6Nf1jszprk7HADF4LvdO9W4aQs1atJUkhRV4Sat++xTfb93j5sjgzfz1Eqdq7g1AUxISJD0+5Ywd911l0qWLOnOcHADGvfyGDVt2kwNG91FAgh4idtq19WKZYv147Gjiq5YSWkH9uvbXd9o0BNPujs0eDNz5X+e8U0gzZo1s//9woULunjRcZF3aGjoVe+12Wyy2WwObYavVVartXiDhMf59JOV2rdvr+YvXOzuUAAUox69+yknO0sJ3e6Tj4+v8vPz1G/gUN3bpoO7QwO8hke8BJKTk6PBgwcrMjJSQUFBKlWqlMNxLcnJyQoLC3M4Xn0l+TpFDnc5kZ6uCePHKvmVV0n2AS+zce1qrV21Us+NeUVvz1moZ14Yq0Ups7Vq5UfuDg1ezGwvgXjENjCJiYnasGGDXnrpJT388MN688039fPPP2vGjBkaP368eva8+qJfKoDmtH7dWg0bmihfX197W15eniwWi3x8fLRtxx6Hc/AebAPj/bp1bKXuvfvp/n92t7fN/e8MrVm1QnMWuW6rDrifO7eBqTL8E5f1fXhiO5f1XVQeMQW8fPlyzZkzR82bN1ffvn119913Ky4uTjExMUpJSblmAmi1Fkz22AfQ+zVo2FCLlzn+H8GL/05SpSpV1Ldff5I/4AZmu3BBPj6OE1Q+Pr4y8t1er4AX89RKnat4RAL422+/qUqVKpJ+X+/322+/SZKaNGmiQYMGuTM0eKigoGDdcsutDm0BgYEKDwsv0A7gxtLo7maaN+ttRZYrr8pVYnXwwPf64P05atuxs7tDA7yGRySAVapU0ZEjR1SxYkVVq1ZNixYt0j/+8Q8tX75c4eHh7g4PAHAdDR3xrP474w29/urLOnPmN5UpU1Yd739AvftREIDrmKwA6BlrACdNmiRfX18NHTpUa9euVceOHWUYhnJzczVx4kQ9/vjjTvXHFDDgvVgDCHgvd64BjBv5qcv6TvtPW5f1XVQekQD+2Q8//KDt27crLi5OtWvXdvp+EkDAe5EAAt7LnQngLU+uclnfB19t47K+i8ojpoD/LCYmRjExMe4OAwAAmITZpoA9IgGcMmXKFdstFov8/f0VFxenpk2b8mYnAABAMfCIBHDSpEn65ZdflJOTY9/4+cyZMwoMDFRwcLBOnTqlKlWqaMOGDYqOjnZztAAAwNuYbRsYj/gmkHHjxunOO+/UwYMHdfr0aZ0+fVoHDhxQgwYN9Prrr+vYsWOKiorSsGHD3B0qAADADc8jXgKJjY3VkiVLVLduXYf2HTt2qGvXrjp8+LC+/PJLde3aVenp6X/ZHy+BAN6Ll0AA7+XOl0CqPbPaZX1/Pz7eZX0XlUdUANPT03XpUsGs7dKlSzpx4oQkqUKFCjp37tz1Dg0AAMDreEQC2KJFCz322GPasWOHvW3Hjh0aNGiQ7rnnHknSnj17VLlyZXeFCAAAvJiPj8VlhyfyiARw5syZioiIUP369e3f7XvHHXcoIiJCM2fOlCQFBwfrtddec3OkAAAANz6PeAs4KipKa9as0ffff68DBw5IkqpWraqqVavar2nRooW7wgMAAF7OZC8Be0YCeFmVKlVksVgUGxurEiU8KjQAAODF2AbGDXJyctSvXz8FBgbqtttu07FjxyRJQ4YM0fjx490cHQAAwPUxbdo01a5dW6GhoQoNDVWjRo306af//z3FzZs3l8VicTgGDhzo9DgekQAmJSVp165d2rhxo/z9/e3trVq10sKFC90YGQAAMAOLxXWHM26++WaNHz9e27dv19dff6177rlHnTp10nfffWe/pn///kpPT7cfEyZMcPp5PWKeddmyZVq4cKEaNmzoUIK97bbbdOjQITdGBgAAcP107NjR4eexY8dq2rRp2rp1q2677TZJUmBgoKKiov7WOB5RAfzll18UGRlZoD07O9t0c/IAAOD6+/O0anEeNptNmZmZDofNZvvLmPLy8rRgwQJlZ2erUaNG9vaUlBSVKVNGNWvWVFJSknJycpx+Xo9IAO+44w6tXLnS/vPlpO/dd991eGAAAIAbTXJyssLCwhyO5OTkq16/Z88eBQcHy2q1auDAgVq6dKlq1KghSerRo4fmzZunDRs2KCkpSXPnzlWvXr2cjskjvgpu8+bNatu2rXr16qXZs2frscce0969e/Xll18qNTVV9evXd6o/vgoO8F58FRzgvdz5VXB1Xlznsr7/92yTAhW/y/seX8nFixd17NgxZWRkaPHixXr33XeVmppqTwL/aP369WrZsqXS0tIUGxtb6Jg8ogLYpEkT7dy5U5cuXVKtWrX02WefKTIyUlu2bHE6+QMAAPAkVqvV/lbv5eNqyZ8k+fn5KS4uTvXr11dycrLq1Kmj119//YrXNmjQQJKUlpbmVEwe8RKIJMXGxuqdd95xdxgAAMCEPPmVg/z8/KuuGdy5c6ckqXz58k716dYE0MfH5y9f8rBYLLp0iTldAADgOp7y0mlSUpLatm2rihUr6ty5c5o/f742btyo1atX69ChQ5o/f77atWun0qVLa/fu3Ro2bJiaNm2q2rVrOzWOWxPApUuXXvXcli1bNGXKFOXn51/HiAAAANzn1KlT6t27t9LT0xUWFqbatWtr9erVuvfee/Xjjz9q7dq1mjx5srKzsxUdHa2uXbvqueeec3ocj3gJ5I/279+vZ555RsuXL1fPnj01ZswYxcTEONUHL4EA3ouXQADv5c6XQOqNWe+yvr954R6X9V1UHvESiCQdP35c/fv3V61atXTp0iXt3LlT7733ntPJHwAAAK7N7S+BZGRkaNy4cZo6darq1q2rdevW6e6773Z3WAAAwEQ8ZQ3g9eLWBHDChAl65ZVXFBUVpffff1+dOnVyZzgAAACm4NY1gD4+PgoICFCrVq3k6+t71es+/PBDp/plDSDgvVgDCHgvd64BvOPlDS7r++vnWris76JyawWwd+/epiu5AgAAuJtbE8DZs2e7c3gAAABJ5lsD6DFvAQMAAOD6cPtbwAAAAO5msgIgCSAAAABTwAAAAPBqVAABAIDpmawASAUQAADAbKgAAgAA02MNIAAAALwaFUAAAGB6JisAUgEEAAAwGyqAAADA9My2BpAEEAAAmJ7J8j+mgAEAAMyGCiAAADA9s00BUwEEAAAwGSqAAADA9KgAAgAAwKtRAQQAAKZnsgIgFUAAAACzoQIIAABMz2xrAEkAAQCA6Zks/2MKGAAAwGyoAAIAANMz2xQwFUAAAACToQIIAABMz2QFQCqAAAAAZkMFEAAAmJ6PyUqAVAABAABMhgogAAAwPZMVAEkAAQAA2AYGAAAAXo0KIAAAMD0fcxUAqQACAACYDRVAAABgeqwBBAAAgFejAggAAEzPZAVAKoAAAACeYtq0aapdu7ZCQ0MVGhqqRo0a6dNPP7Wfv3DhghITE1W6dGkFBwera9euOnnypNPjkAACAADTs7jwjzNuvvlmjR8/Xtu3b9fXX3+te+65R506ddJ3330nSRo2bJiWL1+uDz74QKmpqTp+/Li6dOni/PMahmE4fZeHu3DJ3REAcJXfsi66OwQALlIh3M9tY9/39jaX9f3xgDv/1v0RERF69dVX9cADD6hs2bKaP3++HnjgAUnS999/r+rVq2vLli1q2LBhofukAggAAOBCNptNmZmZDofNZvvL+/Ly8rRgwQJlZ2erUaNG2r59u3Jzc9WqVSv7NdWqVVPFihW1ZcsWp2IiAQQAAKZnsVhcdiQnJyssLMzhSE5Ovmose/bsUXBwsKxWqwYOHKilS5eqRo0aOnHihPz8/BQeHu5wfbly5XTixAmnnpe3gAEAAFwoKSlJw4cPd2izWq1Xvb5q1arauXOnMjIytHjxYiUkJCg1NbVYYyIBBAAApufKbWCsVus1E74/8/PzU1xcnCSpfv362rZtm15//XU9+OCDunjxos6ePetQBTx58qSioqKciokpYAAAAA+Wn58vm82m+vXrq2TJklq3bp393P79+3Xs2DE1atTIqT6pAAIAANPz8ZCdoJOSktS2bVtVrFhR586d0/z587Vx40atXr1aYWFh6tevn4YPH66IiAiFhoZqyJAhatSokVNvAEskgAAAAB7j1KlT6t27t9LT0xUWFqbatWtr9erVuvfeeyVJkyZNko+Pj7p27Sqbzab4+Hi99dZbTo/DPoAAbijsAwh4L3fuA9j1v9td1veSR+q7rO+iogIIAABMz+IhU8DXCy+BAAAAmAwVQAAAYHomKwBSAQQAADAbKoAAAMD0PGUbmOuFCiAAAIDJUAEEAACmZ676HxVAAAAA06ECCAAATM9s+wCSAAIAANPzMVf+xxQwAACA2VABBAAApme2KWAqgAAAACZDBRAAAJieyQqAVAABAADMhgogAAAwPbOtASxUAvjxxx8XusP77ruvyMEAAADA9QqVAHbu3LlQnVksFuXl5f2deAAAAK47s+0DWKgEMD8/39VxAAAAuI3ZpoB5CQQAAMBkivQSSHZ2tlJTU3Xs2DFdvHjR4dzQoUOLJTAAAIDrxVz1vyIkgDt27FC7du2Uk5Oj7OxsRURE6Ndff1VgYKAiIyNJAAEAADyc01PAw4YNU8eOHXXmzBkFBARo69at+uGHH1S/fn395z//cUWMAAAALuVjsbjs8EROJ4A7d+7UiBEj5OPjI19fX9lsNkVHR2vChAl69tlnXREjAAAAipHTCWDJkiXl4/P7bZGRkTp27JgkKSwsTD/++GPxRgcAAHAdWCyuOzyR02sAb7/9dm3btk233HKLmjVrphdeeEG//vqr5s6dq5o1a7oiRgAAABQjpyuA48aNU/ny5SVJY8eOValSpTRo0CD98ssvevvtt4s9QAAAAFezWCwuOzyR0xXAO+64w/73yMhIrVq1qlgDAgAAgGsVaR9AAAAAb+KhhTqXcToBrFy58jXLmYcPH/5bAQEAAFxvnrpdi6s4nQA+8cQTDj/n5uZqx44dWrVqlZ588sniigsAAAAu4nQC+Pjjj1+x/c0339TXX3/9twMCAAC43kxWAHT+LeCradu2rZYsWVJc3QEAAMBFiu0lkMWLFysiIqK4ugMAALhuPHW7Flcp0kbQf/yQDMPQiRMn9Msvv+itt94q1uAAAABQ/JxOADt16uSQAPr4+Khs2bJq3ry5qlWrVqzBFdXNjy5wdwgAXCR712Z3hwDARc7veMNtYxfbmrgbhNMJ4KhRo1wQBgAAAK4XpxNeX19fnTp1qkD76dOn5evrWyxBAQAAXE98FdxfMAzjiu02m01+fn5/OyAAAIDrzccz8zSXKXQCOGXKFEm/Z8jvvvuugoOD7efy8vK0adMmj1kDCAAAgKsrdAI4adIkSb9XAKdPn+4w3evn56dKlSpp+vTpxR8hAACAi5mtAljoNYBHjhzRkSNH1KxZM+3atcv+85EjR7R//36tXr1aDRo0cGWsAAAAXi05OVl33nmnQkJCFBkZqc6dO2v//v0O1zRv3rzAOsOBAwc6NY7TawA3bNjg7C0AAAAezVNe1khNTVViYqLuvPNOXbp0Sc8++6xat26tvXv3KigoyH5d//79NWbMGPvPgYGBTo3jdALYtWtX/eMf/9DTTz/t0D5hwgRt27ZNH3zwgbNdAgAAQNKqVascfp49e7YiIyO1fft2NW3a1N4eGBioqKioIo/j9DYwmzZtUrt27Qq0t23bVps2bSpyIAAAAO7iY3HdYbPZlJmZ6XDYbLZCxZWRkSFJBb5uNyUlRWXKlFHNmjWVlJSknJwc557XqaslZWVlXXG7l5IlSyozM9PZ7gAAALxacnKywsLCHI7k5OS/vC8/P19PPPGEGjdurJo1a9rbe/TooXnz5mnDhg1KSkrS3Llz1atXL6dicnoKuFatWlq4cKFeeOEFh/YFCxaoRo0aznYHAADgdq5cApiUlKThw4c7tFmt1r+8LzExUd9++602b3b8CswBAwbY/16rVi2VL19eLVu21KFDhxQbG1uomJxOAJ9//nl16dJFhw4d0j333CNJWrdunebPn6/Fixc72x0AAIDb+bgwA7RarYVK+P5o8ODBWrFihTZt2qSbb775mtde3oUlLS3NdQlgx44dtWzZMo0bN06LFy9WQECA6tSpo/Xr1xeYnwYAAEDhGYahIUOGaOnSpdq4caMqV678l/fs3LlTklS+fPlCj+N0AihJ7du3V/v27SVJmZmZev/99zVy5Eht375deXl5RekSAADAbZx+KcJFEhMTNX/+fH300UcKCQnRiRMnJElhYWEKCAjQoUOHNH/+fLVr106lS5fW7t27NWzYMDVt2lS1a9cu9DhFft5NmzYpISFBFSpU0GuvvaZ77rlHW7duLWp3AAAApjdt2jRlZGSoefPmKl++vP1YuHChpN+/fW3t2rVq3bq1qlWrphEjRqhr165avny5U+M4VQE8ceKEZs+erZkzZyozM1PdunWTzWbTsmXLeAEEAADcsDxkH2gZhnHN89HR0UpNTf3b4xS6AtixY0dVrVpVu3fv1uTJk3X8+HFNnTr1bwcAAACA66vQFcBPP/1UQ4cO1aBBg3TLLbe4MiYAAIDrypVvAXuiQlcAN2/erHPnzql+/fpq0KCB3njjDf3666+ujA0AAAAuUOgEsGHDhnrnnXeUnp6uxx57TAsWLFCFChWUn5+vNWvW6Ny5c66MEwAAwGUsFtcdnsjpt4CDgoL0yCOPaPPmzdqzZ49GjBih8ePHKzIyUvfdd58rYgQAAHApV34XsCf6W9veVK1aVRMmTNBPP/2k999/v7hiAgAAgAsVaSPoP/P19VXnzp3VuXPn4ugOAADguuIlEAAAAHi1YqkAAgAA3MhMVgCkAggAAGA2VAABAIDpeerbuq5CBRAAAMBkqAACAADTs8hcJUASQAAAYHpMAQMAAMCrUQEEAACmRwUQAAAAXo0KIAAAMD2LyXaCpgIIAABgMlQAAQCA6bEGEAAAAF6NCiAAADA9ky0BJAEEAADwMVkGyBQwAACAyVABBAAApsdLIAAAAPBqVAABAIDpmWwJIBVAAAAAs6ECCAAATM9H5ioBUgEEAAAwGSqAAADA9My2BpAEEAAAmB7bwAAAAMCrUQEEAACmx1fBAQAAwKtRAQQAAKZnsgIgFUAAAACzoQIIAABMjzWAAAAA8GpUAAEAgOmZrABIAggAAGC2KVGzPS8AAIDpkQACAADTs1gsLjuckZycrDvvvFMhISGKjIxU586dtX//fodrLly4oMTERJUuXVrBwcHq2rWrTp486dQ4JIAAAAAeIjU1VYmJidq6davWrFmj3NxctW7dWtnZ2fZrhg0bpuXLl+uDDz5Qamqqjh8/ri5dujg1DmsAAQCA6XnKOyCrVq1y+Hn27NmKjIzU9u3b1bRpU2VkZGjmzJmaP3++7rnnHknSrFmzVL16dW3dulUNGzYs1DhUAAEAAFzIZrMpMzPT4bDZbIW6NyMjQ5IUEREhSdq+fbtyc3PVqlUr+zXVqlVTxYoVtWXLlkLHRAIIAABMz8dicdmRnJyssLAwhyM5OfkvY8rPz9cTTzyhxo0bq2bNmpKkEydOyM/PT+Hh4Q7XlitXTidOnCj08zIFDAAA4EJJSUkaPny4Q5vVav3L+xITE/Xtt99q8+bNxR4TCSAAADA9V64BtFqthUr4/mjw4MFasWKFNm3apJtvvtneHhUVpYsXL+rs2bMOVcCTJ08qKiqq0P0zBQwAAEzPYnHd4QzDMDR48GAtXbpU69evV+XKlR3O169fXyVLltS6devsbfv379exY8fUqFGjQo9DBRAAAMBDJCYmav78+froo48UEhJiX9cXFhamgIAAhYWFqV+/fho+fLgiIiIUGhqqIUOGqFGjRoV+A1giAQQAAHB6w2ZXmTZtmiSpefPmDu2zZs1Snz59JEmTJk2Sj4+PunbtKpvNpvj4eL311ltOjUMCCAAA4CEMw/jLa/z9/fXmm2/qzTffLPI4JIAAAMD0zPZShNmeFwAAwPSoAAIAANPzlDWA1wsVQAAAAJOhAggAAEzPXPU/KoAAAACmQwUQAACYntnWAJIAAgAA0zPblKjZnhcAAMD0qAACAADTM9sUMBVAAAAAk6ECCAAATM9c9T8qgAAAAKZDBRAAAJieyZYAUgEEAAAwGyqAAADA9HxMtgqQBBAAAJgeU8AAAADwalQAAQCA6VlMNgVMBRAAAMBkqAACAADTYw0gAAAAvBoVQAAAYHpm2wbGYyqAn3/+uXr16qVGjRrp559/liTNnTtXmzdvdnNkAAAA3sUjEsAlS5YoPj5eAQEB2rFjh2w2myQpIyND48aNc3N0AADA21ksrjs8kUckgC+//LKmT5+ud955RyVLlrS3N27cWN98840bIwMAAGZAAugG+/fvV9OmTQu0h4WF6ezZs9c/IAAAAC/mEQlgVFSU0tLSCrRv3rxZVapUcUNEAADATCwu/OOJPCIB7N+/vx5//HF99dVXslgsOn78uFJSUjRy5EgNGjTI3eEBAAB4FY/YBuaZZ55Rfn6+WrZsqZycHDVt2lRWq1UjR47UkCFD3B0eAADwcj6eWahzGY9IAC0Wi/7973/rySefVFpamrKyslSjRg0FBwe7OzQAAACv4xEJ4Lx589SlSxcFBgaqRo0a7g4HAACYjKeu1XMVj1gDOGzYMEVGRqpHjx765JNPlJeX5+6QAAAAvJZHJIDp6elasGCBLBaLunXrpvLlyysxMVFffvmlu0MDAAAmwD6AblCiRAl16NBBKSkpOnXqlCZNmqSjR4+qRYsWio2NdXd4AADAy5ltGxiPWAP4R4GBgYqPj9eZM2f0ww8/aN++fe4OCQAAwKt4TAKYk5OjpUuXKiUlRevWrVN0dLS6d++uxYsXuzs0AADg5dgGxg0eeughrVixQoGBgerWrZuef/55NWrUyN1hAQAAeCWPSAB9fX21aNEixcfHy9fX193hAAAAk/HUtXqu4hEJYEpKirtDAAAAMA23JYBTpkzRgAED5O/vrylTplzz2qFDh16nqHAjGNq+ul74Zx1N/2y/npu/Q5JkLemjMQ/drvsbVJRfCR9t+PaEnprztX7JtLk5WgDOGNn3Xr00tJPeSNmgJ/+zRJJUrnSIxj1xv+5pWE0hQVYdOHpKE2au1rJ1O90bLLyKp27X4ipuSwAnTZqknj17yt/fX5MmTbrqdRaLhQQQdrdXjlBC81h9e+yMQ/vL3W/XvXUqqN+bXygzJ1fjH66v2UOaqP3YdW6KFICz6teoqH5dG2v3gZ8c2t99qbfCQwL0zydm6NezWXqw7R2a98ojatxzgnbt/+kqvQE3rk2bNunVV1/V9u3blZ6erqVLl6pz587283369NF7773ncE98fLxWrVpV6DHctg/gkSNHVLp0afvfr3YcPnzYXSHCwwRZS2j6Yw01bNY2ZeTk2ttDAkqqZ9Mqev79Hfp83ynt+uGMhsz8Sg1uKav6saXdGDGAwgoK8NOscX30r5fe19nM8w7nGtaporcWpOrr737Q0Z9P65V3V+vsufO6vUa0m6KFN7K48HBWdna26tSpozfffPOq17Rp00bp6en24/3333dqDI/YCHrMmDHKyckp0H7+/HmNGTPGDRHBE73ycH2t2ZWuTXtPOrTXrVRKfiV8lfqH9rT0c/rx12zdSQII3BAmJz2oVZ9/qw1f7S9wbuuuw3qgdX2VCg2UxWLRP+Pry99aQpu+PuiGSOGtfCwWlx3Oatu2rV5++WXdf//9V73GarUqKirKfpQqVcq553U6KhcYPXq0srKyCrTn5ORo9OjR17zXZrMpMzPT4TDycq95D2489zeoqNoxpfTS4l0FzkWGBciWm6fMHMf/3X/JvKDIsIDrFSKAIvpnfH3VrRat56d+fMXzvZ76r0qW8NXx1AnK+Gqypv77IT04/B0d/vHX6xwpUDRXylVstr+3Rn3jxo2KjIxU1apVNWjQIJ0+fdqp+z0iATQMQ5YrZMi7du1SRETENe9NTk5WWFiYw3F+z0euChVuUCEiUGN71NPAGVtky813dzgAitHN5cL16pNd1fffs2W7eOmK17yY2EHhIQFq+9gUNe41QVPmrde8CY/otrgK1zlaeDNXTgFfKVdJTk4ucqxt2rTRnDlztG7dOr3yyitKTU1V27ZtlZeXV/jnNQzDKHIEf1OpUqVksViUkZGh0NBQhyQwLy9PWVlZGjhw4DXnwG02W4EsunLiR7L4lnRZ3Li+2ta7SXOH3q1Lef+f/JXw9VF+vqF8w9A//5OqpU+3UJV/LXGoAu74T0fN+Gy/pn92wB1hw0Wyd212dwgoRh2b19aiSQN06dL//x9XiRK+ys/PV36+odr3v6S9y0epXteXte/wCfs1K6cP1qEff9XQsQvcETZc5PyON9w29ta0sy7r+/bogAK5itVqldVq/ct7LRZLgZdA/uzw4cOKjY3V2rVr1bJly0LF5NZ9ACdPnizDMPTII49o9OjRCgsLs5/z8/NTpUqV/vIbQa70AZL8eZfP955Uk39/6tA2td8/dPDEOU1ZuU8//5aji5fy1LRGOa34+vc3AuOiQhRdJkjbDjlXEgdwfW34337Vf2CsQ9vbo3tp/5GTem32GgX6+0mS8v9Uq8jLM4q0tgq4Khf+41TYZK+oqlSpojJlyigtLe3GSAATEhIkSZUrV9Zdd92lkiVJ3FBQ1oVL+v7nDIe2nIt5+i3LZm9P2XRYLz10u85mXdS587lK7lVf/zv4q7aTAAIeLSvHpr2H0h3ass9f1G8Z2dp7KF0lSvgo7dgpvfFcdyVNXKrTGdm6r0VttWxYVV0en+6mqAHP8tNPP+n06dMqX758oe9xWwKYmZmp0NBQSdLtt9+u8+fP6/z581e89vJ1wNU89/4O5RvSrMGN5VfSVxv2pOupudvdHRaAv+nSpXx1HjJNLw/tpMWvP6bgQKsO/fiLHn1hrlZv3uvu8OBFPOmr4LKyspSWlmb/+ciRI9q5c6ciIiIUERGh0aNHq2vXroqKitKhQ4f01FNPKS4uTvHx8YUew21rAH19fZWenq7IyEj5+Phc8SWQyy+HOLOoUZLK9GFNCOCtWAMIeC93rgH86lDGX19URA1iw/76oj/YuHGjWrRoUaA9ISFB06ZNU+fOnbVjxw6dPXtWFSpUUOvWrfXSSy+pXLlyhR7DbRXA9evX29/w3bBhg7vCAAAA8KivgmvevLmuVZ9bvXr13x7DbQlgs2bNrvh3AACA682D8r/rwiP2AVy1apU2b/7/aZ0333xTdevWVY8ePXTmzJlr3AkAAABneUQC+OSTTyozM1OStGfPHg0fPlzt2rXTkSNHNHz4cDdHBwAAvJ4nfRnwdeDWbWAuO3LkiGrUqCFJWrJkiTp27Khx48bpm2++Ubt27dwcHQAAgHfxiAqgn5+fcnJyJElr165V69atJUkRERH2yiAAAICrWFz4xxN5RAWwSZMmGj58uBo3bqz//e9/WrhwoSTpwIEDuvnmm90cHQAAgHfxiArgG2+8oRIlSmjx4sWaNm2abrrpJknSp59+qjZt2rg5OgAA4O0sFtcdnsgjKoAVK1bUihUrCrRPmjTJDdEAAAB4N49IACUpLy9Py5Yt0759+yRJt912m+677z75+vq6OTIAAODtPLRQ5zIekQCmpaWpXbt2+vnnn1W1alVJUnJysqKjo7Vy5UrFxsa6OUIAAODVTJYBesQawKFDhyo2NlY//vijvvnmG33zzTc6duyYKleurKFDh7o7PAAAAK/iERXA1NRUbd261f7dwJJUunRpjR8/Xo0bN3ZjZAAAwAw8dbsWV/GICqDVatW5c+cKtGdlZcnPz88NEQEAAHgvj0gAO3TooAEDBuirr76SYRgyDENbt27VwIEDdd9997k7PAAA4OXMtg2MRySAU6ZMUVxcnO666y75+/vL399fjRs3VlxcnF5//XV3hwcAAOBV3LoGMD8/X6+++qo+/vhjXbx4UZ07d1ZCQoIsFouqV6+uuLg4d4YHAABMwkMLdS7j1gRw7NixGjVqlFq1aqWAgAB98sknCgsL03//+193hgUAAODV3DoFPGfOHL311ltavXq1li1bpuXLlyslJUX5+fnuDAsAAJiNxYWHB3JrAnjs2DG1a9fO/nOrVq1ksVh0/PhxN0YFAADMxuLCP57IrQngpUuX5O/v79BWsmRJ5ebmuikiAAAA7+fWNYCGYahPnz6yWq32tgsXLmjgwIEKCgqyt3344YfuCA8AAJiEp27X4ipuTQATEhIKtPXq1csNkQAAAJiHWxPAWbNmuXN4AAAASR77robLeMRG0AAAALh+3FoBBAAA8AgmKwFSAQQAADAZKoAAAMD0PHW/PlehAggAAGAyVAABAIDpsQ8gAACAyZgs/2MKGAAAwGyoAAIAAJisBEgFEAAAwGSoAAIAANNjGxgAAAB4NSqAAADA9My2DQwVQAAAAJOhAggAAEzPZAVAEkAAAACzZYBMAQMAAJgMFUAAAGB6bAMDAAAAr0YCCAAATM9icd3hrE2bNqljx46qUKGCLBaLli1b5nDeMAy98MILKl++vAICAtSqVSsdPHjQqTFIAAEAADxIdna26tSpozfffPOK5ydMmKApU6Zo+vTp+uqrrxQUFKT4+HhduHCh0GOwBhAAAJieJ60AbNu2rdq2bXvFc4ZhaPLkyXruuefUqVMnSdKcOXNUrlw5LVu2TA899FChxqACCAAA4EI2m02ZmZkOh81mK1JfR44c0YkTJ9SqVSt7W1hYmBo0aKAtW7YUuh8SQAAAAIvrjuTkZIWFhTkcycnJRQrzxIkTkqRy5co5tJcrV85+rjCYAgYAAKbnym1gkpKSNHz4cIc2q9XqsvEKgwQQAADAhaxWa7ElfFFRUZKkkydPqnz58vb2kydPqm7duoXuhylgAABgep60Dcy1VK5cWVFRUVq3bp29LTMzU1999ZUaNWpU6H6oAAIAAHiQrKwspaWl2X8+cuSIdu7cqYiICFWsWFFPPPGEXn75Zd1yyy2qXLmynn/+eVWoUEGdO3cu9BgkgAAAwPQ8aRuYr7/+Wi1atLD/fHn9YEJCgmbPnq2nnnpK2dnZGjBggM6ePasmTZpo1apV8vf3L/QYFsMwjGKP3M3K9Fng7hAAuEj2rs3uDgGAi5zf8Ybbxj76a+E3UXZWpTKFT8yuFyqAAAAAnlQCvA54CQQAAMBkqAACAADTc+U+gJ6IBBAAAJhecW/X4umYAgYAADAZKoAAAMD0TFYApAIIAABgNlQAAQCA6bEGEAAAAF6NCiAAAIDJVgFSAQQAADAZKoAAAMD0zLYGkAQQAACYnsnyP6aAAQAAzIYKIAAAMD2zTQFTAQQAADAZKoAAAMD0LCZbBUgFEAAAwGSoAAIAAJirAEgFEAAAwGyoAAIAANMzWQGQBBAAAIBtYAAAAODVqAACAADTYxsYAAAAeDUqgAAAAOYqAFIBBAAAMBsqgAAAwPRMVgCkAggAAGA2VAABAIDpmW0fQBJAAABgemwDAwAAAK9GBRAAAJie2aaAqQACAACYDAkgAACAyZAAAgAAmAxrAAEAgOmxBhAAAABejQogAAAwPbPtA0gCCAAATI8pYAAAAHg1EkAAAGB6Fhcezhg1apQsFovDUa1atb/5dAUxBQwAAOBBbrvtNq1du9b+c4kSxZ+ukQACAAB40BrAEiVKKCoqyqVjMAUMAADgQjabTZmZmQ6HzWa76vUHDx5UhQoVVKVKFfXs2VPHjh0r9phIAAEAgOlZXPgnOTlZYWFhDkdycvIV42jQoIFmz56tVatWadq0aTpy5IjuvvtunTt3rnif1zAMo1h79ABl+ixwdwgAXCR712Z3hwDARc7veMNtY2fZXJcOldTFAhU/q9Uqq9X6l/eePXtWMTExmjhxovr161dsMbEGEAAAmJ4r9wG0+hUu2buS8PBw3XrrrUpLSyvWmJgCBgAA8FBZWVk6dOiQypcvX6z9kgACAADT85R9AEeOHKnU1FQdPXpUX375pe6//375+vqqe/fuf/MJHTEFDAAA4CHbwPz000/q3r27Tp8+rbJly6pJkybaunWrypYtW6zjkAACAAB4iAULrs+LrCSAAADA9CyeUgK8TlgDCAAAYDJUAAEAgOm5chsYT0QFEAAAwGS88ptAYB42m03JyclKSkoq8iabADwTv9+A65AA4oaWmZmpsLAwZWRkKDQ01N3hAChG/H4DrsMUMAAAgMmQAAIAAJgMCSAAAIDJkADihma1WvXiiy+yQBzwQvx+A67DSyAAAAAmQwUQAADAZEgAAQAATIYEEAAAwGRIAGEqlSpV0uTJk90dBoBr2LhxoywWi86ePXvN6/h9BoqOBBDFpk+fPrJYLBo/frxD+7Jly2S5zt+yPXv2bIWHhxdo37ZtmwYMGHBdYwG81eXfeYvFIj8/P8XFxWnMmDG6dOnS3+r3rrvuUnp6usLCwiTx+wy4AgkgipW/v79eeeUVnTlzxt2hXFHZsmUVGBjo7jAAr9GmTRulp6fr4MGDGjFihEaNGqVXX331b/Xp5+enqKiov/wPR36fgaIjAUSxatWqlaKiopScnHzVazZv3qy7775bAQEBio6O1tChQ5WdnW0/n56ervbt2ysgIECVK1fW/PnzC0z1TJw4UbVq1VJQUJCio6P1r3/9S1lZWZJ+nz7q27evMjIy7NWJUaNGSXKcMurRo4cefPBBh9hyc3NVpkwZzZkzR5KUn5+v5ORkVa5cWQEBAapTp44WL15cDJ8U4B2sVquioqIUExOjQYMGqVWrVvr444915swZ9e7dW6VKlVJgYKDatm2rgwcP2u/74Ycf1LFjR5UqVUpBQUG67bbb9Mknn0hynALm9xlwDRJAFCtfX1+NGzdOU6dO1U8//VTg/KFDh9SmTRt17dpVu3fv1sKFC7V582YNHjzYfk3v3r11/Phxbdy4UUuWLNHbb7+tU6dOOfTj4+OjKVOm6LvvvtN7772n9evX66mnnpL0+/TR5MmTFRoaqvT0dKWnp2vkyJEFYunZs6eWL19uTxwlafXq1crJydH9998vSUpOTtacOXM0ffp0fffddxo2bJh69eql1NTUYvm8AG8TEBCgixcvqk+fPvr666/18ccfa8uWLTIMQ+3atVNubq4kKTExUTabTZs2bdKePXv0yiuvKDg4uEB//D4DLmIAxSQhIcHo1KmTYRiG0bBhQ+ORRx4xDMMwli5dalz+R61fv37GgAEDHO77/PPPDR8fH+P8+fPGvn37DEnGtm3b7OcPHjxoSDImTZp01bE/+OADo3Tp0vafZ82aZYSFhRW4LiYmxt5Pbm6uUaZMGWPOnDn28927dzcefPBBwzAM48KFC0ZgYKDx5ZdfOvTRr18/o3v37tf+MAAT+OPvfH5+vrFmzRrDarUanTt3NiQZX3zxhf3aX3/91QgICDAWLVpkGIZh1KpVyxg1atQV+92wYYMhyThz5oxhGPw+A65Qwq3ZJ7zWK6+8onvuuafAf6nv2rVLu3fvVkpKir3NMAzl5+fryJEjOnDggEqUKKF69erZz8fFxalUqVIO/axdu1bJycn6/vvvlZmZqUuXLunChQvKyckp9JqgEiVKqFu3bkpJSdHDDz+s7OxsffTRR1qwYIEkKS0tTTk5Obr33nsd7rt48aJuv/12pz4PwFutWLFCwcHBys3NVX5+vnr06KEuXbpoxYoVatCggf260qVLq2rVqtq3b58kaejQoRo0aJA+++wztWrVSl27dlXt2rWLHAe/z4BzSADhEk2bNlV8fLySkpLUp08fe3tWVpYee+wxDR06tMA9FStW1IEDB/6y76NHj6pDhw4aNGiQxo4dq4iICG3evFn9+vXTxYsXnVoU3rNnTzVr1kynTp3SmjVrFBAQoDZt2thjlaSVK1fqpptucriP7yYFfteiRQtNmzZNfn5+qlChgkqUKKGPP/74L+979NFHFR8fr5UrV+qzzz5TcnKyXnvtNQ0ZMqTIsfD7DBQeCSBcZvz48apbt66qVq1qb6tXr5727t2ruLi4K95TtWpVXbp0STt27FD9+vUl/f5f7n98q3j79u3Kz8/Xa6+9Jh+f35exLlq0yKEfPz8/5eXl/WWMd911l6Kjo7Vw4UJ9+umn+uc//6mSJUtKkmrUqCGr1apjx46pWbNmzj08YBJBQUEFfp+rV6+uS5cu6auvvtJdd90lSTp9+rT279+vGjVq2K+Ljo7WwIEDNXDgQCUlJemdd965YgLI7zNQ/EgA4TK1atVSz549NWXKFHvb008/rYYNG2rw4MF69NFHFRQUpL1792rNmjV64403VK1aNbVq1UoDBgzQtGnTVLJkSY0YMUIBAQH2LSHi4uKUm5urqVOnqmPHjvriiy80ffp0h7ErVaqkrKwsrVu3TnXq1FFgYOBVK4M9evTQ9OnTdeDAAW3YsMHeHhISopEjR2rYsGHKz89XkyZNlJGRoS+++EKhoaFKSEhwwacG3PhuueUWderUSf3799eMGTMUEhKiZ555RjfddJM6deokSXriiSfUtm1b3XrrrTpz5ow2bNig6tWrX7E/fp8BF3D3IkR4jz8uCL/syJEjhp+fn/HHf9T+97//Gffee68RHBxsBAUFGbVr1zbGjh1rP3/8+HGjbdu2htVqNWJiYoz58+cbkZGRxvTp0+3XTJw40ShfvrwREBBgxMfHG3PmzHFYNG4YhjFw4ECjdOnShiTjxRdfNAzDcdH4ZXv37jUkGTExMUZ+fr7Dufz8fGPy5MlG1apVjZIlSxply5Y14uPjjdTU1L/3YQFe4Eq/85f99ttvxsMPP2yEhYXZf08PHDhgPz948GAjNjbWsFqtRtmyZY2HH37Y+PXXXw3DKPgSiGHw+wwUN4thGIYb80/gL/3000+Kjo7W2rVr1bJlS3eHAwDADY8EEB5n/fr1ysrKUq1atZSenq6nnnpKP//8sw4cOGBfzwMAAIqONYDwOLm5uXr22Wd1+PBhhYSE6K677lJKSgrJHwAAxYQKIAAAgMnwVXAAAAAmQwIIAABgMiSAAAAAJkMCCAAAYDIkgAAAACZDAgjAY/Xp00edO3e2/9y8eXM98cQT1z2OjRs3ymKx6OzZs9d9bABwBRJAAE7r06ePLBaLLBaL/Pz8FBcXpzFjxujSpUsuHffDDz/USy+9VKhrSdoA4OrYCBpAkbRp00azZs2SzWbTJ598osTERJUsWVJJSUkO1128eFF+fn7FMmZERESx9AMAZkcFEECRWK1WRUVFKSYmRoMGDVKrVq308ccf26dtx44dqwoVKqhq1aqSpB9//FHdunVTeHi4IiIi1KlTJx09etTeX15enoYPH67w8HCVLl1aTz31lP68T/2fp4BtNpuefvppRUdHy2q1Ki4uTjNnztTRo0fVokULSVKpUqVksVjUp08fSVJ+fr6Sk5NVuXJlBQQEqE6dOlq8eLHDOJ988oluvfVWBQQEqEWLFg5xAoA3IAEEUCwCAgJ08eJFSdK6deu0f/9+rVmzRitWrFBubq7i4+MVEhKizz//XF988YWCg4PVpk0b+z2vvfaaZs+erf/+97/avHmzfvvtNy1duvSaY/bu3Vvvv/++pkyZon379mnGjBkKDg5WdHS0lixZIknav3+/0tPT9frrr0uSkpOTNWfOHE2fPl3fffedhg0bpl69eik1NVXS74lqly5d1LFjR+3cuVOPPvqonnnmGVd9bADgFkwBA/hbDMPQunXrtHr1ag0ZMkS//PKLgoKC9O6779qnfufNm6f8/Hy9++67slgskqRZs2YpPDxcGzduVOvWrTV58mQlJSWpS5cukqTp06dr9erVVx33wIEDWrRokdasWaNWrVpJkqpUqWI/f3m6ODIyUuHh4ZJ+rxiOGzdOa9euVaNGjez3bN68WTNmzFCzZs00bdo0xcbG6rXXXpMkVa1aVXv27NErr7xSjJ8aALgXCSCAIlmxYoWCg4OVm5ur/Px89ejRQ6NGjVJiYqJq1arlsO5v165dSktLU0hIiEMfFy5c0KFDh5SRkaH09HQ1aNDAfq5EiRK64447CkwDX7Zz5075+vqqWbNmhY45LS1NOTk5uvfeex3aL168qNtvv12StG/fPoc4JNmTRQDwFiSAAIqkRYsWmjZtmvz8/FShQgWVKPH//zoJCgpyuDYrK0v169dXSkpKgX7Kli1bpPEDAgKcvicrK0uStHLlSt10000O56xWa5HiAIAbEQkggCIJCgpSXFxcoa6tV6+eFi5cqMjISIWGhl7xmvLly+urr75S06ZNJUmXLl3S9u3bVa9evSteX6tWLeXn5ys1NdU+BfxHlyuQeXl59rYaNWrIarXq2LFjV60cVq9eXR9//LFD29atW//6IQHgBsJLIABcrmfPnipTpow6deqkzz//XEeOHNHGjRs1dOhQ/fTTT5Kkxx9/XOPHj9eyZcv0/fff61//+tc19/CrVKmSEhIS9Mgjj2jZsmX2PhctWiRJiomJkcVi0YoVK/TLL78oKytLISEhGjlypIYNG6b33ntPhw4d0jfffKOpU6fqvffekyQNHDhQBw8e1JNPPqn9+/dr/vz5mj17tqs/IgC4rkgAAbhcYGCgNm3apIoVK6pLly6qXr26+vXrpwsXLtgrgiNGjNDDDz+shIQENWrUSCEhIbr//vuv2e+0adP0wAMP6F//+peqVaum/v37Kzs7W5J00003afTo0XrmmWdUrlw5DR48WJL00ksv6fnnn1dycrKqV6+uNm3aaOXKlapcubIkqWLFilqyZImWLVumOnXqaPr06Ro3bpwLPx0AuP4sxtVWWAMAAMArUQEEAAAwGRJAAAAAkyEBBAAAMBkSQAAAAJMhAQQAADAZEkAAAACTIQEEAAAwGRJAAAAAkyEBBAAAMBkSQAAAAJMhAQQAADCZ/wNQ03l7AUaDCgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a confusion matrix\n",
    "conf_matrix = confusion_matrix(encoded1, clusters)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:29.977992336Z",
     "start_time": "2023-12-01T09:53:29.709185215Z"
    }
   },
   "id": "120a53a4bfd011ac"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.09      0.33      0.14        12\n",
      "    Positive       0.86      0.55      0.67        88\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.47      0.44      0.40       100\n",
      "weighted avg       0.77      0.52      0.60       100\n"
     ]
    }
   ],
   "source": [
    "# Printing a classification report\n",
    "report = classification_report(encoded1, clusters, target_names=['Negative', 'Positive'])\n",
    "print(\"Report:\\n\", report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:53:35.205005460Z",
     "start_time": "2023-12-01T09:53:35.183892087Z"
    }
   },
   "id": "d34c22671d02725a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e33da6e2cace718"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
