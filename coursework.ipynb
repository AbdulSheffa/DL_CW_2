{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f6c751c626a782",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ],
   "id": "f08b627b907be3b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 700000\n",
    "json_file_path = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Read the JSON file in chunks\n",
    "chunks = pd.read_json(json_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "for i, df_chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    chunk_csv_path = f\"chunk_{i + 1}.csv\"\n",
    "    df_chunk.to_csv(chunk_csv_path, index=False)\n",
    "    df_chunk_from_csv = pd.read_csv(chunk_csv_path)"
   ],
   "id": "21e6ff4973b271de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the first 100000\n",
    "chunk_size = 700000\n",
    "file= \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Reading the JSON file in chunks\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Iterate over chunks and process each chunk\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "\n",
    "    # Save the first chunk to a CSV file\n",
    "    if i == 0:\n",
    "        first_chunk_csv_path = \"first_chunk.csv\"\n",
    "        x.to_csv(first_chunk_csv_path, index=False)\n",
    "\n",
    "    df= pd.read_csv(\"first_chunk.csv\")\n",
    "    break"
   ],
   "id": "a1f8a523c16dd08d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ],
   "id": "43d219cd92b812e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Visualization\n",
    "#%%\n",
    "# Data Preprocessing\n",
    "texts = df['text']\n",
    "labels = df['stars'].apply(lambda x: 1 if x > 3 else 0)  \n"
   ],
   "id": "30536b770fd4683"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df\n"
   ],
   "id": "c96b9e8a0791e87b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['stars'].value_counts().sort_index().index, df['stars'].value_counts().sort_index(), color='purple')\n",
    "plt.title('Ratings')\n",
    "plt.xlabel('Stars')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ],
   "id": "88fe3f958ae5c145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['review_length'] = df['text'].apply(len)\n",
    "df['review_length'].hist(bins=50, color='red')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()\n"
   ],
   "id": "4f78414413bc4820"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "df.boxplot(column='useful', by='stars', ax=axes[0])\n",
    "axes[0].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='funny', by='stars', ax=axes[1])\n",
    "axes[1].set_title('Votes by Stars')\n",
    "\n",
    "df.boxplot(column='cool', by='stars', ax=axes[2])\n",
    "axes[2].set_title('Votes by Stars')\n",
    "\n",
    "plt.suptitle('Features by Stars')\n",
    "plt.show()\n"
   ],
   "id": "70e71f732c44e6da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Data Pre-processing\n",
    "#%%\n",
    "#### Drop duplicates\n",
    "df = df.drop_duplicates(subset=['review_id'])\n",
    "df = df.drop_duplicates(subset=['user_id'])\n",
    "df = df.drop_duplicates(subset=['business_id'])\n",
    "df\n"
   ],
   "id": "93c2611f7dc29312"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Null values \n",
    "nulls= df.isnull().sum()\n",
    "print(\"Nulls:\\n\", nulls)\n"
   ],
   "id": "2058e2771de335ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "##%%\n",
    "# Remove special characters \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n"
   ],
   "id": "fbfc907f5245dc80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n"
   ],
   "id": "cf2b2aff60824b9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(texts)\n",
    "s = token.texts_to_sequences(texts)\n",
    "index = token.word_index\n",
    "maximumlength = 100  # Set your desired sequence length\n",
    "data = pad_sequences(s, maxlen=maximumlength)\n"
   ],
   "id": "8c53cbbbccee3e63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode labels\n",
    "l_encode = LabelEncoder()\n",
    "labels = l_encode.fit_transform(labels)\n"
   ],
   "id": "793ccfe3378663ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Train and Test Split\n",
    "#%%\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.18, random_state=42)\n"
   ],
   "id": "bad487cd80c3273c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### LSTM Model\n",
    "#%%\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(index) + 1, 100, input_length=maximumlength))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ],
   "id": "e58988ce9ee38237"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Compile the model\n",
    "#%%\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "id": "e9d0dcb9ab74bbcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with Data Generator\n",
    "def data_generator(data, labels, batch_size):\n",
    "    samples = len(data)\n",
    "    while True:\n",
    "        s_indices = np.arange(samples)\n",
    "        np.random.shuffle(s_indices)\n",
    "        for i in range(0,samples, batch_size):\n",
    "            b_indices = s_indices[i:i+batch_size]\n",
    "            yield data[b_indices], labels[b_indices]\n"
   ],
   "id": "8f86be7d1e34b3d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 33\n",
    "epochs= len(X_train) // batch_size\n",
    "# Training\n",
    "model.fit(data_generator(X_train, y_train, batch_size),\n",
    "          epochs=7,\n",
    "          steps_per_epoch=epochs,\n",
    "          validation_data=(X_test, y_test)) "
   ],
   "id": "f1753315d87b8042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(f\"Accuracy: {accuracy*100}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d751a936b8eb46a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predicted_value = model.predict(X_test)\n",
    "y_binary_value = (y_predicted_value > 0.5).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ba79741794aed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_binary_value, average='binary')\n",
    "\n",
    "print(f\"Precision score: {prec*100:.4f}\")\n",
    "print(f\"Recall score: {rec*100:.4f}\")\n",
    "print(f\"F1 Score: {f1*100:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e5f42428931860"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_binary_value)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e4951bc85962621"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting Actual vs Predicted\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(y_test, y_predicted_value, alpha=0.5)\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5554fb92a7564ffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Bert Super-vised\n",
    "\n",
    "chunk = 100000\n",
    "file = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "chunks1 = pd.read_json(file, lines=True, chunksize=chunk)\n",
    "for i, x in enumerate(chunks1):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "    if i == 0:\n",
    "        csv = \"chunk.csv\"\n",
    "        x.to_csv(csv, index=False)\n",
    "    df1 = pd.read_csv(\"chunk.csv\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f17c0efdfd1445fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentiment(stars):  \n",
    "    if 0 <= stars < 3:\n",
    "        return 0 #negative\n",
    "    elif 3 <= stars <= 5:\n",
    "        return 1 #normal\n",
    "    else:\n",
    "        return 'undefined'\n",
    "\n",
    "df1['sentiment_label'] = df1['stars'].apply(sentiment)\n",
    "le = LabelEncoder()\n",
    "df1['encoded_sentiment'] = le.fit_transform(df1['sentiment_label'])\n",
    "\n",
    "# Extract labels\n",
    "y = df1['encoded_sentiment'].values\n",
    "df1.to_csv('preprocessed_data1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153f07d614f5a5ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7852bd1c1b98f5f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Selecting divided parts\n",
    "positive_reviews = df1[df1['sentiment_label'] == 1].sample(n=2000, random_state=42)\n",
    "negative_reviews = df1[df1['sentiment_label'] == 0].sample(n=2000, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce8501b94f446374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine the selected samples\n",
    "new_data = pd.concat([positive_reviews, negative_reviews], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "956e57b5e4a17d60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca0d0b2cbf66ee05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Load model and tokenizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "551afa8a75c47747"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Tokenizer\n",
    "# Tokenize the texts (reviews)\n",
    "import torch\n",
    "\n",
    "tokenized_vals = []\n",
    "\n",
    "for text in new_data['text']:\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Append individually\n",
    "    tokenized_vals.append({\n",
    "        'input_ids': tokens['input_ids'],\n",
    "        'attention_mask': tokens['attention_mask']\n",
    "    })\n",
    "\n",
    "# Extracting input tensors separately\n",
    "input_ids = torch.cat([entry['input_ids'] for entry in tokenized_vals], dim=0)\n",
    "attention_mask = torch.cat([entry['attention_mask'] for entry in tokenized_vals], dim=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d2e35e34d7703c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Train and test split\n",
    "label = new_data['encoded_sentiment']\n",
    "X_train_id, X_test_id, X_train_m, X_test_m, y_train, y_test = train_test_split(\n",
    "    input_ids.numpy(),\n",
    "    attention_mask.numpy(),\n",
    "    label,\n",
    "    test_size=0.15,\n",
    "    random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e249b8a9999f430"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train_id.shape, X_test_id.shape, X_train_m.shape, X_test_m.shape, y_train.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed4856927171db6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PyTorch tensors \n",
    "X_train_ids= torch.tensor(X_train_id, dtype=torch.long)\n",
    "X_test_ids= torch.tensor(X_test_id, dtype=torch.long)\n",
    "X_train_mask= torch.tensor(X_train_m, dtype=torch.long)\n",
    "X_test_mask= torch.tensor(X_test_m, dtype=torch.long)\n",
    "y_train= torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test= torch.tensor(y_test.values, dtype=torch.long) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad751015748d61c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Loading optimizer\n",
    "criter = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e92b078ad2b1d4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Training the model\n",
    "epochs = 2\n",
    "batch_size = 3\n",
    "\n",
    "train_data = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_data = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_load = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4926b5888ed74d8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_data:\n",
    "        inputs, attention_mask, label = batch\n",
    "        optim.zero_grad()\n",
    "\n",
    "        if label is not None:\n",
    "            print(\"Working\")\n",
    "            batch_output= model(inputs, attention_mask=attention_mask, labels=label)\n",
    "            loss = batch_output.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(\"Outputs:\", batch_output)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss_per_epoch: {total_loss}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9bfc8e4eec69020"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
